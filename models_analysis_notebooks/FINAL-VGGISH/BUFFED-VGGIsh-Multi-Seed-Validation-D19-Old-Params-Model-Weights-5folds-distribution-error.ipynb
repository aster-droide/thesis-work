{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17da2043-9a5b-40fe-b3cb-f755b9a98deb",
   "metadata": {},
   "source": [
    "# VGGIsh Multi Seed Validation\n",
    "#### 5 Random (but reproducible) seeds are selected for 5 different runs of train/test\n",
    "#### Models have been optimised and verified on validation sets thoroughly, running a final train/test evaluation here\n",
    "#### The setup:\n",
    "\n",
    "- 4 fold StratifiedGroupKFold for stratification and ensuring each cat_id group only appears in one set at a time\n",
    "- Final scores averaged over the 4 folds\n",
    "- For each seed run we will explore the cat_id predictions through majority voting\n",
    "- For each run we will explore the potential impact of gender\n",
    "- Includes visualisations & analysis\n",
    "\n",
    "The dataset is highly unbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac1e09ae-387c-4347-b6f5-8d09dd1bf3f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit, GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, RobustScaler, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Imbalanced-learn import\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# TensorFlow and Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, BatchNormalization, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax, AdamW\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.regularizers import l1, l2, L1L2\n",
    "\n",
    "# Optuna import\n",
    "import optuna\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d005cfd2-3eda-44c4-bbb5-af343e979bc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [7270  860 5390 5191 5734]\n"
     ]
    }
   ],
   "source": [
    "# Set an initial seed for reproducibility\n",
    "np.random.seed(42)  \n",
    "\n",
    "# Generate a list of 5 random seeds\n",
    "random_seeds = np.random.randint(0, 10000, size=5)\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5f521e-fe54-47c6-8b43-9eb4858c5216",
   "metadata": {},
   "source": [
    "#### TOTAL RESULTS ACROSS SEED INITIATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "196043c6-e9d9-4194-ad56-355871b49fe9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Lists to collect statistics across all CVs\n",
    "all_losses, all_accuracies, all_precisions, all_recalls, all_f1 = [], [], [], [], []\n",
    "all_majority_vote_accuracies, all_majority_vote_details, all_class_stats, all_gender_stats  = [], [], [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae122a-9f9c-47a3-8835-2d46d2f8e2f9",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5097e68-5153-440c-9294-dfa9e6061652",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_initial_group_split(groups_train, groups_test):\n",
    "    \"\"\"\n",
    "    Check if any group is present in both the train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - groups_train: Array of group identifiers for the train set\n",
    "    - groups_test: Array of group identifiers for the test set\n",
    "\n",
    "    Returns:\n",
    "    - Prints out any groups found in both sets and the count of such groups\n",
    "    \"\"\"\n",
    "    train_groups = set(groups_train)\n",
    "    test_groups = set(groups_test)\n",
    "    common_groups = train_groups.intersection(test_groups)\n",
    "\n",
    "    if common_groups:\n",
    "        print(f\"Warning: Found {len(common_groups)} common groups in both train/validation and test sets: {common_groups}\")\n",
    "    else:\n",
    "        print(\"No common groups found between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88dfe2de-bb1e-4d49-9260-e056c39627bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to perform the swaps based on cat_id, ensuring swaps within the same age_group\n",
    "def swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids):\n",
    "    for cat_id in specific_cat_ids:\n",
    "        # Check if the specific cat_id is not in the training set\n",
    "        if cat_id not in dataframe.iloc[train_val_idx]['cat_id'].values:\n",
    "            # Get the age_group of this cat_id\n",
    "            age_group = dataframe[dataframe['cat_id'] == cat_id]['age_group'].iloc[0]\n",
    "                \n",
    "            # Find a different cat_id within the same age_group in the train set that is not in the test set\n",
    "            other_cat_ids_in_age_group = dataframe[(dataframe['age_group'] == age_group) & \n",
    "                                                   (dataframe['cat_id'] != cat_id) &\n",
    "                                                   (~dataframe['cat_id'].isin(dataframe.iloc[test_idx]['cat_id']))]['cat_id'].unique()\n",
    "            \n",
    "            # Choose one other cat_id for swapping\n",
    "            if len(other_cat_ids_in_age_group) > 0:\n",
    "                other_cat_id = np.random.choice(other_cat_ids_in_age_group)\n",
    "\n",
    "                # Find all instances of the other_cat_id in the train set\n",
    "                other_cat_id_train_val_indices = train_val_idx[dataframe.iloc[train_val_idx]['cat_id'] == other_cat_id]\n",
    "                \n",
    "                # Find all instances of the specific cat_id in the test set\n",
    "                cat_id_test_indices = test_idx[dataframe.iloc[test_idx]['cat_id'] == cat_id]\n",
    "                \n",
    "                # Swap the indices\n",
    "                train_val_idx = np.setdiff1d(train_val_idx, other_cat_id_train_val_indices, assume_unique=True)\n",
    "                test_idx = np.setdiff1d(test_idx, cat_id_test_indices, assume_unique=True)\n",
    "\n",
    "                train_val_idx = np.concatenate((train_val_idx, cat_id_test_indices))\n",
    "                test_idx = np.concatenate((test_idx, other_cat_id_train_val_indices))\n",
    "            else:\n",
    "                print(f\"No alternative cat_id found in the same age_group as {cat_id} for swapping.\")\n",
    "                \n",
    "    return train_val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e6c2d87-cfab-493e-bca8-b3b8976a2bda",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify differences in groups\n",
    "def find_group_differences(original, new):\n",
    "    # Convert numpy arrays to sets for easy difference computation\n",
    "    original_set = set(original)\n",
    "    new_set = set(new)\n",
    "    # Find differences\n",
    "    moved_to_new = new_set - original_set\n",
    "    moved_to_original = original_set - new_set\n",
    "    return moved_to_new, moved_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6164b1c3-75dd-4549-aafd-cb6106297941",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create custom logger function for local logs & stored in a .txt\n",
    "def logger(message, file=None):\n",
    "    print(message)\n",
    "    if file is not None:\n",
    "        with open(file, \"a\") as log_file:\n",
    "            log_file.write(message + \"\\n\")\n",
    "\n",
    "log_file_path = \"multi-seed-val-D13.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "899fb535-0e25-4c7b-b6a5-13231e26c502",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "# Define a custom color palette\n",
    "colors = [\"#6aabd1\", \"#b6e2d3\", \"#dac292\"] \n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "# Function to create bar plots with enhanced style\n",
    "def styled_barplot(data, x, y, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bar_plot = sns.barplot(x=x, y=y, data=data, errorbar=None, width=0.5)  \n",
    "    plt.title(title, fontsize=16, fontweight='bold', color=\"#333333\")\n",
    "    plt.xlabel(xlabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.ylabel(ylabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.xticks(fontsize=12, color=\"#333333\")\n",
    "    plt.yticks(fontsize=12, color=\"#333333\")\n",
    "    plt.ylim(0, 100) \n",
    "\n",
    "    # Adding value labels on top of each bar\n",
    "    for p in bar_plot.patches:\n",
    "        height = p.get_height()\n",
    "        # Annotate the height value on the bar\n",
    "        bar_plot.annotate(f'{height:.1f}', \n",
    "                          (p.get_x() + p.get_width() / 2., height), \n",
    "                          ha='center', va='center', \n",
    "                          xytext=(0, 9), \n",
    "                          textcoords='offset points', fontsize=12, color=\"#333333\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28c8e398-2900-4887-a56b-2b9a6a481b7f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def plot_all_metrics(losses, accuracies, precisions, recalls, f1, metrics_across, x_axis_label):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    seeds = range(1, len(losses) + 1)\n",
    "\n",
    "    ax.plot(seeds, losses, marker='o', color='blue', label='Loss')\n",
    "    ax.plot(seeds, accuracies, marker='o', color='green', label='Accuracy')\n",
    "    ax.plot(seeds, precisions, marker='o', color='red', label='Precision')\n",
    "    ax.plot(seeds, recalls, marker='o', color='purple', label='Recall')\n",
    "    ax.plot(seeds, f1, marker='o', color='orange', label='F1 Score')\n",
    "\n",
    "    ax.set_title(f'Metrics Across {metrics_across}')\n",
    "    ax.set_xlabel(x_axis_label)\n",
    "    ax.set_ylabel('Metric Value')\n",
    "    ax.set_xticks(seeds)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936a24a-13bd-4bc3-be13-0b210a09b5da",
   "metadata": {},
   "source": [
    "# RANDOM SEED 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70136a37-0507-4c2e-8307-c2dd905432e8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14bb802b-f4bd-4464-a5fd-1977438428b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "adult     588\n",
      "senior    534\n",
      "kitten    513\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[0])) \n",
    "np.random.seed(int(random_seeds[0]))\n",
    "tf.random.set_seed(int(random_seeds[0]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_19.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ce97c28-4210-4150-a60b-acdbf0e7716c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c26f17c3-4d21-4537-a090-9ca00f2be51a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91a9c3-3474-4e83-8a5b-c5eb4a9a9223",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4605c0b-f99d-48c3-8c2a-d87dd22c8946",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "020A    23\n",
      "        ..\n",
      "066A     1\n",
      "043A     1\n",
      "073A     1\n",
      "049A     1\n",
      "024A     1\n",
      "Name: cat_id, Length: 91, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "074A    25\n",
      "055A    20\n",
      "097A    16\n",
      "042A    14\n",
      "025A    11\n",
      "036A    11\n",
      "014B    10\n",
      "005A    10\n",
      "007A     6\n",
      "053A     6\n",
      "104A     4\n",
      "062A     4\n",
      "113A     3\n",
      "087A     2\n",
      "032A     2\n",
      "102A     2\n",
      "096A     1\n",
      "026C     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    258\n",
      "X    227\n",
      "F    207\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    121\n",
      "M     79\n",
      "F     45\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 071A, 097B, 028...\n",
      "kitten    [044A, 111A, 040A, 047A, 109A, 050A, 043A, 049...\n",
      "senior    [093A, 057A, 106A, 059A, 116A, 051B, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [103A, 074A, 062A, 005A, 025A, 007A, 087A, 032...\n",
      "kitten                                   [014B, 046A, 042A]\n",
      "senior                             [097A, 104A, 055A, 113A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 13, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 3, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B'\n",
      " '020A' '021A' '022A' '023A' '023B' '024A' '025B' '025C' '026A' '027A'\n",
      " '028A' '029A' '031A' '033A' '034A' '035A' '037A' '038A' '039A' '040A'\n",
      " '041A' '043A' '044A' '045A' '047A' '048A' '049A' '050A' '051A' '051B'\n",
      " '052A' '054A' '056A' '057A' '058A' '059A' '060A' '061A' '063A' '064A'\n",
      " '065A' '066A' '067A' '068A' '069A' '070A' '071A' '072A' '073A' '075A'\n",
      " '076A' '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '099A'\n",
      " '100A' '101A' '105A' '106A' '108A' '109A' '110A' '111A' '115A' '116A'\n",
      " '117A']\n",
      "Unique Test Group IDs:\n",
      "['005A' '007A' '014B' '025A' '026B' '026C' '032A' '036A' '042A' '046A'\n",
      " '053A' '055A' '062A' '074A' '087A' '096A' '097A' '102A' '103A' '104A'\n",
      " '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'040A'}\n",
      "Moved to Test Set:\n",
      "{'040A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B'\n",
      " '020A' '021A' '022A' '023A' '023B' '024A' '025B' '025C' '026A' '027A'\n",
      " '028A' '029A' '031A' '033A' '034A' '035A' '037A' '038A' '039A' '041A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '049A' '050A' '051A' '051B'\n",
      " '052A' '054A' '056A' '057A' '058A' '059A' '060A' '061A' '063A' '064A'\n",
      " '065A' '066A' '067A' '068A' '069A' '070A' '071A' '072A' '073A' '075A'\n",
      " '076A' '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '099A'\n",
      " '100A' '101A' '105A' '106A' '108A' '109A' '110A' '111A' '115A' '116A'\n",
      " '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['005A' '007A' '014B' '025A' '026B' '026C' '032A' '036A' '040A' '042A'\n",
      " '053A' '055A' '062A' '074A' '087A' '096A' '097A' '102A' '103A' '104A'\n",
      " '113A']\n",
      "Length of X_train_val:\n",
      "745\n",
      "Length of y_train_val:\n",
      "745\n",
      "Length of groups_train_val:\n",
      "745\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     473\n",
      "senior    135\n",
      "kitten     84\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     115\n",
      "kitten     87\n",
      "senior     43\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     473\n",
      "kitten    137\n",
      "senior    135\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     115\n",
      "senior     43\n",
      "kitten     34\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 946, 1: 548, 2: 540})\n",
      "Epoch 1/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.9328 - accuracy: 0.5777\n",
      "Epoch 2/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.7583 - accuracy: 0.6534\n",
      "Epoch 3/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.6706 - accuracy: 0.6839\n",
      "Epoch 4/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.6262 - accuracy: 0.7144\n",
      "Epoch 5/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5911 - accuracy: 0.7311\n",
      "Epoch 6/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5705 - accuracy: 0.7404\n",
      "Epoch 7/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5802 - accuracy: 0.7394\n",
      "Epoch 8/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5341 - accuracy: 0.7620\n",
      "Epoch 9/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5083 - accuracy: 0.7650\n",
      "Epoch 10/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4856 - accuracy: 0.7847\n",
      "Epoch 11/1500\n",
      "64/64 [==============================] - 0s 995us/step - loss: 0.4616 - accuracy: 0.7827\n",
      "Epoch 12/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4587 - accuracy: 0.7901\n",
      "Epoch 13/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4505 - accuracy: 0.7906\n",
      "Epoch 14/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4322 - accuracy: 0.8068\n",
      "Epoch 15/1500\n",
      "64/64 [==============================] - 0s 989us/step - loss: 0.4305 - accuracy: 0.8053\n",
      "Epoch 16/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4221 - accuracy: 0.8053\n",
      "Epoch 17/1500\n",
      "64/64 [==============================] - 0s 986us/step - loss: 0.4239 - accuracy: 0.8014\n",
      "Epoch 18/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4042 - accuracy: 0.8309\n",
      "Epoch 19/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3985 - accuracy: 0.8191\n",
      "Epoch 20/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3929 - accuracy: 0.8250\n",
      "Epoch 21/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3674 - accuracy: 0.8333\n",
      "Epoch 22/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3666 - accuracy: 0.8333\n",
      "Epoch 23/1500\n",
      "64/64 [==============================] - 0s 975us/step - loss: 0.3525 - accuracy: 0.8451\n",
      "Epoch 24/1500\n",
      "64/64 [==============================] - 0s 994us/step - loss: 0.3796 - accuracy: 0.8309\n",
      "Epoch 25/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3603 - accuracy: 0.8358\n",
      "Epoch 26/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3594 - accuracy: 0.8378\n",
      "Epoch 27/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3539 - accuracy: 0.8378\n",
      "Epoch 28/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3290 - accuracy: 0.8579\n",
      "Epoch 29/1500\n",
      "64/64 [==============================] - 0s 995us/step - loss: 0.3441 - accuracy: 0.8422\n",
      "Epoch 30/1500\n",
      "64/64 [==============================] - 0s 989us/step - loss: 0.3174 - accuracy: 0.8555\n",
      "Epoch 31/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3230 - accuracy: 0.8476\n",
      "Epoch 32/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3143 - accuracy: 0.8550\n",
      "Epoch 33/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.8648\n",
      "Epoch 34/1500\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.3050 - accuracy: 0.8682\n",
      "Epoch 35/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8491\n",
      "Epoch 36/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3165 - accuracy: 0.8564\n",
      "Epoch 37/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3165 - accuracy: 0.8668\n",
      "Epoch 38/1500\n",
      "64/64 [==============================] - 0s 990us/step - loss: 0.2979 - accuracy: 0.8599\n",
      "Epoch 39/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2747 - accuracy: 0.8771\n",
      "Epoch 40/1500\n",
      "64/64 [==============================] - 0s 980us/step - loss: 0.2983 - accuracy: 0.8712\n",
      "Epoch 41/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2757 - accuracy: 0.8761\n",
      "Epoch 42/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2717 - accuracy: 0.8786\n",
      "Epoch 43/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2949 - accuracy: 0.8712\n",
      "Epoch 44/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2775 - accuracy: 0.8687\n",
      "Epoch 45/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2724 - accuracy: 0.8800\n",
      "Epoch 46/1500\n",
      "64/64 [==============================] - 0s 994us/step - loss: 0.2776 - accuracy: 0.8746\n",
      "Epoch 47/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2660 - accuracy: 0.8850\n",
      "Epoch 48/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2811 - accuracy: 0.8776\n",
      "Epoch 49/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2855 - accuracy: 0.8756\n",
      "Epoch 50/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2752 - accuracy: 0.8648\n",
      "Epoch 51/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2730 - accuracy: 0.8830\n",
      "Epoch 52/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2775 - accuracy: 0.8800\n",
      "Epoch 53/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2660 - accuracy: 0.8800\n",
      "Epoch 54/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2458 - accuracy: 0.8874\n",
      "Epoch 55/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2495 - accuracy: 0.8928\n",
      "Epoch 56/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2414 - accuracy: 0.8963\n",
      "Epoch 57/1500\n",
      "64/64 [==============================] - 0s 993us/step - loss: 0.2373 - accuracy: 0.8923\n",
      "Epoch 58/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.8958\n",
      "Epoch 59/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2366 - accuracy: 0.8972\n",
      "Epoch 60/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2367 - accuracy: 0.9002\n",
      "Epoch 61/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2378 - accuracy: 0.8977\n",
      "Epoch 62/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2545 - accuracy: 0.8864\n",
      "Epoch 63/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2416 - accuracy: 0.8889\n",
      "Epoch 64/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9066\n",
      "Epoch 65/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2348 - accuracy: 0.8972\n",
      "Epoch 66/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2369 - accuracy: 0.9002\n",
      "Epoch 67/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2292 - accuracy: 0.8958\n",
      "Epoch 68/1500\n",
      "64/64 [==============================] - 0s 993us/step - loss: 0.2265 - accuracy: 0.9022\n",
      "Epoch 69/1500\n",
      "64/64 [==============================] - 0s 984us/step - loss: 0.2199 - accuracy: 0.9056\n",
      "Epoch 70/1500\n",
      "64/64 [==============================] - 0s 995us/step - loss: 0.2263 - accuracy: 0.9002\n",
      "Epoch 71/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2419 - accuracy: 0.8958\n",
      "Epoch 72/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2267 - accuracy: 0.8968\n",
      "Epoch 73/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.9076\n",
      "Epoch 74/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9095\n",
      "Epoch 75/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9090\n",
      "Epoch 76/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2117 - accuracy: 0.9110\n",
      "Epoch 77/1500\n",
      "64/64 [==============================] - 0s 997us/step - loss: 0.2027 - accuracy: 0.9125\n",
      "Epoch 78/1500\n",
      "64/64 [==============================] - 0s 973us/step - loss: 0.2108 - accuracy: 0.9130\n",
      "Epoch 79/1500\n",
      "64/64 [==============================] - 0s 977us/step - loss: 0.2057 - accuracy: 0.9179\n",
      "Epoch 80/1500\n",
      "64/64 [==============================] - 0s 981us/step - loss: 0.2053 - accuracy: 0.9130\n",
      "Epoch 81/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2150 - accuracy: 0.9149\n",
      "Epoch 82/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.9145\n",
      "Epoch 83/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1972 - accuracy: 0.9208\n",
      "Epoch 84/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9174\n",
      "Epoch 85/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9095\n",
      "Epoch 86/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1931 - accuracy: 0.9174\n",
      "Epoch 87/1500\n",
      "64/64 [==============================] - 0s 970us/step - loss: 0.1918 - accuracy: 0.9174\n",
      "Epoch 88/1500\n",
      "64/64 [==============================] - 0s 964us/step - loss: 0.1978 - accuracy: 0.9154\n",
      "Epoch 89/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1998 - accuracy: 0.9169\n",
      "Epoch 90/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.9218\n",
      "Epoch 91/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1795 - accuracy: 0.9253\n",
      "Epoch 92/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9233\n",
      "Epoch 93/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.9189\n",
      "Epoch 94/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.9154\n",
      "Epoch 95/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9199\n",
      "Epoch 96/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9248\n",
      "Epoch 97/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1785 - accuracy: 0.9287\n",
      "Epoch 98/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9258\n",
      "Epoch 99/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1748 - accuracy: 0.9263\n",
      "Epoch 100/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9238\n",
      "Epoch 101/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9263\n",
      "Epoch 102/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.9228\n",
      "Epoch 103/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.9356\n",
      "Epoch 104/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1688 - accuracy: 0.9263\n",
      "Epoch 105/1500\n",
      "64/64 [==============================] - 0s 991us/step - loss: 0.1711 - accuracy: 0.9307\n",
      "Epoch 106/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9228\n",
      "Epoch 107/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9317\n",
      "Epoch 108/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9326\n",
      "Epoch 109/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9351\n",
      "Epoch 110/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9341\n",
      "Epoch 111/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9238\n",
      "Epoch 112/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9297\n",
      "Epoch 113/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9415\n",
      "Epoch 114/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.9341\n",
      "Epoch 115/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.9272\n",
      "Epoch 116/1500\n",
      "64/64 [==============================] - 0s 988us/step - loss: 0.1698 - accuracy: 0.9356\n",
      "Epoch 117/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9336\n",
      "Epoch 118/1500\n",
      "64/64 [==============================] - 0s 983us/step - loss: 0.1518 - accuracy: 0.9395\n",
      "Epoch 119/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9351\n",
      "Epoch 120/1500\n",
      "64/64 [==============================] - 0s 983us/step - loss: 0.1517 - accuracy: 0.9435\n",
      "Epoch 121/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9258\n",
      "Epoch 122/1500\n",
      "64/64 [==============================] - 0s 995us/step - loss: 0.1412 - accuracy: 0.9449\n",
      "Epoch 123/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9410\n",
      "Epoch 124/1500\n",
      "64/64 [==============================] - 0s 996us/step - loss: 0.1422 - accuracy: 0.9351\n",
      "Epoch 125/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9454\n",
      "Epoch 126/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.1544 - accuracy: 0.9366\n",
      "Epoch 127/1500\n",
      "64/64 [==============================] - 0s 969us/step - loss: 0.1592 - accuracy: 0.9415\n",
      "Epoch 128/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9326\n",
      "Epoch 129/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9425\n",
      "Epoch 130/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.9444\n",
      "Epoch 131/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1494 - accuracy: 0.9341\n",
      "Epoch 132/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9430\n",
      "Epoch 133/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9494\n",
      "Epoch 134/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9302\n",
      "Epoch 135/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1525 - accuracy: 0.9317\n",
      "Epoch 136/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9390\n",
      "Epoch 137/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9508\n",
      "Epoch 138/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9336\n",
      "Epoch 139/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9430\n",
      "Epoch 140/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9405\n",
      "Epoch 141/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9454\n",
      "Epoch 142/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9371\n",
      "Epoch 143/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9449\n",
      "Epoch 144/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9503\n",
      "Epoch 145/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9558\n",
      "Epoch 146/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9415\n",
      "Epoch 147/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9395\n",
      "Epoch 148/1500\n",
      "64/64 [==============================] - 0s 984us/step - loss: 0.1468 - accuracy: 0.9410\n",
      "Epoch 149/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.9454\n",
      "Epoch 150/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9528\n",
      "Epoch 151/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9518\n",
      "Epoch 152/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9405\n",
      "Epoch 153/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9366\n",
      "Epoch 154/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9435\n",
      "Epoch 155/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9508\n",
      "Epoch 156/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9454\n",
      "Epoch 157/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9499\n",
      "Epoch 158/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9484\n",
      "Epoch 159/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9499\n",
      "Epoch 160/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9474\n",
      "Epoch 161/1500\n",
      "64/64 [==============================] - 0s 986us/step - loss: 0.1337 - accuracy: 0.9494\n",
      "Epoch 162/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9400\n",
      "Epoch 163/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1429 - accuracy: 0.9351\n",
      "Epoch 164/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.9420\n",
      "Epoch 165/1500\n",
      "64/64 [==============================] - 0s 979us/step - loss: 0.1272 - accuracy: 0.9479\n",
      "Epoch 166/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9518\n",
      "Epoch 167/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9410\n",
      "Epoch 168/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9361\n",
      "Epoch 169/1500\n",
      "64/64 [==============================] - 0s 999us/step - loss: 0.1339 - accuracy: 0.9415\n",
      "Epoch 170/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9454\n",
      "Epoch 171/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9543\n",
      "Epoch 172/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9479\n",
      "Epoch 173/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.1213 - accuracy: 0.9533\n",
      "Epoch 174/1500\n",
      "64/64 [==============================] - 0s 978us/step - loss: 0.1329 - accuracy: 0.9435\n",
      "Epoch 175/1500\n",
      "64/64 [==============================] - 0s 982us/step - loss: 0.1209 - accuracy: 0.9523\n",
      "Epoch 176/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9518\n",
      "Epoch 177/1500\n",
      "64/64 [==============================] - 0s 973us/step - loss: 0.1317 - accuracy: 0.9440\n",
      "Epoch 178/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9494\n",
      "Epoch 179/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9533\n",
      "Epoch 180/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9494\n",
      "Epoch 181/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9562\n",
      "Epoch 182/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9508\n",
      "Epoch 183/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9479\n",
      "Epoch 184/1500\n",
      "64/64 [==============================] - 0s 995us/step - loss: 0.1273 - accuracy: 0.9503\n",
      "Epoch 185/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9508\n",
      "Epoch 186/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9489\n",
      "Epoch 187/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9543\n",
      "Epoch 188/1500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.1102 - accuracy: 0.9533\n",
      "Epoch 189/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9523\n",
      "Epoch 190/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9489\n",
      "Epoch 191/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9553\n",
      "Epoch 192/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9499\n",
      "Epoch 193/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9518\n",
      "Epoch 194/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9459\n",
      "Epoch 195/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9626\n",
      "Epoch 196/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9567\n",
      "Epoch 197/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.9523\n",
      "Epoch 198/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9508\n",
      "Epoch 199/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9607\n",
      "Epoch 200/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9513\n",
      "Epoch 201/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9553\n",
      "Epoch 202/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9582\n",
      "Epoch 203/1500\n",
      "64/64 [==============================] - 0s 984us/step - loss: 0.1196 - accuracy: 0.9508\n",
      "Epoch 204/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9617\n",
      "Epoch 205/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9567\n",
      "Epoch 206/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9484\n",
      "Epoch 207/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9562\n",
      "Epoch 208/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9489\n",
      "Epoch 209/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9592\n",
      "Epoch 210/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9523\n",
      "Epoch 211/1500\n",
      "64/64 [==============================] - 0s 992us/step - loss: 0.0988 - accuracy: 0.9641\n",
      "Epoch 212/1500\n",
      "64/64 [==============================] - 0s 994us/step - loss: 0.1142 - accuracy: 0.9523\n",
      "Epoch 213/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9548\n",
      "Epoch 214/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9631\n",
      "Epoch 215/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9646\n",
      "Epoch 216/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9607\n",
      "Epoch 217/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9528\n",
      "Epoch 218/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9641\n",
      "Epoch 219/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9602\n",
      "Epoch 220/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9543\n",
      "Epoch 221/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0977 - accuracy: 0.9646\n",
      "Epoch 222/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9577\n",
      "Epoch 223/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9646\n",
      "Epoch 224/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9676\n",
      "Epoch 225/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.9636\n",
      "Epoch 226/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1135 - accuracy: 0.9558\n",
      "Epoch 227/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.9612\n",
      "Epoch 228/1500\n",
      "64/64 [==============================] - 0s 982us/step - loss: 0.1016 - accuracy: 0.9597\n",
      "Epoch 229/1500\n",
      "64/64 [==============================] - 0s 969us/step - loss: 0.0975 - accuracy: 0.9592\n",
      "Epoch 230/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9602\n",
      "Epoch 231/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9587\n",
      "Epoch 232/1500\n",
      "64/64 [==============================] - 0s 968us/step - loss: 0.0995 - accuracy: 0.9597\n",
      "Epoch 233/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9538\n",
      "Epoch 234/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9621\n",
      "Epoch 235/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.9587\n",
      "Epoch 236/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9685\n",
      "Epoch 237/1500\n",
      "64/64 [==============================] - 0s 967us/step - loss: 0.0929 - accuracy: 0.9597\n",
      "Epoch 238/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9602\n",
      "Epoch 239/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9666\n",
      "Epoch 240/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9548\n",
      "Epoch 241/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9587\n",
      "Epoch 242/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9592\n",
      "Epoch 243/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9587\n",
      "Epoch 244/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.9666\n",
      "Epoch 245/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9612\n",
      "Epoch 246/1500\n",
      "64/64 [==============================] - 0s 990us/step - loss: 0.0908 - accuracy: 0.9631\n",
      "Epoch 247/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9656\n",
      "Epoch 248/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9607\n",
      "Epoch 249/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9548\n",
      "Epoch 250/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9631\n",
      "Epoch 251/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9587\n",
      "Epoch 252/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9631\n",
      "Epoch 253/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9680\n",
      "Epoch 254/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.9685\n",
      "Epoch 255/1500\n",
      "64/64 [==============================] - 0s 998us/step - loss: 0.0948 - accuracy: 0.9631\n",
      "Epoch 256/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9636\n",
      "Epoch 257/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9641\n",
      "Epoch 258/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9695\n",
      "Epoch 259/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9513\n",
      "Epoch 260/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9607\n",
      "Epoch 261/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9607\n",
      "Epoch 262/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9705\n",
      "Epoch 263/1500\n",
      "64/64 [==============================] - 0s 973us/step - loss: 0.0891 - accuracy: 0.9641\n",
      "Epoch 264/1500\n",
      "64/64 [==============================] - 0s 977us/step - loss: 0.1045 - accuracy: 0.9567\n",
      "Epoch 265/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9680\n",
      "Epoch 266/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9641\n",
      "Epoch 267/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.9700\n",
      "Epoch 268/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9626\n",
      "Epoch 269/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.9656\n",
      "Epoch 270/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9690\n",
      "Epoch 271/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9641\n",
      "Epoch 272/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.1125 - accuracy: 0.9513\n",
      "Epoch 273/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0977 - accuracy: 0.9621\n",
      "Epoch 274/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9592\n",
      "Epoch 275/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9646\n",
      "Epoch 276/1500\n",
      "64/64 [==============================] - 0s 997us/step - loss: 0.0863 - accuracy: 0.9685\n",
      "Epoch 277/1500\n",
      "64/64 [==============================] - 0s 990us/step - loss: 0.0805 - accuracy: 0.9676\n",
      "Epoch 278/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.9784\n",
      "Epoch 279/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9690\n",
      "Epoch 280/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.9710\n",
      "Epoch 281/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9715\n",
      "Epoch 282/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9700\n",
      "Epoch 283/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9612\n",
      "Epoch 284/1500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0902 - accuracy: 0.9582\n",
      "Epoch 285/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9661\n",
      "Epoch 286/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.9671\n",
      "Epoch 287/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9646\n",
      "Epoch 288/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.9661\n",
      "Epoch 289/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9661\n",
      "Epoch 290/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9695\n",
      "Epoch 291/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0701 - accuracy: 0.9754\n",
      "Epoch 292/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.9774\n",
      "Epoch 293/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0867 - accuracy: 0.9671\n",
      "Epoch 294/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9680\n",
      "Epoch 295/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9621\n",
      "Epoch 296/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9656\n",
      "Epoch 297/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9725\n",
      "Epoch 298/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9735\n",
      "Epoch 299/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.9754\n",
      "Epoch 300/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.9769\n",
      "Epoch 301/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.9764\n",
      "Epoch 302/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9636\n",
      "Epoch 303/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9715\n",
      "Epoch 304/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9685\n",
      "Epoch 305/1500\n",
      "64/64 [==============================] - 0s 989us/step - loss: 0.0822 - accuracy: 0.9685\n",
      "Epoch 306/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9710\n",
      "Epoch 307/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9661\n",
      "Epoch 308/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.9754\n",
      "Epoch 309/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9749\n",
      "Epoch 310/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9641\n",
      "Epoch 311/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.9730\n",
      "Epoch 312/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9646\n",
      "Epoch 313/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9685\n",
      "Epoch 314/1500\n",
      "64/64 [==============================] - 0s 994us/step - loss: 0.0828 - accuracy: 0.9685\n",
      "Epoch 315/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.9690\n",
      "Epoch 316/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9651\n",
      "Epoch 317/1500\n",
      "64/64 [==============================] - 0s 977us/step - loss: 0.0820 - accuracy: 0.9695\n",
      "Epoch 318/1500\n",
      "64/64 [==============================] - 0s 994us/step - loss: 0.0763 - accuracy: 0.9720\n",
      "Epoch 319/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9735\n",
      "Epoch 320/1500\n",
      "64/64 [==============================] - 0s 999us/step - loss: 0.0874 - accuracy: 0.9671\n",
      "Epoch 321/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.9739\n",
      "Epoch 322/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9725\n",
      "Epoch 323/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.9720\n",
      "Epoch 324/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.9744\n",
      "Epoch 325/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.9739\n",
      "Epoch 326/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.9744\n",
      "Epoch 327/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9715\n",
      "Epoch 328/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.9769\n",
      "Epoch 329/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.9666\n",
      "Epoch 330/1500\n",
      "64/64 [==============================] - 0s 991us/step - loss: 0.0755 - accuracy: 0.9735\n",
      "Epoch 331/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9651\n",
      "Epoch 332/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9715\n",
      "Epoch 333/1500\n",
      "64/64 [==============================] - 0s 975us/step - loss: 0.0919 - accuracy: 0.9651\n",
      "Epoch 334/1500\n",
      "64/64 [==============================] - 0s 977us/step - loss: 0.0729 - accuracy: 0.9705\n",
      "Epoch 335/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9671\n",
      "Epoch 336/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9700\n",
      "Epoch 337/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9720\n",
      "Epoch 338/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9685\n",
      "Epoch 339/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9690\n",
      "Epoch 340/1500\n",
      "64/64 [==============================] - 0s 994us/step - loss: 0.0724 - accuracy: 0.9739\n",
      "Epoch 341/1500\n",
      "64/64 [==============================] - 0s 949us/step - loss: 0.0760 - accuracy: 0.9700\n",
      "Epoch 342/1500\n",
      "64/64 [==============================] - 0s 981us/step - loss: 0.0754 - accuracy: 0.9705\n",
      "Epoch 343/1500\n",
      "64/64 [==============================] - 0s 980us/step - loss: 0.0662 - accuracy: 0.9779\n",
      "Epoch 344/1500\n",
      "64/64 [==============================] - 0s 991us/step - loss: 0.0713 - accuracy: 0.9744\n",
      "Epoch 345/1500\n",
      "64/64 [==============================] - 0s 979us/step - loss: 0.0670 - accuracy: 0.9759\n",
      "Epoch 346/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.9784\n",
      "Epoch 347/1500\n",
      "64/64 [==============================] - 0s 993us/step - loss: 0.0698 - accuracy: 0.9739\n",
      "Epoch 348/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9671\n",
      "Epoch 349/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9730\n",
      "Epoch 350/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.9666\n",
      "Epoch 351/1500\n",
      "64/64 [==============================] - 0s 991us/step - loss: 0.0843 - accuracy: 0.9646\n",
      "Epoch 352/1500\n",
      "64/64 [==============================] - 0s 995us/step - loss: 0.0625 - accuracy: 0.9794\n",
      "Epoch 353/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9680\n",
      "Epoch 354/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9803\n",
      "Epoch 355/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.9676\n",
      "Epoch 356/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.9725\n",
      "Epoch 357/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9759\n",
      "Epoch 358/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9798\n",
      "Epoch 359/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9774\n",
      "Epoch 360/1500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0837 - accuracy: 0.9671\n",
      "Epoch 361/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.9705\n",
      "Epoch 362/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.9700\n",
      "Epoch 363/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9690\n",
      "Epoch 364/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9749\n",
      "Epoch 365/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9735\n",
      "Epoch 366/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.9735\n",
      "Epoch 367/1500\n",
      "64/64 [==============================] - 0s 994us/step - loss: 0.0632 - accuracy: 0.9789\n",
      "Epoch 368/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.9779\n",
      "Epoch 369/1500\n",
      "64/64 [==============================] - 0s 981us/step - loss: 0.0588 - accuracy: 0.9808\n",
      "Epoch 370/1500\n",
      "64/64 [==============================] - 0s 980us/step - loss: 0.0665 - accuracy: 0.9779\n",
      "Epoch 371/1500\n",
      "64/64 [==============================] - 0s 977us/step - loss: 0.0771 - accuracy: 0.9680\n",
      "Epoch 372/1500\n",
      "64/64 [==============================] - 0s 950us/step - loss: 0.0610 - accuracy: 0.9774\n",
      "Epoch 373/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.9764\n",
      "Epoch 374/1500\n",
      "64/64 [==============================] - 0s 974us/step - loss: 0.0777 - accuracy: 0.9735\n",
      "Epoch 375/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.9735\n",
      "Epoch 376/1500\n",
      "64/64 [==============================] - 0s 981us/step - loss: 0.0665 - accuracy: 0.9759\n",
      "Epoch 377/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.9774\n",
      "Epoch 378/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9636\n",
      "Epoch 379/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9730\n",
      "Epoch 380/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9695\n",
      "Epoch 381/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9695\n",
      "Epoch 382/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.9739\n",
      "Epoch 383/1500\n",
      "64/64 [==============================] - 0s 968us/step - loss: 0.0716 - accuracy: 0.9730\n",
      "Epoch 384/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.9749\n",
      "Epoch 385/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9710\n",
      "Epoch 386/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9685\n",
      "Epoch 387/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9685\n",
      "Epoch 388/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9666\n",
      "Epoch 389/1500\n",
      "64/64 [==============================] - 0s 980us/step - loss: 0.0682 - accuracy: 0.9749\n",
      "Epoch 390/1500\n",
      "64/64 [==============================] - 0s 984us/step - loss: 0.0685 - accuracy: 0.9695\n",
      "Epoch 391/1500\n",
      "64/64 [==============================] - 0s 964us/step - loss: 0.0634 - accuracy: 0.9774\n",
      "Epoch 392/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9769\n",
      "Epoch 393/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.9774\n",
      "Epoch 394/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9666\n",
      "Epoch 395/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9646\n",
      "Epoch 396/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9744\n",
      "Epoch 397/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9720\n",
      "Epoch 398/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.9730\n",
      "Epoch 399/1500\n",
      "48/64 [=====================>........] - ETA: 0s - loss: 0.0638 - accuracy: 0.9740Restoring model weights from the end of the best epoch: 369.\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9764\n",
      "Epoch 399: early stopping\n",
      "6/6 [==============================] - 0s 881us/step - loss: 0.9610 - accuracy: 0.7188\n",
      "6/6 [==============================] - 0s 889us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.81 (17/21)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 192, Predictions: 192, Actuals: 192, Gender: 192\n",
      "Final Test Results - Loss: 0.9609705805778503, Accuracy: 0.71875, Precision: 0.7497631997631998, Recall: 0.7369000178433355, F1 Score: 0.7309017891371424\n",
      "Confusion Matrix:\n",
      " [[80  1 34]\n",
      " [ 7 27  0]\n",
      " [12  0 31]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "        ..\n",
      "048A     1\n",
      "066A     1\n",
      "096A     1\n",
      "026C     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 86, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "019A    17\n",
      "028A    13\n",
      "002A    13\n",
      "068A    11\n",
      "051B     9\n",
      "015A     9\n",
      "027A     7\n",
      "099A     7\n",
      "109A     6\n",
      "108A     6\n",
      "023B     5\n",
      "044A     5\n",
      "025C     5\n",
      "070A     5\n",
      "052A     4\n",
      "060A     3\n",
      "012A     3\n",
      "038A     2\n",
      "011A     2\n",
      "043A     1\n",
      "049A     1\n",
      "092A     1\n",
      "091A     1\n",
      "115A     1\n",
      "100A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    306\n",
      "M    264\n",
      "F    197\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    73\n",
      "F    55\n",
      "X    42\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 103A, 071A, 097B, 074...\n",
      "kitten    [014B, 111A, 040A, 046A, 047A, 042A, 050A, 041...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 113...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 028A, 019A, 002B, 091A, 002A, 027A, 038...\n",
      "kitten                       [044A, 109A, 043A, 049A, 115A]\n",
      "senior                                   [051B, 108A, 011A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 11, 'senior': 19}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 5, 'senior': 3}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '003A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '013B' '014A' '014B' '016A' '018A' '019B' '020A' '021A' '022A'\n",
      " '023A' '024A' '025A' '025B' '026A' '026B' '026C' '029A' '031A' '032A'\n",
      " '033A' '034A' '035A' '036A' '037A' '039A' '040A' '041A' '042A' '045A'\n",
      " '046A' '047A' '048A' '050A' '051A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '059A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '069A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '093A'\n",
      " '094A' '095A' '096A' '097A' '097B' '101A' '102A' '103A' '104A' '105A'\n",
      " '106A' '110A' '111A' '113A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '002B' '011A' '012A' '015A' '019A' '023B' '025C' '027A' '028A'\n",
      " '038A' '043A' '044A' '049A' '051B' '052A' '060A' '068A' '070A' '091A'\n",
      " '092A' '099A' '100A' '108A' '109A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '003A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '013B' '014A' '014B' '016A' '018A' '019B' '020A' '021A' '022A'\n",
      " '023A' '024A' '025A' '025B' '026A' '026B' '026C' '029A' '031A' '032A'\n",
      " '033A' '034A' '035A' '036A' '037A' '039A' '040A' '041A' '042A' '045A'\n",
      " '046A' '047A' '048A' '050A' '051A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '059A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '069A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '093A'\n",
      " '094A' '095A' '096A' '097A' '097B' '101A' '102A' '103A' '104A' '105A'\n",
      " '106A' '110A' '111A' '113A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '002B' '011A' '012A' '015A' '019A' '023B' '025C' '027A' '028A'\n",
      " '038A' '043A' '044A' '049A' '051B' '052A' '060A' '068A' '070A' '091A'\n",
      " '092A' '099A' '100A' '108A' '109A' '115A']\n",
      "Length of X_train_val:\n",
      "767\n",
      "Length of y_train_val:\n",
      "767\n",
      "Length of groups_train_val:\n",
      "767\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     449\n",
      "senior    161\n",
      "kitten    157\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     139\n",
      "senior     17\n",
      "kitten     14\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     449\n",
      "senior    161\n",
      "kitten    157\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     139\n",
      "senior     17\n",
      "kitten     14\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 898, 2: 644, 1: 628})\n",
      "Epoch 1/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.9339 - accuracy: 0.5908\n",
      "Epoch 2/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7462 - accuracy: 0.6843\n",
      "Epoch 3/1500\n",
      "68/68 [==============================] - 0s 980us/step - loss: 0.6733 - accuracy: 0.7046\n",
      "Epoch 4/1500\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.6161 - accuracy: 0.7350\n",
      "Epoch 5/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5714 - accuracy: 0.7424\n",
      "Epoch 6/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5647 - accuracy: 0.7548\n",
      "Epoch 7/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5343 - accuracy: 0.7700\n",
      "Epoch 8/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5222 - accuracy: 0.7751\n",
      "Epoch 9/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4845 - accuracy: 0.7779\n",
      "Epoch 10/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5020 - accuracy: 0.7825\n",
      "Epoch 11/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4685 - accuracy: 0.7963\n",
      "Epoch 12/1500\n",
      "68/68 [==============================] - 0s 964us/step - loss: 0.4476 - accuracy: 0.7972\n",
      "Epoch 13/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4358 - accuracy: 0.8060\n",
      "Epoch 14/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4192 - accuracy: 0.8171\n",
      "Epoch 15/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4300 - accuracy: 0.8120\n",
      "Epoch 16/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.4200 - accuracy: 0.8235\n",
      "Epoch 17/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4166 - accuracy: 0.8120\n",
      "Epoch 18/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3805 - accuracy: 0.8332\n",
      "Epoch 19/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3634 - accuracy: 0.8419\n",
      "Epoch 20/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3704 - accuracy: 0.8378\n",
      "Epoch 21/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3761 - accuracy: 0.8447\n",
      "Epoch 22/1500\n",
      "68/68 [==============================] - 0s 976us/step - loss: 0.3591 - accuracy: 0.8548\n",
      "Epoch 23/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.3428 - accuracy: 0.8507\n",
      "Epoch 24/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3433 - accuracy: 0.8424\n",
      "Epoch 25/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.8590\n",
      "Epoch 26/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3690 - accuracy: 0.8396\n",
      "Epoch 27/1500\n",
      "68/68 [==============================] - 0s 977us/step - loss: 0.3385 - accuracy: 0.8512\n",
      "Epoch 28/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3222 - accuracy: 0.8641\n",
      "Epoch 29/1500\n",
      "68/68 [==============================] - 0s 991us/step - loss: 0.3311 - accuracy: 0.8544\n",
      "Epoch 30/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8507\n",
      "Epoch 31/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3014 - accuracy: 0.8724\n",
      "Epoch 32/1500\n",
      "68/68 [==============================] - 0s 995us/step - loss: 0.2957 - accuracy: 0.8816\n",
      "Epoch 33/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.2980 - accuracy: 0.8724\n",
      "Epoch 34/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3018 - accuracy: 0.8700\n",
      "Epoch 35/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3019 - accuracy: 0.8719\n",
      "Epoch 36/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2793 - accuracy: 0.8760\n",
      "Epoch 37/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2850 - accuracy: 0.8806\n",
      "Epoch 38/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2965 - accuracy: 0.8747\n",
      "Epoch 39/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2876 - accuracy: 0.8788\n",
      "Epoch 40/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2794 - accuracy: 0.8829\n",
      "Epoch 41/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2642 - accuracy: 0.8876\n",
      "Epoch 42/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2706 - accuracy: 0.8903\n",
      "Epoch 43/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2600 - accuracy: 0.8876\n",
      "Epoch 44/1500\n",
      "68/68 [==============================] - 0s 982us/step - loss: 0.2787 - accuracy: 0.8839\n",
      "Epoch 45/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.2775 - accuracy: 0.8857\n",
      "Epoch 46/1500\n",
      "68/68 [==============================] - 0s 994us/step - loss: 0.2551 - accuracy: 0.8931\n",
      "Epoch 47/1500\n",
      "68/68 [==============================] - 0s 972us/step - loss: 0.2645 - accuracy: 0.8922\n",
      "Epoch 48/1500\n",
      "68/68 [==============================] - 0s 950us/step - loss: 0.2778 - accuracy: 0.8834\n",
      "Epoch 49/1500\n",
      "68/68 [==============================] - 0s 944us/step - loss: 0.2724 - accuracy: 0.8871\n",
      "Epoch 50/1500\n",
      "68/68 [==============================] - 0s 966us/step - loss: 0.2459 - accuracy: 0.8922\n",
      "Epoch 51/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2528 - accuracy: 0.8972\n",
      "Epoch 52/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2429 - accuracy: 0.8926\n",
      "Epoch 53/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.8972\n",
      "Epoch 54/1500\n",
      "68/68 [==============================] - 0s 998us/step - loss: 0.2344 - accuracy: 0.9046\n",
      "Epoch 55/1500\n",
      "68/68 [==============================] - 0s 954us/step - loss: 0.2218 - accuracy: 0.9101\n",
      "Epoch 56/1500\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.2457 - accuracy: 0.8912\n",
      "Epoch 57/1500\n",
      "68/68 [==============================] - 0s 962us/step - loss: 0.2275 - accuracy: 0.9069\n",
      "Epoch 58/1500\n",
      "68/68 [==============================] - 0s 988us/step - loss: 0.2331 - accuracy: 0.9069\n",
      "Epoch 59/1500\n",
      "68/68 [==============================] - 0s 958us/step - loss: 0.2235 - accuracy: 0.9097\n",
      "Epoch 60/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.2457 - accuracy: 0.9014\n",
      "Epoch 61/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2175 - accuracy: 0.9101\n",
      "Epoch 62/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2199 - accuracy: 0.9101\n",
      "Epoch 63/1500\n",
      "68/68 [==============================] - 0s 986us/step - loss: 0.2275 - accuracy: 0.9032\n",
      "Epoch 64/1500\n",
      "68/68 [==============================] - 0s 977us/step - loss: 0.2165 - accuracy: 0.9120\n",
      "Epoch 65/1500\n",
      "68/68 [==============================] - 0s 996us/step - loss: 0.2195 - accuracy: 0.9083\n",
      "Epoch 66/1500\n",
      "68/68 [==============================] - 0s 991us/step - loss: 0.2180 - accuracy: 0.9065\n",
      "Epoch 67/1500\n",
      "68/68 [==============================] - 0s 964us/step - loss: 0.2025 - accuracy: 0.9166\n",
      "Epoch 68/1500\n",
      "68/68 [==============================] - 0s 954us/step - loss: 0.2301 - accuracy: 0.9065\n",
      "Epoch 69/1500\n",
      "68/68 [==============================] - 0s 947us/step - loss: 0.2116 - accuracy: 0.9051\n",
      "Epoch 70/1500\n",
      "68/68 [==============================] - 0s 958us/step - loss: 0.1939 - accuracy: 0.9272\n",
      "Epoch 71/1500\n",
      "68/68 [==============================] - 0s 958us/step - loss: 0.1989 - accuracy: 0.9203\n",
      "Epoch 72/1500\n",
      "68/68 [==============================] - 0s 991us/step - loss: 0.2070 - accuracy: 0.9106\n",
      "Epoch 73/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9249\n",
      "Epoch 74/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2026 - accuracy: 0.9175\n",
      "Epoch 75/1500\n",
      "68/68 [==============================] - 0s 950us/step - loss: 0.2129 - accuracy: 0.9171\n",
      "Epoch 76/1500\n",
      "68/68 [==============================] - 0s 933us/step - loss: 0.1925 - accuracy: 0.9221\n",
      "Epoch 77/1500\n",
      "68/68 [==============================] - 0s 948us/step - loss: 0.1792 - accuracy: 0.9221\n",
      "Epoch 78/1500\n",
      "68/68 [==============================] - 0s 956us/step - loss: 0.1822 - accuracy: 0.9286\n",
      "Epoch 79/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.1923 - accuracy: 0.9198\n",
      "Epoch 80/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.9253\n",
      "Epoch 81/1500\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.1999 - accuracy: 0.9147\n",
      "Epoch 82/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1796 - accuracy: 0.9323\n",
      "Epoch 83/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9258\n",
      "Epoch 84/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9267\n",
      "Epoch 85/1500\n",
      "68/68 [==============================] - 0s 978us/step - loss: 0.1780 - accuracy: 0.9281\n",
      "Epoch 86/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.1906 - accuracy: 0.9203\n",
      "Epoch 87/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9332\n",
      "Epoch 88/1500\n",
      "68/68 [==============================] - 0s 964us/step - loss: 0.1580 - accuracy: 0.9359\n",
      "Epoch 89/1500\n",
      "68/68 [==============================] - 0s 945us/step - loss: 0.1883 - accuracy: 0.9212\n",
      "Epoch 90/1500\n",
      "68/68 [==============================] - 0s 973us/step - loss: 0.1869 - accuracy: 0.9249\n",
      "Epoch 91/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1927 - accuracy: 0.9240\n",
      "Epoch 92/1500\n",
      "68/68 [==============================] - 0s 976us/step - loss: 0.1847 - accuracy: 0.9217\n",
      "Epoch 93/1500\n",
      "68/68 [==============================] - 0s 964us/step - loss: 0.1771 - accuracy: 0.9313\n",
      "Epoch 94/1500\n",
      "68/68 [==============================] - 0s 957us/step - loss: 0.1679 - accuracy: 0.9318\n",
      "Epoch 95/1500\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.1686 - accuracy: 0.9267\n",
      "Epoch 96/1500\n",
      "68/68 [==============================] - 0s 952us/step - loss: 0.1678 - accuracy: 0.9295\n",
      "Epoch 97/1500\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.1669 - accuracy: 0.9276\n",
      "Epoch 98/1500\n",
      "68/68 [==============================] - 0s 990us/step - loss: 0.1650 - accuracy: 0.9429\n",
      "Epoch 99/1500\n",
      "68/68 [==============================] - 0s 947us/step - loss: 0.1582 - accuracy: 0.9359\n",
      "Epoch 100/1500\n",
      "68/68 [==============================] - 0s 959us/step - loss: 0.1785 - accuracy: 0.9281\n",
      "Epoch 101/1500\n",
      "68/68 [==============================] - 0s 965us/step - loss: 0.1901 - accuracy: 0.9207\n",
      "Epoch 102/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.1693 - accuracy: 0.9327\n",
      "Epoch 103/1500\n",
      "68/68 [==============================] - 0s 965us/step - loss: 0.1567 - accuracy: 0.9401\n",
      "Epoch 104/1500\n",
      "68/68 [==============================] - 0s 966us/step - loss: 0.1573 - accuracy: 0.9369\n",
      "Epoch 105/1500\n",
      "68/68 [==============================] - 0s 958us/step - loss: 0.1713 - accuracy: 0.9327\n",
      "Epoch 106/1500\n",
      "68/68 [==============================] - 0s 958us/step - loss: 0.1744 - accuracy: 0.9304\n",
      "Epoch 107/1500\n",
      "68/68 [==============================] - 0s 976us/step - loss: 0.1600 - accuracy: 0.9401\n",
      "Epoch 108/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.1567 - accuracy: 0.9346\n",
      "Epoch 109/1500\n",
      "68/68 [==============================] - 0s 972us/step - loss: 0.1532 - accuracy: 0.9350\n",
      "Epoch 110/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.1515 - accuracy: 0.9369\n",
      "Epoch 111/1500\n",
      "68/68 [==============================] - 0s 993us/step - loss: 0.1646 - accuracy: 0.9327\n",
      "Epoch 112/1500\n",
      "68/68 [==============================] - 0s 974us/step - loss: 0.1539 - accuracy: 0.9341\n",
      "Epoch 113/1500\n",
      "68/68 [==============================] - 0s 951us/step - loss: 0.1616 - accuracy: 0.9309\n",
      "Epoch 114/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.1449 - accuracy: 0.9401\n",
      "Epoch 115/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9424\n",
      "Epoch 116/1500\n",
      "68/68 [==============================] - 0s 984us/step - loss: 0.1700 - accuracy: 0.9378\n",
      "Epoch 117/1500\n",
      "68/68 [==============================] - 0s 961us/step - loss: 0.1390 - accuracy: 0.9475\n",
      "Epoch 118/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.1455 - accuracy: 0.9479\n",
      "Epoch 119/1500\n",
      "68/68 [==============================] - 0s 965us/step - loss: 0.1544 - accuracy: 0.9359\n",
      "Epoch 120/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9475\n",
      "Epoch 121/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9396\n",
      "Epoch 122/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9355\n",
      "Epoch 123/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9401\n",
      "Epoch 124/1500\n",
      "68/68 [==============================] - 0s 973us/step - loss: 0.1556 - accuracy: 0.9350\n",
      "Epoch 125/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9447\n",
      "Epoch 126/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9438\n",
      "Epoch 127/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9410\n",
      "Epoch 128/1500\n",
      "68/68 [==============================] - 0s 974us/step - loss: 0.1368 - accuracy: 0.9498\n",
      "Epoch 129/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9502\n",
      "Epoch 130/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9488\n",
      "Epoch 131/1500\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.1552 - accuracy: 0.9396\n",
      "Epoch 132/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9530\n",
      "Epoch 133/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9415\n",
      "Epoch 134/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9479\n",
      "Epoch 135/1500\n",
      "68/68 [==============================] - 0s 959us/step - loss: 0.1316 - accuracy: 0.9475\n",
      "Epoch 136/1500\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.1586 - accuracy: 0.9369\n",
      "Epoch 137/1500\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.1339 - accuracy: 0.9438\n",
      "Epoch 138/1500\n",
      "68/68 [==============================] - 0s 988us/step - loss: 0.1394 - accuracy: 0.9419\n",
      "Epoch 139/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9456\n",
      "Epoch 140/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9461\n",
      "Epoch 141/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9447\n",
      "Epoch 142/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9479\n",
      "Epoch 143/1500\n",
      "68/68 [==============================] - 0s 968us/step - loss: 0.1243 - accuracy: 0.9576\n",
      "Epoch 144/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9521\n",
      "Epoch 145/1500\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.1314 - accuracy: 0.9558\n",
      "Epoch 146/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9521\n",
      "Epoch 147/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9516\n",
      "Epoch 148/1500\n",
      "68/68 [==============================] - 0s 951us/step - loss: 0.1378 - accuracy: 0.9493\n",
      "Epoch 149/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.1214 - accuracy: 0.9516\n",
      "Epoch 150/1500\n",
      "68/68 [==============================] - 0s 965us/step - loss: 0.1325 - accuracy: 0.9516\n",
      "Epoch 151/1500\n",
      "68/68 [==============================] - 0s 972us/step - loss: 0.1215 - accuracy: 0.9507\n",
      "Epoch 152/1500\n",
      "68/68 [==============================] - 0s 959us/step - loss: 0.1218 - accuracy: 0.9544\n",
      "Epoch 153/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9461\n",
      "Epoch 154/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9512\n",
      "Epoch 155/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.1482 - accuracy: 0.9401\n",
      "Epoch 156/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9470\n",
      "Epoch 157/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9521\n",
      "Epoch 158/1500\n",
      "68/68 [==============================] - 0s 982us/step - loss: 0.1174 - accuracy: 0.9562\n",
      "Epoch 159/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9512\n",
      "Epoch 160/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9571\n",
      "Epoch 161/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9544\n",
      "Epoch 162/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9493\n",
      "Epoch 163/1500\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.1399 - accuracy: 0.9516\n",
      "Epoch 164/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9558\n",
      "Epoch 165/1500\n",
      "68/68 [==============================] - 0s 993us/step - loss: 0.1215 - accuracy: 0.9553\n",
      "Epoch 166/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.1143 - accuracy: 0.9604\n",
      "Epoch 167/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9512\n",
      "Epoch 168/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.1243 - accuracy: 0.9498\n",
      "Epoch 169/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9571\n",
      "Epoch 170/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9571\n",
      "Epoch 171/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9544\n",
      "Epoch 172/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9641\n",
      "Epoch 173/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.1004 - accuracy: 0.9645\n",
      "Epoch 174/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.0997 - accuracy: 0.9576\n",
      "Epoch 175/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.1250 - accuracy: 0.9507\n",
      "Epoch 176/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9516\n",
      "Epoch 177/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9604\n",
      "Epoch 178/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.1082 - accuracy: 0.9585\n",
      "Epoch 179/1500\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.1216 - accuracy: 0.9548\n",
      "Epoch 180/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9627\n",
      "Epoch 181/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9562\n",
      "Epoch 182/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.1094 - accuracy: 0.9539\n",
      "Epoch 183/1500\n",
      "68/68 [==============================] - 0s 4ms/step - loss: 0.1091 - accuracy: 0.9590\n",
      "Epoch 184/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9535\n",
      "Epoch 185/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9553\n",
      "Epoch 186/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9631\n",
      "Epoch 187/1500\n",
      "68/68 [==============================] - 0s 995us/step - loss: 0.1022 - accuracy: 0.9627\n",
      "Epoch 188/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.1152 - accuracy: 0.9548\n",
      "Epoch 189/1500\n",
      "68/68 [==============================] - 0s 986us/step - loss: 0.0970 - accuracy: 0.9590\n",
      "Epoch 190/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.9608\n",
      "Epoch 191/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9608\n",
      "Epoch 192/1500\n",
      "68/68 [==============================] - 0s 993us/step - loss: 0.0988 - accuracy: 0.9608\n",
      "Epoch 193/1500\n",
      "68/68 [==============================] - 0s 989us/step - loss: 0.1120 - accuracy: 0.9548\n",
      "Epoch 194/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9613\n",
      "Epoch 195/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.1171 - accuracy: 0.9530\n",
      "Epoch 196/1500\n",
      "68/68 [==============================] - 0s 962us/step - loss: 0.1114 - accuracy: 0.9562\n",
      "Epoch 197/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9544\n",
      "Epoch 198/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9585\n",
      "Epoch 199/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9636\n",
      "Epoch 200/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9631\n",
      "Epoch 201/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9530\n",
      "Epoch 202/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9631\n",
      "Epoch 203/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.1163 - accuracy: 0.9530\n",
      "Epoch 204/1500\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.0999 - accuracy: 0.9585\n",
      "Epoch 205/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9645\n",
      "Epoch 206/1500\n",
      "68/68 [==============================] - 0s 952us/step - loss: 0.1007 - accuracy: 0.9571\n",
      "Epoch 207/1500\n",
      "68/68 [==============================] - 0s 952us/step - loss: 0.1004 - accuracy: 0.9673\n",
      "Epoch 208/1500\n",
      "68/68 [==============================] - 0s 980us/step - loss: 0.1035 - accuracy: 0.9613\n",
      "Epoch 209/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.0960 - accuracy: 0.9608\n",
      "Epoch 210/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9622\n",
      "Epoch 211/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9668\n",
      "Epoch 212/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9631\n",
      "Epoch 213/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.9631\n",
      "Epoch 214/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9618\n",
      "Epoch 215/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9594\n",
      "Epoch 216/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9599\n",
      "Epoch 217/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9618\n",
      "Epoch 218/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9622\n",
      "Epoch 219/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9585\n",
      "Epoch 220/1500\n",
      "68/68 [==============================] - 0s 989us/step - loss: 0.1148 - accuracy: 0.9622\n",
      "Epoch 221/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.9627\n",
      "Epoch 222/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9691\n",
      "Epoch 223/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9645\n",
      "Epoch 224/1500\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.0870 - accuracy: 0.9668\n",
      "Epoch 225/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.0959 - accuracy: 0.9668\n",
      "Epoch 226/1500\n",
      "68/68 [==============================] - 0s 958us/step - loss: 0.1009 - accuracy: 0.9608\n",
      "Epoch 227/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9673\n",
      "Epoch 228/1500\n",
      "68/68 [==============================] - 0s 983us/step - loss: 0.0881 - accuracy: 0.9682\n",
      "Epoch 229/1500\n",
      "68/68 [==============================] - 0s 960us/step - loss: 0.0784 - accuracy: 0.9724\n",
      "Epoch 230/1500\n",
      "68/68 [==============================] - 0s 936us/step - loss: 0.0839 - accuracy: 0.9733\n",
      "Epoch 231/1500\n",
      "68/68 [==============================] - 0s 958us/step - loss: 0.0888 - accuracy: 0.9627\n",
      "Epoch 232/1500\n",
      "68/68 [==============================] - 0s 978us/step - loss: 0.0996 - accuracy: 0.9627\n",
      "Epoch 233/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9710\n",
      "Epoch 234/1500\n",
      "68/68 [==============================] - 0s 946us/step - loss: 0.0824 - accuracy: 0.9700\n",
      "Epoch 235/1500\n",
      "68/68 [==============================] - 0s 947us/step - loss: 0.0788 - accuracy: 0.9714\n",
      "Epoch 236/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.1024 - accuracy: 0.9604\n",
      "Epoch 237/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9691\n",
      "Epoch 238/1500\n",
      "68/68 [==============================] - 0s 998us/step - loss: 0.0838 - accuracy: 0.9691\n",
      "Epoch 239/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.1021 - accuracy: 0.9618\n",
      "Epoch 240/1500\n",
      "68/68 [==============================] - 0s 927us/step - loss: 0.0853 - accuracy: 0.9664\n",
      "Epoch 241/1500\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.0848 - accuracy: 0.9705\n",
      "Epoch 242/1500\n",
      "68/68 [==============================] - 0s 956us/step - loss: 0.0812 - accuracy: 0.9774\n",
      "Epoch 243/1500\n",
      "68/68 [==============================] - 0s 974us/step - loss: 0.0928 - accuracy: 0.9631\n",
      "Epoch 244/1500\n",
      "68/68 [==============================] - 0s 945us/step - loss: 0.0962 - accuracy: 0.9668\n",
      "Epoch 245/1500\n",
      "68/68 [==============================] - 0s 938us/step - loss: 0.0962 - accuracy: 0.9627\n",
      "Epoch 246/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9664\n",
      "Epoch 247/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9710\n",
      "Epoch 248/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.9664\n",
      "Epoch 249/1500\n",
      "68/68 [==============================] - 0s 972us/step - loss: 0.0894 - accuracy: 0.9668\n",
      "Epoch 250/1500\n",
      "68/68 [==============================] - 0s 980us/step - loss: 0.0867 - accuracy: 0.9710\n",
      "Epoch 251/1500\n",
      "68/68 [==============================] - 0s 989us/step - loss: 0.0803 - accuracy: 0.9654\n",
      "Epoch 252/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.0965 - accuracy: 0.9641\n",
      "Epoch 253/1500\n",
      "68/68 [==============================] - 0s 942us/step - loss: 0.0787 - accuracy: 0.9733\n",
      "Epoch 254/1500\n",
      "68/68 [==============================] - 0s 993us/step - loss: 0.0788 - accuracy: 0.9742\n",
      "Epoch 255/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9700\n",
      "Epoch 256/1500\n",
      "68/68 [==============================] - 0s 959us/step - loss: 0.0889 - accuracy: 0.9677\n",
      "Epoch 257/1500\n",
      "68/68 [==============================] - 0s 989us/step - loss: 0.0868 - accuracy: 0.9645\n",
      "Epoch 258/1500\n",
      "68/68 [==============================] - 0s 983us/step - loss: 0.0760 - accuracy: 0.9728\n",
      "Epoch 259/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.9705\n",
      "Epoch 260/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9700\n",
      "Epoch 261/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9696\n",
      "Epoch 262/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9673\n",
      "Epoch 263/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.9668\n",
      "Epoch 264/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.9765\n",
      "Epoch 265/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0586 - accuracy: 0.9820\n",
      "Epoch 266/1500\n",
      "68/68 [==============================] - 0s 986us/step - loss: 0.0963 - accuracy: 0.9604\n",
      "Epoch 267/1500\n",
      "68/68 [==============================] - 0s 966us/step - loss: 0.0802 - accuracy: 0.9747\n",
      "Epoch 268/1500\n",
      "68/68 [==============================] - 0s 993us/step - loss: 0.0837 - accuracy: 0.9682\n",
      "Epoch 269/1500\n",
      "68/68 [==============================] - 0s 992us/step - loss: 0.0797 - accuracy: 0.9719\n",
      "Epoch 270/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9687\n",
      "Epoch 271/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9710\n",
      "Epoch 272/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9788\n",
      "Epoch 273/1500\n",
      "68/68 [==============================] - 0s 988us/step - loss: 0.0836 - accuracy: 0.9710\n",
      "Epoch 274/1500\n",
      "68/68 [==============================] - 0s 982us/step - loss: 0.0852 - accuracy: 0.9677\n",
      "Epoch 275/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9659\n",
      "Epoch 276/1500\n",
      "68/68 [==============================] - 0s 990us/step - loss: 0.0714 - accuracy: 0.9705\n",
      "Epoch 277/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9714\n",
      "Epoch 278/1500\n",
      "68/68 [==============================] - 0s 977us/step - loss: 0.0663 - accuracy: 0.9760\n",
      "Epoch 279/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0627 - accuracy: 0.9811\n",
      "Epoch 280/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9728\n",
      "Epoch 281/1500\n",
      "68/68 [==============================] - 0s 994us/step - loss: 0.0696 - accuracy: 0.9728\n",
      "Epoch 282/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9751\n",
      "Epoch 283/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9682\n",
      "Epoch 284/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9682\n",
      "Epoch 285/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0624 - accuracy: 0.9770\n",
      "Epoch 286/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0586 - accuracy: 0.9788\n",
      "Epoch 287/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9747\n",
      "Epoch 288/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.9793\n",
      "Epoch 289/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.9728\n",
      "Epoch 290/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9728\n",
      "Epoch 291/1500\n",
      "68/68 [==============================] - 0s 984us/step - loss: 0.0764 - accuracy: 0.9728\n",
      "Epoch 292/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9724\n",
      "Epoch 293/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.9783\n",
      "Epoch 294/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.9779\n",
      "Epoch 295/1500\n",
      "45/68 [==================>...........] - ETA: 0s - loss: 0.0670 - accuracy: 0.9764Restoring model weights from the end of the best epoch: 265.\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.9774\n",
      "Epoch 295: early stopping\n",
      "6/6 [==============================] - 0s 877us/step - loss: 0.8963 - accuracy: 0.7471\n",
      "6/6 [==============================] - 0s 617us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.69 (18/26)\n",
      "Before appending - Cat IDs: 192, Predictions: 192, Actuals: 192, Gender: 192\n",
      "After appending - Cat IDs: 362, Predictions: 362, Actuals: 362, Gender: 362\n",
      "Final Test Results - Loss: 0.8963047862052917, Accuracy: 0.7470588088035583, Precision: 0.5642174432497012, Recall: 0.6391391088809625, F1 Score: 0.5908710040839319\n",
      "Confusion Matrix:\n",
      " [[110   8  21]\n",
      " [  4  10   0]\n",
      " [ 10   0   7]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "057A    27\n",
      "        ..\n",
      "096A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 90, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "047A    28\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "106A    14\n",
      "051A    12\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "040A    10\n",
      "016A    10\n",
      "033A     9\n",
      "008A     6\n",
      "021A     5\n",
      "105A     4\n",
      "058A     3\n",
      "093A     2\n",
      "069A     2\n",
      "025B     2\n",
      "076A     1\n",
      "073A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    278\n",
      "M    252\n",
      "F    207\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    85\n",
      "X    70\n",
      "F    45\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 015A, 001A, 103A, 071A, 097B, 028...\n",
      "kitten    [044A, 014B, 111A, 046A, 042A, 109A, 050A, 043...\n",
      "senior    [097A, 057A, 104A, 055A, 059A, 113A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 067A, 029A, 039A, 063A, 069A, 000B, 076...\n",
      "kitten                                   [040A, 047A, 110A]\n",
      "senior                 [093A, 106A, 116A, 051A, 058A, 016A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 61, 'kitten': 13, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 13, 'kitten': 3, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B'\n",
      " '020A' '022A' '023A' '023B' '024A' '025A' '025C' '026A' '026B' '026C'\n",
      " '027A' '028A' '031A' '032A' '034A' '035A' '036A' '037A' '038A' '041A'\n",
      " '042A' '043A' '044A' '045A' '046A' '048A' '049A' '050A' '051B' '052A'\n",
      " '053A' '054A' '055A' '056A' '057A' '059A' '060A' '061A' '062A' '064A'\n",
      " '065A' '066A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '091A' '092A' '094A' '095A' '096A' '097A' '097B' '099A' '100A'\n",
      " '101A' '102A' '103A' '104A' '108A' '109A' '111A' '113A' '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '008A' '016A' '021A' '025B' '029A' '033A' '039A' '040A' '047A'\n",
      " '051A' '058A' '063A' '067A' '069A' '073A' '076A' '093A' '105A' '106A'\n",
      " '110A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B'\n",
      " '020A' '022A' '023A' '023B' '024A' '025A' '025C' '026A' '026B' '026C'\n",
      " '027A' '028A' '031A' '032A' '034A' '035A' '036A' '037A' '038A' '041A'\n",
      " '042A' '043A' '044A' '045A' '046A' '048A' '049A' '050A' '051B' '052A'\n",
      " '053A' '054A' '055A' '056A' '057A' '059A' '060A' '061A' '062A' '064A'\n",
      " '065A' '066A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '091A' '092A' '094A' '095A' '096A' '097A' '097B' '099A' '100A'\n",
      " '101A' '102A' '103A' '104A' '108A' '109A' '111A' '113A' '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '008A' '016A' '021A' '025B' '029A' '033A' '039A' '040A' '047A'\n",
      " '051A' '058A' '063A' '067A' '069A' '073A' '076A' '093A' '105A' '106A'\n",
      " '110A' '116A']\n",
      "Length of X_train_val:\n",
      "737\n",
      "Length of y_train_val:\n",
      "737\n",
      "Length of groups_train_val:\n",
      "737\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     480\n",
      "kitten    132\n",
      "senior    125\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     108\n",
      "senior     53\n",
      "kitten     39\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     480\n",
      "kitten    132\n",
      "senior    125\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     108\n",
      "senior     53\n",
      "kitten     39\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 960, 1: 528, 2: 500})\n",
      "Epoch 1/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.8432 - accuracy: 0.5976\n",
      "Epoch 2/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.6477 - accuracy: 0.6831\n",
      "Epoch 3/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5945 - accuracy: 0.7138\n",
      "Epoch 4/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5789 - accuracy: 0.7299\n",
      "Epoch 5/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5130 - accuracy: 0.7656\n",
      "Epoch 6/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5184 - accuracy: 0.7676\n",
      "Epoch 7/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.5155 - accuracy: 0.7621\n",
      "Epoch 8/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4732 - accuracy: 0.7741\n",
      "Epoch 9/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4600 - accuracy: 0.7847\n",
      "Epoch 10/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4621 - accuracy: 0.7827\n",
      "Epoch 11/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4684 - accuracy: 0.7857\n",
      "Epoch 12/1500\n",
      "63/63 [==============================] - 0s 998us/step - loss: 0.4345 - accuracy: 0.7973\n",
      "Epoch 13/1500\n",
      "63/63 [==============================] - 0s 955us/step - loss: 0.3964 - accuracy: 0.8174\n",
      "Epoch 14/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4276 - accuracy: 0.8018\n",
      "Epoch 15/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4255 - accuracy: 0.8038\n",
      "Epoch 16/1500\n",
      "63/63 [==============================] - 0s 984us/step - loss: 0.4100 - accuracy: 0.8204\n",
      "Epoch 17/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3754 - accuracy: 0.8285\n",
      "Epoch 18/1500\n",
      "63/63 [==============================] - 0s 982us/step - loss: 0.3741 - accuracy: 0.8290\n",
      "Epoch 19/1500\n",
      "63/63 [==============================] - 0s 968us/step - loss: 0.3591 - accuracy: 0.8385\n",
      "Epoch 20/1500\n",
      "63/63 [==============================] - 0s 968us/step - loss: 0.3505 - accuracy: 0.8380\n",
      "Epoch 21/1500\n",
      "63/63 [==============================] - 0s 966us/step - loss: 0.3575 - accuracy: 0.8496\n",
      "Epoch 22/1500\n",
      "63/63 [==============================] - 0s 978us/step - loss: 0.3633 - accuracy: 0.8350\n",
      "Epoch 23/1500\n",
      "63/63 [==============================] - 0s 996us/step - loss: 0.3575 - accuracy: 0.8395\n",
      "Epoch 24/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3475 - accuracy: 0.8410\n",
      "Epoch 25/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3627 - accuracy: 0.8365\n",
      "Epoch 26/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3530 - accuracy: 0.8345\n",
      "Epoch 27/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3291 - accuracy: 0.8531\n",
      "Epoch 28/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3310 - accuracy: 0.8511\n",
      "Epoch 29/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3503 - accuracy: 0.8350\n",
      "Epoch 30/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3299 - accuracy: 0.8481\n",
      "Epoch 31/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3331 - accuracy: 0.8556\n",
      "Epoch 32/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3091 - accuracy: 0.8571\n",
      "Epoch 33/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3164 - accuracy: 0.8612\n",
      "Epoch 34/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3048 - accuracy: 0.8652\n",
      "Epoch 35/1500\n",
      "63/63 [==============================] - 0s 951us/step - loss: 0.3163 - accuracy: 0.8516\n",
      "Epoch 36/1500\n",
      "63/63 [==============================] - 0s 978us/step - loss: 0.2981 - accuracy: 0.8647\n",
      "Epoch 37/1500\n",
      "63/63 [==============================] - 0s 998us/step - loss: 0.3046 - accuracy: 0.8612\n",
      "Epoch 38/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3010 - accuracy: 0.8662\n",
      "Epoch 39/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2938 - accuracy: 0.8627\n",
      "Epoch 40/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3072 - accuracy: 0.8677\n",
      "Epoch 41/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3086 - accuracy: 0.8597\n",
      "Epoch 42/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2810 - accuracy: 0.8753\n",
      "Epoch 43/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2925 - accuracy: 0.8697\n",
      "Epoch 44/1500\n",
      "63/63 [==============================] - 0s 986us/step - loss: 0.2934 - accuracy: 0.8692\n",
      "Epoch 45/1500\n",
      "63/63 [==============================] - 0s 989us/step - loss: 0.2927 - accuracy: 0.8697\n",
      "Epoch 46/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2885 - accuracy: 0.8697\n",
      "Epoch 47/1500\n",
      "63/63 [==============================] - 0s 995us/step - loss: 0.2710 - accuracy: 0.8813\n",
      "Epoch 48/1500\n",
      "63/63 [==============================] - 0s 949us/step - loss: 0.2822 - accuracy: 0.8753\n",
      "Epoch 49/1500\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.2804 - accuracy: 0.8707\n",
      "Epoch 50/1500\n",
      "63/63 [==============================] - 0s 980us/step - loss: 0.2742 - accuracy: 0.8687\n",
      "Epoch 51/1500\n",
      "63/63 [==============================] - 0s 979us/step - loss: 0.2701 - accuracy: 0.8823\n",
      "Epoch 52/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2558 - accuracy: 0.8929\n",
      "Epoch 53/1500\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.2688 - accuracy: 0.8722\n",
      "Epoch 54/1500\n",
      "63/63 [==============================] - 0s 998us/step - loss: 0.2726 - accuracy: 0.8818\n",
      "Epoch 55/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2583 - accuracy: 0.8959\n",
      "Epoch 56/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.8929\n",
      "Epoch 57/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2533 - accuracy: 0.8913\n",
      "Epoch 58/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2398 - accuracy: 0.8929\n",
      "Epoch 59/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2478 - accuracy: 0.8913\n",
      "Epoch 60/1500\n",
      "63/63 [==============================] - 0s 976us/step - loss: 0.2374 - accuracy: 0.8873\n",
      "Epoch 61/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.8939\n",
      "Epoch 62/1500\n",
      "63/63 [==============================] - 0s 986us/step - loss: 0.2369 - accuracy: 0.9090\n",
      "Epoch 63/1500\n",
      "63/63 [==============================] - 0s 970us/step - loss: 0.2472 - accuracy: 0.8929\n",
      "Epoch 64/1500\n",
      "63/63 [==============================] - 0s 956us/step - loss: 0.2472 - accuracy: 0.8929\n",
      "Epoch 65/1500\n",
      "63/63 [==============================] - 0s 968us/step - loss: 0.2339 - accuracy: 0.8944\n",
      "Epoch 66/1500\n",
      "63/63 [==============================] - 0s 970us/step - loss: 0.2308 - accuracy: 0.8964\n",
      "Epoch 67/1500\n",
      "63/63 [==============================] - 0s 990us/step - loss: 0.2501 - accuracy: 0.8853\n",
      "Epoch 68/1500\n",
      "63/63 [==============================] - 0s 968us/step - loss: 0.2477 - accuracy: 0.8898\n",
      "Epoch 69/1500\n",
      "63/63 [==============================] - 0s 987us/step - loss: 0.2266 - accuracy: 0.8974\n",
      "Epoch 70/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2275 - accuracy: 0.9029\n",
      "Epoch 71/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2437 - accuracy: 0.8999\n",
      "Epoch 72/1500\n",
      "63/63 [==============================] - 0s 995us/step - loss: 0.2252 - accuracy: 0.9090\n",
      "Epoch 73/1500\n",
      "63/63 [==============================] - 0s 986us/step - loss: 0.2163 - accuracy: 0.9004\n",
      "Epoch 74/1500\n",
      "63/63 [==============================] - 0s 988us/step - loss: 0.2175 - accuracy: 0.9039\n",
      "Epoch 75/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9100\n",
      "Epoch 76/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9100\n",
      "Epoch 77/1500\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.2074 - accuracy: 0.9085\n",
      "Epoch 78/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2269 - accuracy: 0.8964\n",
      "Epoch 79/1500\n",
      "63/63 [==============================] - 0s 971us/step - loss: 0.2176 - accuracy: 0.8969\n",
      "Epoch 80/1500\n",
      "63/63 [==============================] - 0s 990us/step - loss: 0.2024 - accuracy: 0.9120\n",
      "Epoch 81/1500\n",
      "63/63 [==============================] - 0s 964us/step - loss: 0.2068 - accuracy: 0.9120\n",
      "Epoch 82/1500\n",
      "63/63 [==============================] - 0s 987us/step - loss: 0.2148 - accuracy: 0.9054\n",
      "Epoch 83/1500\n",
      "63/63 [==============================] - 0s 982us/step - loss: 0.2073 - accuracy: 0.9090\n",
      "Epoch 84/1500\n",
      "63/63 [==============================] - 0s 936us/step - loss: 0.2155 - accuracy: 0.9115\n",
      "Epoch 85/1500\n",
      "63/63 [==============================] - 0s 947us/step - loss: 0.2080 - accuracy: 0.9105\n",
      "Epoch 86/1500\n",
      "63/63 [==============================] - 0s 949us/step - loss: 0.2064 - accuracy: 0.9115\n",
      "Epoch 87/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9155\n",
      "Epoch 88/1500\n",
      "63/63 [==============================] - 0s 982us/step - loss: 0.1832 - accuracy: 0.9160\n",
      "Epoch 89/1500\n",
      "63/63 [==============================] - 0s 946us/step - loss: 0.2022 - accuracy: 0.9135\n",
      "Epoch 90/1500\n",
      "63/63 [==============================] - 0s 945us/step - loss: 0.1895 - accuracy: 0.9170\n",
      "Epoch 91/1500\n",
      "63/63 [==============================] - 0s 986us/step - loss: 0.1845 - accuracy: 0.9175\n",
      "Epoch 92/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9160\n",
      "Epoch 93/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9155\n",
      "Epoch 94/1500\n",
      "63/63 [==============================] - 0s 973us/step - loss: 0.1806 - accuracy: 0.9205\n",
      "Epoch 95/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9230\n",
      "Epoch 96/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9135\n",
      "Epoch 97/1500\n",
      "63/63 [==============================] - 0s 996us/step - loss: 0.1956 - accuracy: 0.9074\n",
      "Epoch 98/1500\n",
      "63/63 [==============================] - 0s 992us/step - loss: 0.1990 - accuracy: 0.9190\n",
      "Epoch 99/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9155\n",
      "Epoch 100/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1968 - accuracy: 0.9155\n",
      "Epoch 101/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9170\n",
      "Epoch 102/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9125\n",
      "Epoch 103/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9220\n",
      "Epoch 104/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2142 - accuracy: 0.9039\n",
      "Epoch 105/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1944 - accuracy: 0.9200\n",
      "Epoch 106/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1910 - accuracy: 0.9175\n",
      "Epoch 107/1500\n",
      "63/63 [==============================] - 0s 961us/step - loss: 0.1805 - accuracy: 0.9195\n",
      "Epoch 108/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9240\n",
      "Epoch 109/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9205\n",
      "Epoch 110/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9210\n",
      "Epoch 111/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9170\n",
      "Epoch 112/1500\n",
      "63/63 [==============================] - 0s 963us/step - loss: 0.1680 - accuracy: 0.9301\n",
      "Epoch 113/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.9256\n",
      "Epoch 114/1500\n",
      "63/63 [==============================] - 0s 993us/step - loss: 0.1759 - accuracy: 0.9291\n",
      "Epoch 115/1500\n",
      "63/63 [==============================] - 0s 971us/step - loss: 0.1776 - accuracy: 0.9145\n",
      "Epoch 116/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.9175\n",
      "Epoch 117/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9230\n",
      "Epoch 118/1500\n",
      "63/63 [==============================] - 0s 991us/step - loss: 0.1776 - accuracy: 0.9296\n",
      "Epoch 119/1500\n",
      "63/63 [==============================] - 0s 974us/step - loss: 0.1725 - accuracy: 0.9296\n",
      "Epoch 120/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9326\n",
      "Epoch 121/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.9230\n",
      "Epoch 122/1500\n",
      "63/63 [==============================] - 0s 986us/step - loss: 0.1582 - accuracy: 0.9321\n",
      "Epoch 123/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9266\n",
      "Epoch 124/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1795 - accuracy: 0.9185\n",
      "Epoch 125/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9240\n",
      "Epoch 126/1500\n",
      "63/63 [==============================] - 0s 995us/step - loss: 0.1819 - accuracy: 0.9230\n",
      "Epoch 127/1500\n",
      "63/63 [==============================] - 0s 994us/step - loss: 0.1556 - accuracy: 0.9326\n",
      "Epoch 128/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9245\n",
      "Epoch 129/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1635 - accuracy: 0.9316\n",
      "Epoch 130/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9321\n",
      "Epoch 131/1500\n",
      "63/63 [==============================] - 0s 965us/step - loss: 0.1532 - accuracy: 0.9381\n",
      "Epoch 132/1500\n",
      "63/63 [==============================] - 0s 983us/step - loss: 0.1708 - accuracy: 0.9321\n",
      "Epoch 133/1500\n",
      "63/63 [==============================] - 0s 976us/step - loss: 0.1553 - accuracy: 0.9306\n",
      "Epoch 134/1500\n",
      "63/63 [==============================] - 0s 972us/step - loss: 0.1427 - accuracy: 0.9386\n",
      "Epoch 135/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9366\n",
      "Epoch 136/1500\n",
      "63/63 [==============================] - 0s 908us/step - loss: 0.1677 - accuracy: 0.9291\n",
      "Epoch 137/1500\n",
      "63/63 [==============================] - 0s 968us/step - loss: 0.1570 - accuracy: 0.9276\n",
      "Epoch 138/1500\n",
      "63/63 [==============================] - 0s 947us/step - loss: 0.1424 - accuracy: 0.9406\n",
      "Epoch 139/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1468 - accuracy: 0.9401\n",
      "Epoch 140/1500\n",
      "63/63 [==============================] - 0s 989us/step - loss: 0.1663 - accuracy: 0.9351\n",
      "Epoch 141/1500\n",
      "63/63 [==============================] - 0s 952us/step - loss: 0.1585 - accuracy: 0.9321\n",
      "Epoch 142/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1618 - accuracy: 0.9331\n",
      "Epoch 143/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9351\n",
      "Epoch 144/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9215\n",
      "Epoch 145/1500\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.1523 - accuracy: 0.9361\n",
      "Epoch 146/1500\n",
      "63/63 [==============================] - 0s 962us/step - loss: 0.1504 - accuracy: 0.9406\n",
      "Epoch 147/1500\n",
      "63/63 [==============================] - 0s 978us/step - loss: 0.1766 - accuracy: 0.9251\n",
      "Epoch 148/1500\n",
      "63/63 [==============================] - 0s 972us/step - loss: 0.1689 - accuracy: 0.9306\n",
      "Epoch 149/1500\n",
      "63/63 [==============================] - 0s 999us/step - loss: 0.1538 - accuracy: 0.9341\n",
      "Epoch 150/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9311\n",
      "Epoch 151/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9301\n",
      "Epoch 152/1500\n",
      "63/63 [==============================] - 0s 940us/step - loss: 0.1452 - accuracy: 0.9462\n",
      "Epoch 153/1500\n",
      "63/63 [==============================] - 0s 967us/step - loss: 0.1408 - accuracy: 0.9321\n",
      "Epoch 154/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9422\n",
      "Epoch 155/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1566 - accuracy: 0.9326\n",
      "Epoch 156/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9316\n",
      "Epoch 157/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1475 - accuracy: 0.9361\n",
      "Epoch 158/1500\n",
      "63/63 [==============================] - 0s 966us/step - loss: 0.1491 - accuracy: 0.9336\n",
      "Epoch 159/1500\n",
      "63/63 [==============================] - 0s 980us/step - loss: 0.1545 - accuracy: 0.9366\n",
      "Epoch 160/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9457\n",
      "Epoch 161/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9411\n",
      "Epoch 162/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9406\n",
      "Epoch 163/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9396\n",
      "Epoch 164/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9416\n",
      "Epoch 165/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9442\n",
      "Epoch 166/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9396\n",
      "Epoch 167/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9537\n",
      "Epoch 168/1500\n",
      "63/63 [==============================] - 0s 962us/step - loss: 0.1296 - accuracy: 0.9432\n",
      "Epoch 169/1500\n",
      "63/63 [==============================] - 0s 995us/step - loss: 0.1416 - accuracy: 0.9427\n",
      "Epoch 170/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9492\n",
      "Epoch 171/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9422\n",
      "Epoch 172/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.9346\n",
      "Epoch 173/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9472\n",
      "Epoch 174/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9487\n",
      "Epoch 175/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9502\n",
      "Epoch 176/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9502\n",
      "Epoch 177/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9416\n",
      "Epoch 178/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9467\n",
      "Epoch 179/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9472\n",
      "Epoch 180/1500\n",
      "63/63 [==============================] - 0s 994us/step - loss: 0.1243 - accuracy: 0.9487\n",
      "Epoch 181/1500\n",
      "63/63 [==============================] - 0s 969us/step - loss: 0.1324 - accuracy: 0.9452\n",
      "Epoch 182/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9482\n",
      "Epoch 183/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9567\n",
      "Epoch 184/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.9422\n",
      "Epoch 185/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9502\n",
      "Epoch 186/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9532\n",
      "Epoch 187/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9512\n",
      "Epoch 188/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9487\n",
      "Epoch 189/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9472\n",
      "Epoch 190/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9557\n",
      "Epoch 191/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9487\n",
      "Epoch 192/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9467\n",
      "Epoch 193/1500\n",
      "63/63 [==============================] - 0s 999us/step - loss: 0.1139 - accuracy: 0.9542\n",
      "Epoch 194/1500\n",
      "63/63 [==============================] - 0s 988us/step - loss: 0.1196 - accuracy: 0.9487\n",
      "Epoch 195/1500\n",
      "63/63 [==============================] - 0s 959us/step - loss: 0.1199 - accuracy: 0.9472\n",
      "Epoch 196/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.9472\n",
      "Epoch 197/1500\n",
      "63/63 [==============================] - 0s 971us/step - loss: 0.1089 - accuracy: 0.9522\n",
      "Epoch 198/1500\n",
      "63/63 [==============================] - 0s 951us/step - loss: 0.1326 - accuracy: 0.9452\n",
      "Epoch 199/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9517\n",
      "Epoch 200/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9547\n",
      "Epoch 201/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9527\n",
      "Epoch 202/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9427\n",
      "Epoch 203/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9512\n",
      "Epoch 204/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9406\n",
      "Epoch 205/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9582\n",
      "Epoch 206/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9552\n",
      "Epoch 207/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9517\n",
      "Epoch 208/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9532\n",
      "Epoch 209/1500\n",
      "63/63 [==============================] - 0s 984us/step - loss: 0.1216 - accuracy: 0.9507\n",
      "Epoch 210/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9557\n",
      "Epoch 211/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.9532\n",
      "Epoch 212/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9522\n",
      "Epoch 213/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9497\n",
      "Epoch 214/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9598\n",
      "Epoch 215/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9613\n",
      "Epoch 216/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9497\n",
      "Epoch 217/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9517\n",
      "Epoch 218/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9532\n",
      "Epoch 219/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9572\n",
      "Epoch 220/1500\n",
      "63/63 [==============================] - 0s 960us/step - loss: 0.1029 - accuracy: 0.9552\n",
      "Epoch 221/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9577\n",
      "Epoch 222/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9512\n",
      "Epoch 223/1500\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.0965 - accuracy: 0.9588\n",
      "Epoch 224/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9603\n",
      "Epoch 225/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9618\n",
      "Epoch 226/1500\n",
      "63/63 [==============================] - 0s 994us/step - loss: 0.1042 - accuracy: 0.9582\n",
      "Epoch 227/1500\n",
      "63/63 [==============================] - 0s 977us/step - loss: 0.0992 - accuracy: 0.9588\n",
      "Epoch 228/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9588\n",
      "Epoch 229/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9577\n",
      "Epoch 230/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9613\n",
      "Epoch 231/1500\n",
      "63/63 [==============================] - 0s 986us/step - loss: 0.1051 - accuracy: 0.9577\n",
      "Epoch 232/1500\n",
      "63/63 [==============================] - 0s 988us/step - loss: 0.1068 - accuracy: 0.9557\n",
      "Epoch 233/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.9623\n",
      "Epoch 234/1500\n",
      "63/63 [==============================] - 0s 980us/step - loss: 0.1065 - accuracy: 0.9537\n",
      "Epoch 235/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9552\n",
      "Epoch 236/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9552\n",
      "Epoch 237/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9482\n",
      "Epoch 238/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9542\n",
      "Epoch 239/1500\n",
      "63/63 [==============================] - 0s 985us/step - loss: 0.0928 - accuracy: 0.9628\n",
      "Epoch 240/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9593\n",
      "Epoch 241/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9542\n",
      "Epoch 242/1500\n",
      "63/63 [==============================] - 0s 985us/step - loss: 0.0867 - accuracy: 0.9663\n",
      "Epoch 243/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0887 - accuracy: 0.9648\n",
      "Epoch 244/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.9603\n",
      "Epoch 245/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9618\n",
      "Epoch 246/1500\n",
      "63/63 [==============================] - 0s 992us/step - loss: 0.1020 - accuracy: 0.9588\n",
      "Epoch 247/1500\n",
      "63/63 [==============================] - 0s 982us/step - loss: 0.0923 - accuracy: 0.9618\n",
      "Epoch 248/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9593\n",
      "Epoch 249/1500\n",
      "63/63 [==============================] - 0s 991us/step - loss: 0.0899 - accuracy: 0.9643\n",
      "Epoch 250/1500\n",
      "63/63 [==============================] - 0s 967us/step - loss: 0.0927 - accuracy: 0.9638\n",
      "Epoch 251/1500\n",
      "63/63 [==============================] - 0s 983us/step - loss: 0.0913 - accuracy: 0.9683\n",
      "Epoch 252/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9603\n",
      "Epoch 253/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9638\n",
      "Epoch 254/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9572\n",
      "Epoch 255/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9613\n",
      "Epoch 256/1500\n",
      "63/63 [==============================] - 0s 971us/step - loss: 0.1023 - accuracy: 0.9547\n",
      "Epoch 257/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.9588\n",
      "Epoch 258/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9613\n",
      "Epoch 259/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9678\n",
      "Epoch 260/1500\n",
      "63/63 [==============================] - 0s 982us/step - loss: 0.1036 - accuracy: 0.9598\n",
      "Epoch 261/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9668\n",
      "Epoch 262/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9598\n",
      "Epoch 263/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9567\n",
      "Epoch 264/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9623\n",
      "Epoch 265/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.9638\n",
      "Epoch 266/1500\n",
      "63/63 [==============================] - 0s 953us/step - loss: 0.0904 - accuracy: 0.9618\n",
      "Epoch 267/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.9688\n",
      "Epoch 268/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9572\n",
      "Epoch 269/1500\n",
      "63/63 [==============================] - 0s 966us/step - loss: 0.1011 - accuracy: 0.9588\n",
      "Epoch 270/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.9653\n",
      "Epoch 271/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.9713\n",
      "Epoch 272/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9633\n",
      "Epoch 273/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9673\n",
      "Epoch 274/1500\n",
      "63/63 [==============================] - 0s 979us/step - loss: 0.0889 - accuracy: 0.9603\n",
      "Epoch 275/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.9653\n",
      "Epoch 276/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9628\n",
      "Epoch 277/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9593\n",
      "Epoch 278/1500\n",
      "63/63 [==============================] - 0s 985us/step - loss: 0.0869 - accuracy: 0.9678\n",
      "Epoch 279/1500\n",
      "63/63 [==============================] - 0s 992us/step - loss: 0.1025 - accuracy: 0.9552\n",
      "Epoch 280/1500\n",
      "63/63 [==============================] - 0s 997us/step - loss: 0.1032 - accuracy: 0.9653\n",
      "Epoch 281/1500\n",
      "63/63 [==============================] - 0s 982us/step - loss: 0.0872 - accuracy: 0.9663\n",
      "Epoch 282/1500\n",
      "63/63 [==============================] - 0s 987us/step - loss: 0.0854 - accuracy: 0.9648\n",
      "Epoch 283/1500\n",
      "63/63 [==============================] - 0s 999us/step - loss: 0.0929 - accuracy: 0.9663\n",
      "Epoch 284/1500\n",
      "63/63 [==============================] - 0s 990us/step - loss: 0.0877 - accuracy: 0.9683\n",
      "Epoch 285/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9628\n",
      "Epoch 286/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9713\n",
      "Epoch 287/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.9668\n",
      "Epoch 288/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.9764\n",
      "Epoch 289/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.9733\n",
      "Epoch 290/1500\n",
      "63/63 [==============================] - 0s 993us/step - loss: 0.0767 - accuracy: 0.9693\n",
      "Epoch 291/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9698\n",
      "Epoch 292/1500\n",
      "63/63 [==============================] - 0s 973us/step - loss: 0.0897 - accuracy: 0.9673\n",
      "Epoch 293/1500\n",
      "63/63 [==============================] - 0s 967us/step - loss: 0.0733 - accuracy: 0.9723\n",
      "Epoch 294/1500\n",
      "63/63 [==============================] - 0s 974us/step - loss: 0.0858 - accuracy: 0.9658\n",
      "Epoch 295/1500\n",
      "63/63 [==============================] - 0s 977us/step - loss: 0.0871 - accuracy: 0.9683\n",
      "Epoch 296/1500\n",
      "63/63 [==============================] - 0s 982us/step - loss: 0.0863 - accuracy: 0.9698\n",
      "Epoch 297/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9728\n",
      "Epoch 298/1500\n",
      "63/63 [==============================] - 0s 980us/step - loss: 0.0792 - accuracy: 0.9688\n",
      "Epoch 299/1500\n",
      "63/63 [==============================] - 0s 993us/step - loss: 0.0814 - accuracy: 0.9678\n",
      "Epoch 300/1500\n",
      "63/63 [==============================] - 0s 955us/step - loss: 0.0907 - accuracy: 0.9628\n",
      "Epoch 301/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9628\n",
      "Epoch 302/1500\n",
      "63/63 [==============================] - 0s 988us/step - loss: 0.0863 - accuracy: 0.9653\n",
      "Epoch 303/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.9733\n",
      "Epoch 304/1500\n",
      "63/63 [==============================] - 0s 987us/step - loss: 0.0736 - accuracy: 0.9693\n",
      "Epoch 305/1500\n",
      "63/63 [==============================] - 0s 966us/step - loss: 0.0721 - accuracy: 0.9728\n",
      "Epoch 306/1500\n",
      "63/63 [==============================] - 0s 994us/step - loss: 0.0802 - accuracy: 0.9703\n",
      "Epoch 307/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9633\n",
      "Epoch 308/1500\n",
      "63/63 [==============================] - 0s 974us/step - loss: 0.0702 - accuracy: 0.9769\n",
      "Epoch 309/1500\n",
      "63/63 [==============================] - 0s 987us/step - loss: 0.0733 - accuracy: 0.9703\n",
      "Epoch 310/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.9703\n",
      "Epoch 311/1500\n",
      "63/63 [==============================] - 0s 971us/step - loss: 0.0768 - accuracy: 0.9713\n",
      "Epoch 312/1500\n",
      "63/63 [==============================] - 0s 999us/step - loss: 0.0819 - accuracy: 0.9658\n",
      "Epoch 313/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9618\n",
      "Epoch 314/1500\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9733\n",
      "Epoch 315/1500\n",
      "63/63 [==============================] - 0s 981us/step - loss: 0.0825 - accuracy: 0.9663\n",
      "Epoch 316/1500\n",
      "63/63 [==============================] - 0s 971us/step - loss: 0.0940 - accuracy: 0.9633\n",
      "Epoch 317/1500\n",
      "63/63 [==============================] - 0s 972us/step - loss: 0.0734 - accuracy: 0.9743\n",
      "Epoch 318/1500\n",
      "54/63 [========================>.....] - ETA: 0s - loss: 0.0757 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 288.\n",
      "63/63 [==============================] - 0s 994us/step - loss: 0.0762 - accuracy: 0.9678\n",
      "Epoch 318: early stopping\n",
      "7/7 [==============================] - 0s 762us/step - loss: 1.1968 - accuracy: 0.7550\n",
      "7/7 [==============================] - 0s 640us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.73 (16/22)\n",
      "Before appending - Cat IDs: 362, Predictions: 362, Actuals: 362, Gender: 362\n",
      "After appending - Cat IDs: 562, Predictions: 562, Actuals: 562, Gender: 562\n",
      "Final Test Results - Loss: 1.1967754364013672, Accuracy: 0.7549999952316284, Precision: 0.7708047426529677, Recall: 0.7298912361805444, F1 Score: 0.7388953244119567\n",
      "Confusion Matrix:\n",
      " [[94  2 12]\n",
      " [ 3 36  0]\n",
      " [32  0 21]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "        ..\n",
      "091A     1\n",
      "073A     1\n",
      "043A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 89, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "057A    27\n",
      "020A    23\n",
      "101A    15\n",
      "001A    14\n",
      "111A    13\n",
      "071A    10\n",
      "072A     9\n",
      "065A     9\n",
      "010A     8\n",
      "050A     7\n",
      "031A     7\n",
      "037A     6\n",
      "056A     3\n",
      "006A     3\n",
      "014A     3\n",
      "064A     3\n",
      "054A     2\n",
      "018A     2\n",
      "048A     1\n",
      "019B     1\n",
      "066A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    285\n",
      "X    266\n",
      "F    179\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    82\n",
      "F    73\n",
      "M    52\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 103A, 097B, 028A, 019A, 074A, 067...\n",
      "kitten    [044A, 014B, 040A, 046A, 047A, 042A, 109A, 043...\n",
      "senior    [093A, 097A, 106A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 071A, 020A, 101A, 072A, 065...\n",
      "kitten                                   [111A, 050A, 048A]\n",
      "senior                             [057A, 054A, 056A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 58, 'kitten': 13, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 16, 'kitten': 3, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A' '011A'\n",
      " '012A' '013B' '014B' '015A' '016A' '019A' '021A' '022A' '023A' '023B'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '032A'\n",
      " '033A' '034A' '035A' '036A' '038A' '039A' '040A' '041A' '042A' '043A'\n",
      " '044A' '045A' '046A' '047A' '049A' '051A' '051B' '052A' '053A' '055A'\n",
      " '058A' '059A' '060A' '061A' '062A' '063A' '067A' '068A' '069A' '070A'\n",
      " '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A' '092A' '093A'\n",
      " '094A' '095A' '096A' '097A' '097B' '099A' '100A' '102A' '103A' '104A'\n",
      " '105A' '106A' '108A' '109A' '110A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '001A' '006A' '010A' '014A' '018A' '019B' '020A' '024A' '031A'\n",
      " '037A' '048A' '050A' '054A' '056A' '057A' '064A' '065A' '066A' '071A'\n",
      " '072A' '101A' '111A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'069A'}\n",
      "Moved to Test Set:\n",
      "{'069A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A'\n",
      " '011A' '012A' '013B' '014B' '015A' '016A' '019A' '021A' '022A' '023A'\n",
      " '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A'\n",
      " '032A' '033A' '034A' '035A' '036A' '038A' '039A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '049A' '051A' '051B' '052A' '053A'\n",
      " '055A' '058A' '059A' '060A' '061A' '062A' '063A' '067A' '068A' '070A'\n",
      " '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A' '092A' '093A'\n",
      " '094A' '095A' '096A' '097A' '097B' '099A' '100A' '102A' '103A' '104A'\n",
      " '105A' '106A' '108A' '109A' '110A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '006A' '010A' '014A' '018A' '019B' '020A' '024A' '031A' '037A'\n",
      " '048A' '050A' '054A' '056A' '057A' '064A' '065A' '066A' '069A' '071A'\n",
      " '072A' '101A' '111A']\n",
      "Length of X_train_val:\n",
      "767\n",
      "Length of y_train_val:\n",
      "767\n",
      "Length of groups_train_val:\n",
      "767\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     435\n",
      "kitten    150\n",
      "senior    145\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     153\n",
      "senior     33\n",
      "kitten     21\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     472\n",
      "kitten    150\n",
      "senior    145\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     116\n",
      "senior     33\n",
      "kitten     21\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age group distribution: Counter({0: 944, 1: 600, 2: 580})\n",
      "Epoch 1/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.8954 - accuracy: 0.5890\n",
      "Epoch 2/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.7101 - accuracy: 0.6728\n",
      "Epoch 3/1500\n",
      "67/67 [==============================] - 0s 987us/step - loss: 0.6359 - accuracy: 0.7189\n",
      "Epoch 4/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.5968 - accuracy: 0.7208\n",
      "Epoch 5/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.5568 - accuracy: 0.7429\n",
      "Epoch 6/1500\n",
      "67/67 [==============================] - 0s 930us/step - loss: 0.5597 - accuracy: 0.7528\n",
      "Epoch 7/1500\n",
      "67/67 [==============================] - 0s 967us/step - loss: 0.5311 - accuracy: 0.7632\n",
      "Epoch 8/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.4898 - accuracy: 0.7726\n",
      "Epoch 9/1500\n",
      "67/67 [==============================] - 0s 945us/step - loss: 0.4913 - accuracy: 0.7792\n",
      "Epoch 10/1500\n",
      "67/67 [==============================] - 0s 951us/step - loss: 0.4726 - accuracy: 0.7966\n",
      "Epoch 11/1500\n",
      "67/67 [==============================] - 0s 952us/step - loss: 0.4536 - accuracy: 0.7980\n",
      "Epoch 12/1500\n",
      "67/67 [==============================] - 0s 942us/step - loss: 0.4583 - accuracy: 0.7971\n",
      "Epoch 13/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4441 - accuracy: 0.8023\n",
      "Epoch 14/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4166 - accuracy: 0.8098\n",
      "Epoch 15/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4186 - accuracy: 0.8187\n",
      "Epoch 16/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4077 - accuracy: 0.8169\n",
      "Epoch 17/1500\n",
      "67/67 [==============================] - 0s 991us/step - loss: 0.4070 - accuracy: 0.8192\n",
      "Epoch 18/1500\n",
      "67/67 [==============================] - 0s 970us/step - loss: 0.3885 - accuracy: 0.8282\n",
      "Epoch 19/1500\n",
      "67/67 [==============================] - 0s 964us/step - loss: 0.3948 - accuracy: 0.8291\n",
      "Epoch 20/1500\n",
      "67/67 [==============================] - 0s 970us/step - loss: 0.3763 - accuracy: 0.8371\n",
      "Epoch 21/1500\n",
      "67/67 [==============================] - 0s 946us/step - loss: 0.3701 - accuracy: 0.8333\n",
      "Epoch 22/1500\n",
      "67/67 [==============================] - 0s 970us/step - loss: 0.3811 - accuracy: 0.8366\n",
      "Epoch 23/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3730 - accuracy: 0.8395\n",
      "Epoch 24/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3432 - accuracy: 0.8522\n",
      "Epoch 25/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3619 - accuracy: 0.8427\n",
      "Epoch 26/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3434 - accuracy: 0.8526\n",
      "Epoch 27/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3341 - accuracy: 0.8653\n",
      "Epoch 28/1500\n",
      "67/67 [==============================] - 0s 977us/step - loss: 0.3571 - accuracy: 0.8465\n",
      "Epoch 29/1500\n",
      "67/67 [==============================] - 0s 961us/step - loss: 0.3529 - accuracy: 0.8380\n",
      "Epoch 30/1500\n",
      "67/67 [==============================] - 0s 949us/step - loss: 0.3469 - accuracy: 0.8512\n",
      "Epoch 31/1500\n",
      "67/67 [==============================] - 0s 951us/step - loss: 0.3083 - accuracy: 0.8639\n",
      "Epoch 32/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3115 - accuracy: 0.8682\n",
      "Epoch 33/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3293 - accuracy: 0.8573\n",
      "Epoch 34/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.8649\n",
      "Epoch 35/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3190 - accuracy: 0.8668\n",
      "Epoch 36/1500\n",
      "67/67 [==============================] - 0s 984us/step - loss: 0.3038 - accuracy: 0.8734\n",
      "Epoch 37/1500\n",
      "67/67 [==============================] - 0s 926us/step - loss: 0.3055 - accuracy: 0.8677\n",
      "Epoch 38/1500\n",
      "67/67 [==============================] - 0s 960us/step - loss: 0.2906 - accuracy: 0.8757\n",
      "Epoch 39/1500\n",
      "67/67 [==============================] - 0s 939us/step - loss: 0.2934 - accuracy: 0.8757\n",
      "Epoch 40/1500\n",
      "67/67 [==============================] - 0s 986us/step - loss: 0.2924 - accuracy: 0.8691\n",
      "Epoch 41/1500\n",
      "67/67 [==============================] - 0s 955us/step - loss: 0.2837 - accuracy: 0.8847\n",
      "Epoch 42/1500\n",
      "67/67 [==============================] - 0s 954us/step - loss: 0.2831 - accuracy: 0.8748\n",
      "Epoch 43/1500\n",
      "67/67 [==============================] - 0s 978us/step - loss: 0.2866 - accuracy: 0.8757\n",
      "Epoch 44/1500\n",
      "67/67 [==============================] - 0s 975us/step - loss: 0.2911 - accuracy: 0.8785\n",
      "Epoch 45/1500\n",
      "67/67 [==============================] - 0s 971us/step - loss: 0.2882 - accuracy: 0.8757\n",
      "Epoch 46/1500\n",
      "67/67 [==============================] - 0s 997us/step - loss: 0.2785 - accuracy: 0.8823\n",
      "Epoch 47/1500\n",
      "67/67 [==============================] - 0s 987us/step - loss: 0.2658 - accuracy: 0.8927\n",
      "Epoch 48/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2693 - accuracy: 0.8748\n",
      "Epoch 49/1500\n",
      "67/67 [==============================] - 0s 950us/step - loss: 0.2705 - accuracy: 0.8936\n",
      "Epoch 50/1500\n",
      "67/67 [==============================] - 0s 972us/step - loss: 0.2807 - accuracy: 0.8804\n",
      "Epoch 51/1500\n",
      "67/67 [==============================] - 0s 959us/step - loss: 0.2638 - accuracy: 0.8960\n",
      "Epoch 52/1500\n",
      "67/67 [==============================] - 0s 932us/step - loss: 0.2827 - accuracy: 0.8809\n",
      "Epoch 53/1500\n",
      "67/67 [==============================] - 0s 951us/step - loss: 0.2609 - accuracy: 0.8847\n",
      "Epoch 54/1500\n",
      "67/67 [==============================] - 0s 960us/step - loss: 0.2439 - accuracy: 0.8931\n",
      "Epoch 55/1500\n",
      "67/67 [==============================] - 0s 944us/step - loss: 0.2559 - accuracy: 0.8978\n",
      "Epoch 56/1500\n",
      "67/67 [==============================] - 0s 925us/step - loss: 0.2559 - accuracy: 0.8898\n",
      "Epoch 57/1500\n",
      "67/67 [==============================] - 0s 954us/step - loss: 0.2324 - accuracy: 0.9030\n",
      "Epoch 58/1500\n",
      "67/67 [==============================] - 0s 987us/step - loss: 0.2515 - accuracy: 0.8936\n",
      "Epoch 59/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.2509 - accuracy: 0.8997\n",
      "Epoch 60/1500\n",
      "67/67 [==============================] - 0s 948us/step - loss: 0.2424 - accuracy: 0.9002\n",
      "Epoch 61/1500\n",
      "67/67 [==============================] - 0s 947us/step - loss: 0.2395 - accuracy: 0.9011\n",
      "Epoch 62/1500\n",
      "67/67 [==============================] - 0s 967us/step - loss: 0.2330 - accuracy: 0.9035\n",
      "Epoch 63/1500\n",
      "67/67 [==============================] - 0s 944us/step - loss: 0.2305 - accuracy: 0.8983\n",
      "Epoch 64/1500\n",
      "67/67 [==============================] - 0s 983us/step - loss: 0.2207 - accuracy: 0.9049\n",
      "Epoch 65/1500\n",
      "67/67 [==============================] - 0s 966us/step - loss: 0.2275 - accuracy: 0.9138\n",
      "Epoch 66/1500\n",
      "67/67 [==============================] - 0s 981us/step - loss: 0.2274 - accuracy: 0.9011\n",
      "Epoch 67/1500\n",
      "67/67 [==============================] - 0s 967us/step - loss: 0.2227 - accuracy: 0.9063\n",
      "Epoch 68/1500\n",
      "67/67 [==============================] - 0s 953us/step - loss: 0.2170 - accuracy: 0.9134\n",
      "Epoch 69/1500\n",
      "67/67 [==============================] - 0s 960us/step - loss: 0.2311 - accuracy: 0.9025\n",
      "Epoch 70/1500\n",
      "67/67 [==============================] - 0s 955us/step - loss: 0.2141 - accuracy: 0.9073\n",
      "Epoch 71/1500\n",
      "67/67 [==============================] - 0s 967us/step - loss: 0.2186 - accuracy: 0.9148\n",
      "Epoch 72/1500\n",
      "67/67 [==============================] - 0s 960us/step - loss: 0.2242 - accuracy: 0.9138\n",
      "Epoch 73/1500\n",
      "67/67 [==============================] - 0s 944us/step - loss: 0.2205 - accuracy: 0.9096\n",
      "Epoch 74/1500\n",
      "67/67 [==============================] - 0s 955us/step - loss: 0.2223 - accuracy: 0.9054\n",
      "Epoch 75/1500\n",
      "67/67 [==============================] - 0s 936us/step - loss: 0.2158 - accuracy: 0.9148\n",
      "Epoch 76/1500\n",
      "67/67 [==============================] - 0s 925us/step - loss: 0.2249 - accuracy: 0.9101\n",
      "Epoch 77/1500\n",
      "67/67 [==============================] - 0s 951us/step - loss: 0.2042 - accuracy: 0.9185\n",
      "Epoch 78/1500\n",
      "67/67 [==============================] - 0s 935us/step - loss: 0.2229 - accuracy: 0.9068\n",
      "Epoch 79/1500\n",
      "67/67 [==============================] - 0s 939us/step - loss: 0.2119 - accuracy: 0.9167\n",
      "Epoch 80/1500\n",
      "67/67 [==============================] - 0s 976us/step - loss: 0.2209 - accuracy: 0.9091\n",
      "Epoch 81/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.1964 - accuracy: 0.9247\n",
      "Epoch 82/1500\n",
      "67/67 [==============================] - 0s 954us/step - loss: 0.1893 - accuracy: 0.9218\n",
      "Epoch 83/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.1896 - accuracy: 0.9251\n",
      "Epoch 84/1500\n",
      "67/67 [==============================] - 0s 945us/step - loss: 0.1968 - accuracy: 0.9195\n",
      "Epoch 85/1500\n",
      "67/67 [==============================] - 0s 962us/step - loss: 0.2065 - accuracy: 0.9157\n",
      "Epoch 86/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.1838 - accuracy: 0.9256\n",
      "Epoch 87/1500\n",
      "67/67 [==============================] - 0s 970us/step - loss: 0.2039 - accuracy: 0.9143\n",
      "Epoch 88/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.1867 - accuracy: 0.9247\n",
      "Epoch 89/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2072 - accuracy: 0.9176\n",
      "Epoch 90/1500\n",
      "67/67 [==============================] - 0s 960us/step - loss: 0.1918 - accuracy: 0.9233\n",
      "Epoch 91/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.1923 - accuracy: 0.9223\n",
      "Epoch 92/1500\n",
      "67/67 [==============================] - 0s 949us/step - loss: 0.1938 - accuracy: 0.9298\n",
      "Epoch 93/1500\n",
      "67/67 [==============================] - 0s 975us/step - loss: 0.1902 - accuracy: 0.9200\n",
      "Epoch 94/1500\n",
      "67/67 [==============================] - 0s 950us/step - loss: 0.1994 - accuracy: 0.9218\n",
      "Epoch 95/1500\n",
      "67/67 [==============================] - 0s 993us/step - loss: 0.1901 - accuracy: 0.9204\n",
      "Epoch 96/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1706 - accuracy: 0.9346\n",
      "Epoch 97/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.1878 - accuracy: 0.9209\n",
      "Epoch 98/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.1819 - accuracy: 0.9270\n",
      "Epoch 99/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1952 - accuracy: 0.9200\n",
      "Epoch 100/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.9256\n",
      "Epoch 101/1500\n",
      "67/67 [==============================] - 0s 983us/step - loss: 0.1828 - accuracy: 0.9237\n",
      "Epoch 102/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.9218\n",
      "Epoch 103/1500\n",
      "67/67 [==============================] - 0s 931us/step - loss: 0.1833 - accuracy: 0.9261\n",
      "Epoch 104/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9308\n",
      "Epoch 105/1500\n",
      "67/67 [==============================] - 0s 974us/step - loss: 0.1908 - accuracy: 0.9284\n",
      "Epoch 106/1500\n",
      "67/67 [==============================] - 0s 977us/step - loss: 0.1575 - accuracy: 0.9355\n",
      "Epoch 107/1500\n",
      "67/67 [==============================] - 0s 961us/step - loss: 0.1777 - accuracy: 0.9289\n",
      "Epoch 108/1500\n",
      "67/67 [==============================] - 0s 964us/step - loss: 0.1712 - accuracy: 0.9350\n",
      "Epoch 109/1500\n",
      "67/67 [==============================] - 0s 981us/step - loss: 0.1928 - accuracy: 0.9185\n",
      "Epoch 110/1500\n",
      "67/67 [==============================] - 0s 944us/step - loss: 0.1746 - accuracy: 0.9341\n",
      "Epoch 111/1500\n",
      "67/67 [==============================] - 0s 982us/step - loss: 0.1648 - accuracy: 0.9322\n",
      "Epoch 112/1500\n",
      "67/67 [==============================] - 0s 973us/step - loss: 0.1685 - accuracy: 0.9346\n",
      "Epoch 113/1500\n",
      "67/67 [==============================] - 0s 982us/step - loss: 0.1669 - accuracy: 0.9341\n",
      "Epoch 114/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.9280\n",
      "Epoch 115/1500\n",
      "67/67 [==============================] - 0s 992us/step - loss: 0.1724 - accuracy: 0.9303\n",
      "Epoch 116/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9355\n",
      "Epoch 117/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9364\n",
      "Epoch 118/1500\n",
      "67/67 [==============================] - 0s 980us/step - loss: 0.1554 - accuracy: 0.9383\n",
      "Epoch 119/1500\n",
      "67/67 [==============================] - 0s 948us/step - loss: 0.1657 - accuracy: 0.9388\n",
      "Epoch 120/1500\n",
      "67/67 [==============================] - 0s 998us/step - loss: 0.1604 - accuracy: 0.9393\n",
      "Epoch 121/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9355\n",
      "Epoch 122/1500\n",
      "67/67 [==============================] - 0s 978us/step - loss: 0.1591 - accuracy: 0.9364\n",
      "Epoch 123/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9280\n",
      "Epoch 124/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9360\n",
      "Epoch 125/1500\n",
      "67/67 [==============================] - 0s 977us/step - loss: 0.1437 - accuracy: 0.9416\n",
      "Epoch 126/1500\n",
      "67/67 [==============================] - 0s 997us/step - loss: 0.1558 - accuracy: 0.9379\n",
      "Epoch 127/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.9421\n",
      "Epoch 128/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.1561 - accuracy: 0.9374\n",
      "Epoch 129/1500\n",
      "67/67 [==============================] - 0s 993us/step - loss: 0.1569 - accuracy: 0.9435\n",
      "Epoch 130/1500\n",
      "67/67 [==============================] - 0s 949us/step - loss: 0.1709 - accuracy: 0.9294\n",
      "Epoch 131/1500\n",
      "67/67 [==============================] - 0s 973us/step - loss: 0.1639 - accuracy: 0.9355\n",
      "Epoch 132/1500\n",
      "67/67 [==============================] - 0s 974us/step - loss: 0.1634 - accuracy: 0.9416\n",
      "Epoch 133/1500\n",
      "67/67 [==============================] - 0s 948us/step - loss: 0.1546 - accuracy: 0.9383\n",
      "Epoch 134/1500\n",
      "67/67 [==============================] - 0s 960us/step - loss: 0.1546 - accuracy: 0.9388\n",
      "Epoch 135/1500\n",
      "67/67 [==============================] - 0s 950us/step - loss: 0.1499 - accuracy: 0.9463\n",
      "Epoch 136/1500\n",
      "67/67 [==============================] - 0s 992us/step - loss: 0.1400 - accuracy: 0.9463\n",
      "Epoch 137/1500\n",
      "67/67 [==============================] - 0s 952us/step - loss: 0.1411 - accuracy: 0.9440\n",
      "Epoch 138/1500\n",
      "67/67 [==============================] - 0s 961us/step - loss: 0.1418 - accuracy: 0.9421\n",
      "Epoch 139/1500\n",
      "67/67 [==============================] - 0s 984us/step - loss: 0.1430 - accuracy: 0.9440\n",
      "Epoch 140/1500\n",
      "67/67 [==============================] - 0s 983us/step - loss: 0.1387 - accuracy: 0.9444\n",
      "Epoch 141/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.1703 - accuracy: 0.9369\n",
      "Epoch 142/1500\n",
      "67/67 [==============================] - 0s 991us/step - loss: 0.1398 - accuracy: 0.9501\n",
      "Epoch 143/1500\n",
      "67/67 [==============================] - 0s 983us/step - loss: 0.1536 - accuracy: 0.9374\n",
      "Epoch 144/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9430\n",
      "Epoch 145/1500\n",
      "67/67 [==============================] - 0s 964us/step - loss: 0.1382 - accuracy: 0.9492\n",
      "Epoch 146/1500\n",
      "67/67 [==============================] - 0s 988us/step - loss: 0.1394 - accuracy: 0.9430\n",
      "Epoch 147/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9430\n",
      "Epoch 148/1500\n",
      "67/67 [==============================] - 0s 991us/step - loss: 0.1384 - accuracy: 0.9473\n",
      "Epoch 149/1500\n",
      "67/67 [==============================] - 0s 969us/step - loss: 0.1586 - accuracy: 0.9341\n",
      "Epoch 150/1500\n",
      "67/67 [==============================] - 0s 988us/step - loss: 0.1563 - accuracy: 0.9411\n",
      "Epoch 151/1500\n",
      "67/67 [==============================] - 0s 986us/step - loss: 0.1500 - accuracy: 0.9421\n",
      "Epoch 152/1500\n",
      "67/67 [==============================] - 0s 967us/step - loss: 0.1325 - accuracy: 0.9506\n",
      "Epoch 153/1500\n",
      "67/67 [==============================] - 0s 988us/step - loss: 0.1251 - accuracy: 0.9487\n",
      "Epoch 154/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9487\n",
      "Epoch 155/1500\n",
      "67/67 [==============================] - 0s 987us/step - loss: 0.1257 - accuracy: 0.9520\n",
      "Epoch 156/1500\n",
      "67/67 [==============================] - 0s 962us/step - loss: 0.1392 - accuracy: 0.9444\n",
      "Epoch 157/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.1346 - accuracy: 0.9520\n",
      "Epoch 158/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.1452 - accuracy: 0.9482\n",
      "Epoch 159/1500\n",
      "67/67 [==============================] - 0s 969us/step - loss: 0.1470 - accuracy: 0.9397\n",
      "Epoch 160/1500\n",
      "67/67 [==============================] - 0s 959us/step - loss: 0.1293 - accuracy: 0.9520\n",
      "Epoch 161/1500\n",
      "67/67 [==============================] - 0s 985us/step - loss: 0.1315 - accuracy: 0.9463\n",
      "Epoch 162/1500\n",
      "67/67 [==============================] - 0s 974us/step - loss: 0.1229 - accuracy: 0.9482\n",
      "Epoch 163/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9444\n",
      "Epoch 164/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9477\n",
      "Epoch 165/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.1378 - accuracy: 0.9463\n",
      "Epoch 166/1500\n",
      "67/67 [==============================] - 0s 981us/step - loss: 0.1282 - accuracy: 0.9487\n",
      "Epoch 167/1500\n",
      "67/67 [==============================] - 0s 957us/step - loss: 0.1257 - accuracy: 0.9468\n",
      "Epoch 168/1500\n",
      "67/67 [==============================] - 0s 983us/step - loss: 0.1210 - accuracy: 0.9557\n",
      "Epoch 169/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.1174 - accuracy: 0.9572\n",
      "Epoch 170/1500\n",
      "67/67 [==============================] - 0s 998us/step - loss: 0.1377 - accuracy: 0.9435\n",
      "Epoch 171/1500\n",
      "67/67 [==============================] - 0s 967us/step - loss: 0.1201 - accuracy: 0.9520\n",
      "Epoch 172/1500\n",
      "67/67 [==============================] - 0s 973us/step - loss: 0.1261 - accuracy: 0.9454\n",
      "Epoch 173/1500\n",
      "67/67 [==============================] - 0s 959us/step - loss: 0.1429 - accuracy: 0.9430\n",
      "Epoch 174/1500\n",
      "67/67 [==============================] - 0s 998us/step - loss: 0.1319 - accuracy: 0.9529\n",
      "Epoch 175/1500\n",
      "67/67 [==============================] - 0s 967us/step - loss: 0.1215 - accuracy: 0.9534\n",
      "Epoch 176/1500\n",
      "67/67 [==============================] - 0s 977us/step - loss: 0.1219 - accuracy: 0.9543\n",
      "Epoch 177/1500\n",
      "67/67 [==============================] - 0s 975us/step - loss: 0.1288 - accuracy: 0.9501\n",
      "Epoch 178/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9496\n",
      "Epoch 179/1500\n",
      "67/67 [==============================] - 0s 988us/step - loss: 0.1254 - accuracy: 0.9454\n",
      "Epoch 180/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9482\n",
      "Epoch 181/1500\n",
      "67/67 [==============================] - 0s 973us/step - loss: 0.1390 - accuracy: 0.9411\n",
      "Epoch 182/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9492\n",
      "Epoch 183/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9482\n",
      "Epoch 184/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9576\n",
      "Epoch 185/1500\n",
      "67/67 [==============================] - 0s 976us/step - loss: 0.1401 - accuracy: 0.9468\n",
      "Epoch 186/1500\n",
      "67/67 [==============================] - 0s 969us/step - loss: 0.1335 - accuracy: 0.9482\n",
      "Epoch 187/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9529\n",
      "Epoch 188/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9548\n",
      "Epoch 189/1500\n",
      "67/67 [==============================] - 0s 987us/step - loss: 0.1133 - accuracy: 0.9581\n",
      "Epoch 190/1500\n",
      "67/67 [==============================] - 0s 971us/step - loss: 0.1197 - accuracy: 0.9520\n",
      "Epoch 191/1500\n",
      "67/67 [==============================] - 0s 974us/step - loss: 0.1230 - accuracy: 0.9449\n",
      "Epoch 192/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9501\n",
      "Epoch 193/1500\n",
      "67/67 [==============================] - 0s 981us/step - loss: 0.1152 - accuracy: 0.9576\n",
      "Epoch 194/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.1239 - accuracy: 0.9520\n",
      "Epoch 195/1500\n",
      "67/67 [==============================] - 0s 972us/step - loss: 0.1339 - accuracy: 0.9473\n",
      "Epoch 196/1500\n",
      "67/67 [==============================] - 0s 970us/step - loss: 0.1236 - accuracy: 0.9586\n",
      "Epoch 197/1500\n",
      "67/67 [==============================] - 0s 971us/step - loss: 0.1262 - accuracy: 0.9529\n",
      "Epoch 198/1500\n",
      "67/67 [==============================] - 0s 986us/step - loss: 0.1217 - accuracy: 0.9557\n",
      "Epoch 199/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.9520\n",
      "Epoch 200/1500\n",
      "67/67 [==============================] - 0s 997us/step - loss: 0.1004 - accuracy: 0.9619\n",
      "Epoch 201/1500\n",
      "67/67 [==============================] - 0s 975us/step - loss: 0.1131 - accuracy: 0.9576\n",
      "Epoch 202/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.1091 - accuracy: 0.9605\n",
      "Epoch 203/1500\n",
      "67/67 [==============================] - 0s 994us/step - loss: 0.1137 - accuracy: 0.9619\n",
      "Epoch 204/1500\n",
      "67/67 [==============================] - 0s 967us/step - loss: 0.1086 - accuracy: 0.9637\n",
      "Epoch 205/1500\n",
      "67/67 [==============================] - 0s 953us/step - loss: 0.1101 - accuracy: 0.9548\n",
      "Epoch 206/1500\n",
      "67/67 [==============================] - 0s 987us/step - loss: 0.1155 - accuracy: 0.9557\n",
      "Epoch 207/1500\n",
      "67/67 [==============================] - 0s 956us/step - loss: 0.1037 - accuracy: 0.9614\n",
      "Epoch 208/1500\n",
      "67/67 [==============================] - 0s 980us/step - loss: 0.1165 - accuracy: 0.9482\n",
      "Epoch 209/1500\n",
      "67/67 [==============================] - 0s 986us/step - loss: 0.1112 - accuracy: 0.9562\n",
      "Epoch 210/1500\n",
      "67/67 [==============================] - 0s 994us/step - loss: 0.1320 - accuracy: 0.9477\n",
      "Epoch 211/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.1191 - accuracy: 0.9548\n",
      "Epoch 212/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9605\n",
      "Epoch 213/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9623\n",
      "Epoch 214/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9576\n",
      "Epoch 215/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9600\n",
      "Epoch 216/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9633\n",
      "Epoch 217/1500\n",
      "67/67 [==============================] - 0s 982us/step - loss: 0.1032 - accuracy: 0.9609\n",
      "Epoch 218/1500\n",
      "67/67 [==============================] - 0s 984us/step - loss: 0.1239 - accuracy: 0.9520\n",
      "Epoch 219/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.1082 - accuracy: 0.9600\n",
      "Epoch 220/1500\n",
      "67/67 [==============================] - 0s 989us/step - loss: 0.1035 - accuracy: 0.9609\n",
      "Epoch 221/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.9623\n",
      "Epoch 222/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.1011 - accuracy: 0.9652\n",
      "Epoch 223/1500\n",
      "67/67 [==============================] - 0s 966us/step - loss: 0.0954 - accuracy: 0.9656\n",
      "Epoch 224/1500\n",
      "67/67 [==============================] - 0s 973us/step - loss: 0.0908 - accuracy: 0.9661\n",
      "Epoch 225/1500\n",
      "67/67 [==============================] - 0s 984us/step - loss: 0.0976 - accuracy: 0.9637\n",
      "Epoch 226/1500\n",
      "67/67 [==============================] - 0s 961us/step - loss: 0.0966 - accuracy: 0.9670\n",
      "Epoch 227/1500\n",
      "67/67 [==============================] - 0s 974us/step - loss: 0.1196 - accuracy: 0.9534\n",
      "Epoch 228/1500\n",
      "67/67 [==============================] - 0s 998us/step - loss: 0.1079 - accuracy: 0.9595\n",
      "Epoch 229/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9524\n",
      "Epoch 230/1500\n",
      "67/67 [==============================] - 0s 966us/step - loss: 0.1139 - accuracy: 0.9590\n",
      "Epoch 231/1500\n",
      "67/67 [==============================] - 0s 994us/step - loss: 0.0995 - accuracy: 0.9642\n",
      "Epoch 232/1500\n",
      "67/67 [==============================] - 0s 986us/step - loss: 0.1031 - accuracy: 0.9586\n",
      "Epoch 233/1500\n",
      "67/67 [==============================] - 0s 969us/step - loss: 0.1013 - accuracy: 0.9600\n",
      "Epoch 234/1500\n",
      "67/67 [==============================] - 0s 976us/step - loss: 0.1120 - accuracy: 0.9586\n",
      "Epoch 235/1500\n",
      "67/67 [==============================] - 0s 956us/step - loss: 0.1154 - accuracy: 0.9492\n",
      "Epoch 236/1500\n",
      "67/67 [==============================] - 0s 975us/step - loss: 0.1033 - accuracy: 0.9633\n",
      "Epoch 237/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9685\n",
      "Epoch 238/1500\n",
      "67/67 [==============================] - 0s 993us/step - loss: 0.1012 - accuracy: 0.9590\n",
      "Epoch 239/1500\n",
      "67/67 [==============================] - 0s 993us/step - loss: 0.0997 - accuracy: 0.9609\n",
      "Epoch 240/1500\n",
      "67/67 [==============================] - 0s 985us/step - loss: 0.1169 - accuracy: 0.9562\n",
      "Epoch 241/1500\n",
      "67/67 [==============================] - 0s 981us/step - loss: 0.0926 - accuracy: 0.9619\n",
      "Epoch 242/1500\n",
      "67/67 [==============================] - 0s 991us/step - loss: 0.1072 - accuracy: 0.9567\n",
      "Epoch 243/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9581\n",
      "Epoch 244/1500\n",
      "67/67 [==============================] - 0s 980us/step - loss: 0.1086 - accuracy: 0.9581\n",
      "Epoch 245/1500\n",
      "67/67 [==============================] - 0s 991us/step - loss: 0.1093 - accuracy: 0.9595\n",
      "Epoch 246/1500\n",
      "67/67 [==============================] - 0s 986us/step - loss: 0.1103 - accuracy: 0.9586\n",
      "Epoch 247/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9605\n",
      "Epoch 248/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9605\n",
      "Epoch 249/1500\n",
      "67/67 [==============================] - 0s 997us/step - loss: 0.0896 - accuracy: 0.9656\n",
      "Epoch 250/1500\n",
      "67/67 [==============================] - 0s 992us/step - loss: 0.1009 - accuracy: 0.9628\n",
      "Epoch 251/1500\n",
      "67/67 [==============================] - 0s 990us/step - loss: 0.0914 - accuracy: 0.9628\n",
      "Epoch 252/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9633\n",
      "Epoch 253/1500\n",
      "67/67 [==============================] - 0s 946us/step - loss: 0.0934 - accuracy: 0.9637\n",
      "Epoch 254/1500\n",
      "67/67 [==============================] - 0s 978us/step - loss: 0.0884 - accuracy: 0.9647\n",
      "Epoch 255/1500\n",
      "67/67 [==============================] - 0s 988us/step - loss: 0.1070 - accuracy: 0.9605\n",
      "Epoch 256/1500\n",
      "67/67 [==============================] - 0s 984us/step - loss: 0.1067 - accuracy: 0.9572\n",
      "Epoch 257/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9562\n",
      "Epoch 258/1500\n",
      "67/67 [==============================] - 0s 988us/step - loss: 0.0910 - accuracy: 0.9666\n",
      "Epoch 259/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9652\n",
      "Epoch 260/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.0886 - accuracy: 0.9647\n",
      "Epoch 261/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9609\n",
      "Epoch 262/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.0896 - accuracy: 0.9656\n",
      "Epoch 263/1500\n",
      "67/67 [==============================] - 0s 981us/step - loss: 0.0824 - accuracy: 0.9713\n",
      "Epoch 264/1500\n",
      "67/67 [==============================] - 0s 975us/step - loss: 0.0911 - accuracy: 0.9670\n",
      "Epoch 265/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9614\n",
      "Epoch 266/1500\n",
      "67/67 [==============================] - 0s 984us/step - loss: 0.0933 - accuracy: 0.9656\n",
      "Epoch 267/1500\n",
      "51/67 [=====================>........] - ETA: 0s - loss: 0.0961 - accuracy: 0.9626Restoring model weights from the end of the best epoch: 237.\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9637\n",
      "Epoch 267: early stopping\n",
      "6/6 [==============================] - 0s 889us/step - loss: 0.7683 - accuracy: 0.7176\n",
      "6/6 [==============================] - 0s 625us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.78 (18/23)\n",
      "Before appending - Cat IDs: 562, Predictions: 562, Actuals: 562, Gender: 562\n",
      "After appending - Cat IDs: 732, Predictions: 732, Actuals: 732, Gender: 732\n",
      "Final Test Results - Loss: 0.7683061361312866, Accuracy: 0.7176470756530762, Precision: 0.6476325757575757, Recall: 0.633701547494651, F1 Score: 0.6373087128784849\n",
      "Confusion Matrix:\n",
      " [[98  1 17]\n",
      " [ 2 19  0]\n",
      " [28  0  5]]\n",
      "outer_fold 5\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "        ..\n",
      "096A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 92, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "097B    14\n",
      "059A    14\n",
      "045A     9\n",
      "022A     9\n",
      "094A     8\n",
      "095A     8\n",
      "013B     8\n",
      "117A     7\n",
      "023A     6\n",
      "034A     5\n",
      "075A     5\n",
      "035A     4\n",
      "003A     4\n",
      "026A     4\n",
      "009A     4\n",
      "061A     2\n",
      "041A     1\n",
      "004A     1\n",
      "088A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    315\n",
      "M    289\n",
      "F    218\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    48\n",
      "F    34\n",
      "X    33\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 028...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [097B, 022A, 095A, 034A, 009A, 023A, 013B, 026...\n",
      "kitten                                         [041A, 045A]\n",
      "senior                       [059A, 117A, 094A, 061A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 61, 'kitten': 14, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 13, 'kitten': 2, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '005A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '021A' '023B' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '036A' '037A' '038A' '039A' '040A' '042A'\n",
      " '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A' '051B' '052A'\n",
      " '053A' '054A' '055A' '056A' '057A' '058A' '060A' '062A' '063A' '064A'\n",
      " '065A' '066A' '067A' '068A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '087A' '091A' '092A' '093A' '096A' '097A' '099A' '100A' '101A'\n",
      " '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '004A' '009A' '013B' '022A' '023A' '026A' '034A' '035A' '041A'\n",
      " '045A' '059A' '061A' '075A' '088A' '090A' '094A' '095A' '097B' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '005A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '021A' '023B' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '036A' '037A' '038A' '039A' '040A' '042A'\n",
      " '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A' '051B' '052A'\n",
      " '053A' '054A' '055A' '056A' '057A' '058A' '060A' '062A' '063A' '064A'\n",
      " '065A' '066A' '067A' '068A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '087A' '091A' '092A' '093A' '096A' '097A' '099A' '100A' '101A'\n",
      " '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '004A' '009A' '013B' '022A' '023A' '026A' '034A' '035A' '041A'\n",
      " '045A' '059A' '061A' '075A' '088A' '090A' '094A' '095A' '097B' '117A']\n",
      "Length of X_train_val:\n",
      "822\n",
      "Length of y_train_val:\n",
      "822\n",
      "Length of groups_train_val:\n",
      "822\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     515\n",
      "kitten    161\n",
      "senior    146\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     73\n",
      "senior    32\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     515\n",
      "kitten    161\n",
      "senior    146\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     73\n",
      "senior    32\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1030, 1: 644, 2: 584})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.9521 - accuracy: 0.5642\n",
      "Epoch 2/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.7242 - accuracy: 0.6763\n",
      "Epoch 3/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6731 - accuracy: 0.6997\n",
      "Epoch 4/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6098 - accuracy: 0.7086\n",
      "Epoch 5/1500\n",
      "71/71 [==============================] - 0s 977us/step - loss: 0.5982 - accuracy: 0.7259\n",
      "Epoch 6/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5648 - accuracy: 0.7387\n",
      "Epoch 7/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5619 - accuracy: 0.7449\n",
      "Epoch 8/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5241 - accuracy: 0.7573\n",
      "Epoch 9/1500\n",
      "71/71 [==============================] - 0s 998us/step - loss: 0.5168 - accuracy: 0.7613\n",
      "Epoch 10/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5046 - accuracy: 0.7733\n",
      "Epoch 11/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4663 - accuracy: 0.7803\n",
      "Epoch 12/1500\n",
      "71/71 [==============================] - 0s 4ms/step - loss: 0.4733 - accuracy: 0.7905\n",
      "Epoch 13/1500\n",
      "71/71 [==============================] - 0s 4ms/step - loss: 0.4912 - accuracy: 0.7861\n",
      "Epoch 14/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4401 - accuracy: 0.7976\n",
      "Epoch 15/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4234 - accuracy: 0.8162\n",
      "Epoch 16/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4202 - accuracy: 0.8069\n",
      "Epoch 17/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4112 - accuracy: 0.8162\n",
      "Epoch 18/1500\n",
      "71/71 [==============================] - 0s 4ms/step - loss: 0.4094 - accuracy: 0.8171\n",
      "Epoch 19/1500\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 0.4123 - accuracy: 0.8224\n",
      "Epoch 20/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.8255\n",
      "Epoch 21/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3711 - accuracy: 0.8282\n",
      "Epoch 22/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.8277\n",
      "Epoch 23/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8286\n",
      "Epoch 24/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3751 - accuracy: 0.8291\n",
      "Epoch 25/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3625 - accuracy: 0.8335\n",
      "Epoch 26/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3509 - accuracy: 0.8428\n",
      "Epoch 27/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3559 - accuracy: 0.8379\n",
      "Epoch 28/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3475 - accuracy: 0.8423\n",
      "Epoch 29/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3337 - accuracy: 0.8530\n",
      "Epoch 30/1500\n",
      "71/71 [==============================] - 0s 4ms/step - loss: 0.3335 - accuracy: 0.8508\n",
      "Epoch 31/1500\n",
      "71/71 [==============================] - 0s 3ms/step - loss: 0.3275 - accuracy: 0.8512\n",
      "Epoch 32/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3253 - accuracy: 0.8530\n",
      "Epoch 33/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3236 - accuracy: 0.8640\n",
      "Epoch 34/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3197 - accuracy: 0.8685\n",
      "Epoch 35/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3209 - accuracy: 0.8561\n",
      "Epoch 36/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3177 - accuracy: 0.8667\n",
      "Epoch 37/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3048 - accuracy: 0.8667\n",
      "Epoch 38/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.3112 - accuracy: 0.8583\n",
      "Epoch 39/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3185 - accuracy: 0.8676\n",
      "Epoch 40/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2851 - accuracy: 0.8733\n",
      "Epoch 41/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2799 - accuracy: 0.8764\n",
      "Epoch 42/1500\n",
      "71/71 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.8773\n",
      "Epoch 43/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2841 - accuracy: 0.8764\n",
      "Epoch 44/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2743 - accuracy: 0.8795\n",
      "Epoch 45/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2783 - accuracy: 0.8720\n",
      "Epoch 46/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2863 - accuracy: 0.8711\n",
      "Epoch 47/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2787 - accuracy: 0.8738\n",
      "Epoch 48/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2615 - accuracy: 0.8884\n",
      "Epoch 49/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2601 - accuracy: 0.8826\n",
      "Epoch 50/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2786 - accuracy: 0.8804\n",
      "Epoch 51/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2671 - accuracy: 0.8809\n",
      "Epoch 52/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2636 - accuracy: 0.8791\n",
      "Epoch 53/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2592 - accuracy: 0.8857\n",
      "Epoch 54/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2487 - accuracy: 0.8937\n",
      "Epoch 55/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2587 - accuracy: 0.8849\n",
      "Epoch 56/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2463 - accuracy: 0.8955\n",
      "Epoch 57/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2394 - accuracy: 0.8977\n",
      "Epoch 58/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2611 - accuracy: 0.8844\n",
      "Epoch 59/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2514 - accuracy: 0.8942\n",
      "Epoch 60/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2464 - accuracy: 0.8959\n",
      "Epoch 61/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2502 - accuracy: 0.8888\n",
      "Epoch 62/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2561 - accuracy: 0.8946\n",
      "Epoch 63/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2507 - accuracy: 0.8906\n",
      "Epoch 64/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2169 - accuracy: 0.9119\n",
      "Epoch 65/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2343 - accuracy: 0.9026\n",
      "Epoch 66/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2162 - accuracy: 0.9074\n",
      "Epoch 67/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2347 - accuracy: 0.8990\n",
      "Epoch 68/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2244 - accuracy: 0.9035\n",
      "Epoch 69/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9052\n",
      "Epoch 70/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.8849\n",
      "Epoch 71/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2242 - accuracy: 0.8973\n",
      "Epoch 72/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2218 - accuracy: 0.9105\n",
      "Epoch 73/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9021\n",
      "Epoch 74/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2218 - accuracy: 0.9070\n",
      "Epoch 75/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2251 - accuracy: 0.9008\n",
      "Epoch 76/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2064 - accuracy: 0.9136\n",
      "Epoch 77/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2141 - accuracy: 0.9070\n",
      "Epoch 78/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9092\n",
      "Epoch 79/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.9074\n",
      "Epoch 80/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9092\n",
      "Epoch 81/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1938 - accuracy: 0.9247\n",
      "Epoch 82/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9265\n",
      "Epoch 83/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9150\n",
      "Epoch 84/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1957 - accuracy: 0.9194\n",
      "Epoch 85/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9252\n",
      "Epoch 86/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9190\n",
      "Epoch 87/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1908 - accuracy: 0.9212\n",
      "Epoch 88/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1955 - accuracy: 0.9159\n",
      "Epoch 89/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2215 - accuracy: 0.9092\n",
      "Epoch 90/1500\n",
      "71/71 [==============================] - 0s 993us/step - loss: 0.2057 - accuracy: 0.9097\n",
      "Epoch 91/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9136\n",
      "Epoch 92/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9252\n",
      "Epoch 93/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1869 - accuracy: 0.9225\n",
      "Epoch 94/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.9247\n",
      "Epoch 95/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9203\n",
      "Epoch 96/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1943 - accuracy: 0.9176\n",
      "Epoch 97/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.9269\n",
      "Epoch 98/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.9216\n",
      "Epoch 99/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.9269\n",
      "Epoch 100/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1754 - accuracy: 0.9252\n",
      "Epoch 101/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1897 - accuracy: 0.9260\n",
      "Epoch 102/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9296\n",
      "Epoch 103/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1812 - accuracy: 0.9234\n",
      "Epoch 104/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9283\n",
      "Epoch 105/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1717 - accuracy: 0.9291\n",
      "Epoch 106/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1933 - accuracy: 0.9216\n",
      "Epoch 107/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.9349\n",
      "Epoch 108/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9340\n",
      "Epoch 109/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9291\n",
      "Epoch 110/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1717 - accuracy: 0.9252\n",
      "Epoch 111/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9331\n",
      "Epoch 112/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9389\n",
      "Epoch 113/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9331\n",
      "Epoch 114/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9371\n",
      "Epoch 115/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9322\n",
      "Epoch 116/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.9318\n",
      "Epoch 117/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1557 - accuracy: 0.9407\n",
      "Epoch 118/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9402\n",
      "Epoch 119/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9433\n",
      "Epoch 120/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9353\n",
      "Epoch 121/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9296\n",
      "Epoch 122/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1493 - accuracy: 0.9398\n",
      "Epoch 123/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1608 - accuracy: 0.9318\n",
      "Epoch 124/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9367\n",
      "Epoch 125/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9442\n",
      "Epoch 126/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9349\n",
      "Epoch 127/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1547 - accuracy: 0.9402\n",
      "Epoch 128/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9327\n",
      "Epoch 129/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.9389\n",
      "Epoch 130/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9345\n",
      "Epoch 131/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9327\n",
      "Epoch 132/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9318\n",
      "Epoch 133/1500\n",
      "71/71 [==============================] - 0s 983us/step - loss: 0.1571 - accuracy: 0.9314\n",
      "Epoch 134/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9420\n",
      "Epoch 135/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9451\n",
      "Epoch 136/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1608 - accuracy: 0.9367\n",
      "Epoch 137/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9353\n",
      "Epoch 138/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.9362\n",
      "Epoch 139/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9384\n",
      "Epoch 140/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.9353\n",
      "Epoch 141/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9367\n",
      "Epoch 142/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1429 - accuracy: 0.9482\n",
      "Epoch 143/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9460\n",
      "Epoch 144/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9433\n",
      "Epoch 145/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.9367\n",
      "Epoch 146/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9522\n",
      "Epoch 147/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9433\n",
      "Epoch 148/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9407\n",
      "Epoch 149/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9389\n",
      "Epoch 150/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9398\n",
      "Epoch 151/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9473\n",
      "Epoch 152/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9411\n",
      "Epoch 153/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1422 - accuracy: 0.9384\n",
      "Epoch 154/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1330 - accuracy: 0.9433\n",
      "Epoch 155/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9433\n",
      "Epoch 156/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9526\n",
      "Epoch 157/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9486\n",
      "Epoch 158/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9362\n",
      "Epoch 159/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.9495\n",
      "Epoch 160/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9522\n",
      "Epoch 161/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9482\n",
      "Epoch 162/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9477\n",
      "Epoch 163/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9579\n",
      "Epoch 164/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9486\n",
      "Epoch 165/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1330 - accuracy: 0.9473\n",
      "Epoch 166/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9393\n",
      "Epoch 167/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9491\n",
      "Epoch 168/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9420\n",
      "Epoch 169/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9407\n",
      "Epoch 170/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.9557\n",
      "Epoch 171/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9473\n",
      "Epoch 172/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9579\n",
      "Epoch 173/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9451\n",
      "Epoch 174/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9482\n",
      "Epoch 175/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9535\n",
      "Epoch 176/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9500\n",
      "Epoch 177/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.9420\n",
      "Epoch 178/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9442\n",
      "Epoch 179/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9557\n",
      "Epoch 180/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9531\n",
      "Epoch 181/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9522\n",
      "Epoch 182/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9504\n",
      "Epoch 183/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9539\n",
      "Epoch 184/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9539\n",
      "Epoch 185/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9513\n",
      "Epoch 186/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9548\n",
      "Epoch 187/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9579\n",
      "Epoch 188/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9579\n",
      "Epoch 189/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9464\n",
      "Epoch 190/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9508\n",
      "Epoch 191/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9486\n",
      "Epoch 192/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9557\n",
      "Epoch 193/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9588\n",
      "Epoch 194/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9504\n",
      "Epoch 195/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.9517\n",
      "Epoch 196/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9500\n",
      "Epoch 197/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9610\n",
      "Epoch 198/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9539\n",
      "Epoch 199/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9473\n",
      "Epoch 200/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.9526\n",
      "Epoch 201/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9557\n",
      "Epoch 202/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9570\n",
      "Epoch 203/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9597\n",
      "Epoch 204/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9615\n",
      "Epoch 205/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9584\n",
      "Epoch 206/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9517\n",
      "Epoch 207/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9535\n",
      "Epoch 208/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9579\n",
      "Epoch 209/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9610\n",
      "Epoch 210/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9535\n",
      "Epoch 211/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9579\n",
      "Epoch 212/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9570\n",
      "Epoch 213/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9526\n",
      "Epoch 214/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9588\n",
      "Epoch 215/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9655\n",
      "Epoch 216/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9553\n",
      "Epoch 217/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.9557\n",
      "Epoch 218/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9579\n",
      "Epoch 219/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9610\n",
      "Epoch 220/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9641\n",
      "Epoch 221/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9628\n",
      "Epoch 222/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9566\n",
      "Epoch 223/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9539\n",
      "Epoch 224/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9588\n",
      "Epoch 225/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9570\n",
      "Epoch 226/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9601\n",
      "Epoch 227/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9601\n",
      "Epoch 228/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9526\n",
      "Epoch 229/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1111 - accuracy: 0.9588\n",
      "Epoch 230/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9575\n",
      "Epoch 231/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9606\n",
      "Epoch 232/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9610\n",
      "Epoch 233/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9584\n",
      "Epoch 234/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9575\n",
      "Epoch 235/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9575\n",
      "Epoch 236/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9566\n",
      "Epoch 237/1500\n",
      "71/71 [==============================] - 0s 991us/step - loss: 0.1361 - accuracy: 0.9455\n",
      "Epoch 238/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9663\n",
      "Epoch 239/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9615\n",
      "Epoch 240/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9632\n",
      "Epoch 241/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9570\n",
      "Epoch 242/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9655\n",
      "Epoch 243/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9677\n",
      "Epoch 244/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9526\n",
      "Epoch 245/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.9681\n",
      "Epoch 246/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9579\n",
      "Epoch 247/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9553\n",
      "Epoch 248/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9566\n",
      "Epoch 249/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9668\n",
      "Epoch 250/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9668\n",
      "Epoch 251/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9597\n",
      "Epoch 252/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9694\n",
      "Epoch 253/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9712\n",
      "Epoch 254/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9624\n",
      "Epoch 255/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9632\n",
      "Epoch 256/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9601\n",
      "Epoch 257/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9593\n",
      "Epoch 258/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9650\n",
      "Epoch 259/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9712\n",
      "Epoch 260/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9641\n",
      "Epoch 261/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9672\n",
      "Epoch 262/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9588\n",
      "Epoch 263/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9593\n",
      "Epoch 264/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9588\n",
      "Epoch 265/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9548\n",
      "Epoch 266/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9557\n",
      "Epoch 267/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9637\n",
      "Epoch 268/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9588\n",
      "Epoch 269/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9650\n",
      "Epoch 270/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9566\n",
      "Epoch 271/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9646\n",
      "Epoch 272/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9615\n",
      "Epoch 273/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0956 - accuracy: 0.9632\n",
      "Epoch 274/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9579\n",
      "Epoch 275/1500\n",
      "47/71 [==================>...........] - ETA: 0s - loss: 0.0753 - accuracy: 0.9694Restoring model weights from the end of the best epoch: 245.\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9655\n",
      "Epoch 275: early stopping\n",
      "4/4 [==============================] - 0s 987us/step - loss: 0.6697 - accuracy: 0.7217\n",
      "4/4 [==============================] - 0s 774us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.85 (17/20)\n",
      "Before appending - Cat IDs: 732, Predictions: 732, Actuals: 732, Gender: 732\n",
      "After appending - Cat IDs: 847, Predictions: 847, Actuals: 847, Gender: 847\n",
      "Final Test Results - Loss: 0.6696994304656982, Accuracy: 0.7217391133308411, Precision: 0.7106247661803217, Recall: 0.7368721461187215, F1 Score: 0.7152477152477154\n",
      "Confusion Matrix:\n",
      " [[61  2 10]\n",
      " [ 0 10  0]\n",
      " [20  0 12]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6826449091518463\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8984112739562988\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7320389986038208\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6886085455207531\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6953008113036431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=int(random_seeds[0]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train_val),\n",
    "        y=y_train_val\n",
    "    )\n",
    "    weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping], class_weight=weight_dict)\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "# Append averages to total lists\n",
    "all_f1.append(unseen_set_avg_f1)\n",
    "all_losses.append(unseen_set_avg_loss)\n",
    "all_accuracies.append(unseen_set_avg_acc)\n",
    "all_precisions.append(unseen_set_avg_precision)\n",
    "all_recalls.append(unseen_set_avg_recall)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f55886c1-5ef3-4ce9-bb13-1e48bca7704b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed_1_folds_values = (unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1)\n",
    "\n",
    "# Plotting all fold metrics\n",
    "seed_1_folds_plot = plot_all_metrics(unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1, \"Folds\", \"Fold Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79e793-c694-4bc7-8cc6-05e124ddf71f",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c3c239d-6215-4589-a583-b37a18ca22cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 847, Predictions: 847, Actuals: 847, Gender: 847\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99978976-e4a9-4c04-a6b2-6b14a3111277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "189b812b-d37f-462f-a99f-6de8e4a7899f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.76 (84/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "# store for final evaluation \n",
    "all_majority_vote_accuracies.append(majority_vote_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "907d957f-7340-4a76-b3fa-7cf166f43049",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7016fa20-094f-409a-9a2f-b6d6793aed03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, adult, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, kit...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, senior, adult, adult, kitten, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "75    069A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, a...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "69    063A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "61    055A  [adult, senior, senior, senior, senior, senior...        senior           senior                   True\n",
       "59    053A       [adult, adult, senior, senior, adult, adult]         adult            adult                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A           [kitten, kitten, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "80    074A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "81    075A               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [senior, senior, adult]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "102   108A   [senior, senior, senior, senior, senior, senior]        senior           senior                   True\n",
       "100   105A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, s...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, adult, senior, ...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, adult, kitten,...        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "29    025B                                     [adult, adult]         adult            adult                   True\n",
       "28    025A  [senior, senior, adult, senior, adult, adult, ...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, kit...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, adult, s...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A       [adult, senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "7     006A                             [adult, senior, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                     [adult, senior, adult, senior]         adult            adult                   True\n",
       "3     002B  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "34    027A  [adult, senior, adult, adult, kitten, adult, s...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, senior, adult, senior, a...         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "99    104A                     [senior, adult, senior, adult]         adult           senior                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "101   106A  [adult, senior, adult, senior, adult, adult, a...         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "57    051B  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "42    036A  [adult, senior, senior, senior, adult, senior,...        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "56    051A  [senior, adult, adult, adult, adult, adult, ad...         adult           senior                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "60    054A                                    [adult, senior]         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, adult, senior, senior, a...         adult           senior                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "62    056A                             [adult, senior, adult]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "64    058A                             [senior, adult, adult]         adult           senior                  False\n",
       "65    059A  [adult, adult, adult, adult, senior, adult, ad...         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "109   117A  [senior, senior, adult, adult, adult, adult, a...         adult           senior                  False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "994531fd-e6fb-4491-9eab-86e41ac1ed3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     64\n",
      "kitten    12\n",
      "senior     8\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "229af3e2-6a64-4f3d-a594-e04108fce87f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             64  87.671233\n",
      "1           kitten           15             12  80.000000\n",
      "2           senior           22              8  36.363636\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# store for final evaluation \n",
    "all_majority_vote_details.append(class_stats)\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3960450a-db52-4464-a965-00f1e600bda5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbf768-64c2-48ec-80e3-3a961b0b12a6",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe16003e-4015-4d28-ae37-ca0c94952758",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          551            443  80.399274\n",
      "1           kitten          118            102  86.440678\n",
      "2           senior          178             76  42.696629\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# store for final evaluation \n",
    "all_class_stats.append(class_stats)\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b5b8766-b4bd-4dd2-92e2-6513741b03ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efa447-1e32-42df-8664-a3f1f0e0f812",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5eecd6a6-4094-4747-8029-f795a87f8ff0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      165     77.46\n",
      "1          M    337      234     69.44\n",
      "2          X    297      222     74.75\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)\n",
    "\n",
    "# store for final evaluation \n",
    "all_gender_stats.append(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "115592e9-47eb-42fe-8cb5-26beeb7328ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bdf9c0-a94a-45a4-ac2b-36314d303346",
   "metadata": {},
   "source": [
    "### Log current total CV Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5c379093-52d3-42eb-b4c6-a42832422573",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Majority Vote Accuracy so far: [0.7636363636363637]\n",
      "Total Class Statistics so far:\n",
      " [  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          551            443  80.399274\n",
      "1           kitten          118            102  86.440678\n",
      "2           senior          178             76  42.696629]\n",
      "Total Gender Accuracy so far:\n",
      " [  all_gender  count  correct  accuracy\n",
      "0          F    213      165     77.46\n",
      "1          M    337      234     69.44\n",
      "2          X    297      222     74.75]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Majority Vote Accuracy so far:\", all_majority_vote_accuracies)\n",
    "print(\"Total Class Statistics so far:\\n\", all_class_stats)\n",
    "print(\"Total Gender Accuracy so far:\\n\", all_gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae066af0-659b-43f0-924c-2c50be0e6c40",
   "metadata": {},
   "source": [
    "# RANDOM SEED 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc76296a-4029-447e-b12c-41520160251a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "adult     588\n",
      "senior    534\n",
      "kitten    513\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[1]))\n",
    "np.random.seed(int(random_seeds[1]))\n",
    "tf.random.set_seed(int(random_seeds[1]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_19.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "093977a2-1813-4a02-9573-f757b3d2a567",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0c89dbfa-4c9b-4e46-a0d9-f659db30532f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a62fdf-86b5-45e7-9249-73a55985936a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee1b36c7-4a91-4071-a713-c904da913c3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "101A    15\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "039A    12\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "051B     9\n",
      "022A     9\n",
      "045A     9\n",
      "065A     9\n",
      "072A     9\n",
      "015A     9\n",
      "033A     9\n",
      "094A     8\n",
      "013B     8\n",
      "010A     8\n",
      "095A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "037A     6\n",
      "023A     6\n",
      "109A     6\n",
      "053A     6\n",
      "008A     6\n",
      "007A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "021A     5\n",
      "023B     5\n",
      "070A     5\n",
      "075A     5\n",
      "035A     4\n",
      "009A     4\n",
      "062A     4\n",
      "105A     4\n",
      "104A     4\n",
      "003A     4\n",
      "058A     3\n",
      "064A     3\n",
      "056A     3\n",
      "060A     3\n",
      "012A     3\n",
      "113A     3\n",
      "011A     2\n",
      "025B     2\n",
      "032A     2\n",
      "018A     2\n",
      "093A     2\n",
      "038A     2\n",
      "054A     2\n",
      "069A     2\n",
      "087A     2\n",
      "073A     1\n",
      "088A     1\n",
      "090A     1\n",
      "091A     1\n",
      "115A     1\n",
      "019B     1\n",
      "048A     1\n",
      "004A     1\n",
      "043A     1\n",
      "076A     1\n",
      "026C     1\n",
      "096A     1\n",
      "041A     1\n",
      "049A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000B    19\n",
      "019A    17\n",
      "097A    16\n",
      "097B    14\n",
      "059A    14\n",
      "068A    11\n",
      "005A    10\n",
      "027A     7\n",
      "099A     7\n",
      "108A     6\n",
      "026A     4\n",
      "052A     4\n",
      "006A     3\n",
      "014A     3\n",
      "061A     2\n",
      "102A     2\n",
      "092A     1\n",
      "066A     1\n",
      "110A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    295\n",
      "X    289\n",
      "F    209\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    59\n",
      "F    43\n",
      "M    42\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 001A, 103A, 071A, 028A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 097B, 019A, 005A, 027A, 099A, 014A, 000...\n",
      "kitten                                               [110A]\n",
      "senior                             [097A, 059A, 108A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 58, 'kitten': 15, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 16, 'kitten': 1, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014B' '015A' '016A' '018A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '024A' '025A' '025B' '025C' '026C' '028A' '029A'\n",
      " '031A' '032A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '045A' '046A' '047A' '048A' '049A' '050A'\n",
      " '051A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '062A'\n",
      " '063A' '064A' '065A' '067A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '088A' '090A' '091A' '093A' '094A' '095A' '096A'\n",
      " '101A' '103A' '104A' '105A' '106A' '109A' '111A' '113A' '115A' '116A'\n",
      " '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '005A' '006A' '014A' '019A' '026A' '026B' '027A' '052A' '059A'\n",
      " '061A' '066A' '068A' '092A' '097A' '097B' '099A' '100A' '102A' '108A'\n",
      " '110A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014B' '015A' '016A' '018A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '024A' '025A' '025B' '025C' '026C' '028A' '029A'\n",
      " '031A' '032A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '045A' '046A' '047A' '048A' '049A' '050A'\n",
      " '051A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '062A'\n",
      " '063A' '064A' '065A' '067A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '088A' '090A' '091A' '093A' '094A' '095A' '096A'\n",
      " '101A' '103A' '104A' '105A' '106A' '109A' '111A' '113A' '115A' '116A'\n",
      " '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '005A' '006A' '014A' '019A' '026A' '026B' '027A' '052A' '059A'\n",
      " '061A' '066A' '068A' '092A' '097A' '097B' '099A' '100A' '102A' '108A'\n",
      " '110A']\n",
      "Length of X_train_val:\n",
      "793\n",
      "Length of y_train_val:\n",
      "793\n",
      "Length of groups_train_val:\n",
      "793\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     483\n",
      "kitten    170\n",
      "senior    140\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     105\n",
      "senior     38\n",
      "kitten      1\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     483\n",
      "kitten    170\n",
      "senior    140\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     105\n",
      "senior     38\n",
      "kitten      1\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 966, 1: 680, 2: 560})\n",
      "Epoch 1/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.9239 - accuracy: 0.5734\n",
      "Epoch 2/1500\n",
      "69/69 [==============================] - 0s 977us/step - loss: 0.7249 - accuracy: 0.6700\n",
      "Epoch 3/1500\n",
      "69/69 [==============================] - 0s 976us/step - loss: 0.6816 - accuracy: 0.7004\n",
      "Epoch 4/1500\n",
      "69/69 [==============================] - 0s 941us/step - loss: 0.6128 - accuracy: 0.7303\n",
      "Epoch 5/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.5814 - accuracy: 0.7452\n",
      "Epoch 6/1500\n",
      "69/69 [==============================] - 0s 961us/step - loss: 0.5602 - accuracy: 0.7529\n",
      "Epoch 7/1500\n",
      "69/69 [==============================] - 0s 966us/step - loss: 0.5026 - accuracy: 0.7693\n",
      "Epoch 8/1500\n",
      "69/69 [==============================] - 0s 950us/step - loss: 0.5090 - accuracy: 0.7747\n",
      "Epoch 9/1500\n",
      "69/69 [==============================] - 0s 985us/step - loss: 0.4873 - accuracy: 0.7847\n",
      "Epoch 10/1500\n",
      "69/69 [==============================] - 0s 981us/step - loss: 0.4728 - accuracy: 0.7919\n",
      "Epoch 11/1500\n",
      "69/69 [==============================] - 0s 941us/step - loss: 0.4662 - accuracy: 0.8015\n",
      "Epoch 12/1500\n",
      "69/69 [==============================] - 0s 966us/step - loss: 0.4532 - accuracy: 0.8015\n",
      "Epoch 13/1500\n",
      "69/69 [==============================] - 0s 941us/step - loss: 0.4338 - accuracy: 0.8083\n",
      "Epoch 14/1500\n",
      "69/69 [==============================] - 0s 947us/step - loss: 0.4317 - accuracy: 0.8155\n",
      "Epoch 15/1500\n",
      "69/69 [==============================] - 0s 943us/step - loss: 0.4237 - accuracy: 0.8196\n",
      "Epoch 16/1500\n",
      "69/69 [==============================] - 0s 943us/step - loss: 0.4020 - accuracy: 0.8250\n",
      "Epoch 17/1500\n",
      "69/69 [==============================] - 0s 943us/step - loss: 0.4110 - accuracy: 0.8264\n",
      "Epoch 18/1500\n",
      "69/69 [==============================] - 0s 964us/step - loss: 0.3981 - accuracy: 0.8223\n",
      "Epoch 19/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.3833 - accuracy: 0.8323\n",
      "Epoch 20/1500\n",
      "69/69 [==============================] - 0s 990us/step - loss: 0.3730 - accuracy: 0.8418\n",
      "Epoch 21/1500\n",
      "69/69 [==============================] - 0s 980us/step - loss: 0.3816 - accuracy: 0.8354\n",
      "Epoch 22/1500\n",
      "69/69 [==============================] - 0s 979us/step - loss: 0.3761 - accuracy: 0.8432\n",
      "Epoch 23/1500\n",
      "69/69 [==============================] - 0s 980us/step - loss: 0.3784 - accuracy: 0.8400\n",
      "Epoch 24/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8536\n",
      "Epoch 25/1500\n",
      "69/69 [==============================] - 0s 947us/step - loss: 0.3590 - accuracy: 0.8509\n",
      "Epoch 26/1500\n",
      "69/69 [==============================] - 0s 948us/step - loss: 0.3445 - accuracy: 0.8477\n",
      "Epoch 27/1500\n",
      "69/69 [==============================] - 0s 934us/step - loss: 0.3451 - accuracy: 0.8513\n",
      "Epoch 28/1500\n",
      "69/69 [==============================] - 0s 943us/step - loss: 0.3394 - accuracy: 0.8531\n",
      "Epoch 29/1500\n",
      "69/69 [==============================] - 0s 965us/step - loss: 0.3309 - accuracy: 0.8586\n",
      "Epoch 30/1500\n",
      "69/69 [==============================] - 0s 961us/step - loss: 0.3151 - accuracy: 0.8672\n",
      "Epoch 31/1500\n",
      "69/69 [==============================] - 0s 993us/step - loss: 0.3096 - accuracy: 0.8694\n",
      "Epoch 32/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3046 - accuracy: 0.8672\n",
      "Epoch 33/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3164 - accuracy: 0.8762\n",
      "Epoch 34/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3008 - accuracy: 0.8762\n",
      "Epoch 35/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3001 - accuracy: 0.8749\n",
      "Epoch 36/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3080 - accuracy: 0.8726\n",
      "Epoch 37/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2910 - accuracy: 0.8762\n",
      "Epoch 38/1500\n",
      "69/69 [==============================] - 0s 964us/step - loss: 0.3007 - accuracy: 0.8762\n",
      "Epoch 39/1500\n",
      "69/69 [==============================] - 0s 962us/step - loss: 0.3003 - accuracy: 0.8821\n",
      "Epoch 40/1500\n",
      "69/69 [==============================] - 0s 966us/step - loss: 0.2816 - accuracy: 0.8844\n",
      "Epoch 41/1500\n",
      "69/69 [==============================] - 0s 950us/step - loss: 0.3074 - accuracy: 0.8781\n",
      "Epoch 42/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2847 - accuracy: 0.8812\n",
      "Epoch 43/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3081 - accuracy: 0.8704\n",
      "Epoch 44/1500\n",
      "69/69 [==============================] - 0s 993us/step - loss: 0.2825 - accuracy: 0.8753\n",
      "Epoch 45/1500\n",
      "69/69 [==============================] - 0s 943us/step - loss: 0.2683 - accuracy: 0.8812\n",
      "Epoch 46/1500\n",
      "69/69 [==============================] - 0s 958us/step - loss: 0.2582 - accuracy: 0.8926\n",
      "Epoch 47/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.2584 - accuracy: 0.8908\n",
      "Epoch 48/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.2522 - accuracy: 0.8898\n",
      "Epoch 49/1500\n",
      "69/69 [==============================] - 0s 961us/step - loss: 0.2458 - accuracy: 0.9016\n",
      "Epoch 50/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2606 - accuracy: 0.8889\n",
      "Epoch 51/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.2527 - accuracy: 0.8912\n",
      "Epoch 52/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.2396 - accuracy: 0.9025\n",
      "Epoch 53/1500\n",
      "69/69 [==============================] - 0s 941us/step - loss: 0.2522 - accuracy: 0.8898\n",
      "Epoch 54/1500\n",
      "69/69 [==============================] - 0s 953us/step - loss: 0.2412 - accuracy: 0.9048\n",
      "Epoch 55/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.2324 - accuracy: 0.9048\n",
      "Epoch 56/1500\n",
      "69/69 [==============================] - 0s 952us/step - loss: 0.2476 - accuracy: 0.9003\n",
      "Epoch 57/1500\n",
      "69/69 [==============================] - 0s 969us/step - loss: 0.2287 - accuracy: 0.9125\n",
      "Epoch 58/1500\n",
      "69/69 [==============================] - 0s 971us/step - loss: 0.2365 - accuracy: 0.8994\n",
      "Epoch 59/1500\n",
      "69/69 [==============================] - 0s 986us/step - loss: 0.2358 - accuracy: 0.9007\n",
      "Epoch 60/1500\n",
      "69/69 [==============================] - 0s 968us/step - loss: 0.2247 - accuracy: 0.9089\n",
      "Epoch 61/1500\n",
      "69/69 [==============================] - 0s 980us/step - loss: 0.2304 - accuracy: 0.9166\n",
      "Epoch 62/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2237 - accuracy: 0.9152\n",
      "Epoch 63/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2308 - accuracy: 0.9044\n",
      "Epoch 64/1500\n",
      "69/69 [==============================] - 0s 973us/step - loss: 0.2090 - accuracy: 0.9202\n",
      "Epoch 65/1500\n",
      "69/69 [==============================] - 0s 968us/step - loss: 0.2267 - accuracy: 0.9048\n",
      "Epoch 66/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.2232 - accuracy: 0.9125\n",
      "Epoch 67/1500\n",
      "69/69 [==============================] - 0s 948us/step - loss: 0.2181 - accuracy: 0.9084\n",
      "Epoch 68/1500\n",
      "69/69 [==============================] - 0s 999us/step - loss: 0.2102 - accuracy: 0.9170\n",
      "Epoch 69/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2142 - accuracy: 0.9161\n",
      "Epoch 70/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.2032 - accuracy: 0.9193\n",
      "Epoch 71/1500\n",
      "69/69 [==============================] - 0s 989us/step - loss: 0.2219 - accuracy: 0.9102\n",
      "Epoch 72/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9134\n",
      "Epoch 73/1500\n",
      "69/69 [==============================] - 0s 942us/step - loss: 0.2013 - accuracy: 0.9121\n",
      "Epoch 74/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.1994 - accuracy: 0.9229\n",
      "Epoch 75/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2072 - accuracy: 0.9089\n",
      "Epoch 76/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2198 - accuracy: 0.9107\n",
      "Epoch 77/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9229\n",
      "Epoch 78/1500\n",
      "69/69 [==============================] - 0s 961us/step - loss: 0.1918 - accuracy: 0.9238\n",
      "Epoch 79/1500\n",
      "69/69 [==============================] - 0s 993us/step - loss: 0.1921 - accuracy: 0.9238\n",
      "Epoch 80/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9220\n",
      "Epoch 81/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1923 - accuracy: 0.9248\n",
      "Epoch 82/1500\n",
      "69/69 [==============================] - 0s 975us/step - loss: 0.2016 - accuracy: 0.9184\n",
      "Epoch 83/1500\n",
      "69/69 [==============================] - 0s 967us/step - loss: 0.2111 - accuracy: 0.9107\n",
      "Epoch 84/1500\n",
      "69/69 [==============================] - 0s 943us/step - loss: 0.1981 - accuracy: 0.9288\n",
      "Epoch 85/1500\n",
      "69/69 [==============================] - 0s 998us/step - loss: 0.1788 - accuracy: 0.9275\n",
      "Epoch 86/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.9180\n",
      "Epoch 87/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9211\n",
      "Epoch 88/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.1882 - accuracy: 0.9257\n",
      "Epoch 89/1500\n",
      "69/69 [==============================] - 0s 984us/step - loss: 0.1712 - accuracy: 0.9275\n",
      "Epoch 90/1500\n",
      "69/69 [==============================] - 0s 946us/step - loss: 0.1841 - accuracy: 0.9297\n",
      "Epoch 91/1500\n",
      "69/69 [==============================] - 0s 970us/step - loss: 0.1864 - accuracy: 0.9225\n",
      "Epoch 92/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.1879 - accuracy: 0.9238\n",
      "Epoch 93/1500\n",
      "69/69 [==============================] - 0s 991us/step - loss: 0.1718 - accuracy: 0.9329\n",
      "Epoch 94/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1796 - accuracy: 0.9288\n",
      "Epoch 95/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.9275\n",
      "Epoch 96/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9356\n",
      "Epoch 97/1500\n",
      "69/69 [==============================] - 0s 968us/step - loss: 0.1833 - accuracy: 0.9252\n",
      "Epoch 98/1500\n",
      "69/69 [==============================] - 0s 989us/step - loss: 0.1575 - accuracy: 0.9374\n",
      "Epoch 99/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9379\n",
      "Epoch 100/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9356\n",
      "Epoch 101/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1754 - accuracy: 0.9293\n",
      "Epoch 102/1500\n",
      "69/69 [==============================] - 0s 998us/step - loss: 0.1782 - accuracy: 0.9334\n",
      "Epoch 103/1500\n",
      "69/69 [==============================] - 0s 965us/step - loss: 0.1656 - accuracy: 0.9316\n",
      "Epoch 104/1500\n",
      "69/69 [==============================] - 0s 972us/step - loss: 0.1700 - accuracy: 0.9347\n",
      "Epoch 105/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9297\n",
      "Epoch 106/1500\n",
      "69/69 [==============================] - 0s 982us/step - loss: 0.1571 - accuracy: 0.9329\n",
      "Epoch 107/1500\n",
      "69/69 [==============================] - 0s 961us/step - loss: 0.1738 - accuracy: 0.9279\n",
      "Epoch 108/1500\n",
      "69/69 [==============================] - 0s 999us/step - loss: 0.1654 - accuracy: 0.9370\n",
      "Epoch 109/1500\n",
      "69/69 [==============================] - 0s 992us/step - loss: 0.1602 - accuracy: 0.9420\n",
      "Epoch 110/1500\n",
      "69/69 [==============================] - 0s 946us/step - loss: 0.1593 - accuracy: 0.9383\n",
      "Epoch 111/1500\n",
      "69/69 [==============================] - 0s 957us/step - loss: 0.1604 - accuracy: 0.9397\n",
      "Epoch 112/1500\n",
      "69/69 [==============================] - 0s 950us/step - loss: 0.1502 - accuracy: 0.9393\n",
      "Epoch 113/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.1525 - accuracy: 0.9420\n",
      "Epoch 114/1500\n",
      "69/69 [==============================] - 0s 971us/step - loss: 0.1578 - accuracy: 0.9361\n",
      "Epoch 115/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9356\n",
      "Epoch 116/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9343\n",
      "Epoch 117/1500\n",
      "69/69 [==============================] - 0s 975us/step - loss: 0.1445 - accuracy: 0.9402\n",
      "Epoch 118/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1569 - accuracy: 0.9406\n",
      "Epoch 119/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9424\n",
      "Epoch 120/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9424\n",
      "Epoch 121/1500\n",
      "69/69 [==============================] - 0s 980us/step - loss: 0.1576 - accuracy: 0.9424\n",
      "Epoch 122/1500\n",
      "69/69 [==============================] - 0s 987us/step - loss: 0.1722 - accuracy: 0.9388\n",
      "Epoch 123/1500\n",
      "69/69 [==============================] - 0s 970us/step - loss: 0.1448 - accuracy: 0.9479\n",
      "Epoch 124/1500\n",
      "69/69 [==============================] - 0s 957us/step - loss: 0.1505 - accuracy: 0.9370\n",
      "Epoch 125/1500\n",
      "69/69 [==============================] - 0s 973us/step - loss: 0.1613 - accuracy: 0.9388\n",
      "Epoch 126/1500\n",
      "69/69 [==============================] - 0s 951us/step - loss: 0.1691 - accuracy: 0.9374\n",
      "Epoch 127/1500\n",
      "69/69 [==============================] - 0s 964us/step - loss: 0.1424 - accuracy: 0.9483\n",
      "Epoch 128/1500\n",
      "69/69 [==============================] - 0s 965us/step - loss: 0.1362 - accuracy: 0.9506\n",
      "Epoch 129/1500\n",
      "69/69 [==============================] - 0s 948us/step - loss: 0.1372 - accuracy: 0.9433\n",
      "Epoch 130/1500\n",
      "69/69 [==============================] - 0s 984us/step - loss: 0.1427 - accuracy: 0.9383\n",
      "Epoch 131/1500\n",
      "69/69 [==============================] - 0s 987us/step - loss: 0.1487 - accuracy: 0.9383\n",
      "Epoch 132/1500\n",
      "69/69 [==============================] - 0s 968us/step - loss: 0.1396 - accuracy: 0.9447\n",
      "Epoch 133/1500\n",
      "69/69 [==============================] - 0s 960us/step - loss: 0.1369 - accuracy: 0.9451\n",
      "Epoch 134/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.1554 - accuracy: 0.9424\n",
      "Epoch 135/1500\n",
      "69/69 [==============================] - 0s 949us/step - loss: 0.1406 - accuracy: 0.9438\n",
      "Epoch 136/1500\n",
      "69/69 [==============================] - 0s 951us/step - loss: 0.1349 - accuracy: 0.9465\n",
      "Epoch 137/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9533\n",
      "Epoch 138/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9560\n",
      "Epoch 139/1500\n",
      "69/69 [==============================] - 0s 995us/step - loss: 0.1208 - accuracy: 0.9533\n",
      "Epoch 140/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9438\n",
      "Epoch 141/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.1309 - accuracy: 0.9501\n",
      "Epoch 142/1500\n",
      "69/69 [==============================] - 0s 960us/step - loss: 0.1322 - accuracy: 0.9501\n",
      "Epoch 143/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1320 - accuracy: 0.9519\n",
      "Epoch 144/1500\n",
      "69/69 [==============================] - 0s 958us/step - loss: 0.1214 - accuracy: 0.9547\n",
      "Epoch 145/1500\n",
      "69/69 [==============================] - 0s 945us/step - loss: 0.1282 - accuracy: 0.9510\n",
      "Epoch 146/1500\n",
      "69/69 [==============================] - 0s 970us/step - loss: 0.1367 - accuracy: 0.9456\n",
      "Epoch 147/1500\n",
      "69/69 [==============================] - 0s 961us/step - loss: 0.1307 - accuracy: 0.9497\n",
      "Epoch 148/1500\n",
      "69/69 [==============================] - 0s 960us/step - loss: 0.1431 - accuracy: 0.9438\n",
      "Epoch 149/1500\n",
      "69/69 [==============================] - 0s 957us/step - loss: 0.1338 - accuracy: 0.9501\n",
      "Epoch 150/1500\n",
      "69/69 [==============================] - 0s 953us/step - loss: 0.1281 - accuracy: 0.9515\n",
      "Epoch 151/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9533\n",
      "Epoch 152/1500\n",
      "69/69 [==============================] - 0s 958us/step - loss: 0.1370 - accuracy: 0.9483\n",
      "Epoch 153/1500\n",
      "69/69 [==============================] - 0s 951us/step - loss: 0.1207 - accuracy: 0.9556\n",
      "Epoch 154/1500\n",
      "69/69 [==============================] - 0s 946us/step - loss: 0.1327 - accuracy: 0.9461\n",
      "Epoch 155/1500\n",
      "69/69 [==============================] - 0s 950us/step - loss: 0.1355 - accuracy: 0.9451\n",
      "Epoch 156/1500\n",
      "69/69 [==============================] - 0s 966us/step - loss: 0.1256 - accuracy: 0.9501\n",
      "Epoch 157/1500\n",
      "69/69 [==============================] - 0s 985us/step - loss: 0.1258 - accuracy: 0.9519\n",
      "Epoch 158/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9587\n",
      "Epoch 159/1500\n",
      "69/69 [==============================] - 0s 956us/step - loss: 0.1089 - accuracy: 0.9619\n",
      "Epoch 160/1500\n",
      "69/69 [==============================] - 0s 948us/step - loss: 0.1168 - accuracy: 0.9560\n",
      "Epoch 161/1500\n",
      "69/69 [==============================] - 0s 951us/step - loss: 0.1147 - accuracy: 0.9578\n",
      "Epoch 162/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.1178 - accuracy: 0.9538\n",
      "Epoch 163/1500\n",
      "69/69 [==============================] - 0s 986us/step - loss: 0.1270 - accuracy: 0.9519\n",
      "Epoch 164/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9465\n",
      "Epoch 165/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9515\n",
      "Epoch 166/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9665\n",
      "Epoch 167/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9560\n",
      "Epoch 168/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1244 - accuracy: 0.9542\n",
      "Epoch 169/1500\n",
      "69/69 [==============================] - 0s 949us/step - loss: 0.1248 - accuracy: 0.9529\n",
      "Epoch 170/1500\n",
      "69/69 [==============================] - 0s 973us/step - loss: 0.1180 - accuracy: 0.9542\n",
      "Epoch 171/1500\n",
      "69/69 [==============================] - 0s 982us/step - loss: 0.1159 - accuracy: 0.9565\n",
      "Epoch 172/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9610\n",
      "Epoch 173/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9551\n",
      "Epoch 174/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9547\n",
      "Epoch 175/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9560\n",
      "Epoch 176/1500\n",
      "69/69 [==============================] - 0s 998us/step - loss: 0.1158 - accuracy: 0.9569\n",
      "Epoch 177/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9474\n",
      "Epoch 178/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9538\n",
      "Epoch 179/1500\n",
      "69/69 [==============================] - 0s 997us/step - loss: 0.1276 - accuracy: 0.9510\n",
      "Epoch 180/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9565\n",
      "Epoch 181/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9560\n",
      "Epoch 182/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9583\n",
      "Epoch 183/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9592\n",
      "Epoch 184/1500\n",
      "69/69 [==============================] - 0s 994us/step - loss: 0.1048 - accuracy: 0.9606\n",
      "Epoch 185/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9624\n",
      "Epoch 186/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9492\n",
      "Epoch 187/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9542\n",
      "Epoch 188/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9583\n",
      "Epoch 189/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9547\n",
      "Epoch 190/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9592\n",
      "Epoch 191/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9583\n",
      "Epoch 192/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9538\n",
      "Epoch 193/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9619\n",
      "Epoch 194/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9597\n",
      "Epoch 195/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9538\n",
      "Epoch 196/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9651\n",
      "Epoch 197/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9633\n",
      "Epoch 198/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9542\n",
      "Epoch 199/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9592\n",
      "Epoch 200/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9674\n",
      "Epoch 201/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9610\n",
      "Epoch 202/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9542\n",
      "Epoch 203/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9642\n",
      "Epoch 204/1500\n",
      "69/69 [==============================] - 0s 987us/step - loss: 0.1028 - accuracy: 0.9628\n",
      "Epoch 205/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9678\n",
      "Epoch 206/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9669\n",
      "Epoch 207/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9674\n",
      "Epoch 208/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9646\n",
      "Epoch 209/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9606\n",
      "Epoch 210/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9597\n",
      "Epoch 211/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9538\n",
      "Epoch 212/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9660\n",
      "Epoch 213/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9624\n",
      "Epoch 214/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9655\n",
      "Epoch 215/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9723\n",
      "Epoch 216/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9592\n",
      "Epoch 217/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9583\n",
      "Epoch 218/1500\n",
      "69/69 [==============================] - 0s 960us/step - loss: 0.1012 - accuracy: 0.9628\n",
      "Epoch 219/1500\n",
      "69/69 [==============================] - 0s 972us/step - loss: 0.1068 - accuracy: 0.9610\n",
      "Epoch 220/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9597\n",
      "Epoch 221/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9646\n",
      "Epoch 222/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9569\n",
      "Epoch 223/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9651\n",
      "Epoch 224/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9628\n",
      "Epoch 225/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9578\n",
      "Epoch 226/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9597\n",
      "Epoch 227/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.9669\n",
      "Epoch 228/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9642\n",
      "Epoch 229/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9610\n",
      "Epoch 230/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9569\n",
      "Epoch 231/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9660\n",
      "Epoch 232/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9642\n",
      "Epoch 233/1500\n",
      "69/69 [==============================] - 0s 965us/step - loss: 0.0945 - accuracy: 0.9651\n",
      "Epoch 234/1500\n",
      "69/69 [==============================] - 0s 976us/step - loss: 0.0980 - accuracy: 0.9665\n",
      "Epoch 235/1500\n",
      "69/69 [==============================] - 0s 990us/step - loss: 0.1009 - accuracy: 0.9606\n",
      "Epoch 236/1500\n",
      "69/69 [==============================] - 0s 958us/step - loss: 0.1113 - accuracy: 0.9565\n",
      "Epoch 237/1500\n",
      "69/69 [==============================] - 0s 970us/step - loss: 0.0869 - accuracy: 0.9669\n",
      "Epoch 238/1500\n",
      "69/69 [==============================] - 0s 973us/step - loss: 0.0975 - accuracy: 0.9619\n",
      "Epoch 239/1500\n",
      "69/69 [==============================] - 0s 982us/step - loss: 0.0892 - accuracy: 0.9633\n",
      "Epoch 240/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.9687\n",
      "Epoch 241/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9746\n",
      "Epoch 242/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.9660\n",
      "Epoch 243/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.9646\n",
      "Epoch 244/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9678\n",
      "Epoch 245/1500\n",
      "69/69 [==============================] - 0s 998us/step - loss: 0.0826 - accuracy: 0.9696\n",
      "Epoch 246/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9646\n",
      "Epoch 247/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9606\n",
      "Epoch 248/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9678\n",
      "Epoch 249/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9606\n",
      "Epoch 250/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9565\n",
      "Epoch 251/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.9615\n",
      "Epoch 252/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9642\n",
      "Epoch 253/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9619\n",
      "Epoch 254/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9755\n",
      "Epoch 255/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9719\n",
      "Epoch 256/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9755\n",
      "Epoch 257/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9624\n",
      "Epoch 258/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9665\n",
      "Epoch 259/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9728\n",
      "Epoch 260/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.9637\n",
      "Epoch 261/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9601\n",
      "Epoch 262/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9692\n",
      "Epoch 263/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.0903 - accuracy: 0.9637\n",
      "Epoch 264/1500\n",
      "69/69 [==============================] - 0s 944us/step - loss: 0.0824 - accuracy: 0.9687\n",
      "Epoch 265/1500\n",
      "69/69 [==============================] - 0s 973us/step - loss: 0.1071 - accuracy: 0.9569\n",
      "Epoch 266/1500\n",
      "69/69 [==============================] - 0s 974us/step - loss: 0.0773 - accuracy: 0.9728\n",
      "Epoch 267/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9642\n",
      "Epoch 268/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9601\n",
      "Epoch 269/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.0853 - accuracy: 0.9665\n",
      "Epoch 270/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9660\n",
      "Epoch 271/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9687\n",
      "Epoch 272/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.9746\n",
      "Epoch 273/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9687\n",
      "Epoch 274/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9601\n",
      "Epoch 275/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9601\n",
      "Epoch 276/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9669\n",
      "Epoch 277/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9710\n",
      "Epoch 278/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9642\n",
      "Epoch 279/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9619\n",
      "Epoch 280/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9719\n",
      "Epoch 281/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9678\n",
      "Epoch 282/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9660\n",
      "Epoch 283/1500\n",
      "69/69 [==============================] - 0s 999us/step - loss: 0.1016 - accuracy: 0.9606\n",
      "Epoch 284/1500\n",
      "51/69 [=====================>........] - ETA: 0s - loss: 0.0912 - accuracy: 0.9602Restoring model weights from the end of the best epoch: 254.\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9633\n",
      "Epoch 284: early stopping\n",
      "5/5 [==============================] - 0s 890us/step - loss: 0.7777 - accuracy: 0.7222\n",
      "5/5 [==============================] - 0s 804us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (16/21)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 144, Predictions: 144, Actuals: 144, Gender: 144\n",
      "Final Test Results - Loss: 0.7776740193367004, Accuracy: 0.7222222089767456, Precision: 0.45259293970634173, Recall: 0.4588972431077694, F1 Score: 0.4543186025919665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[81  3 21]\n",
      " [ 1  0  0]\n",
      " [15  0 23]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "097B    14\n",
      "001A    14\n",
      "042A    14\n",
      "059A    14\n",
      "028A    13\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "051A    12\n",
      "063A    11\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "071A    10\n",
      "014B    10\n",
      "005A    10\n",
      "040A    10\n",
      "072A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "065A     9\n",
      "015A     9\n",
      "013B     8\n",
      "010A     8\n",
      "095A     8\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "027A     7\n",
      "108A     6\n",
      "007A     6\n",
      "109A     6\n",
      "008A     6\n",
      "053A     6\n",
      "075A     5\n",
      "023B     5\n",
      "044A     5\n",
      "025C     5\n",
      "070A     5\n",
      "026A     4\n",
      "052A     4\n",
      "104A     4\n",
      "003A     4\n",
      "035A     4\n",
      "009A     4\n",
      "012A     3\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "014A     3\n",
      "038A     2\n",
      "061A     2\n",
      "011A     2\n",
      "087A     2\n",
      "069A     2\n",
      "025B     2\n",
      "032A     2\n",
      "054A     2\n",
      "018A     2\n",
      "102A     2\n",
      "110A     1\n",
      "091A     1\n",
      "100A     1\n",
      "090A     1\n",
      "115A     1\n",
      "043A     1\n",
      "019B     1\n",
      "004A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "074A    25\n",
      "067A    19\n",
      "002A    13\n",
      "016A    10\n",
      "022A     9\n",
      "094A     8\n",
      "117A     7\n",
      "037A     6\n",
      "023A     6\n",
      "021A     5\n",
      "034A     5\n",
      "105A     4\n",
      "062A     4\n",
      "113A     3\n",
      "060A     3\n",
      "093A     2\n",
      "076A     1\n",
      "073A     1\n",
      "048A     1\n",
      "088A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    329\n",
      "M    257\n",
      "F    178\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    80\n",
      "F    74\n",
      "X    19\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 097B, 028...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [097A, 057A, 106A, 104A, 055A, 059A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 074A, 067A, 062A, 022A, 034A, 002A, 023...\n",
      "kitten                                               [048A]\n",
      "senior                 [093A, 113A, 117A, 016A, 094A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 59, 'kitten': 15, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 15, 'kitten': 1, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002B' '003A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B'\n",
      " '020A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '043A' '044A' '045A' '046A' '047A' '049A' '050A' '051A' '051B'\n",
      " '052A' '053A' '054A' '055A' '056A' '057A' '058A' '059A' '061A' '063A'\n",
      " '064A' '065A' '066A' '068A' '069A' '070A' '071A' '072A' '075A' '087A'\n",
      " '090A' '091A' '092A' '095A' '096A' '097A' '097B' '099A' '100A' '101A'\n",
      " '102A' '103A' '104A' '106A' '108A' '109A' '110A' '111A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002A' '016A' '021A' '022A' '023A' '024A' '034A' '037A' '048A'\n",
      " '060A' '062A' '067A' '073A' '074A' '076A' '088A' '093A' '094A' '105A'\n",
      " '113A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'027A'}\n",
      "Moved to Test Set:\n",
      "{'027A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '006A' '007A' '008A'\n",
      " '009A' '010A' '011A' '012A' '013B' '014A' '014B' '015A' '018A' '019A'\n",
      " '019B' '020A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '031A' '032A' '033A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '043A' '044A' '045A' '046A' '047A' '049A' '050A' '051A' '051B'\n",
      " '052A' '053A' '054A' '055A' '056A' '057A' '058A' '059A' '061A' '063A'\n",
      " '064A' '065A' '066A' '068A' '069A' '070A' '071A' '072A' '075A' '087A'\n",
      " '090A' '091A' '092A' '095A' '096A' '097A' '097B' '099A' '100A' '101A'\n",
      " '102A' '103A' '104A' '106A' '108A' '109A' '110A' '111A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '016A' '021A' '022A' '023A' '024A' '027A' '034A' '037A' '048A'\n",
      " '060A' '062A' '067A' '073A' '074A' '076A' '088A' '093A' '094A' '105A'\n",
      " '113A' '117A']\n",
      "Length of X_train_val:\n",
      "796\n",
      "Length of y_train_val:\n",
      "796\n",
      "Length of groups_train_val:\n",
      "796\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     447\n",
      "kitten    170\n",
      "senior    147\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     141\n",
      "senior     31\n",
      "kitten      1\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     479\n",
      "kitten    170\n",
      "senior    147\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     109\n",
      "senior     31\n",
      "kitten      1\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 958, 1: 680, 2: 588})\n",
      "Epoch 1/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8857 - accuracy: 0.5979\n",
      "Epoch 2/1500\n",
      "70/70 [==============================] - 0s 989us/step - loss: 0.6898 - accuracy: 0.6851\n",
      "Epoch 3/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6465 - accuracy: 0.7107\n",
      "Epoch 4/1500\n",
      "70/70 [==============================] - 0s 935us/step - loss: 0.5896 - accuracy: 0.7273\n",
      "Epoch 5/1500\n",
      "70/70 [==============================] - 0s 953us/step - loss: 0.5574 - accuracy: 0.7507\n",
      "Epoch 6/1500\n",
      "70/70 [==============================] - 0s 954us/step - loss: 0.5368 - accuracy: 0.7610\n",
      "Epoch 7/1500\n",
      "70/70 [==============================] - 0s 955us/step - loss: 0.5058 - accuracy: 0.7808\n",
      "Epoch 8/1500\n",
      "70/70 [==============================] - 0s 936us/step - loss: 0.5080 - accuracy: 0.7772\n",
      "Epoch 9/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4976 - accuracy: 0.7826\n",
      "Epoch 10/1500\n",
      "70/70 [==============================] - 0s 961us/step - loss: 0.4577 - accuracy: 0.7884\n",
      "Epoch 11/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4726 - accuracy: 0.7987\n",
      "Epoch 12/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4530 - accuracy: 0.8046\n",
      "Epoch 13/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4224 - accuracy: 0.8122\n",
      "Epoch 14/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4369 - accuracy: 0.8122\n",
      "Epoch 15/1500\n",
      "70/70 [==============================] - 0s 982us/step - loss: 0.4018 - accuracy: 0.8181\n",
      "Epoch 16/1500\n",
      "70/70 [==============================] - 0s 950us/step - loss: 0.4105 - accuracy: 0.8194\n",
      "Epoch 17/1500\n",
      "70/70 [==============================] - 0s 972us/step - loss: 0.3910 - accuracy: 0.8306\n",
      "Epoch 18/1500\n",
      "70/70 [==============================] - 0s 930us/step - loss: 0.3982 - accuracy: 0.8181\n",
      "Epoch 19/1500\n",
      "70/70 [==============================] - 0s 972us/step - loss: 0.3956 - accuracy: 0.8266\n",
      "Epoch 20/1500\n",
      "70/70 [==============================] - 0s 981us/step - loss: 0.3831 - accuracy: 0.8315\n",
      "Epoch 21/1500\n",
      "70/70 [==============================] - 0s 964us/step - loss: 0.3699 - accuracy: 0.8342\n",
      "Epoch 22/1500\n",
      "70/70 [==============================] - 0s 953us/step - loss: 0.3593 - accuracy: 0.8423\n",
      "Epoch 23/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3472 - accuracy: 0.8531\n",
      "Epoch 24/1500\n",
      "70/70 [==============================] - 0s 927us/step - loss: 0.3486 - accuracy: 0.8535\n",
      "Epoch 25/1500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.3419 - accuracy: 0.8509\n",
      "Epoch 26/1500\n",
      "70/70 [==============================] - 0s 927us/step - loss: 0.3380 - accuracy: 0.8571\n",
      "Epoch 27/1500\n",
      "70/70 [==============================] - 0s 931us/step - loss: 0.3339 - accuracy: 0.8567\n",
      "Epoch 28/1500\n",
      "70/70 [==============================] - 0s 927us/step - loss: 0.3326 - accuracy: 0.8518\n",
      "Epoch 29/1500\n",
      "70/70 [==============================] - 0s 924us/step - loss: 0.3151 - accuracy: 0.8625\n",
      "Epoch 30/1500\n",
      "70/70 [==============================] - 0s 929us/step - loss: 0.3296 - accuracy: 0.8576\n",
      "Epoch 31/1500\n",
      "70/70 [==============================] - 0s 934us/step - loss: 0.3155 - accuracy: 0.8621\n",
      "Epoch 32/1500\n",
      "70/70 [==============================] - 0s 927us/step - loss: 0.3101 - accuracy: 0.8625\n",
      "Epoch 33/1500\n",
      "70/70 [==============================] - 0s 936us/step - loss: 0.3105 - accuracy: 0.8679\n",
      "Epoch 34/1500\n",
      "70/70 [==============================] - 0s 946us/step - loss: 0.3007 - accuracy: 0.8702\n",
      "Epoch 35/1500\n",
      "70/70 [==============================] - 0s 939us/step - loss: 0.3055 - accuracy: 0.8724\n",
      "Epoch 36/1500\n",
      "70/70 [==============================] - 0s 949us/step - loss: 0.2946 - accuracy: 0.8756\n",
      "Epoch 37/1500\n",
      "70/70 [==============================] - 0s 936us/step - loss: 0.2828 - accuracy: 0.8787\n",
      "Epoch 38/1500\n",
      "70/70 [==============================] - 0s 932us/step - loss: 0.2919 - accuracy: 0.8778\n",
      "Epoch 39/1500\n",
      "70/70 [==============================] - 0s 934us/step - loss: 0.2930 - accuracy: 0.8706\n",
      "Epoch 40/1500\n",
      "70/70 [==============================] - 0s 924us/step - loss: 0.2852 - accuracy: 0.8801\n",
      "Epoch 41/1500\n",
      "70/70 [==============================] - 0s 927us/step - loss: 0.2893 - accuracy: 0.8765\n",
      "Epoch 42/1500\n",
      "70/70 [==============================] - 0s 958us/step - loss: 0.2750 - accuracy: 0.8796\n",
      "Epoch 43/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2765 - accuracy: 0.8792\n",
      "Epoch 44/1500\n",
      "70/70 [==============================] - 0s 945us/step - loss: 0.2675 - accuracy: 0.8886\n",
      "Epoch 45/1500\n",
      "70/70 [==============================] - 0s 931us/step - loss: 0.2685 - accuracy: 0.8872\n",
      "Epoch 46/1500\n",
      "70/70 [==============================] - 0s 947us/step - loss: 0.2870 - accuracy: 0.8760\n",
      "Epoch 47/1500\n",
      "70/70 [==============================] - 0s 938us/step - loss: 0.2842 - accuracy: 0.8756\n",
      "Epoch 48/1500\n",
      "70/70 [==============================] - 0s 944us/step - loss: 0.2603 - accuracy: 0.8886\n",
      "Epoch 49/1500\n",
      "70/70 [==============================] - 0s 942us/step - loss: 0.2704 - accuracy: 0.8854\n",
      "Epoch 50/1500\n",
      "70/70 [==============================] - 0s 939us/step - loss: 0.2515 - accuracy: 0.8944\n",
      "Epoch 51/1500\n",
      "70/70 [==============================] - 0s 944us/step - loss: 0.2667 - accuracy: 0.8886\n",
      "Epoch 52/1500\n",
      "70/70 [==============================] - 0s 938us/step - loss: 0.2473 - accuracy: 0.8922\n",
      "Epoch 53/1500\n",
      "70/70 [==============================] - 0s 952us/step - loss: 0.2565 - accuracy: 0.8895\n",
      "Epoch 54/1500\n",
      "70/70 [==============================] - 0s 989us/step - loss: 0.2512 - accuracy: 0.8940\n",
      "Epoch 55/1500\n",
      "70/70 [==============================] - 0s 983us/step - loss: 0.2475 - accuracy: 0.8881\n",
      "Epoch 56/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2359 - accuracy: 0.9021\n",
      "Epoch 57/1500\n",
      "70/70 [==============================] - 0s 978us/step - loss: 0.2399 - accuracy: 0.8998\n",
      "Epoch 58/1500\n",
      "70/70 [==============================] - 0s 930us/step - loss: 0.2294 - accuracy: 0.9070\n",
      "Epoch 59/1500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.2212 - accuracy: 0.9039\n",
      "Epoch 60/1500\n",
      "70/70 [==============================] - 0s 987us/step - loss: 0.2520 - accuracy: 0.8980\n",
      "Epoch 61/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2445 - accuracy: 0.8971\n",
      "Epoch 62/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2273 - accuracy: 0.8998\n",
      "Epoch 63/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2221 - accuracy: 0.9119\n",
      "Epoch 64/1500\n",
      "70/70 [==============================] - 0s 958us/step - loss: 0.2270 - accuracy: 0.9043\n",
      "Epoch 65/1500\n",
      "70/70 [==============================] - 0s 947us/step - loss: 0.2340 - accuracy: 0.9030\n",
      "Epoch 66/1500\n",
      "70/70 [==============================] - 0s 947us/step - loss: 0.2268 - accuracy: 0.9102\n",
      "Epoch 67/1500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.2193 - accuracy: 0.9102\n",
      "Epoch 68/1500\n",
      "70/70 [==============================] - 0s 948us/step - loss: 0.2251 - accuracy: 0.9111\n",
      "Epoch 69/1500\n",
      "70/70 [==============================] - 0s 973us/step - loss: 0.2324 - accuracy: 0.9034\n",
      "Epoch 70/1500\n",
      "70/70 [==============================] - 0s 942us/step - loss: 0.2195 - accuracy: 0.9070\n",
      "Epoch 71/1500\n",
      "70/70 [==============================] - 0s 988us/step - loss: 0.2278 - accuracy: 0.9079\n",
      "Epoch 72/1500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.2296 - accuracy: 0.9079\n",
      "Epoch 73/1500\n",
      "70/70 [==============================] - 0s 961us/step - loss: 0.2096 - accuracy: 0.9142\n",
      "Epoch 74/1500\n",
      "70/70 [==============================] - 0s 954us/step - loss: 0.2039 - accuracy: 0.9196\n",
      "Epoch 75/1500\n",
      "70/70 [==============================] - 0s 930us/step - loss: 0.2161 - accuracy: 0.9111\n",
      "Epoch 76/1500\n",
      "70/70 [==============================] - 0s 961us/step - loss: 0.2014 - accuracy: 0.9151\n",
      "Epoch 77/1500\n",
      "70/70 [==============================] - 0s 930us/step - loss: 0.2133 - accuracy: 0.9093\n",
      "Epoch 78/1500\n",
      "70/70 [==============================] - 0s 944us/step - loss: 0.2112 - accuracy: 0.9106\n",
      "Epoch 79/1500\n",
      "70/70 [==============================] - 0s 935us/step - loss: 0.2013 - accuracy: 0.9200\n",
      "Epoch 80/1500\n",
      "70/70 [==============================] - 0s 958us/step - loss: 0.1940 - accuracy: 0.9164\n",
      "Epoch 81/1500\n",
      "70/70 [==============================] - 0s 941us/step - loss: 0.1983 - accuracy: 0.9151\n",
      "Epoch 82/1500\n",
      "70/70 [==============================] - 0s 960us/step - loss: 0.1872 - accuracy: 0.9218\n",
      "Epoch 83/1500\n",
      "70/70 [==============================] - 0s 966us/step - loss: 0.1847 - accuracy: 0.9241\n",
      "Epoch 84/1500\n",
      "70/70 [==============================] - 0s 938us/step - loss: 0.1914 - accuracy: 0.9272\n",
      "Epoch 85/1500\n",
      "70/70 [==============================] - 0s 939us/step - loss: 0.1862 - accuracy: 0.9209\n",
      "Epoch 86/1500\n",
      "70/70 [==============================] - 0s 947us/step - loss: 0.1774 - accuracy: 0.9326\n",
      "Epoch 87/1500\n",
      "70/70 [==============================] - 0s 960us/step - loss: 0.1891 - accuracy: 0.9227\n",
      "Epoch 88/1500\n",
      "70/70 [==============================] - 0s 959us/step - loss: 0.1637 - accuracy: 0.9322\n",
      "Epoch 89/1500\n",
      "70/70 [==============================] - 0s 940us/step - loss: 0.1996 - accuracy: 0.9151\n",
      "Epoch 90/1500\n",
      "70/70 [==============================] - 0s 924us/step - loss: 0.1739 - accuracy: 0.9272\n",
      "Epoch 91/1500\n",
      "70/70 [==============================] - 0s 930us/step - loss: 0.2003 - accuracy: 0.9232\n",
      "Epoch 92/1500\n",
      "70/70 [==============================] - 0s 988us/step - loss: 0.1720 - accuracy: 0.9272\n",
      "Epoch 93/1500\n",
      "70/70 [==============================] - 0s 949us/step - loss: 0.1782 - accuracy: 0.9268\n",
      "Epoch 94/1500\n",
      "70/70 [==============================] - 0s 953us/step - loss: 0.1756 - accuracy: 0.9286\n",
      "Epoch 95/1500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.1706 - accuracy: 0.9385\n",
      "Epoch 96/1500\n",
      "70/70 [==============================] - 0s 959us/step - loss: 0.1755 - accuracy: 0.9272\n",
      "Epoch 97/1500\n",
      "70/70 [==============================] - 0s 965us/step - loss: 0.1736 - accuracy: 0.9299\n",
      "Epoch 98/1500\n",
      "70/70 [==============================] - 0s 963us/step - loss: 0.1631 - accuracy: 0.9349\n",
      "Epoch 99/1500\n",
      "70/70 [==============================] - 0s 950us/step - loss: 0.1700 - accuracy: 0.9304\n",
      "Epoch 100/1500\n",
      "70/70 [==============================] - 0s 931us/step - loss: 0.1770 - accuracy: 0.9299\n",
      "Epoch 101/1500\n",
      "70/70 [==============================] - 0s 951us/step - loss: 0.1697 - accuracy: 0.9353\n",
      "Epoch 102/1500\n",
      "70/70 [==============================] - 0s 963us/step - loss: 0.1563 - accuracy: 0.9394\n",
      "Epoch 103/1500\n",
      "70/70 [==============================] - 0s 934us/step - loss: 0.1630 - accuracy: 0.9344\n",
      "Epoch 104/1500\n",
      "70/70 [==============================] - 0s 939us/step - loss: 0.1657 - accuracy: 0.9304\n",
      "Epoch 105/1500\n",
      "70/70 [==============================] - 0s 937us/step - loss: 0.1699 - accuracy: 0.9358\n",
      "Epoch 106/1500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.1644 - accuracy: 0.9317\n",
      "Epoch 107/1500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.1598 - accuracy: 0.9376\n",
      "Epoch 108/1500\n",
      "70/70 [==============================] - 0s 949us/step - loss: 0.1715 - accuracy: 0.9322\n",
      "Epoch 109/1500\n",
      "70/70 [==============================] - 0s 941us/step - loss: 0.1706 - accuracy: 0.9317\n",
      "Epoch 110/1500\n",
      "70/70 [==============================] - 0s 949us/step - loss: 0.1731 - accuracy: 0.9299\n",
      "Epoch 111/1500\n",
      "70/70 [==============================] - 0s 930us/step - loss: 0.1684 - accuracy: 0.9394\n",
      "Epoch 112/1500\n",
      "70/70 [==============================] - 0s 978us/step - loss: 0.1782 - accuracy: 0.9277\n",
      "Epoch 113/1500\n",
      "70/70 [==============================] - 0s 989us/step - loss: 0.1663 - accuracy: 0.9322\n",
      "Epoch 114/1500\n",
      "70/70 [==============================] - 0s 921us/step - loss: 0.1642 - accuracy: 0.9335\n",
      "Epoch 115/1500\n",
      "70/70 [==============================] - 0s 939us/step - loss: 0.1603 - accuracy: 0.9376\n",
      "Epoch 116/1500\n",
      "70/70 [==============================] - 0s 947us/step - loss: 0.1614 - accuracy: 0.9340\n",
      "Epoch 117/1500\n",
      "70/70 [==============================] - 0s 954us/step - loss: 0.1556 - accuracy: 0.9371\n",
      "Epoch 118/1500\n",
      "70/70 [==============================] - 0s 967us/step - loss: 0.1502 - accuracy: 0.9367\n",
      "Epoch 119/1500\n",
      "70/70 [==============================] - 0s 947us/step - loss: 0.1526 - accuracy: 0.9420\n",
      "Epoch 120/1500\n",
      "70/70 [==============================] - 0s 952us/step - loss: 0.1520 - accuracy: 0.9367\n",
      "Epoch 121/1500\n",
      "70/70 [==============================] - 0s 950us/step - loss: 0.1492 - accuracy: 0.9434\n",
      "Epoch 122/1500\n",
      "70/70 [==============================] - 0s 966us/step - loss: 0.1533 - accuracy: 0.9438\n",
      "Epoch 123/1500\n",
      "70/70 [==============================] - 0s 936us/step - loss: 0.1552 - accuracy: 0.9349\n",
      "Epoch 124/1500\n",
      "70/70 [==============================] - 0s 934us/step - loss: 0.1527 - accuracy: 0.9416\n",
      "Epoch 125/1500\n",
      "70/70 [==============================] - 0s 950us/step - loss: 0.1717 - accuracy: 0.9304\n",
      "Epoch 126/1500\n",
      "70/70 [==============================] - 0s 952us/step - loss: 0.1424 - accuracy: 0.9443\n",
      "Epoch 127/1500\n",
      "70/70 [==============================] - 0s 959us/step - loss: 0.1405 - accuracy: 0.9488\n",
      "Epoch 128/1500\n",
      "70/70 [==============================] - 0s 992us/step - loss: 0.1414 - accuracy: 0.9452\n",
      "Epoch 129/1500\n",
      "70/70 [==============================] - 0s 955us/step - loss: 0.1382 - accuracy: 0.9479\n",
      "Epoch 130/1500\n",
      "70/70 [==============================] - 0s 939us/step - loss: 0.1345 - accuracy: 0.9492\n",
      "Epoch 131/1500\n",
      "70/70 [==============================] - 0s 988us/step - loss: 0.1481 - accuracy: 0.9447\n",
      "Epoch 132/1500\n",
      "70/70 [==============================] - 0s 966us/step - loss: 0.1383 - accuracy: 0.9407\n",
      "Epoch 133/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9479\n",
      "Epoch 134/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9380\n",
      "Epoch 135/1500\n",
      "70/70 [==============================] - 0s 946us/step - loss: 0.1496 - accuracy: 0.9447\n",
      "Epoch 136/1500\n",
      "70/70 [==============================] - 0s 955us/step - loss: 0.1398 - accuracy: 0.9438\n",
      "Epoch 137/1500\n",
      "70/70 [==============================] - 0s 951us/step - loss: 0.1435 - accuracy: 0.9429\n",
      "Epoch 138/1500\n",
      "70/70 [==============================] - 0s 951us/step - loss: 0.1385 - accuracy: 0.9474\n",
      "Epoch 139/1500\n",
      "70/70 [==============================] - 0s 964us/step - loss: 0.1397 - accuracy: 0.9434\n",
      "Epoch 140/1500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.1474 - accuracy: 0.9416\n",
      "Epoch 141/1500\n",
      "70/70 [==============================] - 0s 967us/step - loss: 0.1394 - accuracy: 0.9447\n",
      "Epoch 142/1500\n",
      "70/70 [==============================] - 0s 953us/step - loss: 0.1528 - accuracy: 0.9425\n",
      "Epoch 143/1500\n",
      "70/70 [==============================] - 0s 952us/step - loss: 0.1445 - accuracy: 0.9425\n",
      "Epoch 144/1500\n",
      "70/70 [==============================] - 0s 938us/step - loss: 0.1379 - accuracy: 0.9483\n",
      "Epoch 145/1500\n",
      "70/70 [==============================] - 0s 942us/step - loss: 0.1559 - accuracy: 0.9376\n",
      "Epoch 146/1500\n",
      "70/70 [==============================] - 0s 927us/step - loss: 0.1461 - accuracy: 0.9456\n",
      "Epoch 147/1500\n",
      "70/70 [==============================] - 0s 951us/step - loss: 0.1433 - accuracy: 0.9438\n",
      "Epoch 148/1500\n",
      "70/70 [==============================] - 0s 991us/step - loss: 0.1422 - accuracy: 0.9470\n",
      "Epoch 149/1500\n",
      "70/70 [==============================] - 0s 962us/step - loss: 0.1250 - accuracy: 0.9519\n",
      "Epoch 150/1500\n",
      "70/70 [==============================] - 0s 972us/step - loss: 0.1307 - accuracy: 0.9447\n",
      "Epoch 151/1500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.1416 - accuracy: 0.9412\n",
      "Epoch 152/1500\n",
      "70/70 [==============================] - 0s 946us/step - loss: 0.1263 - accuracy: 0.9519\n",
      "Epoch 153/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9515\n",
      "Epoch 154/1500\n",
      "70/70 [==============================] - 0s 936us/step - loss: 0.1205 - accuracy: 0.9515\n",
      "Epoch 155/1500\n",
      "70/70 [==============================] - 0s 995us/step - loss: 0.1339 - accuracy: 0.9542\n",
      "Epoch 156/1500\n",
      "70/70 [==============================] - 0s 987us/step - loss: 0.1291 - accuracy: 0.9474\n",
      "Epoch 157/1500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.1347 - accuracy: 0.9461\n",
      "Epoch 158/1500\n",
      "70/70 [==============================] - 0s 985us/step - loss: 0.1355 - accuracy: 0.9497\n",
      "Epoch 159/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9506\n",
      "Epoch 160/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1702 - accuracy: 0.9308\n",
      "Epoch 161/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9600\n",
      "Epoch 162/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9555\n",
      "Epoch 163/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9479\n",
      "Epoch 164/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9569\n",
      "Epoch 165/1500\n",
      "70/70 [==============================] - 0s 999us/step - loss: 0.1362 - accuracy: 0.9483\n",
      "Epoch 166/1500\n",
      "70/70 [==============================] - 0s 997us/step - loss: 0.1295 - accuracy: 0.9461\n",
      "Epoch 167/1500\n",
      "70/70 [==============================] - 0s 960us/step - loss: 0.1198 - accuracy: 0.9537\n",
      "Epoch 168/1500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.1484 - accuracy: 0.9483\n",
      "Epoch 169/1500\n",
      "70/70 [==============================] - 0s 995us/step - loss: 0.1190 - accuracy: 0.9555\n",
      "Epoch 170/1500\n",
      "70/70 [==============================] - 0s 945us/step - loss: 0.1175 - accuracy: 0.9582\n",
      "Epoch 171/1500\n",
      "70/70 [==============================] - 0s 953us/step - loss: 0.1211 - accuracy: 0.9546\n",
      "Epoch 172/1500\n",
      "70/70 [==============================] - 0s 984us/step - loss: 0.1108 - accuracy: 0.9623\n",
      "Epoch 173/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9542\n",
      "Epoch 174/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9645\n",
      "Epoch 175/1500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.1215 - accuracy: 0.9519\n",
      "Epoch 176/1500\n",
      "70/70 [==============================] - 0s 983us/step - loss: 0.1120 - accuracy: 0.9515\n",
      "Epoch 177/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9546\n",
      "Epoch 178/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9587\n",
      "Epoch 179/1500\n",
      "70/70 [==============================] - 0s 972us/step - loss: 0.1077 - accuracy: 0.9596\n",
      "Epoch 180/1500\n",
      "70/70 [==============================] - 0s 939us/step - loss: 0.1442 - accuracy: 0.9456\n",
      "Epoch 181/1500\n",
      "70/70 [==============================] - 0s 937us/step - loss: 0.1107 - accuracy: 0.9560\n",
      "Epoch 182/1500\n",
      "70/70 [==============================] - 0s 990us/step - loss: 0.1116 - accuracy: 0.9551\n",
      "Epoch 183/1500\n",
      "70/70 [==============================] - 0s 960us/step - loss: 0.1031 - accuracy: 0.9623\n",
      "Epoch 184/1500\n",
      "70/70 [==============================] - 0s 954us/step - loss: 0.1146 - accuracy: 0.9573\n",
      "Epoch 185/1500\n",
      "70/70 [==============================] - 0s 953us/step - loss: 0.1218 - accuracy: 0.9510\n",
      "Epoch 186/1500\n",
      "70/70 [==============================] - 0s 956us/step - loss: 0.1099 - accuracy: 0.9600\n",
      "Epoch 187/1500\n",
      "70/70 [==============================] - 0s 958us/step - loss: 0.1070 - accuracy: 0.9605\n",
      "Epoch 188/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9569\n",
      "Epoch 189/1500\n",
      "70/70 [==============================] - 0s 991us/step - loss: 0.1179 - accuracy: 0.9591\n",
      "Epoch 190/1500\n",
      "70/70 [==============================] - 0s 965us/step - loss: 0.1060 - accuracy: 0.9591\n",
      "Epoch 191/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9650\n",
      "Epoch 192/1500\n",
      "70/70 [==============================] - 0s 941us/step - loss: 0.1163 - accuracy: 0.9524\n",
      "Epoch 193/1500\n",
      "70/70 [==============================] - 0s 951us/step - loss: 0.0938 - accuracy: 0.9654\n",
      "Epoch 194/1500\n",
      "70/70 [==============================] - 0s 949us/step - loss: 0.1012 - accuracy: 0.9623\n",
      "Epoch 195/1500\n",
      "70/70 [==============================] - 0s 949us/step - loss: 0.1086 - accuracy: 0.9555\n",
      "Epoch 196/1500\n",
      "70/70 [==============================] - 0s 920us/step - loss: 0.1101 - accuracy: 0.9524\n",
      "Epoch 197/1500\n",
      "70/70 [==============================] - 0s 952us/step - loss: 0.1101 - accuracy: 0.9542\n",
      "Epoch 198/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9542\n",
      "Epoch 199/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9591\n",
      "Epoch 200/1500\n",
      "70/70 [==============================] - 0s 970us/step - loss: 0.1131 - accuracy: 0.9573\n",
      "Epoch 201/1500\n",
      "70/70 [==============================] - 0s 974us/step - loss: 0.1065 - accuracy: 0.9614\n",
      "Epoch 202/1500\n",
      "70/70 [==============================] - 0s 948us/step - loss: 0.1038 - accuracy: 0.9591\n",
      "Epoch 203/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.9636\n",
      "Epoch 204/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9605\n",
      "Epoch 205/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9582\n",
      "Epoch 206/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9663\n",
      "Epoch 207/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9605\n",
      "Epoch 208/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9609\n",
      "Epoch 209/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9578\n",
      "Epoch 210/1500\n",
      "70/70 [==============================] - 0s 940us/step - loss: 0.1125 - accuracy: 0.9573\n",
      "Epoch 211/1500\n",
      "70/70 [==============================] - 0s 984us/step - loss: 0.1121 - accuracy: 0.9582\n",
      "Epoch 212/1500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.1096 - accuracy: 0.9528\n",
      "Epoch 213/1500\n",
      "70/70 [==============================] - 0s 983us/step - loss: 0.1073 - accuracy: 0.9623\n",
      "Epoch 214/1500\n",
      "70/70 [==============================] - 0s 962us/step - loss: 0.1062 - accuracy: 0.9627\n",
      "Epoch 215/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.9591\n",
      "Epoch 216/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.9654\n",
      "Epoch 217/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9627\n",
      "Epoch 218/1500\n",
      "70/70 [==============================] - 0s 979us/step - loss: 0.1025 - accuracy: 0.9605\n",
      "Epoch 219/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9690\n",
      "Epoch 220/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9632\n",
      "Epoch 221/1500\n",
      "70/70 [==============================] - 0s 977us/step - loss: 0.1006 - accuracy: 0.9591\n",
      "Epoch 222/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9641\n",
      "Epoch 223/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9591\n",
      "Epoch 224/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9591\n",
      "Epoch 225/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9618\n",
      "Epoch 226/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9600\n",
      "Epoch 227/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9569\n",
      "Epoch 228/1500\n",
      "70/70 [==============================] - 0s 983us/step - loss: 0.1007 - accuracy: 0.9605\n",
      "Epoch 229/1500\n",
      "70/70 [==============================] - 0s 993us/step - loss: 0.0889 - accuracy: 0.9636\n",
      "Epoch 230/1500\n",
      "70/70 [==============================] - 0s 973us/step - loss: 0.1148 - accuracy: 0.9546\n",
      "Epoch 231/1500\n",
      "70/70 [==============================] - 0s 974us/step - loss: 0.1009 - accuracy: 0.9623\n",
      "Epoch 232/1500\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1114 - accuracy: 0.9560\n",
      "Epoch 233/1500\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1201 - accuracy: 0.9506\n",
      "Epoch 234/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9672\n",
      "Epoch 235/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.9686\n",
      "Epoch 236/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9605\n",
      "Epoch 237/1500\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.1041 - accuracy: 0.9600\n",
      "Epoch 238/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.9668\n",
      "Epoch 239/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9654\n",
      "Epoch 240/1500\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0919 - accuracy: 0.9654\n",
      "Epoch 241/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9609\n",
      "Epoch 242/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9632\n",
      "Epoch 243/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9614\n",
      "Epoch 244/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9641\n",
      "Epoch 245/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9627\n",
      "Epoch 246/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9708\n",
      "Epoch 247/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9721\n",
      "Epoch 248/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9712\n",
      "Epoch 249/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9596\n",
      "Epoch 250/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.9681\n",
      "Epoch 251/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9578\n",
      "Epoch 252/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9618\n",
      "Epoch 253/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9582\n",
      "Epoch 254/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9641\n",
      "Epoch 255/1500\n",
      "70/70 [==============================] - 0s 988us/step - loss: 0.0915 - accuracy: 0.9614\n",
      "Epoch 256/1500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.1051 - accuracy: 0.9591\n",
      "Epoch 257/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9555\n",
      "Epoch 258/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9659\n",
      "Epoch 259/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9672\n",
      "Epoch 260/1500\n",
      "70/70 [==============================] - 0s 962us/step - loss: 0.1019 - accuracy: 0.9600\n",
      "Epoch 261/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9654\n",
      "Epoch 262/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9708\n",
      "Epoch 263/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9681\n",
      "Epoch 264/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9730\n",
      "Epoch 265/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9712\n",
      "Epoch 266/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9632\n",
      "Epoch 267/1500\n",
      "70/70 [==============================] - 0s 973us/step - loss: 0.0907 - accuracy: 0.9663\n",
      "Epoch 268/1500\n",
      "70/70 [==============================] - 0s 989us/step - loss: 0.0765 - accuracy: 0.9717\n",
      "Epoch 269/1500\n",
      "70/70 [==============================] - 0s 985us/step - loss: 0.1021 - accuracy: 0.9636\n",
      "Epoch 270/1500\n",
      "70/70 [==============================] - 0s 972us/step - loss: 0.0963 - accuracy: 0.9600\n",
      "Epoch 271/1500\n",
      "70/70 [==============================] - 0s 969us/step - loss: 0.0759 - accuracy: 0.9744\n",
      "Epoch 272/1500\n",
      "70/70 [==============================] - 0s 992us/step - loss: 0.0826 - accuracy: 0.9726\n",
      "Epoch 273/1500\n",
      "70/70 [==============================] - 0s 937us/step - loss: 0.0814 - accuracy: 0.9717\n",
      "Epoch 274/1500\n",
      "70/70 [==============================] - 0s 989us/step - loss: 0.0777 - accuracy: 0.9726\n",
      "Epoch 275/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.9717\n",
      "Epoch 276/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9659\n",
      "Epoch 277/1500\n",
      "70/70 [==============================] - 0s 985us/step - loss: 0.0693 - accuracy: 0.9753\n",
      "Epoch 278/1500\n",
      "70/70 [==============================] - 0s 966us/step - loss: 0.0986 - accuracy: 0.9609\n",
      "Epoch 279/1500\n",
      "70/70 [==============================] - 0s 964us/step - loss: 0.0878 - accuracy: 0.9686\n",
      "Epoch 280/1500\n",
      "70/70 [==============================] - 0s 963us/step - loss: 0.0836 - accuracy: 0.9668\n",
      "Epoch 281/1500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.0851 - accuracy: 0.9681\n",
      "Epoch 282/1500\n",
      "70/70 [==============================] - 0s 974us/step - loss: 0.0913 - accuracy: 0.9645\n",
      "Epoch 283/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9677\n",
      "Epoch 284/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0816 - accuracy: 0.9686\n",
      "Epoch 285/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9686\n",
      "Epoch 286/1500\n",
      "70/70 [==============================] - 0s 991us/step - loss: 0.0787 - accuracy: 0.9704\n",
      "Epoch 287/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.9717\n",
      "Epoch 288/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9668\n",
      "Epoch 289/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9672\n",
      "Epoch 290/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9677\n",
      "Epoch 291/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0977 - accuracy: 0.9632\n",
      "Epoch 292/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9712\n",
      "Epoch 293/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9704\n",
      "Epoch 294/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.9695\n",
      "Epoch 295/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9677\n",
      "Epoch 296/1500\n",
      "70/70 [==============================] - 0s 959us/step - loss: 0.0883 - accuracy: 0.9677\n",
      "Epoch 297/1500\n",
      "70/70 [==============================] - 0s 968us/step - loss: 0.0832 - accuracy: 0.9690\n",
      "Epoch 298/1500\n",
      "70/70 [==============================] - 0s 985us/step - loss: 0.0625 - accuracy: 0.9789\n",
      "Epoch 299/1500\n",
      "70/70 [==============================] - 0s 981us/step - loss: 0.0705 - accuracy: 0.9735\n",
      "Epoch 300/1500\n",
      "70/70 [==============================] - 0s 991us/step - loss: 0.0715 - accuracy: 0.9735\n",
      "Epoch 301/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9695\n",
      "Epoch 302/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9686\n",
      "Epoch 303/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.9766\n",
      "Epoch 304/1500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.0742 - accuracy: 0.9730\n",
      "Epoch 305/1500\n",
      "70/70 [==============================] - 0s 953us/step - loss: 0.0720 - accuracy: 0.9712\n",
      "Epoch 306/1500\n",
      "70/70 [==============================] - 0s 982us/step - loss: 0.0958 - accuracy: 0.9636\n",
      "Epoch 307/1500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.0730 - accuracy: 0.9744\n",
      "Epoch 308/1500\n",
      "70/70 [==============================] - 0s 951us/step - loss: 0.0673 - accuracy: 0.9717\n",
      "Epoch 309/1500\n",
      "70/70 [==============================] - 0s 946us/step - loss: 0.0675 - accuracy: 0.9690\n",
      "Epoch 310/1500\n",
      "70/70 [==============================] - 0s 950us/step - loss: 0.0821 - accuracy: 0.9726\n",
      "Epoch 311/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9650\n",
      "Epoch 312/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.9708\n",
      "Epoch 313/1500\n",
      "70/70 [==============================] - 0s 986us/step - loss: 0.0802 - accuracy: 0.9704\n",
      "Epoch 314/1500\n",
      "70/70 [==============================] - 0s 994us/step - loss: 0.0764 - accuracy: 0.9708\n",
      "Epoch 315/1500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.0672 - accuracy: 0.9753\n",
      "Epoch 316/1500\n",
      "70/70 [==============================] - 0s 949us/step - loss: 0.0787 - accuracy: 0.9726\n",
      "Epoch 317/1500\n",
      "70/70 [==============================] - 0s 971us/step - loss: 0.0751 - accuracy: 0.9704\n",
      "Epoch 318/1500\n",
      "70/70 [==============================] - 0s 981us/step - loss: 0.0813 - accuracy: 0.9654\n",
      "Epoch 319/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9695\n",
      "Epoch 320/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9726\n",
      "Epoch 321/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9677\n",
      "Epoch 322/1500\n",
      "70/70 [==============================] - 0s 990us/step - loss: 0.0663 - accuracy: 0.9762\n",
      "Epoch 323/1500\n",
      "70/70 [==============================] - 0s 955us/step - loss: 0.0644 - accuracy: 0.9753\n",
      "Epoch 324/1500\n",
      "70/70 [==============================] - 0s 964us/step - loss: 0.0809 - accuracy: 0.9704\n",
      "Epoch 325/1500\n",
      "70/70 [==============================] - 0s 968us/step - loss: 0.0883 - accuracy: 0.9663\n",
      "Epoch 326/1500\n",
      "70/70 [==============================] - 0s 998us/step - loss: 0.0869 - accuracy: 0.9690\n",
      "Epoch 327/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.9712\n",
      "Epoch 328/1500\n",
      "47/70 [===================>..........] - ETA: 0s - loss: 0.0653 - accuracy: 0.9754Restoring model weights from the end of the best epoch: 298.\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9762\n",
      "Epoch 328: early stopping\n",
      "5/5 [==============================] - 0s 974us/step - loss: 0.6875 - accuracy: 0.7872\n",
      "5/5 [==============================] - 0s 644us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.73 (16/22)\n",
      "Before appending - Cat IDs: 144, Predictions: 144, Actuals: 144, Gender: 144\n",
      "After appending - Cat IDs: 285, Predictions: 285, Actuals: 285, Gender: 285\n",
      "Final Test Results - Loss: 0.6874774098396301, Accuracy: 0.7872340679168701, Precision: 0.5033816425120773, Recall: 0.4471737200355135, F1 Score: 0.46817765567765574\n",
      "Confusion Matrix:\n",
      " [[97  5  7]\n",
      " [ 1  0  0]\n",
      " [17  0 14]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "097B    14\n",
      "042A    14\n",
      "001A    14\n",
      "059A    14\n",
      "002A    13\n",
      "028A    13\n",
      "116A    12\n",
      "039A    12\n",
      "025A    11\n",
      "068A    11\n",
      "063A    11\n",
      "005A    10\n",
      "016A    10\n",
      "014B    10\n",
      "040A    10\n",
      "071A    10\n",
      "022A     9\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "094A     8\n",
      "013B     8\n",
      "031A     7\n",
      "117A     7\n",
      "027A     7\n",
      "099A     7\n",
      "007A     6\n",
      "053A     6\n",
      "037A     6\n",
      "108A     6\n",
      "023A     6\n",
      "044A     5\n",
      "070A     5\n",
      "034A     5\n",
      "021A     5\n",
      "023B     5\n",
      "075A     5\n",
      "025C     5\n",
      "009A     4\n",
      "062A     4\n",
      "003A     4\n",
      "052A     4\n",
      "026A     4\n",
      "035A     4\n",
      "105A     4\n",
      "012A     3\n",
      "060A     3\n",
      "058A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "014A     3\n",
      "061A     2\n",
      "025B     2\n",
      "102A     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "011A     2\n",
      "093A     2\n",
      "087A     2\n",
      "038A     2\n",
      "073A     1\n",
      "019B     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "110A     1\n",
      "066A     1\n",
      "088A     1\n",
      "004A     1\n",
      "048A     1\n",
      "091A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "076A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "002B    32\n",
      "055A    20\n",
      "029A    17\n",
      "101A    15\n",
      "106A    14\n",
      "111A    13\n",
      "051A    12\n",
      "036A    11\n",
      "045A     9\n",
      "072A     9\n",
      "015A     9\n",
      "010A     8\n",
      "095A     8\n",
      "050A     7\n",
      "008A     6\n",
      "109A     6\n",
      "104A     4\n",
      "018A     2\n",
      "096A     1\n",
      "043A     1\n",
      "049A     1\n",
      "115A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    241\n",
      "X    226\n",
      "F    201\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    122\n",
      "M     96\n",
      "F     51\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 103A, 071A, 097B, 028...\n",
      "kitten     [044A, 014B, 040A, 047A, 042A, 041A, 048A, 110A]\n",
      "senior    [093A, 097A, 057A, 059A, 113A, 116A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 002B, 101A, 029A, 095A, 072A, 096A, 018...\n",
      "kitten     [111A, 046A, 109A, 050A, 043A, 049A, 045A, 115A]\n",
      "senior                             [106A, 104A, 055A, 051A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 63, 'kitten': 8, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 11, 'kitten': 8, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C'\n",
      " '027A' '028A' '031A' '032A' '033A' '034A' '035A' '037A' '038A' '039A'\n",
      " '040A' '041A' '042A' '044A' '047A' '048A' '051B' '052A' '053A' '054A'\n",
      " '056A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '069A' '070A' '071A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '090A' '091A' '092A' '093A' '094A' '097A' '097B' '099A'\n",
      " '100A' '102A' '103A' '105A' '108A' '110A' '113A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '008A' '010A' '015A' '018A' '029A' '036A' '043A' '045A' '046A'\n",
      " '049A' '050A' '051A' '055A' '072A' '095A' '096A' '101A' '104A' '106A'\n",
      " '109A' '111A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'014B'}\n",
      "Moved to Test Set:\n",
      "{'014B'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '011A' '012A' '013B' '014A' '016A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '044A' '046A' '047A' '048A' '051B' '052A' '053A' '054A'\n",
      " '056A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '069A' '070A' '071A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '090A' '091A' '092A' '093A' '094A' '097A' '097B' '099A'\n",
      " '100A' '102A' '103A' '105A' '108A' '110A' '113A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '008A' '010A' '014B' '015A' '018A' '029A' '036A' '043A' '045A'\n",
      " '049A' '050A' '051A' '055A' '072A' '095A' '096A' '101A' '104A' '106A'\n",
      " '109A' '111A' '115A']\n",
      "Length of X_train_val:\n",
      "721\n",
      "Length of y_train_val:\n",
      "721\n",
      "Length of groups_train_val:\n",
      "721\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     470\n",
      "senior    128\n",
      "kitten     70\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     118\n",
      "kitten    101\n",
      "senior     50\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     470\n",
      "senior    128\n",
      "kitten    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     118\n",
      "senior     50\n",
      "kitten     48\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 940, 2: 512, 1: 492})\n",
      "Epoch 1/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.8985 - accuracy: 0.5921\n",
      "Epoch 2/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.7083 - accuracy: 0.6764\n",
      "Epoch 3/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.6255 - accuracy: 0.7047\n",
      "Epoch 4/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.5804 - accuracy: 0.7279\n",
      "Epoch 5/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.5562 - accuracy: 0.7330\n",
      "Epoch 6/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.5296 - accuracy: 0.7567\n",
      "Epoch 7/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4989 - accuracy: 0.7654\n",
      "Epoch 8/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4748 - accuracy: 0.7731\n",
      "Epoch 9/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4656 - accuracy: 0.7881\n",
      "Epoch 10/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4476 - accuracy: 0.7932\n",
      "Epoch 11/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4547 - accuracy: 0.7922\n",
      "Epoch 12/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4330 - accuracy: 0.7906\n",
      "Epoch 13/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.4271 - accuracy: 0.8061\n",
      "Epoch 14/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3935 - accuracy: 0.8200\n",
      "Epoch 15/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3906 - accuracy: 0.8246\n",
      "Epoch 16/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3957 - accuracy: 0.8323\n",
      "Epoch 17/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3801 - accuracy: 0.8256\n",
      "Epoch 18/1500\n",
      "61/61 [==============================] - 0s 980us/step - loss: 0.3787 - accuracy: 0.8349\n",
      "Epoch 19/1500\n",
      "61/61 [==============================] - 0s 1000us/step - loss: 0.3674 - accuracy: 0.8246\n",
      "Epoch 20/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3775 - accuracy: 0.8282\n",
      "Epoch 21/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3622 - accuracy: 0.8344\n",
      "Epoch 22/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3428 - accuracy: 0.8400\n",
      "Epoch 23/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3321 - accuracy: 0.8477\n",
      "Epoch 24/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3310 - accuracy: 0.8477\n",
      "Epoch 25/1500\n",
      "61/61 [==============================] - 0s 987us/step - loss: 0.3297 - accuracy: 0.8483\n",
      "Epoch 26/1500\n",
      "61/61 [==============================] - 0s 973us/step - loss: 0.3344 - accuracy: 0.8519\n",
      "Epoch 27/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8565\n",
      "Epoch 28/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2973 - accuracy: 0.8575\n",
      "Epoch 29/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3136 - accuracy: 0.8513\n",
      "Epoch 30/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3053 - accuracy: 0.8652\n",
      "Epoch 31/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3166 - accuracy: 0.8555\n",
      "Epoch 32/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2953 - accuracy: 0.8668\n",
      "Epoch 33/1500\n",
      "61/61 [==============================] - 0s 984us/step - loss: 0.2806 - accuracy: 0.8683\n",
      "Epoch 34/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2914 - accuracy: 0.8719\n",
      "Epoch 35/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2960 - accuracy: 0.8570\n",
      "Epoch 36/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2982 - accuracy: 0.8678\n",
      "Epoch 37/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.3038 - accuracy: 0.8652\n",
      "Epoch 38/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2698 - accuracy: 0.8663\n",
      "Epoch 39/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2802 - accuracy: 0.8755\n",
      "Epoch 40/1500\n",
      "61/61 [==============================] - 0s 989us/step - loss: 0.2645 - accuracy: 0.8776\n",
      "Epoch 41/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2790 - accuracy: 0.8755\n",
      "Epoch 42/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2686 - accuracy: 0.8837\n",
      "Epoch 43/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8755\n",
      "Epoch 44/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2478 - accuracy: 0.8858\n",
      "Epoch 45/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.8884\n",
      "Epoch 46/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2452 - accuracy: 0.8920\n",
      "Epoch 47/1500\n",
      "61/61 [==============================] - 0s 965us/step - loss: 0.2346 - accuracy: 0.8868\n",
      "Epoch 48/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2654 - accuracy: 0.8853\n",
      "Epoch 49/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2657 - accuracy: 0.8760\n",
      "Epoch 50/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2382 - accuracy: 0.8951\n",
      "Epoch 51/1500\n",
      "61/61 [==============================] - 0s 985us/step - loss: 0.2532 - accuracy: 0.8812\n",
      "Epoch 52/1500\n",
      "61/61 [==============================] - 0s 988us/step - loss: 0.2423 - accuracy: 0.8971\n",
      "Epoch 53/1500\n",
      "61/61 [==============================] - 0s 985us/step - loss: 0.2368 - accuracy: 0.8956\n",
      "Epoch 54/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2237 - accuracy: 0.8961\n",
      "Epoch 55/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2285 - accuracy: 0.9043\n",
      "Epoch 56/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2177 - accuracy: 0.8997\n",
      "Epoch 57/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2345 - accuracy: 0.9033\n",
      "Epoch 58/1500\n",
      "61/61 [==============================] - 0s 991us/step - loss: 0.2328 - accuracy: 0.8909\n",
      "Epoch 59/1500\n",
      "61/61 [==============================] - 0s 950us/step - loss: 0.2364 - accuracy: 0.9012\n",
      "Epoch 60/1500\n",
      "61/61 [==============================] - 0s 964us/step - loss: 0.2204 - accuracy: 0.9048\n",
      "Epoch 61/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2451 - accuracy: 0.8904\n",
      "Epoch 62/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2050 - accuracy: 0.9110\n",
      "Epoch 63/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2079 - accuracy: 0.9069\n",
      "Epoch 64/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2150 - accuracy: 0.9074\n",
      "Epoch 65/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2037 - accuracy: 0.9079\n",
      "Epoch 66/1500\n",
      "61/61 [==============================] - 0s 994us/step - loss: 0.2059 - accuracy: 0.9100\n",
      "Epoch 67/1500\n",
      "61/61 [==============================] - 0s 945us/step - loss: 0.2201 - accuracy: 0.9053\n",
      "Epoch 68/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2146 - accuracy: 0.9033\n",
      "Epoch 69/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9084\n",
      "Epoch 70/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2202 - accuracy: 0.9002\n",
      "Epoch 71/1500\n",
      "61/61 [==============================] - 0s 988us/step - loss: 0.2033 - accuracy: 0.9079\n",
      "Epoch 72/1500\n",
      "61/61 [==============================] - 0s 984us/step - loss: 0.2136 - accuracy: 0.9017\n",
      "Epoch 73/1500\n",
      "61/61 [==============================] - 0s 980us/step - loss: 0.1897 - accuracy: 0.9162\n",
      "Epoch 74/1500\n",
      "61/61 [==============================] - 0s 988us/step - loss: 0.1895 - accuracy: 0.9203\n",
      "Epoch 75/1500\n",
      "61/61 [==============================] - 0s 938us/step - loss: 0.2067 - accuracy: 0.9172\n",
      "Epoch 76/1500\n",
      "61/61 [==============================] - 0s 971us/step - loss: 0.1909 - accuracy: 0.9203\n",
      "Epoch 77/1500\n",
      "61/61 [==============================] - 0s 983us/step - loss: 0.1887 - accuracy: 0.9151\n",
      "Epoch 78/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.2054 - accuracy: 0.9146\n",
      "Epoch 79/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.9234\n",
      "Epoch 80/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.9213\n",
      "Epoch 81/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1891 - accuracy: 0.9151\n",
      "Epoch 82/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9146\n",
      "Epoch 83/1500\n",
      "61/61 [==============================] - 0s 963us/step - loss: 0.1725 - accuracy: 0.9254\n",
      "Epoch 84/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1995 - accuracy: 0.9120\n",
      "Epoch 85/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1964 - accuracy: 0.9187\n",
      "Epoch 86/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.9249\n",
      "Epoch 87/1500\n",
      "61/61 [==============================] - 0s 990us/step - loss: 0.1761 - accuracy: 0.9218\n",
      "Epoch 88/1500\n",
      "61/61 [==============================] - 0s 976us/step - loss: 0.1783 - accuracy: 0.9295\n",
      "Epoch 89/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.9218\n",
      "Epoch 90/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9311\n",
      "Epoch 91/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9270\n",
      "Epoch 92/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9300\n",
      "Epoch 93/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9254\n",
      "Epoch 94/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9383\n",
      "Epoch 95/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9213\n",
      "Epoch 96/1500\n",
      "61/61 [==============================] - 0s 989us/step - loss: 0.1505 - accuracy: 0.9321\n",
      "Epoch 97/1500\n",
      "61/61 [==============================] - 0s 966us/step - loss: 0.1741 - accuracy: 0.9244\n",
      "Epoch 98/1500\n",
      "61/61 [==============================] - 0s 981us/step - loss: 0.1731 - accuracy: 0.9300\n",
      "Epoch 99/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9429\n",
      "Epoch 100/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9347\n",
      "Epoch 101/1500\n",
      "61/61 [==============================] - 0s 963us/step - loss: 0.1686 - accuracy: 0.9311\n",
      "Epoch 102/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9321\n",
      "Epoch 103/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9275\n",
      "Epoch 104/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9306\n",
      "Epoch 105/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9228\n",
      "Epoch 106/1500\n",
      "61/61 [==============================] - 0s 957us/step - loss: 0.1552 - accuracy: 0.9367\n",
      "Epoch 107/1500\n",
      "61/61 [==============================] - 0s 968us/step - loss: 0.1642 - accuracy: 0.9208\n",
      "Epoch 108/1500\n",
      "61/61 [==============================] - 0s 998us/step - loss: 0.1514 - accuracy: 0.9352\n",
      "Epoch 109/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.9336\n",
      "Epoch 110/1500\n",
      "61/61 [==============================] - 0s 999us/step - loss: 0.1503 - accuracy: 0.9311\n",
      "Epoch 111/1500\n",
      "61/61 [==============================] - 0s 989us/step - loss: 0.1419 - accuracy: 0.9424\n",
      "Epoch 112/1500\n",
      "61/61 [==============================] - 0s 994us/step - loss: 0.1554 - accuracy: 0.9347\n",
      "Epoch 113/1500\n",
      "61/61 [==============================] - 0s 963us/step - loss: 0.1579 - accuracy: 0.9321\n",
      "Epoch 114/1500\n",
      "61/61 [==============================] - 0s 989us/step - loss: 0.1554 - accuracy: 0.9342\n",
      "Epoch 115/1500\n",
      "61/61 [==============================] - 0s 995us/step - loss: 0.1399 - accuracy: 0.9455\n",
      "Epoch 116/1500\n",
      "61/61 [==============================] - 0s 975us/step - loss: 0.1431 - accuracy: 0.9352\n",
      "Epoch 117/1500\n",
      "61/61 [==============================] - 0s 993us/step - loss: 0.1400 - accuracy: 0.9429\n",
      "Epoch 118/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9403\n",
      "Epoch 119/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9403\n",
      "Epoch 120/1500\n",
      "61/61 [==============================] - 0s 958us/step - loss: 0.1402 - accuracy: 0.9424\n",
      "Epoch 121/1500\n",
      "61/61 [==============================] - 0s 978us/step - loss: 0.1319 - accuracy: 0.9475\n",
      "Epoch 122/1500\n",
      "61/61 [==============================] - 0s 968us/step - loss: 0.1436 - accuracy: 0.9424\n",
      "Epoch 123/1500\n",
      "61/61 [==============================] - 0s 970us/step - loss: 0.1400 - accuracy: 0.9429\n",
      "Epoch 124/1500\n",
      "61/61 [==============================] - 0s 982us/step - loss: 0.1394 - accuracy: 0.9414\n",
      "Epoch 125/1500\n",
      "61/61 [==============================] - 0s 982us/step - loss: 0.1364 - accuracy: 0.9398\n",
      "Epoch 126/1500\n",
      "61/61 [==============================] - 0s 973us/step - loss: 0.1299 - accuracy: 0.9470\n",
      "Epoch 127/1500\n",
      "61/61 [==============================] - 0s 1000us/step - loss: 0.1335 - accuracy: 0.9429\n",
      "Epoch 128/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9444\n",
      "Epoch 129/1500\n",
      "61/61 [==============================] - 0s 980us/step - loss: 0.1341 - accuracy: 0.9444\n",
      "Epoch 130/1500\n",
      "61/61 [==============================] - 0s 982us/step - loss: 0.1553 - accuracy: 0.9367\n",
      "Epoch 131/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9434\n",
      "Epoch 132/1500\n",
      "61/61 [==============================] - 0s 961us/step - loss: 0.1281 - accuracy: 0.9465\n",
      "Epoch 133/1500\n",
      "61/61 [==============================] - 0s 998us/step - loss: 0.1436 - accuracy: 0.9403\n",
      "Epoch 134/1500\n",
      "61/61 [==============================] - 0s 974us/step - loss: 0.1469 - accuracy: 0.9475\n",
      "Epoch 135/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9501\n",
      "Epoch 136/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9455\n",
      "Epoch 137/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9506\n",
      "Epoch 138/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9516\n",
      "Epoch 139/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9465\n",
      "Epoch 140/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9542\n",
      "Epoch 141/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9480\n",
      "Epoch 142/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9460\n",
      "Epoch 143/1500\n",
      "61/61 [==============================] - 0s 995us/step - loss: 0.1281 - accuracy: 0.9403\n",
      "Epoch 144/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9450\n",
      "Epoch 145/1500\n",
      "61/61 [==============================] - 0s 977us/step - loss: 0.1310 - accuracy: 0.9516\n",
      "Epoch 146/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9419\n",
      "Epoch 147/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9506\n",
      "Epoch 148/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9475\n",
      "Epoch 149/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9552\n",
      "Epoch 150/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9491\n",
      "Epoch 151/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9486\n",
      "Epoch 152/1500\n",
      "61/61 [==============================] - 0s 989us/step - loss: 0.1215 - accuracy: 0.9444\n",
      "Epoch 153/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9475\n",
      "Epoch 154/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9480\n",
      "Epoch 155/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9537\n",
      "Epoch 156/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9568\n",
      "Epoch 157/1500\n",
      "61/61 [==============================] - 0s 981us/step - loss: 0.1156 - accuracy: 0.9568\n",
      "Epoch 158/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9552\n",
      "Epoch 159/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9460\n",
      "Epoch 160/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9480\n",
      "Epoch 161/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9486\n",
      "Epoch 162/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9542\n",
      "Epoch 163/1500\n",
      "61/61 [==============================] - 0s 967us/step - loss: 0.1079 - accuracy: 0.9578\n",
      "Epoch 164/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9547\n",
      "Epoch 165/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9594\n",
      "Epoch 166/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9455\n",
      "Epoch 167/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.9486\n",
      "Epoch 168/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9542\n",
      "Epoch 169/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9537\n",
      "Epoch 170/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9501\n",
      "Epoch 171/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9522\n",
      "Epoch 172/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9583\n",
      "Epoch 173/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9552\n",
      "Epoch 174/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9573\n",
      "Epoch 175/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9537\n",
      "Epoch 176/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9614\n",
      "Epoch 177/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9563\n",
      "Epoch 178/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9537\n",
      "Epoch 179/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9522\n",
      "Epoch 180/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9511\n",
      "Epoch 181/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9511\n",
      "Epoch 182/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9491\n",
      "Epoch 183/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9522\n",
      "Epoch 184/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9506\n",
      "Epoch 185/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9532\n",
      "Epoch 186/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9568\n",
      "Epoch 187/1500\n",
      "61/61 [==============================] - 0s 998us/step - loss: 0.1124 - accuracy: 0.9578\n",
      "Epoch 188/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9455\n",
      "Epoch 189/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9511\n",
      "Epoch 190/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9671\n",
      "Epoch 191/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9686\n",
      "Epoch 192/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9604\n",
      "Epoch 193/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9552\n",
      "Epoch 194/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9573\n",
      "Epoch 195/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9491\n",
      "Epoch 196/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9604\n",
      "Epoch 197/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.9624\n",
      "Epoch 198/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.9588\n",
      "Epoch 199/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9609\n",
      "Epoch 200/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9645\n",
      "Epoch 201/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9671\n",
      "Epoch 202/1500\n",
      "61/61 [==============================] - 0s 990us/step - loss: 0.0977 - accuracy: 0.9609\n",
      "Epoch 203/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9604\n",
      "Epoch 204/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.9578\n",
      "Epoch 205/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9588\n",
      "Epoch 206/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9552\n",
      "Epoch 207/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9558\n",
      "Epoch 208/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9552\n",
      "Epoch 209/1500\n",
      "61/61 [==============================] - 0s 997us/step - loss: 0.0955 - accuracy: 0.9619\n",
      "Epoch 210/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.9702\n",
      "Epoch 211/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9630\n",
      "Epoch 212/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9614\n",
      "Epoch 213/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9604\n",
      "Epoch 214/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9614\n",
      "Epoch 215/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9563\n",
      "Epoch 216/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.9578\n",
      "Epoch 217/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9568\n",
      "Epoch 218/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9619\n",
      "Epoch 219/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.9619\n",
      "Epoch 220/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9697\n",
      "Epoch 221/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9609\n",
      "Epoch 222/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9681\n",
      "Epoch 223/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9594\n",
      "Epoch 224/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.9655\n",
      "Epoch 225/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9733\n",
      "Epoch 226/1500\n",
      "61/61 [==============================] - 0s 979us/step - loss: 0.0789 - accuracy: 0.9691\n",
      "Epoch 227/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9717\n",
      "Epoch 228/1500\n",
      "61/61 [==============================] - 0s 986us/step - loss: 0.0977 - accuracy: 0.9604\n",
      "Epoch 229/1500\n",
      "61/61 [==============================] - 0s 975us/step - loss: 0.0818 - accuracy: 0.9650\n",
      "Epoch 230/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9743\n",
      "Epoch 231/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9655\n",
      "Epoch 232/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.9758\n",
      "Epoch 233/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9619\n",
      "Epoch 234/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9660\n",
      "Epoch 235/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9686\n",
      "Epoch 236/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.9630\n",
      "Epoch 237/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9578\n",
      "Epoch 238/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9635\n",
      "Epoch 239/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9676\n",
      "Epoch 240/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9712\n",
      "Epoch 241/1500\n",
      "61/61 [==============================] - 0s 6ms/step - loss: 0.0914 - accuracy: 0.9645\n",
      "Epoch 242/1500\n",
      "61/61 [==============================] - 0s 2ms/step - loss: 0.0917 - accuracy: 0.9655\n",
      "Epoch 243/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9655\n",
      "Epoch 244/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9594\n",
      "Epoch 245/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9712\n",
      "Epoch 246/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9650\n",
      "Epoch 247/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9681\n",
      "Epoch 248/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9655\n",
      "Epoch 249/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.9707\n",
      "Epoch 250/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9681\n",
      "Epoch 251/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9691\n",
      "Epoch 252/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9697\n",
      "Epoch 253/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9552\n",
      "Epoch 254/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9681\n",
      "Epoch 255/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9619\n",
      "Epoch 256/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9619\n",
      "Epoch 257/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9650\n",
      "Epoch 258/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9645\n",
      "Epoch 259/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9671\n",
      "Epoch 260/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9733\n",
      "Epoch 261/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.9748\n",
      "Epoch 262/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9702\n",
      "Epoch 263/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.9753\n",
      "Epoch 264/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.9676\n",
      "Epoch 265/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9733\n",
      "Epoch 266/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9666\n",
      "Epoch 267/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0665 - accuracy: 0.9794\n",
      "Epoch 268/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9717\n",
      "Epoch 269/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.9753\n",
      "Epoch 270/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.9635\n",
      "Epoch 271/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9655\n",
      "Epoch 272/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9650\n",
      "Epoch 273/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.9691\n",
      "Epoch 274/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9722\n",
      "Epoch 275/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0615 - accuracy: 0.9748\n",
      "Epoch 276/1500\n",
      "61/61 [==============================] - 0s 982us/step - loss: 0.0728 - accuracy: 0.9717\n",
      "Epoch 277/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9733\n",
      "Epoch 278/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.9738\n",
      "Epoch 279/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.9763\n",
      "Epoch 280/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9676\n",
      "Epoch 281/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.9655\n",
      "Epoch 282/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9645\n",
      "Epoch 283/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9758\n",
      "Epoch 284/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9681\n",
      "Epoch 285/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0726 - accuracy: 0.9738\n",
      "Epoch 286/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9743\n",
      "Epoch 287/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0585 - accuracy: 0.9794\n",
      "Epoch 288/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9743\n",
      "Epoch 289/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9722\n",
      "Epoch 290/1500\n",
      "61/61 [==============================] - 0s 985us/step - loss: 0.0633 - accuracy: 0.9758\n",
      "Epoch 291/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9650\n",
      "Epoch 292/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9681\n",
      "Epoch 293/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9681\n",
      "Epoch 294/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9666\n",
      "Epoch 295/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0719 - accuracy: 0.9660\n",
      "Epoch 296/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.9794\n",
      "Epoch 297/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9619\n",
      "Epoch 298/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.9763\n",
      "Epoch 299/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9697\n",
      "Epoch 300/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9733\n",
      "Epoch 301/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0584 - accuracy: 0.9820\n",
      "Epoch 302/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.9748\n",
      "Epoch 303/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9748\n",
      "Epoch 304/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.9727\n",
      "Epoch 305/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.9733\n",
      "Epoch 306/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9671\n",
      "Epoch 307/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.9748\n",
      "Epoch 308/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0522 - accuracy: 0.9825\n",
      "Epoch 309/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9697\n",
      "Epoch 310/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9702\n",
      "Epoch 311/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.9799\n",
      "Epoch 312/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.9727\n",
      "Epoch 313/1500\n",
      "61/61 [==============================] - 0s 988us/step - loss: 0.0738 - accuracy: 0.9676\n",
      "Epoch 314/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.9691\n",
      "Epoch 315/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9697\n",
      "Epoch 316/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9743\n",
      "Epoch 317/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0527 - accuracy: 0.9799\n",
      "Epoch 318/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0554 - accuracy: 0.9784\n",
      "Epoch 319/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.9738\n",
      "Epoch 320/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.9748\n",
      "Epoch 321/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.9794\n",
      "Epoch 322/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.9727\n",
      "Epoch 323/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9727\n",
      "Epoch 324/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9686\n",
      "Epoch 325/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9712\n",
      "Epoch 326/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.9774\n",
      "Epoch 327/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9748\n",
      "Epoch 328/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9753\n",
      "Epoch 329/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9784\n",
      "Epoch 330/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0587 - accuracy: 0.9779\n",
      "Epoch 331/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.9794\n",
      "Epoch 332/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.9727\n",
      "Epoch 333/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.9722\n",
      "Epoch 334/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.9763\n",
      "Epoch 335/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9712\n",
      "Epoch 336/1500\n",
      "61/61 [==============================] - 0s 992us/step - loss: 0.0601 - accuracy: 0.9799\n",
      "Epoch 337/1500\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0622 - accuracy: 0.9805\n",
      "Epoch 338/1500\n",
      "50/61 [=======================>......] - ETA: 0s - loss: 0.0600 - accuracy: 0.9769Restoring model weights from the end of the best epoch: 308.\n",
      "61/61 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.9738\n",
      "Epoch 338: early stopping\n",
      "7/7 [==============================] - 0s 747us/step - loss: 0.8547 - accuracy: 0.7685\n",
      "7/7 [==============================] - 0s 668us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (19/23)\n",
      "Before appending - Cat IDs: 285, Predictions: 285, Actuals: 285, Gender: 285\n",
      "After appending - Cat IDs: 501, Predictions: 501, Actuals: 501, Gender: 501\n",
      "Final Test Results - Loss: 0.8546939492225647, Accuracy: 0.7685185074806213, Precision: 0.7833333333333333, Recall: 0.7259133709981168, F1 Score: 0.7485246162411516\n",
      "Confusion Matrix:\n",
      " [[102   0  16]\n",
      " [  8  40   0]\n",
      " [ 26   0  24]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "036A    11\n",
      "025A    11\n",
      "063A    11\n",
      "068A    11\n",
      "005A    10\n",
      "016A    10\n",
      "040A    10\n",
      "022A     9\n",
      "033A     9\n",
      "051B     9\n",
      "015A     9\n",
      "072A     9\n",
      "045A     9\n",
      "095A     8\n",
      "010A     8\n",
      "094A     8\n",
      "099A     7\n",
      "027A     7\n",
      "050A     7\n",
      "117A     7\n",
      "037A     6\n",
      "008A     6\n",
      "109A     6\n",
      "023A     6\n",
      "108A     6\n",
      "053A     6\n",
      "021A     5\n",
      "025C     5\n",
      "034A     5\n",
      "023B     5\n",
      "105A     4\n",
      "052A     4\n",
      "026A     4\n",
      "003A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "060A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "093A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "073A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "004A     1\n",
      "088A     1\n",
      "043A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "057A    27\n",
      "020A    23\n",
      "001A    14\n",
      "042A    14\n",
      "039A    12\n",
      "014B    10\n",
      "071A    10\n",
      "065A     9\n",
      "013B     8\n",
      "031A     7\n",
      "007A     6\n",
      "044A     5\n",
      "075A     5\n",
      "070A     5\n",
      "009A     4\n",
      "058A     3\n",
      "064A     3\n",
      "012A     3\n",
      "038A     2\n",
      "087A     2\n",
      "069A     2\n",
      "032A     2\n",
      "054A     2\n",
      "091A     1\n",
      "041A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    283\n",
      "X    256\n",
      "F    217\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    92\n",
      "M    54\n",
      "F    35\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 103A, 097B, 028A, 019...\n",
      "kitten    [111A, 040A, 046A, 047A, 109A, 050A, 043A, 049...\n",
      "senior    [093A, 097A, 106A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [001A, 071A, 020A, 065A, 091A, 039A, 009A, 038...\n",
      "kitten                             [044A, 014B, 042A, 041A]\n",
      "senior                                   [057A, 054A, 058A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 12, 'senior': 19}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 4, 'senior': 3}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '008A' '010A'\n",
      " '011A' '014A' '015A' '016A' '018A' '019A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A'\n",
      " '033A' '034A' '035A' '036A' '037A' '040A' '043A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '059A'\n",
      " '060A' '061A' '062A' '063A' '066A' '067A' '068A' '072A' '073A' '074A'\n",
      " '076A' '088A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A'\n",
      " '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '007A' '009A' '012A' '013B' '014B' '019B' '020A' '031A' '032A'\n",
      " '038A' '039A' '041A' '042A' '044A' '054A' '057A' '058A' '064A' '065A'\n",
      " '069A' '070A' '071A' '075A' '087A' '091A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '008A' '010A'\n",
      " '011A' '014A' '015A' '016A' '018A' '019A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A'\n",
      " '033A' '034A' '035A' '036A' '037A' '040A' '043A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '059A'\n",
      " '060A' '061A' '062A' '063A' '066A' '067A' '068A' '072A' '073A' '074A'\n",
      " '076A' '088A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A'\n",
      " '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '007A' '009A' '012A' '013B' '014B' '019B' '020A' '031A' '032A'\n",
      " '038A' '039A' '041A' '042A' '044A' '054A' '057A' '058A' '064A' '065A'\n",
      " '069A' '070A' '071A' '075A' '087A' '091A']\n",
      "Length of X_train_val:\n",
      "756\n",
      "Length of y_train_val:\n",
      "756\n",
      "Length of groups_train_val:\n",
      "756\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     469\n",
      "senior    146\n",
      "kitten    141\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     119\n",
      "senior     32\n",
      "kitten     30\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     469\n",
      "senior    146\n",
      "kitten    141\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     119\n",
      "senior     32\n",
      "kitten     30\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 938, 2: 584, 1: 564})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.9555 - accuracy: 0.5623\n",
      "Epoch 2/1500\n",
      "66/66 [==============================] - 0s 989us/step - loss: 0.7398 - accuracy: 0.6592\n",
      "Epoch 3/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.6563 - accuracy: 0.6932\n",
      "Epoch 4/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.6043 - accuracy: 0.7325\n",
      "Epoch 5/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.5765 - accuracy: 0.7454\n",
      "Epoch 6/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.5706 - accuracy: 0.7464\n",
      "Epoch 7/1500\n",
      "66/66 [==============================] - 0s 989us/step - loss: 0.5347 - accuracy: 0.7450\n",
      "Epoch 8/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.5002 - accuracy: 0.7756\n",
      "Epoch 9/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.4894 - accuracy: 0.7886\n",
      "Epoch 10/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.4516 - accuracy: 0.8073\n",
      "Epoch 11/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.4622 - accuracy: 0.7943\n",
      "Epoch 12/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.4381 - accuracy: 0.8035\n",
      "Epoch 13/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.4368 - accuracy: 0.8073\n",
      "Epoch 14/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.4227 - accuracy: 0.8145\n",
      "Epoch 15/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3984 - accuracy: 0.8303\n",
      "Epoch 16/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.4032 - accuracy: 0.8130\n",
      "Epoch 17/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3935 - accuracy: 0.8317\n",
      "Epoch 18/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3923 - accuracy: 0.8260\n",
      "Epoch 19/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3931 - accuracy: 0.8332\n",
      "Epoch 20/1500\n",
      "66/66 [==============================] - 0s 965us/step - loss: 0.3592 - accuracy: 0.8370\n",
      "Epoch 21/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3824 - accuracy: 0.8413\n",
      "Epoch 22/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3735 - accuracy: 0.8432\n",
      "Epoch 23/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3837 - accuracy: 0.8308\n",
      "Epoch 24/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3647 - accuracy: 0.8337\n",
      "Epoch 25/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3565 - accuracy: 0.8432\n",
      "Epoch 26/1500\n",
      "66/66 [==============================] - 0s 995us/step - loss: 0.3413 - accuracy: 0.8480\n",
      "Epoch 27/1500\n",
      "66/66 [==============================] - 0s 980us/step - loss: 0.3273 - accuracy: 0.8557\n",
      "Epoch 28/1500\n",
      "66/66 [==============================] - 0s 983us/step - loss: 0.3267 - accuracy: 0.8619\n",
      "Epoch 29/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3207 - accuracy: 0.8610\n",
      "Epoch 30/1500\n",
      "66/66 [==============================] - 0s 991us/step - loss: 0.3221 - accuracy: 0.8543\n",
      "Epoch 31/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3161 - accuracy: 0.8514\n",
      "Epoch 32/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3222 - accuracy: 0.8586\n",
      "Epoch 33/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3037 - accuracy: 0.8734\n",
      "Epoch 34/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3087 - accuracy: 0.8706\n",
      "Epoch 35/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2742 - accuracy: 0.8811\n",
      "Epoch 36/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2866 - accuracy: 0.8730\n",
      "Epoch 37/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3018 - accuracy: 0.8749\n",
      "Epoch 38/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2898 - accuracy: 0.8701\n",
      "Epoch 39/1500\n",
      "66/66 [==============================] - 0s 980us/step - loss: 0.2894 - accuracy: 0.8730\n",
      "Epoch 40/1500\n",
      "66/66 [==============================] - 0s 969us/step - loss: 0.2921 - accuracy: 0.8845\n",
      "Epoch 41/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2847 - accuracy: 0.8849\n",
      "Epoch 42/1500\n",
      "66/66 [==============================] - 0s 995us/step - loss: 0.2780 - accuracy: 0.8869\n",
      "Epoch 43/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2741 - accuracy: 0.8811\n",
      "Epoch 44/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2880 - accuracy: 0.8773\n",
      "Epoch 45/1500\n",
      "66/66 [==============================] - 0s 995us/step - loss: 0.2755 - accuracy: 0.8797\n",
      "Epoch 46/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2746 - accuracy: 0.8802\n",
      "Epoch 47/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.8840\n",
      "Epoch 48/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2754 - accuracy: 0.8873\n",
      "Epoch 49/1500\n",
      "66/66 [==============================] - 0s 983us/step - loss: 0.2669 - accuracy: 0.8864\n",
      "Epoch 50/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2538 - accuracy: 0.8950\n",
      "Epoch 51/1500\n",
      "66/66 [==============================] - 0s 986us/step - loss: 0.2566 - accuracy: 0.8859\n",
      "Epoch 52/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2382 - accuracy: 0.8917\n",
      "Epoch 53/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2482 - accuracy: 0.8931\n",
      "Epoch 54/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2575 - accuracy: 0.8936\n",
      "Epoch 55/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.8979\n",
      "Epoch 56/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.8826\n",
      "Epoch 57/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2418 - accuracy: 0.8960\n",
      "Epoch 58/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2455 - accuracy: 0.9003\n",
      "Epoch 59/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2406 - accuracy: 0.8945\n",
      "Epoch 60/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2574 - accuracy: 0.8969\n",
      "Epoch 61/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2430 - accuracy: 0.9003\n",
      "Epoch 62/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2336 - accuracy: 0.8998\n",
      "Epoch 63/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.8941\n",
      "Epoch 64/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9003\n",
      "Epoch 65/1500\n",
      "66/66 [==============================] - 0s 956us/step - loss: 0.2419 - accuracy: 0.8974\n",
      "Epoch 66/1500\n",
      "66/66 [==============================] - 0s 961us/step - loss: 0.2213 - accuracy: 0.9080\n",
      "Epoch 67/1500\n",
      "66/66 [==============================] - 0s 974us/step - loss: 0.2224 - accuracy: 0.9070\n",
      "Epoch 68/1500\n",
      "66/66 [==============================] - 0s 988us/step - loss: 0.2304 - accuracy: 0.8979\n",
      "Epoch 69/1500\n",
      "66/66 [==============================] - 0s 995us/step - loss: 0.2221 - accuracy: 0.9046\n",
      "Epoch 70/1500\n",
      "66/66 [==============================] - 0s 987us/step - loss: 0.2053 - accuracy: 0.9142\n",
      "Epoch 71/1500\n",
      "66/66 [==============================] - 0s 975us/step - loss: 0.2079 - accuracy: 0.9147\n",
      "Epoch 72/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2159 - accuracy: 0.9142\n",
      "Epoch 73/1500\n",
      "66/66 [==============================] - 0s 986us/step - loss: 0.2225 - accuracy: 0.9046\n",
      "Epoch 74/1500\n",
      "66/66 [==============================] - 0s 952us/step - loss: 0.2044 - accuracy: 0.9185\n",
      "Epoch 75/1500\n",
      "66/66 [==============================] - 0s 971us/step - loss: 0.2168 - accuracy: 0.9161\n",
      "Epoch 76/1500\n",
      "66/66 [==============================] - 0s 998us/step - loss: 0.2115 - accuracy: 0.9151\n",
      "Epoch 77/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9195\n",
      "Epoch 78/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9151\n",
      "Epoch 79/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9108\n",
      "Epoch 80/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2086 - accuracy: 0.9156\n",
      "Epoch 81/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9075\n",
      "Epoch 82/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.9175\n",
      "Epoch 83/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2075 - accuracy: 0.9147\n",
      "Epoch 84/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2090 - accuracy: 0.9113\n",
      "Epoch 85/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2121 - accuracy: 0.9118\n",
      "Epoch 86/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1942 - accuracy: 0.9161\n",
      "Epoch 87/1500\n",
      "66/66 [==============================] - 0s 995us/step - loss: 0.2112 - accuracy: 0.9080\n",
      "Epoch 88/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9233\n",
      "Epoch 89/1500\n",
      "66/66 [==============================] - 0s 995us/step - loss: 0.1859 - accuracy: 0.9238\n",
      "Epoch 90/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9319\n",
      "Epoch 91/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2089 - accuracy: 0.9080\n",
      "Epoch 92/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1968 - accuracy: 0.9175\n",
      "Epoch 93/1500\n",
      "66/66 [==============================] - 0s 986us/step - loss: 0.1934 - accuracy: 0.9238\n",
      "Epoch 94/1500\n",
      "66/66 [==============================] - 0s 982us/step - loss: 0.1876 - accuracy: 0.9233\n",
      "Epoch 95/1500\n",
      "66/66 [==============================] - 0s 992us/step - loss: 0.1823 - accuracy: 0.9267\n",
      "Epoch 96/1500\n",
      "66/66 [==============================] - 0s 996us/step - loss: 0.1856 - accuracy: 0.9228\n",
      "Epoch 97/1500\n",
      "66/66 [==============================] - 0s 993us/step - loss: 0.1866 - accuracy: 0.9271\n",
      "Epoch 98/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9209\n",
      "Epoch 99/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9219\n",
      "Epoch 100/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9291\n",
      "Epoch 101/1500\n",
      "66/66 [==============================] - 0s 997us/step - loss: 0.1949 - accuracy: 0.9137\n",
      "Epoch 102/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9314\n",
      "Epoch 103/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1812 - accuracy: 0.9291\n",
      "Epoch 104/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.9228\n",
      "Epoch 105/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.9310\n",
      "Epoch 106/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1752 - accuracy: 0.9310\n",
      "Epoch 107/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9314\n",
      "Epoch 108/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9223\n",
      "Epoch 109/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.9300\n",
      "Epoch 110/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9453\n",
      "Epoch 111/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.9410\n",
      "Epoch 112/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9425\n",
      "Epoch 113/1500\n",
      "66/66 [==============================] - 0s 1000us/step - loss: 0.1642 - accuracy: 0.9329\n",
      "Epoch 114/1500\n",
      "66/66 [==============================] - 0s 992us/step - loss: 0.1667 - accuracy: 0.9338\n",
      "Epoch 115/1500\n",
      "66/66 [==============================] - 0s 985us/step - loss: 0.1599 - accuracy: 0.9367\n",
      "Epoch 116/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9425\n",
      "Epoch 117/1500\n",
      "66/66 [==============================] - 0s 997us/step - loss: 0.1494 - accuracy: 0.9372\n",
      "Epoch 118/1500\n",
      "66/66 [==============================] - 0s 990us/step - loss: 0.1543 - accuracy: 0.9406\n",
      "Epoch 119/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9305\n",
      "Epoch 120/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9367\n",
      "Epoch 121/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1551 - accuracy: 0.9372\n",
      "Epoch 122/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9271\n",
      "Epoch 123/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9314\n",
      "Epoch 124/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9343\n",
      "Epoch 125/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9410\n",
      "Epoch 126/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9415\n",
      "Epoch 127/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.9305\n",
      "Epoch 128/1500\n",
      "66/66 [==============================] - 0s 986us/step - loss: 0.1635 - accuracy: 0.9396\n",
      "Epoch 129/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9391\n",
      "Epoch 130/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9362\n",
      "Epoch 131/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1477 - accuracy: 0.9377\n",
      "Epoch 132/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9377\n",
      "Epoch 133/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9281\n",
      "Epoch 134/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.9382\n",
      "Epoch 135/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9439\n",
      "Epoch 136/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9406\n",
      "Epoch 137/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9382\n",
      "Epoch 138/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9362\n",
      "Epoch 139/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9406\n",
      "Epoch 140/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9463\n",
      "Epoch 141/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9497\n",
      "Epoch 142/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9453\n",
      "Epoch 143/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9444\n",
      "Epoch 144/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9420\n",
      "Epoch 145/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9372\n",
      "Epoch 146/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9463\n",
      "Epoch 147/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9525\n",
      "Epoch 148/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9463\n",
      "Epoch 149/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1365 - accuracy: 0.9473\n",
      "Epoch 150/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9501\n",
      "Epoch 151/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.9386\n",
      "Epoch 152/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9434\n",
      "Epoch 153/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9410\n",
      "Epoch 154/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.9492\n",
      "Epoch 155/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9545\n",
      "Epoch 156/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9492\n",
      "Epoch 157/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9367\n",
      "Epoch 158/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9425\n",
      "Epoch 159/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9449\n",
      "Epoch 160/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9401\n",
      "Epoch 161/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9430\n",
      "Epoch 162/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9545\n",
      "Epoch 163/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9463\n",
      "Epoch 164/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9458\n",
      "Epoch 165/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9487\n",
      "Epoch 166/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9559\n",
      "Epoch 167/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9511\n",
      "Epoch 168/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9492\n",
      "Epoch 169/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9482\n",
      "Epoch 170/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9463\n",
      "Epoch 171/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9487\n",
      "Epoch 172/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9540\n",
      "Epoch 173/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9463\n",
      "Epoch 174/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9482\n",
      "Epoch 175/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9549\n",
      "Epoch 176/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9530\n",
      "Epoch 177/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9593\n",
      "Epoch 178/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9525\n",
      "Epoch 179/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9516\n",
      "Epoch 180/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9569\n",
      "Epoch 181/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9506\n",
      "Epoch 182/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9545\n",
      "Epoch 183/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9631\n",
      "Epoch 184/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9501\n",
      "Epoch 185/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9564\n",
      "Epoch 186/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9530\n",
      "Epoch 187/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9540\n",
      "Epoch 188/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9554\n",
      "Epoch 189/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9540\n",
      "Epoch 190/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9549\n",
      "Epoch 191/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.9564\n",
      "Epoch 192/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9593\n",
      "Epoch 193/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9655\n",
      "Epoch 194/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9597\n",
      "Epoch 195/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9497\n",
      "Epoch 196/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9573\n",
      "Epoch 197/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9506\n",
      "Epoch 198/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9569\n",
      "Epoch 199/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.9607\n",
      "Epoch 200/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9511\n",
      "Epoch 201/1500\n",
      "66/66 [==============================] - 0s 996us/step - loss: 0.1165 - accuracy: 0.9516\n",
      "Epoch 202/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9602\n",
      "Epoch 203/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9535\n",
      "Epoch 204/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1111 - accuracy: 0.9583\n",
      "Epoch 205/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9616\n",
      "Epoch 206/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9477\n",
      "Epoch 207/1500\n",
      "66/66 [==============================] - 0s 1000us/step - loss: 0.1033 - accuracy: 0.9559\n",
      "Epoch 208/1500\n",
      "66/66 [==============================] - 0s 973us/step - loss: 0.1220 - accuracy: 0.9549\n",
      "Epoch 209/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9506\n",
      "Epoch 210/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9593\n",
      "Epoch 211/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9530\n",
      "Epoch 212/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9578\n",
      "Epoch 213/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9554\n",
      "Epoch 214/1500\n",
      "66/66 [==============================] - 0s 1000us/step - loss: 0.1066 - accuracy: 0.9573\n",
      "Epoch 215/1500\n",
      "66/66 [==============================] - 0s 965us/step - loss: 0.1010 - accuracy: 0.9612\n",
      "Epoch 216/1500\n",
      "66/66 [==============================] - 0s 973us/step - loss: 0.1073 - accuracy: 0.9573\n",
      "Epoch 217/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9636\n",
      "Epoch 218/1500\n",
      "66/66 [==============================] - 0s 996us/step - loss: 0.1022 - accuracy: 0.9640\n",
      "Epoch 219/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9521\n",
      "Epoch 220/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.9593\n",
      "Epoch 221/1500\n",
      "66/66 [==============================] - 0s 989us/step - loss: 0.1032 - accuracy: 0.9631\n",
      "Epoch 222/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.9597\n",
      "Epoch 223/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9660\n",
      "Epoch 224/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9655\n",
      "Epoch 225/1500\n",
      "66/66 [==============================] - 0s 979us/step - loss: 0.0869 - accuracy: 0.9645\n",
      "Epoch 226/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9521\n",
      "Epoch 227/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9588\n",
      "Epoch 228/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9616\n",
      "Epoch 229/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9607\n",
      "Epoch 230/1500\n",
      "66/66 [==============================] - 0s 993us/step - loss: 0.0935 - accuracy: 0.9660\n",
      "Epoch 231/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9602\n",
      "Epoch 232/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9612\n",
      "Epoch 233/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9569\n",
      "Epoch 234/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9645\n",
      "Epoch 235/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9564\n",
      "Epoch 236/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.9660\n",
      "Epoch 237/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9650\n",
      "Epoch 238/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9645\n",
      "Epoch 239/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9569\n",
      "Epoch 240/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9569\n",
      "Epoch 241/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9602\n",
      "Epoch 242/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.9621\n",
      "Epoch 243/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9626\n",
      "Epoch 244/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9698\n",
      "Epoch 245/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9631\n",
      "Epoch 246/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9626\n",
      "Epoch 247/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9640\n",
      "Epoch 248/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9655\n",
      "Epoch 249/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.9631\n",
      "Epoch 250/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9583\n",
      "Epoch 251/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9698\n",
      "Epoch 252/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9688\n",
      "Epoch 253/1500\n",
      "66/66 [==============================] - 0s 984us/step - loss: 0.0964 - accuracy: 0.9645\n",
      "Epoch 254/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9626\n",
      "Epoch 255/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.9669\n",
      "Epoch 256/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9612\n",
      "Epoch 257/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9583\n",
      "Epoch 258/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9669\n",
      "Epoch 259/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9717\n",
      "Epoch 260/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9684\n",
      "Epoch 261/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9703\n",
      "Epoch 262/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9684\n",
      "Epoch 263/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9708\n",
      "Epoch 264/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9674\n",
      "Epoch 265/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9621\n",
      "Epoch 266/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9588\n",
      "Epoch 267/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9655\n",
      "Epoch 268/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9631\n",
      "Epoch 269/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9664\n",
      "Epoch 270/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9703\n",
      "Epoch 271/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9664\n",
      "Epoch 272/1500\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.0951 - accuracy: 0.9626\n",
      "Epoch 273/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9674\n",
      "Epoch 274/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9698\n",
      "Epoch 275/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9674\n",
      "Epoch 276/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9655\n",
      "Epoch 277/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9631\n",
      "Epoch 278/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9712\n",
      "Epoch 279/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9679\n",
      "Epoch 280/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9688\n",
      "Epoch 281/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9655\n",
      "Epoch 282/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9549\n",
      "Epoch 283/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9669\n",
      "Epoch 284/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9597\n",
      "Epoch 285/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.9640\n",
      "Epoch 286/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9597\n",
      "Epoch 287/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9655\n",
      "Epoch 288/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9679\n",
      "Epoch 289/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.9679\n",
      "Epoch 290/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9631\n",
      "Epoch 291/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9655\n",
      "Epoch 292/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9583\n",
      "Epoch 293/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9669\n",
      "Epoch 294/1500\n",
      "66/66 [==============================] - 0s 999us/step - loss: 0.1006 - accuracy: 0.9593\n",
      "Epoch 295/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.9708\n",
      "Epoch 296/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.9669\n",
      "Epoch 297/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.9645\n",
      "Epoch 298/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9593\n",
      "Epoch 299/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9684\n",
      "Epoch 300/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9631\n",
      "Epoch 301/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9760\n",
      "Epoch 302/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.9756\n",
      "Epoch 303/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9679\n",
      "Epoch 304/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9674\n",
      "Epoch 305/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9626\n",
      "Epoch 306/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9684\n",
      "Epoch 307/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9717\n",
      "Epoch 308/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9650\n",
      "Epoch 309/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.9660\n",
      "Epoch 310/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9593\n",
      "Epoch 311/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9722\n",
      "Epoch 312/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9669\n",
      "Epoch 313/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.9722\n",
      "Epoch 314/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9674\n",
      "Epoch 315/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9693\n",
      "Epoch 316/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9722\n",
      "Epoch 317/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9688\n",
      "Epoch 318/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9650\n",
      "Epoch 319/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9669\n",
      "Epoch 320/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.9775\n",
      "Epoch 321/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.9760\n",
      "Epoch 322/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9717\n",
      "Epoch 323/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9746\n",
      "Epoch 324/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.9770\n",
      "Epoch 325/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9645\n",
      "Epoch 326/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.9722\n",
      "Epoch 327/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9712\n",
      "Epoch 328/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9650\n",
      "Epoch 329/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9765\n",
      "Epoch 330/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9616\n",
      "Epoch 331/1500\n",
      "66/66 [==============================] - 0s 974us/step - loss: 0.0843 - accuracy: 0.9636\n",
      "Epoch 332/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9751\n",
      "Epoch 333/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9693\n",
      "Epoch 334/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.9775\n",
      "Epoch 335/1500\n",
      "66/66 [==============================] - 0s 990us/step - loss: 0.0714 - accuracy: 0.9741\n",
      "Epoch 336/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9679\n",
      "Epoch 337/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0719 - accuracy: 0.9712\n",
      "Epoch 338/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9693\n",
      "Epoch 339/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9674\n",
      "Epoch 340/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9712\n",
      "Epoch 341/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9717\n",
      "Epoch 342/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9727\n",
      "Epoch 343/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.9698\n",
      "Epoch 344/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0884 - accuracy: 0.9636\n",
      "Epoch 345/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9645\n",
      "Epoch 346/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9703\n",
      "Epoch 347/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9602\n",
      "Epoch 348/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.9612\n",
      "Epoch 349/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9708\n",
      "Epoch 350/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9756\n",
      "Epoch 351/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9660\n",
      "Epoch 352/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.9741\n",
      "Epoch 353/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.9765\n",
      "Epoch 354/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9712\n",
      "Epoch 355/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9751\n",
      "Epoch 356/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.9727\n",
      "Epoch 357/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.9741\n",
      "Epoch 358/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9693\n",
      "Epoch 359/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9679\n",
      "Epoch 360/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9525\n",
      "Epoch 361/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9463\n",
      "Epoch 362/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9660\n",
      "Epoch 363/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9770\n",
      "Epoch 364/1500\n",
      "51/66 [======================>.......] - ETA: 0s - loss: 0.0731 - accuracy: 0.9724Restoring model weights from the end of the best epoch: 334.\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9722\n",
      "Epoch 364: early stopping\n",
      "6/6 [==============================] - 0s 765us/step - loss: 0.7805 - accuracy: 0.7624\n",
      "6/6 [==============================] - 0s 607us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.85 (22/26)\n",
      "Before appending - Cat IDs: 501, Predictions: 501, Actuals: 501, Gender: 501\n",
      "After appending - Cat IDs: 682, Predictions: 682, Actuals: 682, Gender: 682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Results - Loss: 0.7804884314537048, Accuracy: 0.7624309659004211, Precision: 0.7099107142857143, Recall: 0.6780753968253969, F1 Score: 0.6913149363240106\n",
      "Confusion Matrix:\n",
      " [[102   3  14]\n",
      " [  5  25   0]\n",
      " [ 21   0  11]]\n",
      "outer_fold 5\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "097B    14\n",
      "106A    14\n",
      "001A    14\n",
      "059A    14\n",
      "042A    14\n",
      "002A    13\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "014B    10\n",
      "005A    10\n",
      "071A    10\n",
      "016A    10\n",
      "045A     9\n",
      "072A     9\n",
      "065A     9\n",
      "015A     9\n",
      "022A     9\n",
      "095A     8\n",
      "010A     8\n",
      "094A     8\n",
      "013B     8\n",
      "031A     7\n",
      "050A     7\n",
      "027A     7\n",
      "117A     7\n",
      "099A     7\n",
      "109A     6\n",
      "023A     6\n",
      "108A     6\n",
      "008A     6\n",
      "037A     6\n",
      "007A     6\n",
      "044A     5\n",
      "021A     5\n",
      "070A     5\n",
      "034A     5\n",
      "075A     5\n",
      "009A     4\n",
      "052A     4\n",
      "062A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "058A     3\n",
      "113A     3\n",
      "060A     3\n",
      "064A     3\n",
      "012A     3\n",
      "014A     3\n",
      "006A     3\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "038A     2\n",
      "087A     2\n",
      "073A     1\n",
      "019B     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "091A     1\n",
      "048A     1\n",
      "088A     1\n",
      "043A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "047A    28\n",
      "028A    13\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "040A    10\n",
      "033A     9\n",
      "051B     9\n",
      "053A     6\n",
      "025C     5\n",
      "023B     5\n",
      "035A     4\n",
      "003A     4\n",
      "056A     3\n",
      "011A     2\n",
      "025B     2\n",
      "026C     1\n",
      "004A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    292\n",
      "M    272\n",
      "F    203\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    65\n",
      "X    56\n",
      "F    49\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 015A, 001A, 071A, 097B, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 046A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 113...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 103A, 028A, 063A, 025A, 025C, 003A, 023...\n",
      "kitten                                         [040A, 047A]\n",
      "senior                       [116A, 051B, 056A, 011A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 61, 'kitten': 14, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 13, 'kitten': 2, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B'\n",
      " '020A' '021A' '022A' '023A' '024A' '026A' '026B' '027A' '029A' '031A'\n",
      " '032A' '034A' '036A' '037A' '038A' '039A' '041A' '042A' '043A' '044A'\n",
      " '045A' '046A' '048A' '049A' '050A' '051A' '052A' '054A' '055A' '057A'\n",
      " '058A' '059A' '060A' '061A' '062A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '070A' '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A'\n",
      " '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A'\n",
      " '101A' '102A' '104A' '105A' '106A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '004A' '011A' '023B' '025A' '025B' '025C' '026C' '028A' '033A'\n",
      " '035A' '040A' '047A' '051B' '053A' '056A' '063A' '090A' '103A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B'\n",
      " '020A' '021A' '022A' '023A' '024A' '026A' '026B' '027A' '029A' '031A'\n",
      " '032A' '034A' '036A' '037A' '038A' '039A' '041A' '042A' '043A' '044A'\n",
      " '045A' '046A' '048A' '049A' '050A' '051A' '052A' '054A' '055A' '057A'\n",
      " '058A' '059A' '060A' '061A' '062A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '070A' '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A'\n",
      " '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A'\n",
      " '101A' '102A' '104A' '105A' '106A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '004A' '011A' '023B' '025A' '025B' '025C' '026C' '028A' '033A'\n",
      " '035A' '040A' '047A' '051B' '053A' '056A' '063A' '090A' '103A' '116A']\n",
      "Length of X_train_val:\n",
      "767\n",
      "Length of y_train_val:\n",
      "767\n",
      "Length of groups_train_val:\n",
      "767\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     483\n",
      "senior    151\n",
      "kitten    133\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     105\n",
      "kitten     38\n",
      "senior     27\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     483\n",
      "senior    151\n",
      "kitten    133\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     105\n",
      "kitten     38\n",
      "senior     27\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 966, 2: 604, 1: 532})\n",
      "Epoch 1/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.9183 - accuracy: 0.5728\n",
      "Epoch 2/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.7474 - accuracy: 0.6594\n",
      "Epoch 3/1500\n",
      "66/66 [==============================] - 0s 979us/step - loss: 0.6477 - accuracy: 0.7003\n",
      "Epoch 4/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.6172 - accuracy: 0.7250\n",
      "Epoch 5/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.5522 - accuracy: 0.7512\n",
      "Epoch 6/1500\n",
      "66/66 [==============================] - 0s 971us/step - loss: 0.5508 - accuracy: 0.7536\n",
      "Epoch 7/1500\n",
      "66/66 [==============================] - 0s 952us/step - loss: 0.5397 - accuracy: 0.7664\n",
      "Epoch 8/1500\n",
      "66/66 [==============================] - 0s 993us/step - loss: 0.5013 - accuracy: 0.7802\n",
      "Epoch 9/1500\n",
      "66/66 [==============================] - 0s 975us/step - loss: 0.5023 - accuracy: 0.7835\n",
      "Epoch 10/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.4768 - accuracy: 0.7969\n",
      "Epoch 11/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.4667 - accuracy: 0.7902\n",
      "Epoch 12/1500\n",
      "66/66 [==============================] - 0s 996us/step - loss: 0.4456 - accuracy: 0.8040\n",
      "Epoch 13/1500\n",
      "66/66 [==============================] - 0s 946us/step - loss: 0.4287 - accuracy: 0.8102\n",
      "Epoch 14/1500\n",
      "66/66 [==============================] - 0s 940us/step - loss: 0.4277 - accuracy: 0.8073\n",
      "Epoch 15/1500\n",
      "66/66 [==============================] - 0s 937us/step - loss: 0.4293 - accuracy: 0.8135\n",
      "Epoch 16/1500\n",
      "66/66 [==============================] - 0s 976us/step - loss: 0.4133 - accuracy: 0.8197\n",
      "Epoch 17/1500\n",
      "66/66 [==============================] - 0s 997us/step - loss: 0.4137 - accuracy: 0.8178\n",
      "Epoch 18/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8287\n",
      "Epoch 19/1500\n",
      "66/66 [==============================] - 0s 955us/step - loss: 0.3873 - accuracy: 0.8278\n",
      "Epoch 20/1500\n",
      "66/66 [==============================] - 0s 943us/step - loss: 0.3898 - accuracy: 0.8302\n",
      "Epoch 21/1500\n",
      "66/66 [==============================] - 0s 954us/step - loss: 0.3512 - accuracy: 0.8411\n",
      "Epoch 22/1500\n",
      "66/66 [==============================] - 0s 945us/step - loss: 0.3715 - accuracy: 0.8411\n",
      "Epoch 23/1500\n",
      "66/66 [==============================] - 0s 955us/step - loss: 0.3555 - accuracy: 0.8416\n",
      "Epoch 24/1500\n",
      "66/66 [==============================] - 0s 950us/step - loss: 0.3614 - accuracy: 0.8425\n",
      "Epoch 25/1500\n",
      "66/66 [==============================] - 0s 940us/step - loss: 0.3534 - accuracy: 0.8501\n",
      "Epoch 26/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3392 - accuracy: 0.8478\n",
      "Epoch 27/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8639\n",
      "Epoch 28/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.3387 - accuracy: 0.8487\n",
      "Epoch 29/1500\n",
      "66/66 [==============================] - 0s 979us/step - loss: 0.3113 - accuracy: 0.8658\n",
      "Epoch 30/1500\n",
      "66/66 [==============================] - 0s 947us/step - loss: 0.3306 - accuracy: 0.8511\n",
      "Epoch 31/1500\n",
      "66/66 [==============================] - 0s 949us/step - loss: 0.3146 - accuracy: 0.8649\n",
      "Epoch 32/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2960 - accuracy: 0.8744\n",
      "Epoch 33/1500\n",
      "66/66 [==============================] - 0s 994us/step - loss: 0.3111 - accuracy: 0.8677\n",
      "Epoch 34/1500\n",
      "66/66 [==============================] - 0s 945us/step - loss: 0.2988 - accuracy: 0.8649\n",
      "Epoch 35/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2912 - accuracy: 0.8711\n",
      "Epoch 36/1500\n",
      "66/66 [==============================] - 0s 996us/step - loss: 0.3051 - accuracy: 0.8620\n",
      "Epoch 37/1500\n",
      "66/66 [==============================] - 0s 999us/step - loss: 0.2933 - accuracy: 0.8635\n",
      "Epoch 38/1500\n",
      "66/66 [==============================] - 0s 956us/step - loss: 0.2746 - accuracy: 0.8882\n",
      "Epoch 39/1500\n",
      "66/66 [==============================] - 0s 942us/step - loss: 0.3020 - accuracy: 0.8644\n",
      "Epoch 40/1500\n",
      "66/66 [==============================] - 0s 965us/step - loss: 0.2730 - accuracy: 0.8782\n",
      "Epoch 41/1500\n",
      "66/66 [==============================] - 0s 937us/step - loss: 0.2836 - accuracy: 0.8758\n",
      "Epoch 42/1500\n",
      "66/66 [==============================] - 0s 960us/step - loss: 0.2749 - accuracy: 0.8801\n",
      "Epoch 43/1500\n",
      "66/66 [==============================] - 0s 938us/step - loss: 0.2620 - accuracy: 0.8877\n",
      "Epoch 44/1500\n",
      "66/66 [==============================] - 0s 945us/step - loss: 0.2699 - accuracy: 0.8896\n",
      "Epoch 45/1500\n",
      "66/66 [==============================] - 0s 939us/step - loss: 0.2852 - accuracy: 0.8739\n",
      "Epoch 46/1500\n",
      "66/66 [==============================] - 0s 951us/step - loss: 0.2758 - accuracy: 0.8830\n",
      "Epoch 47/1500\n",
      "66/66 [==============================] - 0s 944us/step - loss: 0.2575 - accuracy: 0.8896\n",
      "Epoch 48/1500\n",
      "66/66 [==============================] - 0s 940us/step - loss: 0.2667 - accuracy: 0.8782\n",
      "Epoch 49/1500\n",
      "66/66 [==============================] - 0s 954us/step - loss: 0.2506 - accuracy: 0.8868\n",
      "Epoch 50/1500\n",
      "66/66 [==============================] - 0s 941us/step - loss: 0.2473 - accuracy: 0.8968\n",
      "Epoch 51/1500\n",
      "66/66 [==============================] - 0s 956us/step - loss: 0.2543 - accuracy: 0.8925\n",
      "Epoch 52/1500\n",
      "66/66 [==============================] - 0s 958us/step - loss: 0.2412 - accuracy: 0.8949\n",
      "Epoch 53/1500\n",
      "66/66 [==============================] - 0s 933us/step - loss: 0.2598 - accuracy: 0.8868\n",
      "Epoch 54/1500\n",
      "66/66 [==============================] - 0s 980us/step - loss: 0.2468 - accuracy: 0.8958\n",
      "Epoch 55/1500\n",
      "66/66 [==============================] - 0s 949us/step - loss: 0.2395 - accuracy: 0.8977\n",
      "Epoch 56/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2401 - accuracy: 0.8915\n",
      "Epoch 57/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2398 - accuracy: 0.8982\n",
      "Epoch 58/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.8863\n",
      "Epoch 59/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2308 - accuracy: 0.9010\n",
      "Epoch 60/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2185 - accuracy: 0.9068\n",
      "Epoch 61/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2444 - accuracy: 0.8968\n",
      "Epoch 62/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2203 - accuracy: 0.9034\n",
      "Epoch 63/1500\n",
      "66/66 [==============================] - 0s 967us/step - loss: 0.2184 - accuracy: 0.9039\n",
      "Epoch 64/1500\n",
      "66/66 [==============================] - 0s 976us/step - loss: 0.2188 - accuracy: 0.9096\n",
      "Epoch 65/1500\n",
      "66/66 [==============================] - 0s 932us/step - loss: 0.2239 - accuracy: 0.9087\n",
      "Epoch 66/1500\n",
      "66/66 [==============================] - 0s 955us/step - loss: 0.1976 - accuracy: 0.9153\n",
      "Epoch 67/1500\n",
      "66/66 [==============================] - 0s 952us/step - loss: 0.1899 - accuracy: 0.9186\n",
      "Epoch 68/1500\n",
      "66/66 [==============================] - 0s 993us/step - loss: 0.2029 - accuracy: 0.9120\n",
      "Epoch 69/1500\n",
      "66/66 [==============================] - 0s 989us/step - loss: 0.2015 - accuracy: 0.9144\n",
      "Epoch 70/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2127 - accuracy: 0.9082\n",
      "Epoch 71/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9129\n",
      "Epoch 72/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2038 - accuracy: 0.9210\n",
      "Epoch 73/1500\n",
      "66/66 [==============================] - 0s 959us/step - loss: 0.2126 - accuracy: 0.9029\n",
      "Epoch 74/1500\n",
      "66/66 [==============================] - 0s 947us/step - loss: 0.1945 - accuracy: 0.9144\n",
      "Epoch 75/1500\n",
      "66/66 [==============================] - 0s 987us/step - loss: 0.1958 - accuracy: 0.9139\n",
      "Epoch 76/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9144\n",
      "Epoch 77/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9110\n",
      "Epoch 78/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9163\n",
      "Epoch 79/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1806 - accuracy: 0.9172\n",
      "Epoch 80/1500\n",
      "66/66 [==============================] - 0s 974us/step - loss: 0.1993 - accuracy: 0.9144\n",
      "Epoch 81/1500\n",
      "66/66 [==============================] - 0s 951us/step - loss: 0.1968 - accuracy: 0.9158\n",
      "Epoch 82/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1940 - accuracy: 0.9125\n",
      "Epoch 83/1500\n",
      "66/66 [==============================] - 0s 984us/step - loss: 0.1891 - accuracy: 0.9177\n",
      "Epoch 84/1500\n",
      "66/66 [==============================] - 0s 996us/step - loss: 0.1803 - accuracy: 0.9234\n",
      "Epoch 85/1500\n",
      "66/66 [==============================] - 0s 980us/step - loss: 0.1748 - accuracy: 0.9258\n",
      "Epoch 86/1500\n",
      "66/66 [==============================] - 0s 987us/step - loss: 0.1954 - accuracy: 0.9186\n",
      "Epoch 87/1500\n",
      "66/66 [==============================] - 0s 952us/step - loss: 0.1815 - accuracy: 0.9277\n",
      "Epoch 88/1500\n",
      "66/66 [==============================] - 0s 946us/step - loss: 0.1877 - accuracy: 0.9191\n",
      "Epoch 89/1500\n",
      "66/66 [==============================] - 0s 947us/step - loss: 0.1808 - accuracy: 0.9225\n",
      "Epoch 90/1500\n",
      "66/66 [==============================] - 0s 966us/step - loss: 0.1763 - accuracy: 0.9182\n",
      "Epoch 91/1500\n",
      "66/66 [==============================] - 0s 950us/step - loss: 0.1973 - accuracy: 0.9144\n",
      "Epoch 92/1500\n",
      "66/66 [==============================] - 0s 959us/step - loss: 0.1911 - accuracy: 0.9191\n",
      "Epoch 93/1500\n",
      "66/66 [==============================] - 0s 989us/step - loss: 0.1831 - accuracy: 0.9305\n",
      "Epoch 94/1500\n",
      "66/66 [==============================] - 0s 987us/step - loss: 0.1702 - accuracy: 0.9225\n",
      "Epoch 95/1500\n",
      "66/66 [==============================] - 0s 987us/step - loss: 0.1890 - accuracy: 0.9248\n",
      "Epoch 96/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.9201\n",
      "Epoch 97/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9182\n",
      "Epoch 98/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.9324\n",
      "Epoch 99/1500\n",
      "66/66 [==============================] - 0s 995us/step - loss: 0.1886 - accuracy: 0.9201\n",
      "Epoch 100/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9410\n",
      "Epoch 101/1500\n",
      "66/66 [==============================] - 0s 953us/step - loss: 0.1465 - accuracy: 0.9396\n",
      "Epoch 102/1500\n",
      "66/66 [==============================] - 0s 947us/step - loss: 0.1735 - accuracy: 0.9277\n",
      "Epoch 103/1500\n",
      "66/66 [==============================] - 0s 966us/step - loss: 0.1665 - accuracy: 0.9244\n",
      "Epoch 104/1500\n",
      "66/66 [==============================] - 0s 953us/step - loss: 0.1855 - accuracy: 0.9263\n",
      "Epoch 105/1500\n",
      "66/66 [==============================] - 0s 946us/step - loss: 0.1623 - accuracy: 0.9324\n",
      "Epoch 106/1500\n",
      "66/66 [==============================] - 0s 959us/step - loss: 0.1605 - accuracy: 0.9286\n",
      "Epoch 107/1500\n",
      "66/66 [==============================] - 0s 952us/step - loss: 0.1694 - accuracy: 0.9258\n",
      "Epoch 108/1500\n",
      "66/66 [==============================] - 0s 929us/step - loss: 0.1732 - accuracy: 0.9296\n",
      "Epoch 109/1500\n",
      "66/66 [==============================] - 0s 952us/step - loss: 0.1512 - accuracy: 0.9415\n",
      "Epoch 110/1500\n",
      "66/66 [==============================] - 0s 944us/step - loss: 0.1587 - accuracy: 0.9305\n",
      "Epoch 111/1500\n",
      "66/66 [==============================] - 0s 939us/step - loss: 0.1521 - accuracy: 0.9372\n",
      "Epoch 112/1500\n",
      "66/66 [==============================] - 0s 922us/step - loss: 0.1681 - accuracy: 0.9258\n",
      "Epoch 113/1500\n",
      "66/66 [==============================] - 0s 980us/step - loss: 0.1483 - accuracy: 0.9424\n",
      "Epoch 114/1500\n",
      "66/66 [==============================] - 0s 991us/step - loss: 0.1565 - accuracy: 0.9363\n",
      "Epoch 115/1500\n",
      "66/66 [==============================] - 0s 972us/step - loss: 0.1518 - accuracy: 0.9315\n",
      "Epoch 116/1500\n",
      "66/66 [==============================] - 0s 959us/step - loss: 0.1578 - accuracy: 0.9339\n",
      "Epoch 117/1500\n",
      "66/66 [==============================] - 0s 969us/step - loss: 0.1441 - accuracy: 0.9410\n",
      "Epoch 118/1500\n",
      "66/66 [==============================] - 0s 967us/step - loss: 0.1504 - accuracy: 0.9363\n",
      "Epoch 119/1500\n",
      "66/66 [==============================] - 0s 942us/step - loss: 0.1519 - accuracy: 0.9367\n",
      "Epoch 120/1500\n",
      "66/66 [==============================] - 0s 962us/step - loss: 0.1417 - accuracy: 0.9448\n",
      "Epoch 121/1500\n",
      "66/66 [==============================] - 0s 977us/step - loss: 0.1366 - accuracy: 0.9467\n",
      "Epoch 122/1500\n",
      "66/66 [==============================] - 0s 970us/step - loss: 0.1476 - accuracy: 0.9401\n",
      "Epoch 123/1500\n",
      "66/66 [==============================] - 0s 956us/step - loss: 0.1512 - accuracy: 0.9386\n",
      "Epoch 124/1500\n",
      "66/66 [==============================] - 0s 949us/step - loss: 0.1538 - accuracy: 0.9382\n",
      "Epoch 125/1500\n",
      "66/66 [==============================] - 0s 966us/step - loss: 0.1553 - accuracy: 0.9367\n",
      "Epoch 126/1500\n",
      "66/66 [==============================] - 0s 952us/step - loss: 0.1452 - accuracy: 0.9372\n",
      "Epoch 127/1500\n",
      "66/66 [==============================] - 0s 939us/step - loss: 0.1379 - accuracy: 0.9443\n",
      "Epoch 128/1500\n",
      "66/66 [==============================] - 0s 971us/step - loss: 0.1494 - accuracy: 0.9296\n",
      "Epoch 129/1500\n",
      "66/66 [==============================] - 0s 973us/step - loss: 0.1347 - accuracy: 0.9429\n",
      "Epoch 130/1500\n",
      "66/66 [==============================] - 0s 981us/step - loss: 0.1412 - accuracy: 0.9410\n",
      "Epoch 131/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9429\n",
      "Epoch 132/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.9410\n",
      "Epoch 133/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9348\n",
      "Epoch 134/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9391\n",
      "Epoch 135/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9405\n",
      "Epoch 136/1500\n",
      "66/66 [==============================] - 0s 974us/step - loss: 0.1560 - accuracy: 0.9343\n",
      "Epoch 137/1500\n",
      "66/66 [==============================] - 0s 940us/step - loss: 0.1373 - accuracy: 0.9401\n",
      "Epoch 138/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.9405\n",
      "Epoch 139/1500\n",
      "66/66 [==============================] - 0s 1000us/step - loss: 0.1375 - accuracy: 0.9453\n",
      "Epoch 140/1500\n",
      "66/66 [==============================] - 0s 966us/step - loss: 0.1282 - accuracy: 0.9577\n",
      "Epoch 141/1500\n",
      "66/66 [==============================] - 0s 973us/step - loss: 0.1569 - accuracy: 0.9377\n",
      "Epoch 142/1500\n",
      "66/66 [==============================] - 0s 983us/step - loss: 0.1439 - accuracy: 0.9429\n",
      "Epoch 143/1500\n",
      "66/66 [==============================] - 0s 979us/step - loss: 0.1353 - accuracy: 0.9443\n",
      "Epoch 144/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.9424\n",
      "Epoch 145/1500\n",
      "66/66 [==============================] - 0s 974us/step - loss: 0.1334 - accuracy: 0.9453\n",
      "Epoch 146/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9420\n",
      "Epoch 147/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9372\n",
      "Epoch 148/1500\n",
      "66/66 [==============================] - 0s 977us/step - loss: 0.1311 - accuracy: 0.9453\n",
      "Epoch 149/1500\n",
      "66/66 [==============================] - 0s 978us/step - loss: 0.1349 - accuracy: 0.9486\n",
      "Epoch 150/1500\n",
      "66/66 [==============================] - 0s 986us/step - loss: 0.1243 - accuracy: 0.9491\n",
      "Epoch 151/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9453\n",
      "Epoch 152/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9515\n",
      "Epoch 153/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9491\n",
      "Epoch 154/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9467\n",
      "Epoch 155/1500\n",
      "66/66 [==============================] - 0s 992us/step - loss: 0.1291 - accuracy: 0.9534\n",
      "Epoch 156/1500\n",
      "66/66 [==============================] - 0s 946us/step - loss: 0.1198 - accuracy: 0.9534\n",
      "Epoch 157/1500\n",
      "66/66 [==============================] - 0s 987us/step - loss: 0.1321 - accuracy: 0.9410\n",
      "Epoch 158/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9477\n",
      "Epoch 159/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9591\n",
      "Epoch 160/1500\n",
      "66/66 [==============================] - 0s 991us/step - loss: 0.1342 - accuracy: 0.9448\n",
      "Epoch 161/1500\n",
      "66/66 [==============================] - 0s 961us/step - loss: 0.1342 - accuracy: 0.9515\n",
      "Epoch 162/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9500\n",
      "Epoch 163/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9520\n",
      "Epoch 164/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9496\n",
      "Epoch 165/1500\n",
      "66/66 [==============================] - 0s 996us/step - loss: 0.1197 - accuracy: 0.9491\n",
      "Epoch 166/1500\n",
      "66/66 [==============================] - 0s 943us/step - loss: 0.1202 - accuracy: 0.9543\n",
      "Epoch 167/1500\n",
      "66/66 [==============================] - 0s 946us/step - loss: 0.1137 - accuracy: 0.9524\n",
      "Epoch 168/1500\n",
      "66/66 [==============================] - 0s 953us/step - loss: 0.1151 - accuracy: 0.9467\n",
      "Epoch 169/1500\n",
      "66/66 [==============================] - 0s 954us/step - loss: 0.1286 - accuracy: 0.9453\n",
      "Epoch 170/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9410\n",
      "Epoch 171/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9529\n",
      "Epoch 172/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9500\n",
      "Epoch 173/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9558\n",
      "Epoch 174/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9434\n",
      "Epoch 175/1500\n",
      "66/66 [==============================] - 0s 971us/step - loss: 0.1106 - accuracy: 0.9591\n",
      "Epoch 176/1500\n",
      "66/66 [==============================] - 0s 972us/step - loss: 0.1108 - accuracy: 0.9553\n",
      "Epoch 177/1500\n",
      "66/66 [==============================] - 0s 1000us/step - loss: 0.1156 - accuracy: 0.9581\n",
      "Epoch 178/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9520\n",
      "Epoch 179/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9572\n",
      "Epoch 180/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9567\n",
      "Epoch 181/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9558\n",
      "Epoch 182/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9577\n",
      "Epoch 183/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9477\n",
      "Epoch 184/1500\n",
      "66/66 [==============================] - 0s 990us/step - loss: 0.1167 - accuracy: 0.9515\n",
      "Epoch 185/1500\n",
      "66/66 [==============================] - 0s 994us/step - loss: 0.1171 - accuracy: 0.9562\n",
      "Epoch 186/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9567\n",
      "Epoch 187/1500\n",
      "66/66 [==============================] - 0s 954us/step - loss: 0.0845 - accuracy: 0.9681\n",
      "Epoch 188/1500\n",
      "66/66 [==============================] - 0s 956us/step - loss: 0.1084 - accuracy: 0.9562\n",
      "Epoch 189/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9619\n",
      "Epoch 190/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9505\n",
      "Epoch 191/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9567\n",
      "Epoch 192/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9472\n",
      "Epoch 193/1500\n",
      "66/66 [==============================] - 0s 983us/step - loss: 0.1150 - accuracy: 0.9529\n",
      "Epoch 194/1500\n",
      "66/66 [==============================] - 0s 4ms/step - loss: 0.1150 - accuracy: 0.9539\n",
      "Epoch 195/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9634\n",
      "Epoch 196/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9610\n",
      "Epoch 197/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9524\n",
      "Epoch 198/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.9567\n",
      "Epoch 199/1500\n",
      "66/66 [==============================] - 0s 985us/step - loss: 0.1157 - accuracy: 0.9539\n",
      "Epoch 200/1500\n",
      "66/66 [==============================] - 0s 953us/step - loss: 0.1245 - accuracy: 0.9472\n",
      "Epoch 201/1500\n",
      "66/66 [==============================] - 0s 969us/step - loss: 0.0932 - accuracy: 0.9657\n",
      "Epoch 202/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9539\n",
      "Epoch 203/1500\n",
      "66/66 [==============================] - 0s 989us/step - loss: 0.1032 - accuracy: 0.9624\n",
      "Epoch 204/1500\n",
      "66/66 [==============================] - 0s 955us/step - loss: 0.1009 - accuracy: 0.9577\n",
      "Epoch 205/1500\n",
      "66/66 [==============================] - 0s 963us/step - loss: 0.1130 - accuracy: 0.9553\n",
      "Epoch 206/1500\n",
      "66/66 [==============================] - 0s 994us/step - loss: 0.0998 - accuracy: 0.9610\n",
      "Epoch 207/1500\n",
      "66/66 [==============================] - 0s 1000us/step - loss: 0.0930 - accuracy: 0.9624\n",
      "Epoch 208/1500\n",
      "66/66 [==============================] - 0s 945us/step - loss: 0.1055 - accuracy: 0.9558\n",
      "Epoch 209/1500\n",
      "66/66 [==============================] - 0s 973us/step - loss: 0.0948 - accuracy: 0.9605\n",
      "Epoch 210/1500\n",
      "66/66 [==============================] - 0s 993us/step - loss: 0.1120 - accuracy: 0.9496\n",
      "Epoch 211/1500\n",
      "66/66 [==============================] - 0s 965us/step - loss: 0.0871 - accuracy: 0.9629\n",
      "Epoch 212/1500\n",
      "66/66 [==============================] - 0s 955us/step - loss: 0.0907 - accuracy: 0.9667\n",
      "Epoch 213/1500\n",
      "66/66 [==============================] - 0s 990us/step - loss: 0.0941 - accuracy: 0.9634\n",
      "Epoch 214/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9539\n",
      "Epoch 215/1500\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9553\n",
      "Epoch 216/1500\n",
      "66/66 [==============================] - 0s 987us/step - loss: 0.1033 - accuracy: 0.9577\n",
      "Epoch 217/1500\n",
      "52/66 [======================>.......] - ETA: 0s - loss: 0.1162 - accuracy: 0.9495Restoring model weights from the end of the best epoch: 187.\n",
      "66/66 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9486\n",
      "Epoch 217: early stopping\n",
      "6/6 [==============================] - 0s 933us/step - loss: 1.1135 - accuracy: 0.6412\n",
      "6/6 [==============================] - 0s 648us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.65 (13/20)\n",
      "Before appending - Cat IDs: 682, Predictions: 682, Actuals: 682, Gender: 682\n",
      "After appending - Cat IDs: 852, Predictions: 852, Actuals: 852, Gender: 852\n",
      "Final Test Results - Loss: 1.1135371923446655, Accuracy: 0.6411764621734619, Precision: 0.6721492015609662, Recall: 0.7034437946718648, F1 Score: 0.6534138522577829\n",
      "Confusion Matrix:\n",
      " [[56  3 46]\n",
      " [ 2 36  0]\n",
      " [10  0 17]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6031499326185135\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8427742004394532\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.736316442489624\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6242735662796866\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6027007051277322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=int(random_seeds[1]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train_val),\n",
    "        y=y_train_val\n",
    "    )\n",
    "    weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping], class_weight=weight_dict)\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "# Append averages to total lists\n",
    "all_f1.append(unseen_set_avg_f1)\n",
    "all_losses.append(unseen_set_avg_loss)\n",
    "all_accuracies.append(unseen_set_avg_acc)\n",
    "all_precisions.append(unseen_set_avg_precision)\n",
    "all_recalls.append(unseen_set_avg_recall)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fde45c61-b809-4c9f-8754-70d344a20190",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed_2_folds_values = (unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1)\n",
    "\n",
    "# Plotting all fold metrics\n",
    "seed_2_folds_plot = plot_all_metrics(unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1, \"Folds\", \"Fold Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab4362-eb48-4e85-a963-5fa11d9788b4",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4c960e52-8c1c-4e31-9e1b-d2ccb3b4a895",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 852, Predictions: 852, Actuals: 852, Gender: 852\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "647f78df-682b-4082-acb9-1387c1d7bed0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfc64ac1-f1f5-4af3-b1bc-6057d9aba1e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.76 (84/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "# store for final evaluation \n",
    "all_majority_vote_accuracies.append(majority_vote_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60d3fda6-5e34-4f5b-b0d0-fc9086cafccd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3782ea5-9117-496a-9507-5bd7087ed100",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, kitten, adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, adult, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, senior, adult, senior, senior,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, kitten, adult, adult, ad...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "65    059A  [adult, adult, senior, senior, senior, adult, ...        senior           senior                   True\n",
       "64    058A                           [senior, senior, senior]        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A            [kitten, kitten, adult, kitten, kitten]        kitten           kitten                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "102   108A     [senior, adult, adult, senior, senior, senior]        senior           senior                   True\n",
       "100   105A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, adult, adult, senior,...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "17    015A  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "34    027A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "32    026B                                            [adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, senior, adult, adult, a...         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A       [adult, senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                     [senior, adult, senior, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "40    034A               [adult, senior, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, kitten, adult, ki...         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "6     005A  [senior, senior, senior, adult, senior, senior...        senior            adult                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "101   106A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "103   109A        [adult, adult, kitten, adult, adult, adult]         adult           kitten                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "18    016A  [senior, adult, senior, adult, senior, senior,...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "42    036A  [senior, senior, senior, senior, adult, adult,...        senior            adult                  False\n",
       "56    051A  [senior, adult, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "57    051B  [senior, adult, adult, adult, senior, adult, s...         adult           senior                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "59    053A   [senior, senior, senior, senior, senior, senior]        senior            adult                  False\n",
       "60    054A                                    [adult, senior]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, senior, a...         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "69    063A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "74    068A  [adult, senior, senior, senior, senior, adult,...        senior            adult                  False\n",
       "30    025C            [senior, senior, senior, senior, adult]        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "109   117A  [senior, senior, adult, adult, adult, adult, a...         adult           senior                  False"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d9247461-bb2f-4a9f-8810-8732407763f7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     61\n",
      "kitten    12\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16e409aa-1496-4526-8133-118ee04c7cbe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             61  83.561644\n",
      "1           kitten           15             12  80.000000\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# store for final evaluation \n",
    "all_majority_vote_details.append(class_stats)\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5803a711-1fb5-42bb-ba2c-227e8d91de6c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbb732a-a8ec-4e08-b320-67f16cebd21c",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca565717-6538-443a-91ef-3127a74a1887",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          556            438  78.776978\n",
      "1           kitten          118            101  85.593220\n",
      "2           senior          178             89  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# store for final evaluation \n",
    "all_class_stats.append(class_stats)\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "698469f4-d547-4b7b-a358-33e235e22b15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABhFUlEQVR4nO3dd1iV9f/H8ecBQZYioqi4t6m5B67cM1c5+2VDc+XKMrPcZWblyJ3mytA0K/dKc5SiZG5NRFFRFHcuhsg4vz+4uL+cAEWGgOf1uK6ui3PP9308d+d1Pvfn/twms9lsRkRERETESthkdAEiIiIiIs+SArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErEq2jC5AxBqFhoaydu1afHx8uHDhAnfv3iV79uzky5eP6tWr8+qrr1KqVKmMLjPNBAcH0759e+P1wYMHjb/btWvH1atXAZg3bx41atRI9nbDw8Np1aoVoaGhAJQtW5bly5enUdWSUo/7984IGzduZPz48cbrYcOG8dprr2VcQU8hKiqK7du3s337ds6dO8ft27cxm83kypWLMmXK0LRpU1q1akW2bPo6F3kaOmNEnrHDhw/zySefcPv2bYvpkZGRhISEcO7cOX7++We6dOnCBx98oC+2x9i+fbsRfgH8/f35559/qFChQgZWJZnN+vXrLV6vWbMmSwTgwMBAxo4dy6lTpxLMu379OtevX2fPnj0sX76cb775hvz582dAlSJZk75ZRZ6h48ePM3jwYCIiIgCwtbWlVq1aFCtWjPDwcP7++2+uXLmC2Wxm1apV/Pvvv3z55ZcZXHXmtW7dugTT1qxZowAshkuXLnH48GGLaefPn+fo0aNUqVIlY4pKhsuXL9OzZ08ePHgAgI2NDdWrV6dkyZJERERw/Phxzp07B8DZs2cZMmQIy5cvx87OLiPLFskyFIBFnpGIiAhGjx5thN+CBQsydepUi64O0dHRLFy4kAULFgDw+++/s2bNGl555ZUMqTkzCwwM5NixYwDkzJmT+/fvA7Bt2zbef/99nJ2dM7I8ySTit/7G/5ysWbMm0wbgqKgoPvroIyP85s+fn6lTp1K2bFmL5X7++We++uorIDbUb9q0iY4dOz7rckWyJAVgkWfkt99+Izg4GIhtzZk8eXKCfr62trb069ePCxcu8PvvvwOwZMkSOnbsyJ9//smwYcMA8PT0ZN26dZhMJov1u3TpwoULFwCYPn069evXB2LD98qVK9myZQtBQUHY29tTunRpXn31VVq2bGmxnYMHD9K/f38AmjdvTps2bZg2bRrXrl0jX758zJkzh4IFC3Lr1i0WLVrE/v37uXHjBtHR0eTKlYvy5cvTs2dPKlWqlA7v4v/Eb/3t0qULvr6+/PPPP4SFhbF161Y6deqU5LqnT5/G29ubw4cPc/fuXXLnzk3JkiXp3r07devWTbB8SEgIy5cvZ9euXVy+fBk7Ozs8PT1p0aIFXbp0wcnJyVh2/PjxbNy4EYA+ffrQr18/Y17897ZAgQJs2LDBmBfX99nd3Z0FCxYwfvx4/Pz8yJkzJx999BFNmzbl0aNHLF++nO3btxMUFERERATOzs4UL16cTp068fLLL6e49l69enH8+HEAhg4dSo8ePSy2s2LFCqZOnQpA/fr1mT59epLv7389evSIJUuWsGHDBv79918KFSpE+/bt6d69u9HFZ9SoUfz2228AdO3alY8++shiG7t37+bDDz8EoGTJkvz0009P3G9UVJTxbwGx/zYffPABEPvj8sMPPyRHjhyJrhsaGsrixYvZvn07t27dwtPTk86dO9OtWze8vLyIjo5O8G8IsZ+txYsXc/jwYUJDQ/Hw8KBOnTr07NmTfPnyJev9+v333zlz5gwQ+/+KadOmUaZMmQTLdenShXPnznHv3j1KlChByZIljXnJPY8Brl69yqpVq9izZw/Xrl0jW7ZslCpVijZt2tC+ffsE3bDi99Nfv349np6eFu9xYp//DRs28OmnnwLQo0cPXnvtNebMmcO+ffuIiIjghRdeoE+fPtSsWTNZ75FIaikAizwjf/75p/F3zZo1E/1Ci/P6668bATg4OJiAgADq1auHu7s7t2/fJjg4mGPHjlm0YPn5+RnhN2/evNSpUweI/SIfNGgQJ06cMJaNiIjg8OHDHD58GF9fX8aNG5cgTEPspdWPPvqIyMhIILafsqenJ3fu3KFv375cunTJYvnbt2+zZ88e9u3bx8yZM6ldu/ZTvkvJExUVxaZNm4zX7dq1I3/+/Pzzzz9AbOteUgF448aNTJgwgejoaGNaXH/Kffv2MWjQIN5++21j3rVr13j33XcJCgoypj18+BB/f3/8/f3ZsWMH8+bNswjBqfHw4UMGDRpk/Fi6ffs2ZcqUISYmhlGjRrFr1y6L5R88eMDx48c5fvw4ly9ftgjcT1N7+/btjQC8bdu2BAF4+/btxt9t27Z9qmMaOnQoBw4cMF6fP3+e6dOnc+zYMb7++mtMJhMdOnQwAvCOHTv48MMPsbH530BFKdm/j48Pt27dAqBq1aq89NJLVKpUiePHjxMREcGmTZvo3r17gvVCQkLo06cPZ8+eNaYFBgYyZcoUAgICktzf1q1bGTdunMVn68qVK/zyyy9s376dWbNmUb58+SfWHf9Yvby8Hvv/io8//viJ20vqPAbYt28fI0eOJCQkxGKdo0ePcvToUbZu3cq0adNwcXF54n6SKzg4mB49enDnzh1j2uHDhxk4cCBjxoyhXbt2abYvkaRoGDSRZyT+l+mTLr2+8MILFn35/Pz8yJYtm8UX/9atWy3W2bx5s/H3yy+/jK2tLQBTp041wq+joyPt2rXj5ZdfJnv27EBsIFyzZk2idQQGBmIymWjXrh3NmjWjdevWmEwmvv/+eyP8FixYkO7du/Pqq6+SJ08eILYrx8qVKx97jKmxZ88e/v33XyA22BQqVIgWLVrg6OgIxLbC+fn5JVjv/PnzTJw40QgopUuXpkuXLnh5eRnLzJ49G39/f+P1qFGjjADp4uJC27Zt6dChg9HF4tSpU3z77bdpdmyhoaEEBwfToEEDXnnlFWrXrk3hwoXZu3evEX6dnZ3p0KED3bt3twhHP/74I2azOUW1t2jRwgjxp06d4vLly8Z2rl27ZnyGcubMyUsvvfRUx3TgwAFeeOEFunTpQrly5Yzpu3btMlrya9asabRI3r59m0OHDhnLRUREsGfPHiD2Kknr1q2Ttd/4Vwnizp0OHToY09auXZvoejNnzrQ4X+vWrcurr76Kp6cna9eutQi4cS5evGjxw6pChQoWx3vv3j0++eQTowvU45w+fdr4u3Llyk9c/kmSOo+Dg4P55JNPjPCbL18+XnnlFZo0aWK0+h4+fJgxY8akuob4du7cyZ07d6hbty6vvPIKHh4eAMTExPDll18ao8KIpCe1AIs8I/FbO9zd3R+7bLZs2ciZM6cxUsTdu3cBaN++PUuXLgViW4k+/PBDsmXLRnR0NNu2bTPWjxuC6tatW0ZLqZ2dHYsXL6Z06dIAdO7cmXfeeYeYmBiWLVvGq6++mmgtQ4YMSdBKVrhwYVq2bMmlS5eYMWMGuXPnBqB169b06dMHiG35Si/xg01ca5GzszPNmjUzLkmvXr2aUaNGWay3YsUKoxWsUaNGfPnll8YX/eeff87atWtxdnbmwIEDlC1blmPHjhn9jJ2dnVm2bBmFChUy9tu7d29sbW35559/iImJsWixTI3GjRszefJki2n29vZ07NiRs2fP0r9/f6OF/+HDhzRv3pzw8HBCQ0O5e/cubm5uT127k5MTzZo1M/rMbtu2jV69egGxl+TjgnWLFi2wt7d/quNp3rw5EydOxMbGhpiYGMaMGWO09q5evZqOHTsaAW3evHnG/uMuh/v4+BAWFgZA7dq1jR9aj3Pr1i18fHyA2B9+zZs3N2qZOnUqYWFhBAQEcPz4cYvuOuHh4RZXF+J3BwkNDaVPnz5G94T4Vq5caYTbVq1aMWHCBEwmEzExMQwbNow9e/Zw5coVdu7c+cQAH3+EmLhzK05UVJTFD7b4EuuSESex83jJkiXGKCrly5dn7ty5RkvvkSNH6N+/P9HR0ezZs4eDBw8+1RCFT/Lhhx8a9dy5c4cePXpw/fp1IiIiWLNmDQMGDEizfYkkRi3AIs9IVFSU8Xf8VrqkxF8m7u+iRYtStWpVILZFaf/+/UBsC1vcl2aVKlUoUqQIAIcOHTJapKpUqWKEX4AXX3yRYsWKAbF3ysddcv+vli1bJpjWuXNnJk6ciLe3N7lz5+bevXvs3bvXIjgkp6UrJW7cuGEct6OjI82aNTPmxW/d27ZtmxGa4sQfj7Zr164WfRsHDhzI2rVr2b17N2+88UaC5V966SUjQELs+7ls2TL+/PNPFi9enGbhFxJ/z728vBg9ejRLly6lTp06REREcPToUby9vS0+K3Hve0pq/+/7FyeuOw48ffcHgJ49exr7sLGx4c033zTm+fv7Gz9K2rZtayy3c+dO45yJ3yUguZfHN27caHz2mzRpYrRuOzk5GWEYSHD1w8/Pz3gPc+TIYREanZ2dLWqPL34Xj06dOhldimxsbCz6Zv/1119PrD3u6gyQaGtzSiT2mYr/vg4aNMiim0PVqlVp0aKF8Xr37t1pUgfENgB07drVeO3m5kaXLl2M13E/3ETSk1qARZ4RV1dXbt68CWD0S0zKo0ePuHfvnvE6V65cxt8dOnTgyJEjQGw3iAYNGlh0f4j/AIJr164Zf//999+PbcG5cOGCxc0sAA4ODri5uSW6/MmTJ1m3bh2HDh1K0BcYYi9npocNGzYYocDW1ta4MSqOyWTCbDYTGhrKb7/9ZjGCxo0bN4y/CxQoYLGem5tbgmN93PKAxeX85EjOD5+k9gWx/56rV6/G19cXf3//RMNR3PuektorV65MsWLFCAwMJCAggAsXLuDo6MjJkycBKFasGBUrVkzWMcQX94MsTtwPL4gNePfu3SNPnjzkz58fLy8v9u3bx7179/jrr7+oXr06e/fuBWIDaXK7X8Qf/eHUqVMWLYrxz7/t27czbNgwI/zFnaMQ273nvzeAFS9ePNH9xT/X4q6CJCaun/7j5MuXj/PnzwOx/dPjs7Gx4a233jJeBwQEGC3dSUnsPL57965Fv9/EPg/lypVjy5YtABb9yB8nOed94cKFE/xgjP++/neMdJH0oAAs8oyUKVPG+HKN378xMcePH7cIN/G/nJo1a8bkyZMJDQ3lzz//5MGDB/zxxx9Awtat+F9G2bNnf+yNLHGtcPElNZTYihUrmDZtGmazGQcHBxo2bEiVKlXInz8/n3zyyWOPLTXMZrNFsAkJCbFoefuvxw0h97Qtaylpiftv4E3sPU5MYu/7sWPHGDx4MGFhYZhMJqpUqUK1atWoVKkSn3/+uUVw+6+nqb1Dhw7MmDEDiG0Fjn9zX0pafyH2uB0cHJKsJ66/OsT+gNu3b5+x//DwcMLDw4HY7gvxW0eTcvjwYYsfZRcuXEgyeD58+JDNmzcbLZLx/82e5kdc/GVz5cplcUzxJefBNhUqVDAC8H+fomdjY8PgwYON1xs2bHhiAE7s85ScOuK/F4ndJAsJ36PkfMYfPXqUYFr8ex6S2pdIWlIAFnlGGjRoYHxRHTlyhBMnTvDiiy8muqy3t7fxd/78+S26Ljg4ONCiRQvWrFlDeHg4c+fONS71N2vWzLgRDGJHg4hTtWpVZs+ebbGf6OjoJL+ogUQH1b9//z6zZs3CbDZjZ2fHqlWrjJbjuC/t9HLo0KGn6lt86tQp/P39jfFTPTw8jJaswMBAi5bIS5cu8euvv1KiRAnKli1LuXLljJtzIPYmp//69ttvyZEjByVLlqRq1ao4ODhYtGw9fPjQYvm4vtxPktj7Pm3aNOPfecKECbRq1cqYF797TZyU1A6xN1DOmTOHqKgotm3bZoQnGxsb2rRpk6z6/+vs2bNUq1bNeB0/nGbPnp2cOXMarxs2bEiuXLm4e/cuu3fvNsbtheR3f0jsASmPs3btWiMAxz9ngoODiYqKsgiLSY0C4eHhYXw2p02bZtGv+Enn2X+1bt3a6Mt74sQJDh06RPXq1RNdNjkhPbHPk4uLCy4uLkYrsL+/f4IhyOLfDFq4cGHj77i+3JDwMx7/ylVS4obwi/9jJv5nIv6/gUh6UR9gkWekbdu2xs07ZrOZjz76KMEjTiMjI5k2bZpFi87bb7+d4HJh/L6av/76q/F3/O4PANWrVzdaUw4dOmTxhXbmzBkaNGhAt27dGDVqVIIvMki8JebixYtGC46tra3FOKrxu2KkRxeI+Hftd+/enYMHDyb6X61atYzlVq9ebfwdP0SsWrXKorVq1apVLF++nAkTJrBo0aIEy+/fv9948hbE3qm/aNEipk+fztChQ433JH6Y++8Pgh07diTrOJMaki5O/C4x+/fvt7jBMu59T0ntEHvTVYMGDYDYf+u4z2itWrUsQvXTWLx4sRHSzWazcSMnQMWKFS3CoZ2dnRG0Q0NDjdEfihQpkuQPxvhCQkIs3udly5Yl+hnZuHGj8T6fOXPG6ObxwgsvGMEsJCTEYjST+/fv8/333ye63/gBf8WKFRaf/48//pgWLVrQv39/i363SalZs6bF9kaOHGkMURffzp07mTNnzhO3l1SLavzuJHPmzLF4rPjRo0ct+oE3adLE+Dv+OR//M379+nWL4RaT8uDBA4vPQEhIiMV5Gnefg0h6UguwyDPi4ODAxIkTGThwIFFRUdy8eZO3336bGjVqULJkScLCwvD19bXo8/fSSy8lOp5txYoVKVmyJOfOnTO+aIsWLZpgeLUCBQrQuHFjdu7cSWRkJL169aJJkyY4Ozvz+++/8+jRI86dO0eJEiUsLlE/Tvw78B8+fEjPnj2pXbs2fn5+Fl/SaX0T3IMHDyzGwI1/89t/tWzZ0ugasXXrVoYOHYqjoyPdu3dn48aNREVFceDAAV577TVq1qzJlStXjMvuAN26dQNibxaLP25sz549adiwIQ4ODhZBpk2bNkbwjd9av2/fPiZNmkTZsmX5448/nnip+nHy5Mlj3Kg4cuRIWrRowe3bty3Gl4b/ve8pqT1Ohw4dEow3nNLuDwC+vr706NGDGjVqcPLkSSNsAhY3Q8Xf/48//pii/W/dutX4MVeoUKEk+2nnz5+fKlWqGP3pV69eTcWKFXFycqJdu3b88ssvQOwDZQ4ePEjevHnZt29fgj65cV577TU2b95MdHQ027dv5+LFi1StWpULFy4Yn8W7d+8yfPjwJx6DyWTi008/pUePHty7d4/bt2/zzjvvULVqVcqUKUNERESife+f9umHb775Jjt27CAiIoKTJ0/SrVs36tSpw/379/njjz+MriqNGjWyCKVlypTh77//BmDKlCncuHEDs9nMypUrje4qT/Ldd99x5MgRihQpwv79+43PtqOjo8UPfJH0ohZgkWeoevXqzJ492xgGLSYmhgMHDrBixQrWrVtn8eXasWNHvvrqqyRbb/77JZHU5eGRI0dSokQJIDYcbdmyhV9++cW4HF+qVClGjBiR7GMoUKCARfgMDAzkp59+4vjx42TLls0I0vfu3bO4fJ1aW7ZsMcJd3rx5Hzs+apMmTYzLvnE3w0HssX7yySdGi2NgYCA///yzRfjt2bOnxc2Cn3/+uTE+bVhYGFu2bGHNmjXGpeMSJUowdOhQi33HLQ+xLfRffPEFPj4+Fne6P624kSkgtiXyl19+YdeuXURHR1v07Y5/s9LT1h6nTp06FpehnZ2dadSoUYrqLlOmDNWqVSMgIICVK1dahN/27dvTtGnTBOuULFnS4ma7p+l+Eb+P+ON+JIHlyAjbt2833pdBgwYZ5wzA3r17WbNmDdevX7cI4vGvzJQpU4bhw4dbtCr/9NNPRvg1mUx89NFHFk9re5wCBQqwbNky48EZZrOZw4cPs3LlStasWWMRfm1tbWnTps1Tj0ddqlQpPvvsMyM4X7t2jTVr1rBjxw6jxb569eqMHz/eYr3XX3/dOM5///2X6dOnM2PGDO7fv5+sHyrFihWjYMGC/P333/z6668WT8gcNWpUiq80iDwNBWCRZ6xGjRqsW7eO4cOH4+Xlhbu7O9myZTMeadu5c2eWLVvG6NGjE+27F6dNmzbGfFtb2yS/eHLlysUPP/zAgAEDKFu2LE5OTjg5OVGqVCneffddFi5caHFJPTk+++wzBgwYQLFixbC3t8fV1ZX69euzcOFCGjduDMR+Ye/cufOptvs48ft1NmnS5LE3yuTIkcPikcbxh7rq0KEDS5YsoXnz5ri7u2Nra0vOnDmpXbs2U6ZMYeDAgRbb8vT0xNvbm169elG8eHGyZ89O9uzZKVmyJH379mXp0qW4uroayzs6OrJw4UJat25Nrly5cHBwoGLFinz++eeJhs3k6tKlC19++SXly5fHyckJR0dHKlasyIQJEyy2G//y/9PWHsfW1pYKFSoYr5s1a5bsKwT/ZW9vz+zZs+nTpw+enp7Y29tTokQJPv7448c+YCF+d4caNWqQP3/+J+7r7NmzFt2KnhSAmzVrZvwYCg8PNx4u4+LiwuLFi+nevTseHh7Y29tTpkwZvvjiC15//XVj/f++J507d2bRokU0a9aMPHnyYGdnR758+XjppZdYsGABnTt3fuIxxFegQAGWLFnCpEmTaNq0KQUKFMDe3p7s2bOTP39+6tWrx9ChQ9mwYQOfffZZkiO2PE7Tpk1ZsWIFb7zxBsWLF8fBwQFnZ2cqV67MqFGjmDNnToKbZ+vXr88333xDpUqVjBEmWrRowbJly5I1Skju3LlZsmQJL7/8Mjlz5sTBwYHq1avz7bffWvRtF0lPJnNyx+URERGrcOnSJbp37270DZ4/f36SN2Glh7t379KlSxejb/P48eNT1QXjaS1atIicOXPi6upKmTJlLG6W3Lhxo9Ei2qBBA7755ptnVldWtmHDBj799FMgtr/0d999l8EVibVTH2AREeHq1ausWrWK6Ohotm7daoTfkiVLPpPwGx4ezrfffoutra3xqFyIHZ/5SS25aW39+vXGiA45cuSgadOmODs7c+3aNeOmPIhtCRWRrCnTBuDr16/TrVs3pkyZYtEfLygoiGnTpnHkyBFsbW1p1qwZgwcPtrhEExYWxqxZs9i5cydhYWFUrVqVDz74wOJXvIiI/I/JZLIYfg9iR2RIzk1baSF79uysWrXKYkg3k8nEBx98kOLuFynVv39/xo4di9ls5sGDBxajj8SpVKlSsodlE5HMJ1MG4GvXrjF48GCLp9RA7F3g/fv3x93dnfHjx3Pnzh1mzpxJcHAws2bNMpYbNWoUJ0+eZMiQITg7O7NgwQL69+/PqlWrEtztLCIisTcWFi5cmBs3buDg4EDZsmXp1avXY58emJZsbGx48cUX8fPzw87OjuLFi9OjRw+L4beeldatW1OgQAFWrVrFP//8w61bt4iKisLJyYnixYvTpEkTunbtir29/TOvTUTSRqbqAxwTE8OmTZuYPn06EHsX+bx584z/AS9ZsoRFixaxceNG46YdHx8f3nvvPRYuXEiVKlU4fvw4vXr1YsaMGdSrVw+AO3fu0L59e95++23eeeedjDg0EREREckkMtUoEGfPnmXSpEm8/PLLRmf5+Pbv30/VqlUt7lj38vLC2dnZGF9z//79ODo64uXlZSzj5uZGtWrVUjUGp4iIiIg8HzJVAM6fPz9r1qxJss9XYGAgRYoUsZhma2uLp6en8ajPwMBAChYsmOCxk4ULF070caAiIiIiYl0yVR9gV1fXRMekjBMSEpLok26cnJyMRzgmZ5mn5e/vb6z7uHFZRURERCTjREZGYjKZnvhI7UwVgJ8k/rPV/yvuiTzJWSYl4rpKxw0NJCIiIiJZU5YKwC4uLoSFhSWYHhoaajw60cXFhX///TfRZf77NJvkKlu2LCdOnMBsNlOqVKkUbUNERERE0ldAQMBjnxQaJ0sF4KJFi1o85x4gOjqa4OBg4/GrRYsWxdfXl5iYGIsW36CgoFSPA2wymXByckrVNkREREQkfSQn/EImuwnuSby8vDh8+LDxhCAAX19fwsLCjFEfvLy8CA0NZf/+/cYyd+7c4ciRIxYjQ4iIiIiIdcpSAbhz585kz56dgQMHsmvXLtauXcuYMWOoW7culStXBmKfMV69enXGjBnD2rVr2bVrFwMGDCBHjhx07tw5g49ARERERDJaluoC4ebmxrx585g2bRqjR4/G2dmZpk2bMnToUIvlJk+ezDfffMOMGTOIiYmhcuXKTJo0SU+BExEREZHM9SS4zOzEiRMAvPjiixlciYiIiIgkJrl5LUt1gRARERERSS0FYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSrZMroAERFJvTVr1rBixQqCg4PJnz8/Xbt2pUuXLphMJgDeeecdjh07lmC9H374gfLlyye53RMnTjB79mz++ecfnJycqFOnDu+99x65c+dOt2MREUlvCsAiIlnc2rVrmThxIt26daNhw4YcOXKEyZMn8+jRI3r06IHZbCYgIIDXX3+dZs2aWaxbvHjxJLfr5+dH//79qVWrFlOmTOHmzZvMnj2boKAgFi9enN6HJSKSbhSARUSyuPXr11OlShWGDx8OQK1atbh48SKrVq2iR48eXL58mdDQUOrVq8eLL76Y7O3OnDmTsmXLMnXqVGxsYnvMOTs7M3XqVK5cuULBggXT5XhERNKb+gCLiGRxERERODs7W0xzdXXl3r17APj7+wNQpkyZZG/z7t27HDp0iM6dOxvhF6BJkyZs2rRJ4VdEsjQFYBGRLO61117D19eXzZs3ExISwv79+9m0aRNt2rQB4MyZMzg5OTFjxgyaNm1K3bp1GTJkCIGBgUluMyAggJiYGNzc3Bg9ejQvvfQSDRo0YOzYsTx48OAZHZmISPpQFwgRkSyuZcuWHDp0iLFjxxrT6tSpw7Bhw4DYABwWFkaOHDmYMmUKV69eZcGCBfTp04cff/yRvHnzJtjmnTt3APjss8+oW7cuU6ZM4dKlS8yZM4crV66wcOFC4wY7EZGsRgFYRCSLGzZsGEePHmXIkCFUqFCBgIAAvvvuO0aMGMGUKVMYMGAAb775JtWqVQOgatWqVKpUiS5durBixQqGDBmSYJuRkZEAlCtXjjFjxgCxfYtz5MjBqFGj+Ouvv/Dy8np2BykikoYUgEVEsrBjx46xb98+Ro8eTceOHQGoXr06BQsWZOjQoezdu5cGDRokWK9QoUIUL16cs2fPJrpdJycngATr1q1bF4DTp08rAItIlqUALCKShV29ehWAypUrW0yPa+09d+4c9+7do0iRIlSqVMlimYcPH5IrV65Et1ukSBEAHj16ZDE9KioKAAcHh1TXLiKSUXQTnIhIFlasWDEAjhw5YjE97qEXhQoVYsGCBcyYMcNi/unTp7l8+TI1atRIdLvFixfH09OTbdu2YTabjel//PEHAFWqVEmjIxARefbUAiwikoWVK1eOJk2a8M0333D//n0qVqzI+fPn+e6773jhhRdo1KgRDx8+ZPz48YwdO5Y2bdpw7do15s2bR5kyZWjbti0Q29Lr7++Ph4cH+fLlw2QyMWTIED755BNGjhxJx44duXDhAnPnzqVJkyaUK1cug49cRCTlTOb4P+0lSSdOnAB4qkHkRUSehcjISBYtWsTmzZu5efMm+fPnp1GjRvTp08foy7t9+3Z++OEHLly4gKOjI40aNWLQoEG4uroCEBwcTPv27enTpw/9+vUztr1nzx4WLFhAQEAAOXPmpHXr1rz77rvY29tnyLGKiDxOcvOaAnAyKQCLiIiIZG7JzWvqAywiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYReQoxGjo909K/jYgklx6FLJnCwYMH6d+/f5Lz+/btS9++fTly5Ahz5szh7NmzuLi40LhxY959912cnZ0fu/3du3ezcOFCLl68iLu7O23atKFnz57Y2dml9aHIc87GZGKl7xlu3A/L6FIkHo+cTnT3KpPRZYhIFqEALJlCuXLlWLJkSYLp3377Lf/88w8tW7bk3LlzDBw4kCpVqjBp0iRu3LjBrFmzuHLlCt98802S2/b19WX48OE0b96cQYMGcf78eebMmcPdu3f56KOP0vOw5Dl1434YwXdCM7oMERFJIQVgyRRcXFwSPLbwjz/+4MCBA3z55ZcULVqUOXPmYDKZmDJlCk5OTgBER0czadIkrl69SoECBRLd9oYNG8ifPz8TJkzA1tYWLy8v/v33X5YvX84HH3xAtmw6DURERKyJ+gBLpvTw4UMmT55M/fr1adasGQARERFky5YNBwcHYzlXV1cA7t27l+S2Hj16hKOjI7a2thbrRUZGEhqqVjwRERFrowAsmdLKlSu5efMmw4YNM6a1b98egG+++Ya7d+9y7tw5FixYQKlSpShdunSS2+rSpQuXLl3C29ubBw8ecOLECVasWEG9evWMAC0iIiLWQ9d+JdOJjIxkxYoVtGjRgsKFCxvTS5UqxeDBg/n6669ZsWIFAAUKFGDBggUWrbv/VbNmTd58801mzJjBjBkzAChbtiwTJ05M3wMRERGRTEktwJLp7Nixg9u3b/PGG29YTP/+++/58ssv6dSpE99++y2TJk3CycmJAQMGcPv27SS3N2nSJH744Qfeeecd5s2bx7hx47h//z6DBw/m4cOH6X04IiIiksmoBVgynR07dlCiRAnKlPnfkEZRUVEsXLiQ1q1bM2LECGN69erV6dixI97e3gwdOjTBtm7cuMGaNWvo2bMn7777rjG9QoUKdO3alXXr1tGtW7d0PR4RERHJXNQCLJlKVFQU+/fvp3nz5hbT7969y8OHD6lcubLF9Ny5c1O0aFHOnz+f6PauXbuG2WxOsF6JEiVwdXVNcj0RERF5fikAS6YSEBCQaNB1c3PD1dWVI0eOWEy/e/culy5domDBgolur3Dhwtja2nL06FGL6YGBgdy7dy/J9UREROT5lSW7QKxZs4YVK1YQHBxM/vz56dq1K126dMFkMgEQFBTEtGnTOHLkCLa2tjRr1ozBgwfj4uKSwZXLkwQEBACxLbTx2dra0rdvXyZPnoyzszPNmjXj7t27fP/999jY2PD6668by544cQI3NzcKFSqEm5sbr732Gj/88AMAtWvX5urVqyxYsIACBQrwyiuvPLuDExERkUwhywXgtWvXMnHiRLp160bDhg05cuQIkydP5tGjR/To0YMHDx7Qv39/3N3dGT9+PHfu3GHmzJkEBwcza9asjC5fniDuZrYcOXIkmNetWzdy5MjBsmXL2LBhA7ly5aJKlSpMnjzZoiW3Z8+etG3blvHjxwPw3nvv4eHhwa+//sqyZcvIkycPXl5eDBgwINH9iIiIyPPNZDabzRldxNPo1asXNjY2LFy40Jg2cuRITp48yfr161myZAmLFi1i48aN5MqVCwAfHx/ee+89Fi5cSJUqVVK03xMnTgAkeFqZiFifmduO6lHImYynmzNDWlTJ6DJEJIMlN69luT7AERERODs7W0xzdXU1ngS2f/9+qlataoRfAC8vL5ydnfHx8XmWpYqIiIhIJpTlAvBrr72Gr68vmzdvJiQkhP3797Np0ybatGkDxN7cVKRIEYt1bG1t8fT05OLFixlRsoiIiIhkIlmuD3DLli05dOgQY8eONabVqVPHeGRuSEhIghZiACcnJ0JDU3fJ0mw2ExYWlqptiEjWZTKZcHR0zOgy5DHCw8PJYj37RCQNmc1mY1CEx8lyAXjYsGEcPXqUIUOGUKFCBQICAvjuu+8YMWIEU6ZMISYmJsl1bWxS1+AdGRmJn59fqrYhIlmXo6Mj5cuXz+gy5DEuXLhAeHh4RpchIhnI3t7+ictkqQB87Ngx9u3bx+jRo+nYsSMQ+ySwggULMnToUPbu3YuLi0uirbShoaF4eHikav92dnaUKlUqVdsQkawrOa0KkrGKFy+uFmARKxY3nOqTZKkAfPXqVYAED0moVq0aAOfOnaNo0aIEBQVZzI+OjiY4OJjGjRunav8mkwknJ6dUbUNERNKPuqiIWLfkNlRkqZvgihUrBpDgaWDHjh0DoFChQnh5eXH48GHu3LljzPf19SUsLAwvL69nVquIiIiIZE5ZqgW4XLlyNGnShG+++Yb79+9TsWJFzp8/z3fffccLL7xAo0aNqF69Oj/99BMDBw6kT58+3Lt3j5kzZ1K3bt0ELcfWLMZsxkaXczMt/fuIiIiknyz3IIzIyEgWLVrE5s2buXnzJvnz56dRo0b06dPH6J4QEBDAtGnTOHbsGM7OzjRs2JChQ4cmOjpEcj2PD8JY6XuGG/c1qkVm45HTie5eZTK6DHkMPQgj89GDMEQEkp/XslQLMMTeiNa/f3/69++f5DKlSpVi7ty5z7CqrOnG/TB9iYuIiIjVyVJ9gEVEREREUksBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiViVbala+fPky169f586dO2TLlo1cuXJRokQJcubMmVb1iYiIiIikqacOwCdPnmTNmjX4+vpy8+bNRJcpUqQIDRo0oF27dpQoUSLVRYqIiIiIpJVkB+CjR48yc+ZMTp48CYDZbE5y2YsXL3Lp0iWWL19OlSpVGDp0KOXLl099tSIiIiIiqZSsADxx4kTWr19PTEwMAMWKFePFF1+kdOnS5M2bF2dnZwDu37/PzZs3OXv2LKdPn+b8+fMcOXKEnj170qZNG8aNG5d+RyIiIiIikgzJCsBr167Fw8ODV199lWbNmlG0aNFkbfz27dv8/vvvrF69mk2bNikAi4iIiEiGS1YA/vrrr2nYsCE2Nk83aIS7uzvdunWjW7du+Pr6pqhAEREREZG0lKwA3Lhx41TvyMvLK9XbEBERERFJrVQNgwYQEhLCt99+y969e7l9+zYeHh60atWKnj17YmdnlxY1ioiIiIikmVQH4M8++4xdu3YZr4OCgli4cCHh4eG89957qd28iIiIiEiaSlUAjoyM5I8//qBJkya88cYb5MqVi5CQENatW8dvv/2mACwiIiIimU6y7mqbOHEit27dSjA9IiKCmJgYSpQoQYUKFShUqBDlypWjQoUKREREpHmxIiIiIiKplexh0LZs2ULXrl15++23jUcdu7i4ULp0aRYtWsTy5cvJkSMHYWFhhIaG0rBhw3QtXEREREQkJZLVAvzpp5/i7u6Ot7c3HTp0YMmSJTx8+NCYV6xYMcLDw7lx4wYhISFUqlSJ4cOHp2vhIiIiIiIpkawW4DZt2tCiRQtWr17N4sWLmTt3Lj/99BO9e/fmlVde4aeffuLq1av8+++/eHh44OHhkd51i4iIiIikSLKfbJEtWza6du3K2rVreffdd3n06BFff/01nTt35rfffsPT05OKFSsq/IqIiIhIpvZ0j3YDHBwc6NWrF+vWreONN97g5s2bjB07lv/7v//Dx8cnPWoUEREREUkzyQ7At2/fZtOmTXh7e/Pbb79hMpkYPHgwa9eu5ZVXXuHChQu8//779O3bl+PHj6dnzSIiIiIiKZasPsAHDx5k2LBhhIeHG9Pc3NyYP38+xYoV45NPPuGNN97g22+/Zfv27fTu3Zv69eszbdq0dCtcRERERCQlktUCPHPmTLJly0a9evVo2bIlDRs2JFu2bMydO9dYplChQkycOJFly5ZRp04d9u7dm25Fi4iIiIikVLJagAMDA5k5cyZVqlQxpj148IDevXsnWLZMmTLMmDGDo0ePplWNIiIiIiJpJlkBOH/+/EyYMIG6devi4uJCeHg4R48epUCBAkmuEz8si4iIiIhkFskKwL169WLcuHGsXLkSk8mE2WzGzs7OoguEiIiIiEhWkKwA3KpVK4oXL84ff/xhPOyiRYsWFCpUKL3rExERERFJU8kKwABly5albNmy6VmLiIiIiEi6S9YoEMOGDePAgQMp3smpU6cYPXp0itf/rxMnTtCvXz/q169PixYtGDduHP/++68xPygoiPfff59GjRrRtGlTJk2aREhISJrtX0RERESyrmS1AO/Zs4c9e/ZQqFAhmjZtSqNGjXjhhRewsUk8P0dFRXHs2DEOHDjAnj17CAgIAODzzz9PdcF+fn7079+fWrVqMWXKFG7evMns2bMJCgpi8eLFPHjwgP79++Pu7s748eO5c+cOM2fOJDg4mFmzZqV6/yIiIiKStSUrAC9YsICvvvqKs2fPsnTpUpYuXYqdnR3Fixcnb968ODs7YzKZCAsL49q1a1y6dImIiAgAzGYz5cqVY9iwYWlS8MyZMylbtixTp041ArizszNTp07lypUrbNu2jXv37rF8+XJy5coFgIeHB++99x5Hjx7V6BQiIiIiVi5ZAbhy5cosW7aMHTt24O3tjZ+fH48ePcLf358zZ85YLGs2mwEwmUzUqlWLTp060ahRI0wmU6qLvXv3LocOHWL8+PEWrc9NmjShSZMmAOzfv5+qVasa4RfAy8sLZ2dnfHx8FIBFRERErFyyb4KzsbGhefPmNG/enODgYPbt28exY8e4efOm0f82d+7cFCpUiCpVqlCzZk3y5cuXpsUGBAQQExODm5sbo0eP5s8//8RsNtO4cWOGDx9Ojhw5CAwMpHnz5hbr2dra4unpycWLF1O1f7PZTFhYWKq2kRmYTCYcHR0zugx5gvDwcOMHpWQOOncyP503ItbNbDYnq9E12QE4Pk9PTzp37kznzp1TsnqK3blzB4DPPvuMunXrMmXKFC5dusScOXO4cuUKCxcuJCQkBGdn5wTrOjk5ERoamqr9R0ZG4ufnl6ptZAaOjo6UL18+o8uQJ7hw4QLh4eEZXYbEo3Mn89N5IyL29vZPXCZFATijREZGAlCuXDnGjBkDQK1atciRIwejRo3ir7/+IiYmJsn1k7ppL7ns7OwoVapUqraRGaRFdxRJf8WLF1dLViajcyfz03kjYt3iBl54kiwVgJ2cnABo0KCBxfS6desCcPr0aVxcXBLtphAaGoqHh0eq9m8ymYwaRNKbLrWLPD2dNyLWLbkNFalrEn3GihQpAsCjR48spkdFRQHg4OBA0aJFCQoKspgfHR1NcHAwxYoVeyZ1ioiIiEjmlaUCcPHixfH09GTbtm0Wl7j++OMPAKpUqYKXlxeHDx82+gsD+Pr6EhYWhpeX1zOvWUREREQylywVgE0mE0OGDOHEiROMHDmSv/76i5UrVzJt2jSaNGlCuXLl6Ny5M9mzZ2fgwIHs2rWLtWvXMmbMGOrWrUvlypUz+hBEREREJIOlqA/wyZMnqVixYlrXkizNmjUje/bsLFiwgPfff5+cOXPSqVMn3n33XQDc3NyYN28e06ZNY/To0Tg7O9O0aVOGDh2aIfWKiIiISOaSogDcs2dPihcvzssvv0ybNm3ImzdvWtf1WA0aNEhwI1x8pUqVYu7cuc+wIhERERHJKlLcBSIwMJA5c+bQtm1bBg0axG+//WY8/lhEREREJLNKUQvwW2+9xY4dO7h8+TJms5kDBw5w4MABnJycaN68OS+//LIeOSwiIiIimVKKAvCgQYMYNGgQ/v7+/P777+zYsYOgoCBCQ0NZt24d69atw9PTk7Zt29K2bVvy58+f1nWLiIiIiKRIqh6EUbZsWcqWLcvAgQM5c+YMq1atYt26dQAEBwfz3XffsXDhQjp16sSwYcNS/SQ2ERERkbQSERHBSy+9RHR0tMV0R0dH9uzZA8CpU6eYPn06fn5+ODs7065dO/r27Yudnd1jt+3r68vcuXM5d+4c7u7udOnShR49euiJkplEqp8E9+DBA3bs2MH27ds5dOgQJpMJs9lsjNMbHR3Nzz//TM6cOenXr1+qCxYRERFJC+fOnSM6OpoJEyZQqFAhY3pcg93ly5cZMGAAlSpVYtKkSQQGBjJ37lzu3bvHyJEjk9zuiRMnGDp0KM2bN6d///4cPXqUmTNnEh0dzdtvv53ehyXJkKIAHBYWxu7du9m2bRsHDhwwnsRmNpuxsbGhdu3atG/fHpPJxKxZswgODmbr1q0KwCIiIpJpnDlzBltbW5o2bYq9vX2C+UuXLsXZ2ZmpU6diZ2dH/fr1cXBw4Ouvv6ZXr15JdvGcP38+ZcuWZcKECQDUrVuXqKgolixZQvfu3XFwcEjX45InS1EAbt68OZGRkQBGS6+npyft2rVL0OfXw8ODd955hxs3bqRBuSIiIiJpw9/fn2LFiiUafiG2G0O9evUsujs0bdqUL7/8kv379/PKK68kWOfRo0ccOnQoQaNf06ZN+eGHHzh69KieTJsJpCgAP3r0CAB7e3uaNGlChw4dqFGjRqLLenp6ApAjR44UligiIiKS9uJagAcOHMixY8ewt7c3Hp5la2vL1atXKVKkiMU6bm5uODs7c/HixUS3eeXKFSIjIxOsV7hwYQAuXryoAJwJpCgAv/DCC7Rv355WrVrh4uLy2GUdHR2ZM2cOBQsWTFGBIiIiImnNbDYTEBCA2WymY8eOvPPOO5w6dYoFCxZw4cIFJk2aBJBoznF2diY0NDTR7YaEhBjLxOfk5ASQ5HrybKUoAP/www9AbF/gyMhI49LAxYsXyZMnj8U/urOzM7Vq1UqDUkVERETShtlsZurUqbi5uVGyZEkAqlWrhru7O2PGjOHgwYOPXT+p0RxiYmIeu55GxMocUvyvsG7dOtq2bcuJEyeMacuWLaN169asX78+TYoTERERSQ82NjbUqFHDCL9x6tevD8R2ZYDEW2xDQ0OTvAIeNz0sLCzBOvHnS8ZKUQD28fHh888/JyQkhICAAGN6YGAg4eHhfP755xw4cCDNihQRERFJSzdv3mTNmjVcu3bNYnpERAQAefLkwcPDg8uXL1vM//fffwkNDaV48eKJbrdQoULY2toSFBRkMT3udbFixdLoCCQ1UhSAly9fDkCBAgUsfjm9/vrrFC5cGLPZjLe3d9pUKCIiIpLGoqOjmThxIr/++qvF9G3btmFra0vVqlWpXbs2e/bsMW7+B9i5cye2trbUrFkz0e1mz56dqlWrsmvXLmOkrLj1XFxcqFixYvockDyVFPUBPnfuHCaTibFjx1K9enVjeqNGjXB1daVv376cPXs2zYoUERERSUv58+enXbt2eHt7kz17dipVqsTRo0dZsmQJXbt2pWjRorz11lts27aNIUOG8Prrr3Px4kXmzp3LK6+8Ygz5+ujRI/z9/fHw8CBfvnwAvPPOOwwYMICPP/6Y9u3bc/z4cby9vRk0aJDGAM4kUtQCHHeHo5ubW4J5ccOdPXjwIBVliYiIiKSvTz75hN69e7N582aGDh3K5s2b6devH++//z4Q211h9uzZPHz4kBEjRvDjjz/yf//3f3z44YfGNm7dukXPnj1Zu3atMa1mzZp8/fXXXLx4kQ8//JCtW7fy3nvv8dZbbz3rQ5QkpKgFOF++fFy+fJnVq1dbfAjMZjMrV640lhERERHJrOzt7enduze9e/dOcpmqVavy/fffJznf09Mz0REjGjduTOPGjdOiTEkHKQrAjRo1wtvbm1WrVuHr60vp0qWJiorizJkzXL16FZPJRMOGDdO6VhERERGRVEtRAO7Vqxe7d+8mKCiIS5cucenSJWOe2WymcOHCvPPOO2lWpIiIiIhIWklRH2AXFxeWLFlCx44dcXFxwWw2YzabcXZ2pmPHjixevFjj3ImIiIhIppSiFmAAV1dXRo0axciRI7l79y5msxk3N7ckn4wiIiIiIpIZpPp5fCaTCTc3N3Lnzm2E35iYGPbt25fq4kRERERE0lqKWoDNZjOLFy/mzz//5P79+xbPvY6KiuLu3btERUXx119/pVmhIiIiIiJpIUUB+KeffmLevHmYTCaLp5wAxjR1hRARERGRzChFXSA2bdoEgKOjI4ULF8ZkMlGhQgWKFy9uhN8RI0akaaEiIiKSdcX8p8FMMg9r/LdJUQvw5cuXMZlMfPXVV7i5udGjRw/69etHnTp1+Oabb/jxxx8JDAxM41JFREQkq7IxmVjpe4Yb98MyuhSJxyOnE929ymR0Gc9cigJwREQEAEWKFKFAgQI4OTlx8uRJ6tSpwyuvvMKPP/6Ij48Pw4YNS9NiRUREJOu6cT+M4DuhGV2GSMq6QOTOnRsAf39/TCYTpUuXxsfHB4htHQa4ceNGGpUoIiIiIpJ2UhSAK1eujNlsZsyYMQQFBVG1alVOnTpF165dGTlyJPC/kCwiIiIikpmkKAD37t2bnDlzEhkZSd68eWnZsiUmk4nAwEDCw8MxmUw0a9YsrWsVEREREUm1FAXg4sWL4+3tTZ8+fXBwcKBUqVKMGzeOfPnykTNnTjp06EC/fv3SulYRERERkVRL0U1wPj4+VKpUid69exvT2rRpQ5s2bdKsMBERERGR9JCiFuCxY8fSqlUr/vzzz7SuR0REREQkXaUoAD98+JDIyEiKFSuWxuWIiIiIiKSvFAXgpk2bArBr1640LUZEREREJL2lqA9wmTJl2Lt3L3PmzGH16tWUKFECFxcXsmX73+ZMJhNjx45Ns0JFRERERNJCigLwjBkzMJlMAFy9epWrV68mupwCsIiIiIhkNikKwABms/mx8+MCsoiIiIhIZpKiALx+/fq0rkNERERE5JlIUQAuUKBAWtchIiIiIvJMpCgAHz58OFnLVatWLSWbFxERERFJNykKwP369XtiH1+TycRff/2VoqJERERERNJLut0EJyIiIiKSGaUoAPfp08fitdls5tGjR1y7do1du3ZRrlw5evXqlSYFioiIiIikpRQF4L59+yY57/fff2fkyJE8ePAgxUWJiIiIiKSXFD0K+XGaNGkCwIoVK9J60yIiIiIiqZbmAfjvv//GbDZz7ty5tN60iIiIiEiqpagLRP/+/RNMi4mJISQkhPPnzwOQO3fu1FUmIiIiIpIOUhSADx06lOQwaHGjQ7Rt2zblVYmIiIiIpJM0HQbNzs6OvHnz0rJlS3r37p2qwpJr+PDhnD59mg0bNhjTgoKCmDZtGkeOHMHW1pZmzZoxePBgXFxcnklNIiIiIpJ5pSgA//3332ldR4ps3ryZXbt2WTya+cGDB/Tv3x93d3fGjx/PnTt3mDlzJsHBwcyaNSsDqxURERGRzCDFLcCJiYyMxM7OLi03maSbN28yZcoU8uXLZzH9l19+4d69eyxfvpxcuXIB4OHhwXvvvcfRo0epUqXKM6lPRERERDKnFI8C4e/vz4ABAzh9+rQxbebMmfTu3ZuzZ8+mSXGPM2HCBGrXrk3NmjUtpu/fv5+qVasa4RfAy8sLZ2dnfHx80r0uEREREcncUhSAz58/T79+/Th48KBF2A0MDOTYsWP07duXwMDAtKoxgbVr13L69GlGjBiRYF5gYCBFihSxmGZra4unpycXL15Mt5pEREREJGtIUReIxYsXExoair29vcVoEC+88AKHDx8mNDSU77//nvHjx6dVnYarV6/yzTffMHbsWItW3jghISE4OzsnmO7k5ERoaGiq9m02mwkLC0vVNjIDk8mEo6NjRpchTxAeHp7ozaaScXTuZH46bzInnTuZ3/Ny7pjN5iRHKosvRQH46NGjmEwmRo8eTevWrY3pAwYMoFSpUowaNYojR46kZNOPZTab+eyzz6hbty5NmzZNdJmYmJgk17exSd1zPyIjI/Hz80vVNjIDR0dHypcvn9FlyBNcuHCB8PDwjC5D4tG5k/npvMmcdO5kfs/TuWNvb//EZVIUgP/9918AKlasmGBe2bJlAbh161ZKNv1Yq1at4uzZs6xcuZKoqCjgf8OxRUVFYWNjg4uLS6KttKGhoXh4eKRq/3Z2dpQqVSpV28gMkvPLSDJe8eLFn4tf488TnTuZn86bzEnnTub3vJw7AQEByVouRQHY1dWV27dv8/fff1O4cGGLefv27QMgR44cKdn0Y+3YsYO7d+/SqlWrBPO8vLzo06cPRYsWJSgoyGJedHQ0wcHBNG7cOFX7N5lMODk5pWobIsmly4UiT0/njUjKPC/nTnJ/bKUoANeoUYOtW7cydepU/Pz8KFu2LFFRUZw6dYrt27djMpkSjM6QFkaOHJmgdXfBggX4+fkxbdo08ubNi42NDT/88AN37tzBzc0NAF9fX8LCwvDy8krzmkREREQka0lRAO7duzd//vkn4eHhrFu3zmKe2WzG0dGRd955J00KjK9YsWIJprm6umJnZ2f0LercuTM//fQTAwcOpE+fPty7d4+ZM2dSt25dKleunOY1iYiIiEjWkqK7wooWLcqsWbMoUqQIZrPZ4r8iRYowa9asRMPqs+Dm5sa8efPIlSsXo0ePZu7cuTRt2pRJkyZlSD0iIiIikrmk+ElwlSpV4pdffsHf35+goCDMZjOFCxembNmyz7Sze2JDrZUqVYq5c+c+sxpEREREJOtI1aOQw8LCKFGihDHyw8WLFwkLC0t0HF4RERERkcwgxQPjrlu3jrZt23LixAlj2rJly2jdujXr169Pk+JERERERNJaigKwj48Pn3/+OSEhIRbjrQUGBhIeHs7nn3/OgQMH0qxIEREREZG0kqIAvHz5cgAKFChAyZIljemvv/46hQsXxmw24+3tnTYVioiIiIikoRT1AT537hwmk4mxY8dSvXp1Y3qjRo1wdXWlb9++nD17Ns2KFBERERFJKylqAQ4JCQEwHjQRX9wT4B48eJCKskRERERE0keKAnC+fPkAWL16tcV0s9nMypUrLZYREREREclMUtQFolGjRnh7e7Nq1Sp8fX0pXbo0UVFRnDlzhqtXr2IymWjYsGFa1yoiIiIikmopCsC9evVi9+7dBAUFcenSJS5dumTMi3sgRno8CllEREREJLVS1AXCxcWFJUuW0LFjR1xcXIzHIDs7O9OxY0cWL16Mi4tLWtcqIiIiIpJqKX4SnKurK6NGjWLkyJHcvXsXs9mMm5vbM30MsoiIiIjI00rxk+DimEwm3NzcyJ07NyaTifDwcNasWcObb76ZFvWJiIiIiKSpFLcA/5efnx+rV69m27ZthIeHp9VmRURERETSVKoCcFhYGFu2bGHt2rX4+/sb081ms7pCiIiIiEimlKIA/M8//7BmzRq2b99utPaazWYAbG1tadiwIZ06dUq7KkVERERE0kiyA3BoaChbtmxhzZo1xmOO40JvHJPJxMaNG8mTJ0/aVikiIiIikkaSFYA/++wzfv/9dx4+fGgRep2cnGjSpAn58+dn4cKFAAq/IiIiIpKpJSsAb9iwAZPJhNlsJlu2bHh5edG6dWsaNmxI9uzZ2b9/f3rXKSIiIiKSJp5qGDSTyYSHhwcVK1akfPnyZM+ePb3qEhERERFJF8lqAa5SpQpHjx4F4OrVq8yfP5/58+dTvnx5WrVqpae+iYiIiEiWkawAvGDBAi5dusTatWvZvHkzt2/fBuDUqVOcOnXKYtno6GhsbW3TvlIRERERkTSQ7C4QRYoUYciQIWzatInJkydTv359o19w/HF/W7VqxfTp0zl37ly6FS0iIiIiklJPPQ6wra0tjRo1olGjRty6dYv169ezYcMGLl++DMC9e/f48ccfWbFiBX/99VeaFywiIiIikhpPdRPcf+XJk4devXqxZs0avv32W1q1aoWdnZ3RKiwiIiIiktmk6lHI8dWoUYMaNWowYsQINm/ezPr169Nq0yIiIiIiaSbNAnAcFxcXunbtSteuXdN60yIiIiIiqZaqLhAiIiIiIlmNArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq5Itowt4WjExMaxevZpffvmFK1eukDt3bl566SX69euHi4sLAEFBQUybNo0jR45ga2tLs2bNGDx4sDFfRERERKxXlgvAP/zwA99++y1vvPEGNWvW5NKlS8ybN49z584xZ84cQkJC6N+/P+7u7owfP547d+4wc+ZMgoODmTVrVkaXLyIiIiIZLEsF4JiYGJYuXcqrr77KoEGDAKhduzaurq6MHDkSPz8//vrrL+7du8fy5cvJlSsXAB4eHrz33nscPXqUKlWqZNwBiIiIiEiGy1J9gENDQ2nTpg0tW7a0mF6sWDEALl++zP79+6lataoRfgG8vLxwdnbGx8fnGVYrIiIiIplRlmoBzpEjB8OHD08wfffu3QCUKFGCwMBAmjdvbjHf1tYWT09PLl68+CzKFBEREZFMLEsF4MScPHmSpUuX0qBBA0qVKkVISAjOzs4JlnNyciI0NDRV+zKbzYSFhaVqG5mByWTC0dExo8uQJwgPD8dsNmd0GRKPzp3MT+dN5qRzJ/N7Xs4ds9mMyWR64nJZOgAfPXqU999/H09PT8aNGwfE9hNOio1N6np8REZG4ufnl6ptZAaOjo6UL18+o8uQJ7hw4QLh4eEZXYbEo3Mn89N5kznp3Mn8nqdzx97e/onLZNkAvG3bNj799FOKFCnCrFmzjD6/Li4uibbShoaG4uHhkap92tnZUapUqVRtIzNIzi8jyXjFixd/Ln6NP0907mR+Om8yJ507md/zcu4EBAQka7ksGYC9vb2ZOXMm1atXZ8qUKRbj+xYtWpSgoCCL5aOjowkODqZx48ap2q/JZMLJySlV2xBJLl0uFHl6Om9EUuZ5OXeS+2MrS40CAfDrr78yY8YMmjVrxqxZsxI83MLLy4vDhw9z584dY5qvry9hYWF4eXk963JFREREJJPJUi3At27dYtq0aXh6etKtWzdOnz5tMb9QoUJ07tyZn376iYEDB9KnTx/u3bvHzJkzqVu3LpUrV86gykVEREQks8hSAdjHx4eIiAiCg4Pp3bt3gvnjxo2jXbt2zJs3j2nTpjF69GicnZ1p2rQpQ4cOffYFi4iIiEimk6UCcIcOHejQocMTlytVqhRz5859BhWJiIiISFaT5foAi4iIiIikhgKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVuW5DsC+vr68+eab1KtXj/bt2+Pt7Y3ZbM7oskREREQkAz23AfjEiRMMHTqUokWLMnnyZFq1asXMmTNZunRpRpcmIiIiIhkoW0YXkF7mz59P2bJlmTBhAgB169YlKiqKJUuW0L17dxwcHDK4QhERERHJCM9lC/CjR484dOgQjRs3tpjetGlTQkNDOXr0aMYUJiIiIiIZ7rkMwFeuXCEyMpIiRYpYTC9cuDAAFy9ezIiyRERERCQTeC67QISEhADg7OxsMd3JyQmA0NDQp9qev78/jx49AuD48eNpUGHGM5lM1ModQ3QudQXJbGxtYjhx4oRu2MykdO5kTjpvMj+dO5nT83buREZGYjKZnrjccxmAY2JiHjvfxubpG77j3szkvKlZhXN2u4wuQR7jefqsPW907mReOm8yN507mdfzcu6YTCbrDcAuLi4AhIWFWUyPa/mNm59cZcuWTZvCRERERCTDPZd9gAsVKoStrS1BQUEW0+NeFytWLAOqEhEREZHM4LkMwNmzZ6dq1ars2rXLok/Lzp07cXFxoWLFihlYnYiIiIhkpOcyAAO88847nDx5ko8//hgfHx++/fZbvL296dmzp8YAFhEREbFiJvPzcttfInbt2sX8+fO5ePEiHh4edOnShR49emR0WSIiIiKSgZ7rACwiIiIi8l/PbRcIEREREZHEKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYLF6GglQnneJfcb1uRcRa6YALFlScHAwNWrUYMOGDSle58GDB4wdO5YjR46kV5ki6aJdu3aMHz8+0Xnz58+nRo0axuujR4/y3nvvWSyzcOFCvL2907NEEauSku8kyVgKwGK1/P392bx5MzExMRldikia6dixI0uWLDFer127lgsXLlgsM2/ePMLDw591aSLPrTx58rBkyRLq16+f0aVIMmXL6AJERCTt5MuXj3z58mV0GSJWxd7enhdffDGjy5CnoBZgyXAPHz5k9uzZvPLKK9SpU4eGDRsyYMAA/P39jWV27tzJa6+9Rr169Xj99dc5c+aMxTY2bNhAjRo1CA4Otpie1KXigwcP0r9/fwD69+9P37590/7ARJ6RdevWUbNmTRYuXGjRBWL8+PFs3LiRq1evGpdn4+YtWLDAoqtEQEAAQ4cOpWHDhjRs2JAPP/yQy5cvG/MPHjxIjRo1OHDgAAMHDqRevXq0bNmSmTNnEh0d/WwPWOQp+Pn58e6779KwYUNeeuklBgwYwIkTJ4z5R44coW/fvtSrV48mTZowbtw47ty5Y8zfsGEDtWvX5uTJk/Ts2ZO6devStm1bi25EiXWBuHTpEh999BEtW7akfv369OvXj6NHjyZYZ9myZXTq1Il69eqxfv369H0zxKAALBlu3LhxrF+/nrfffpvZs2fz/vvvc/78eUaPHo3ZbObPP/9kxIgRlCpViilTptC8eXPGjBmTqn2WK1eOESNGADBixAg+/vjjtDgUkWdu27ZtTJw4kd69e9O7d2+Leb1796ZevXq4u7sbl2fjukd06NDB+PvixYu88847/Pvvv4wfP54xY8Zw5coVY1p8Y8aMoWrVqkyfPp2WLVvyww8/sHbt2mdyrCJPKyQkhMGDB5MrVy6+/vprvvjiC8LDwxk0aBAhISEcPnyYd999FwcHB7788ks++OADDh06RL9+/Xj48KGxnZiYGD7++GNatGjBjBkzqFKlCjNmzGD//v2J7vf8+fO88cYbXL16leHDh/P5559jMpno378/hw4dslh2wYIFvPXWW3z22WfUrl07Xd8P+R91gZAMFRkZSVhYGMOHD6d58+YAVK9enZCQEKZPn87t27dZuHAhFSpUYMKECQDUqVMHgNmzZ6d4vy4uLhQvXhyA4sWLU6JEiVQeicizt2fPHsaOHcvbb79Nv379EswvVKgQbm5uFpdn3dzcAPDw8DCmLViwAAcHB+bOnYuLiwsANWvWpEOHDnh7e1vcRNexY0cjaNesWZM//viDvXv30qlTp3Q9VpGUuHDhAnfv3qV79+5UrlwZgGLFirF69WpCQ0OZPXs2RYsW5ZtvvsHW1haAF198ka5du7J+/Xq6du0KxI6a0rt3bzp27AhA5cqV2bVrF3v27DG+k+JbsGABdnZ2zJs3D2dnZwDq169Pt27dmDFjBj/88IOxbLNmzWjfvn16vg2SCLUAS4ays7Nj1qxZNG/enBs3bnDw4EF+/fVX9u7dC8QGZD8/Pxo0aGCxXlxYFrFWfn5+fPzxx3h4eBjdeVLq77//plq1ajg4OBAVFUVUVBTOzs5UrVqVv/76y2LZ//Zz9PDw0A11kmmVLFkSNzc33n//fb744gt27dqFu7s7Q4YMwdXVlZMnT1K/fn3MZrPx2S9YsCDFihVL8NmvVKmS8be9vT25cuVK8rN/6NAhGjRoYIRfgGzZstGiRQv8/PwICwszppcpUyaNj1qSQy3AkuH279/P1KlTCQwMxNnZmdKlS+Pk5ATAjRs3MJvN5MqVy2KdPHnyZEClIpnHuXPnqF+/Pnv37mXVqlV07949xdu6e/cu27dvZ/v27QnmxbUYx3FwcLB4bTKZNJKKZFpOTk4sWLCARYsWsX37dlavXk327Nl5+eWX6dmzJzExMSxdupSlS5cmWDd79uwWr//72bexsUlyPO179+7h7u6eYLq7uztms5nQ0FCLGuXZUwCWDHX58mU+/PBDGjZsyPTp0ylYsCAmk4mff/6Zffv24erqio2NTYJ+iPfu3bN4bTKZABJ8Ecf/lS3yPKlbty7Tp0/nk08+Ye7cuTRq1Ij8+fOnaFs5cuSgVq1a9OjRI8G8uMvCIllVsWLFmDBhAtHR0fzzzz9s3ryZX375BQ8PD0wmE//3f/9Hy5YtE6z338D7NFxdXbl9+3aC6XHTXF1duXXrVoq3L6mnLhCSofz8/IiIiODtt9+mUKFCRpDdt28fEHvJqFKlSuzcudPil/aff/5psZ24y0zXr183pgUGBiYIyvHpi12ysty5cwMwbNgwbGxs+PLLLxNdzsYm4f/m/zutWrVqXLhwgTJlylC+fHnKly/PCy+8wPLly9m9e3ea1y7yrPz+++80a9aMW7duYWtrS6VKlfj444/JkSMHt2/fply5cgQGBhqf+/Lly1OiRAnmz5+f4Ga1p1GtWjX27Nlj0dIbHR3Nb7/9Rvny5bG3t0+Lw5NUUACWDFWuXDlsbW2ZNWsWvr6+7Nmzh+HDhxt9gB8+fMjAgQM5f/48w4cPZ9++faxYsYL58+dbbKdGjRpkz56d6dOn4+Pjw7Zt2xg2bBiurq5J7jtHjhwA+Pj4JBhWTSSryJMnDwMHDmTv3r1s3bo1wfwcOXLw77//4uPjY7Q45ciRg2PHjnH48GHMZjN9+vQhKCiI999/n927d7N//34++ugjtm3bRunSpZ/1IYmkmSpVqhATE8OHH37I7t27+fvvv5k4cSIhISE0bdqUgQMH4uvry+jRo9m7dy9//vknQ4YM4e+//6ZcuXIp3m+fPn2IiIigf//+/P777/zxxx8MHjyYK1euMHDgwDQ8QkkpBWDJUIULF2bixIlcv36dYcOG8cUXXwCxj3M1mUwcOXKEqlWrMnPmTG7cuMHw4cNZvXo1Y8eOtdhOjhw5mDx5MtHR0Xz44YfMmzePPn36UL58+ST3XaJECVq2bMmqVasYPXp0uh6nSHrq1KkTFSpUYOrUqQmuerRr144CBQowbNgwNm7cCEDPnj3x8/NjyJAhXL9+ndKlS7Nw4UJMJhPjxo1jxIgR3Lp1iylTptCkSZOMOCSRNJEnTx5mzZqFi4sLEyZMYOjQofj7+/P1119To0YNvLy8mDVrFtevX2fEiBGMHTsWW1tb5s6dm6oHW5QsWZKFCxfi5ubGZ599ZnxnzZ8/X0OdZRImc1I9uEVEREREnkNqARYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKpky+gCRESeB3369OHIkSNA7MMnxo0bl8EVJRQQEMCvv/7KgQMHuHXrFo8ePcLNzY0XXniB9u3b07Bhw4wuUUTkmdCDMEREUunixYt06tTJeO3g4MDWrVtxcXHJwKosff/998ybN4+oqKgkl2ndujWffvopNja6OCgizzf9X05EJJXWrVtn8frhw4ds3rw5g6pJaNWqVcyePZuoqCjy5cvHyJEj+fnnn1m5ciVDhw7F2dkZgC1btvDjjz9mcLUiIulPLcAiIqkQFRXFyy+/zO3bt/H09OT69etER0dTpkyZTBEmb926Rbt27YiMjCRfvnz88MMPuLu7Wyzj4+PDe++9B0DevHnZvHkzJpMpI8oVEXkm1AdYRCQV9u7dy+3btwFo3749J0+eZO/evZw5c4aTJ09SsWLFBOsEBwcze/ZsfH19iYyMpGrVqnzwwQd88cUXHD58mGrVqvHdd98ZywcGBjJ//nz+/vtvwsLCKFCgAK1bt+aNN94ge/bsj61v48aNREZGAtC7d+8E4RegXr16DB06FE9PT8qXL2+E3w0bNvDpp58CMG3aNJYuXcqpU6dwc3PD29sbd3d3IiMjWblyJVu3biUoKAiAkiVL0rFjR9q3b28RpPv27cvhw4cBOHjwoDH94MGD9O/fH4jtS92vXz+L5cuUKcNXX33FjBkz+PvvvzGZTNSpU4fBgwfj6en52OMXEUmMArCISCrE7/7QsmVLChcuzN69ewFYvXp1ggB89epV3nrrLe7cuWNM27dvH6dOnUq0z/A///zDgAEDCA0NNaZdvHiRefPmceDAAebOnUu2bEn/rzwucAJ4eXkluVyPHj0ec5Qwbtw4Hjx4AIC7uzvu7u6EhYXRt29fTp8+bbHsiRMnOHHiBD4+PkyaNAlbW9vHbvtJ7ty5Q8+ePbl7964xbfv27Rw+fJilS5eSP3/+VG1fRKyP+gCLiKTQzZs32bdvHwDly5encOHCNGzY0OhTu337dkJCQizWmT17thF+W7duzYoVK/j222/JnTs3ly9ftljWbDbz2WefERoaSq5cuZg8eTK//vorw4cPx8bGhsOHD/PTTz89tsbr168bf+fNm9di3q1bt7h+/XqC/x49epRgO5GRkUybNo0ff/yRDz74AIDp06cb4bdFixYsW7aMxYsXU7t2bQB27tyJt7f349/EZLh58yY5c+Zk9uzZrFixgtatWwNw+/ZtZs2alerti4j1UQAWEUmhDRs2EB0dDUCrVq2A2BEgGjduDEB4eDhbt241lo+JiTFah/Ply8e4ceMoXbo0NWvWZOLEiQm2f/bsWc6dOwdA27ZtKV++PA4ODjRq1Ihq1aoBsGnTpsfWGH9Eh/+OAPHmm2/y8ssvJ/jv+PHjCbbTrFkzXnrpJcqUKUPVqlUJDQ019l2yZEkmTJhAuXLlqFSpElOmTDG6WjwpoCfXmDFj8PLyonTp0owbN44CBQoAsGfPHuPfQEQkuRSARURSwGw2s379euO1i4sL+/btY9++fRaX5NesWWP8fefOHaMrQ/ny5S26LpQuXdpoOY5z6dIl4+9ly5ZZhNS4PrTnzp1LtMU2Tr58+Yy/g4ODn/YwDSVLlkxQW0REBAA1atSw6Obg6OhIpUqVgNjW2/hdF1LCZDJZdCXJli0b5cuXByAsLCzV2xcR66M+wCIiKXDo0CGLLgufffZZosv5+/vzzz//UKFCBezs7IzpyRmAJzl9Z6Ojo7l//z558uRJdH6tWrWMVue9e/dSokQJY178odrGjx/Pxo0bk9zPf/snP6m2Jx1fdHS0sY24IP24bUVFRSX5/mnEChF5WmoBFhFJgf+O/fs4ca3AOXPmJEeOHAD4+flZdEk4ffq0xY1uAIULFzb+HjBgAAcPHjT+W7ZsGVu3buXgwYNJhl+I7Zvr4OAAwNKlS5NsBf7vvv/rvzfaFSxYEHt7eyB2FIeYmBhjXnh4OCdOnABiW6Bz5coFYCz/3/1du3btsfuG2B8ccaKjo/H39wdig3nc9kVEkksBWETkKT148ICdO3cC4Orqyv79+y3C6cGDB9m6davRwrlt2zYj8LVs2RKIvTnt008/JSAgAF9fX0aNGpVgPyVLlqRMmTJAbBeI3377jcuXL7N582beeustWrVqxfDhwx9ba548eXj//fcBuHfvHj179uTnn38mMDCQwMBAtm7dSr9+/di1a9dTvQfOzs40bdoUiO2GMXbsWE6fPs2JEyf46KOPjKHhunbtaqwT/ya8FStWEBMTg7+/P0uXLn3i/r788kv27NlDQEAAX375JVeuXAGgUaNGenKdiDw1dYEQEXlKW7ZsMS7bt2nTxuLSfJw8efLQsGFDdu7cSVhYGFu3bqVTp0706tWLXbt2cfv2bbZs2cKWLVsAyJ8/P46OjoSHhxuX9E0mE8OGDWPIkCHcv38/QUh2dXU1xsx9nE6dOhEZGcmMGTO4ffs2X331VaLL2dra0qFDB6N/7ZMMHz6cM2fOcO7cObZu3Wpxwx9AkyZNLIZXa9myJRs2bABgwYIFLFy4ELPZzIsvvvjE/slms9kI8nHy5s3LoEGDklWriEh8+tksIvKU4nd/6NChQ5LLderUyfg7rhuEh4cHixYtonHjxjg7O+Ps7EyTJk1YuHCh0UUgfleB6tWr8/3339O8eXPc3d2xs7MjX758tGvXju+//55SpUolq+bu3bvz888/07NnT8qWLYurqyt2dnbkyZOHWrVqMWjQIDZs2MDIkSNxcnJK1jZz5syJt7c37733Hi+88AJOTk44ODhQsWJFRo8ezVdffWXRV9jLy4sJEyZQsmRJ7O3tKVCgAH369OGbb7554r7i3jNHR0dcXFxo0aIFS5YseWz3DxGRpOhRyCIiz5Cvry/29vZ4eHiQP39+o29tTEwMDRo0ICIighYtWvDFF19kcKUZL6knx4mIpJa6QIiIPEM//fQTe/bsAaBjx4689dZbPHr0iI0bNxrdKpLbBUFERFJGAVhE5Bnq1q0bPj4+xMTEsHbtWtauXWsxP1++fLRv3z5jihMRsRLqAywi8gx5eXkxd+5cGjRogLu7O7a2ttjb21OoUCE6derE999/T86cOTO6TBGR55r6AIuIiIiIVVELsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFiV/wcMDrk3PYEm6QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb39b9-957e-4ecc-b126-d1934ace7f33",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "55e13822-de5c-4b44-ab3f-292065eaa8f1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      158     74.18\n",
      "1          M    344      254     73.84\n",
      "2          X    295      216     73.22\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)\n",
    "\n",
    "# store for final evaluation \n",
    "all_gender_stats.append(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b0397818-7e1c-4d02-9feb-a38c70b46b6e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91ab56b-3b38-491b-a70b-53841a5799f2",
   "metadata": {},
   "source": [
    "### Log current total CV Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4bd9137c-b17f-447e-bc42-44f9d5c64b48",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Majority Vote Accuracy so far: [0.7636363636363637, 0.7636363636363637]\n",
      "Total Class Statistics so far:\n",
      " [  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          551            443  80.399274\n",
      "1           kitten          118            102  86.440678\n",
      "2           senior          178             76  42.696629,   actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          556            438  78.776978\n",
      "1           kitten          118            101  85.593220\n",
      "2           senior          178             89  50.000000]\n",
      "Total Gender Accuracy so far:\n",
      " [  all_gender  count  correct  accuracy\n",
      "0          F    213      165     77.46\n",
      "1          M    337      234     69.44\n",
      "2          X    297      222     74.75,   all_gender  count  correct  accuracy\n",
      "0          F    213      158     74.18\n",
      "1          M    344      254     73.84\n",
      "2          X    295      216     73.22]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Majority Vote Accuracy so far:\", all_majority_vote_accuracies)\n",
    "print(\"Total Class Statistics so far:\\n\", all_class_stats)\n",
    "print(\"Total Gender Accuracy so far:\\n\", all_gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58f49-cac6-40b1-8422-e3d95576c453",
   "metadata": {},
   "source": [
    "# RANDOM SEED 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf9d1b64-575e-47ca-bf37-e8916438d506",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "adult     588\n",
      "senior    534\n",
      "kitten    513\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[2]))\n",
    "np.random.seed(int(random_seeds[2]))\n",
    "tf.random.set_seed(int(random_seeds[2]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_19.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91d5db1d-6c9f-4047-97b9-5a0eef4fbaaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e435598c-cabc-4129-8504-68aac9cc06bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29b526-5098-4ce6-bc77-a93d1e6e1397",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79a922e6-198c-4c43-a6f2-90dc580ae875",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "019A    17\n",
      "101A    15\n",
      "042A    14\n",
      "059A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "002A    13\n",
      "028A    13\n",
      "111A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "025A    11\n",
      "063A    11\n",
      "040A    10\n",
      "071A    10\n",
      "005A    10\n",
      "014B    10\n",
      "016A    10\n",
      "051B     9\n",
      "045A     9\n",
      "065A     9\n",
      "072A     9\n",
      "022A     9\n",
      "094A     8\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "027A     7\n",
      "099A     7\n",
      "050A     7\n",
      "109A     6\n",
      "007A     6\n",
      "008A     6\n",
      "053A     6\n",
      "021A     5\n",
      "075A     5\n",
      "025C     5\n",
      "023B     5\n",
      "044A     5\n",
      "026A     4\n",
      "052A     4\n",
      "009A     4\n",
      "062A     4\n",
      "104A     4\n",
      "035A     4\n",
      "056A     3\n",
      "058A     3\n",
      "006A     3\n",
      "012A     3\n",
      "113A     3\n",
      "014A     3\n",
      "064A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "032A     2\n",
      "087A     2\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "069A     2\n",
      "038A     2\n",
      "088A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "091A     1\n",
      "073A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "026C     1\n",
      "096A     1\n",
      "041A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "029A    17\n",
      "097A    16\n",
      "036A    11\n",
      "068A    11\n",
      "033A     9\n",
      "015A     9\n",
      "117A     7\n",
      "031A     7\n",
      "023A     6\n",
      "108A     6\n",
      "037A     6\n",
      "070A     5\n",
      "034A     5\n",
      "105A     4\n",
      "003A     4\n",
      "060A     3\n",
      "025B     2\n",
      "092A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    270\n",
      "M    255\n",
      "F    222\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    82\n",
      "X    78\n",
      "F    30\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 050...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 002B, 029A, 034A, 023A, 070A, 031...\n",
      "kitten                                               [047A]\n",
      "senior                                   [097A, 117A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 15, 'senior': 19}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 1, 'senior': 3}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '020A'\n",
      " '021A' '022A' '023B' '024A' '025A' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '032A' '035A' '038A' '039A' '040A' '041A' '042A' '043A' '044A'\n",
      " '045A' '046A' '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '071A' '072A' '073A' '074A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '093A' '094A' '095A' '096A' '097B' '099A' '100A'\n",
      " '101A' '102A' '103A' '104A' '106A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '003A' '015A' '019B' '023A' '025B' '029A' '031A' '033A' '034A'\n",
      " '036A' '037A' '047A' '060A' '068A' '070A' '092A' '097A' '105A' '108A'\n",
      " '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '020A'\n",
      " '021A' '022A' '023B' '024A' '025A' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '032A' '035A' '038A' '039A' '040A' '041A' '042A' '043A' '044A'\n",
      " '045A' '046A' '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '071A' '072A' '073A' '074A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '093A' '094A' '095A' '096A' '097B' '099A' '100A'\n",
      " '101A' '102A' '103A' '104A' '106A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '003A' '015A' '019B' '023A' '025B' '029A' '031A' '033A' '034A'\n",
      " '036A' '037A' '047A' '060A' '068A' '070A' '092A' '097A' '105A' '108A'\n",
      " '117A']\n",
      "Length of X_train_val:\n",
      "747\n",
      "Length of y_train_val:\n",
      "747\n",
      "Length of groups_train_val:\n",
      "747\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     455\n",
      "senior    149\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     133\n",
      "senior     29\n",
      "kitten     28\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     455\n",
      "senior    149\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     133\n",
      "senior     29\n",
      "kitten     28\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 910, 2: 596, 1: 572})\n",
      "Epoch 1/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.9190 - accuracy: 0.5847\n",
      "Epoch 2/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.7132 - accuracy: 0.6858\n",
      "Epoch 3/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.6355 - accuracy: 0.7166\n",
      "Epoch 4/1500\n",
      "65/65 [==============================] - 0s 988us/step - loss: 0.6043 - accuracy: 0.7368\n",
      "Epoch 5/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5995 - accuracy: 0.7320\n",
      "Epoch 6/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5460 - accuracy: 0.7575\n",
      "Epoch 7/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5311 - accuracy: 0.7695\n",
      "Epoch 8/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5309 - accuracy: 0.7661\n",
      "Epoch 9/1500\n",
      "65/65 [==============================] - 0s 956us/step - loss: 0.4920 - accuracy: 0.7772\n",
      "Epoch 10/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4638 - accuracy: 0.7960\n",
      "Epoch 11/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4468 - accuracy: 0.8017\n",
      "Epoch 12/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4385 - accuracy: 0.8070\n",
      "Epoch 13/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4441 - accuracy: 0.8017\n",
      "Epoch 14/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4267 - accuracy: 0.8186\n",
      "Epoch 15/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4354 - accuracy: 0.8013\n",
      "Epoch 16/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4124 - accuracy: 0.8244\n",
      "Epoch 17/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3962 - accuracy: 0.8234\n",
      "Epoch 18/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3820 - accuracy: 0.8263\n",
      "Epoch 19/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3628 - accuracy: 0.8431\n",
      "Epoch 20/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3774 - accuracy: 0.8388\n",
      "Epoch 21/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3790 - accuracy: 0.8378\n",
      "Epoch 22/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8340\n",
      "Epoch 23/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3847 - accuracy: 0.8446\n",
      "Epoch 24/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3444 - accuracy: 0.8523\n",
      "Epoch 25/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3759 - accuracy: 0.8321\n",
      "Epoch 26/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3176 - accuracy: 0.8696\n",
      "Epoch 27/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3202 - accuracy: 0.8609\n",
      "Epoch 28/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3505 - accuracy: 0.8417\n",
      "Epoch 29/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3250 - accuracy: 0.8643\n",
      "Epoch 30/1500\n",
      "65/65 [==============================] - 0s 963us/step - loss: 0.3182 - accuracy: 0.8653\n",
      "Epoch 31/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3272 - accuracy: 0.8585\n",
      "Epoch 32/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2978 - accuracy: 0.8710\n",
      "Epoch 33/1500\n",
      "65/65 [==============================] - 0s 967us/step - loss: 0.3066 - accuracy: 0.8643\n",
      "Epoch 34/1500\n",
      "65/65 [==============================] - 0s 975us/step - loss: 0.3337 - accuracy: 0.8527\n",
      "Epoch 35/1500\n",
      "65/65 [==============================] - 0s 961us/step - loss: 0.3049 - accuracy: 0.8710\n",
      "Epoch 36/1500\n",
      "65/65 [==============================] - 0s 965us/step - loss: 0.2897 - accuracy: 0.8720\n",
      "Epoch 37/1500\n",
      "65/65 [==============================] - 0s 953us/step - loss: 0.3164 - accuracy: 0.8653\n",
      "Epoch 38/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3165 - accuracy: 0.8571\n",
      "Epoch 39/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2832 - accuracy: 0.8826\n",
      "Epoch 40/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2725 - accuracy: 0.8840\n",
      "Epoch 41/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2804 - accuracy: 0.8787\n",
      "Epoch 42/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2691 - accuracy: 0.8859\n",
      "Epoch 43/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2625 - accuracy: 0.8888\n",
      "Epoch 44/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.8840\n",
      "Epoch 45/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2671 - accuracy: 0.8879\n",
      "Epoch 46/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2606 - accuracy: 0.8946\n",
      "Epoch 47/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2456 - accuracy: 0.9004\n",
      "Epoch 48/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2479 - accuracy: 0.8917\n",
      "Epoch 49/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2651 - accuracy: 0.8826\n",
      "Epoch 50/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2586 - accuracy: 0.8917\n",
      "Epoch 51/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2316 - accuracy: 0.9023\n",
      "Epoch 52/1500\n",
      "65/65 [==============================] - 0s 984us/step - loss: 0.2368 - accuracy: 0.9013\n",
      "Epoch 53/1500\n",
      "65/65 [==============================] - 0s 947us/step - loss: 0.2434 - accuracy: 0.8970\n",
      "Epoch 54/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2256 - accuracy: 0.8965\n",
      "Epoch 55/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2296 - accuracy: 0.9071\n",
      "Epoch 56/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2375 - accuracy: 0.8908\n",
      "Epoch 57/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9028\n",
      "Epoch 58/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2366 - accuracy: 0.8941\n",
      "Epoch 59/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2228 - accuracy: 0.9066\n",
      "Epoch 60/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2275 - accuracy: 0.9057\n",
      "Epoch 61/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2268 - accuracy: 0.9018\n",
      "Epoch 62/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2265 - accuracy: 0.9090\n",
      "Epoch 63/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2275 - accuracy: 0.9115\n",
      "Epoch 64/1500\n",
      "65/65 [==============================] - 0s 988us/step - loss: 0.2055 - accuracy: 0.9105\n",
      "Epoch 65/1500\n",
      "65/65 [==============================] - 0s 992us/step - loss: 0.2179 - accuracy: 0.9143\n",
      "Epoch 66/1500\n",
      "65/65 [==============================] - 0s 997us/step - loss: 0.2049 - accuracy: 0.9177\n",
      "Epoch 67/1500\n",
      "65/65 [==============================] - 0s 968us/step - loss: 0.2249 - accuracy: 0.9062\n",
      "Epoch 68/1500\n",
      "65/65 [==============================] - 0s 974us/step - loss: 0.2263 - accuracy: 0.9095\n",
      "Epoch 69/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2113 - accuracy: 0.9090\n",
      "Epoch 70/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9076\n",
      "Epoch 71/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2228 - accuracy: 0.9076\n",
      "Epoch 72/1500\n",
      "65/65 [==============================] - 0s 984us/step - loss: 0.2104 - accuracy: 0.9139\n",
      "Epoch 73/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2039 - accuracy: 0.9110\n",
      "Epoch 74/1500\n",
      "65/65 [==============================] - 0s 965us/step - loss: 0.2061 - accuracy: 0.9129\n",
      "Epoch 75/1500\n",
      "65/65 [==============================] - 0s 961us/step - loss: 0.1985 - accuracy: 0.9172\n",
      "Epoch 76/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2057 - accuracy: 0.9158\n",
      "Epoch 77/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1955 - accuracy: 0.9139\n",
      "Epoch 78/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9100\n",
      "Epoch 79/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9090\n",
      "Epoch 80/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2001 - accuracy: 0.9192\n",
      "Epoch 81/1500\n",
      "65/65 [==============================] - 0s 987us/step - loss: 0.2043 - accuracy: 0.9134\n",
      "Epoch 82/1500\n",
      "65/65 [==============================] - 0s 987us/step - loss: 0.2223 - accuracy: 0.9042\n",
      "Epoch 83/1500\n",
      "65/65 [==============================] - 0s 973us/step - loss: 0.1803 - accuracy: 0.9288\n",
      "Epoch 84/1500\n",
      "65/65 [==============================] - 0s 986us/step - loss: 0.2001 - accuracy: 0.9115\n",
      "Epoch 85/1500\n",
      "65/65 [==============================] - 0s 983us/step - loss: 0.1868 - accuracy: 0.9216\n",
      "Epoch 86/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9220\n",
      "Epoch 87/1500\n",
      "65/65 [==============================] - 0s 992us/step - loss: 0.1881 - accuracy: 0.9196\n",
      "Epoch 88/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.9293\n",
      "Epoch 89/1500\n",
      "65/65 [==============================] - 0s 979us/step - loss: 0.1691 - accuracy: 0.9278\n",
      "Epoch 90/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.9273\n",
      "Epoch 91/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.9249\n",
      "Epoch 92/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9273\n",
      "Epoch 93/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9225\n",
      "Epoch 94/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9244\n",
      "Epoch 95/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9264\n",
      "Epoch 96/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9312\n",
      "Epoch 97/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1683 - accuracy: 0.9336\n",
      "Epoch 98/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9302\n",
      "Epoch 99/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9341\n",
      "Epoch 100/1500\n",
      "65/65 [==============================] - 0s 956us/step - loss: 0.1772 - accuracy: 0.9259\n",
      "Epoch 101/1500\n",
      "65/65 [==============================] - 0s 948us/step - loss: 0.1553 - accuracy: 0.9317\n",
      "Epoch 102/1500\n",
      "65/65 [==============================] - 0s 960us/step - loss: 0.1831 - accuracy: 0.9283\n",
      "Epoch 103/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.9220\n",
      "Epoch 104/1500\n",
      "65/65 [==============================] - 0s 971us/step - loss: 0.1708 - accuracy: 0.9283\n",
      "Epoch 105/1500\n",
      "65/65 [==============================] - 0s 953us/step - loss: 0.1763 - accuracy: 0.9331\n",
      "Epoch 106/1500\n",
      "65/65 [==============================] - 0s 957us/step - loss: 0.1719 - accuracy: 0.9235\n",
      "Epoch 107/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9384\n",
      "Epoch 108/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9461\n",
      "Epoch 109/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9331\n",
      "Epoch 110/1500\n",
      "65/65 [==============================] - 0s 970us/step - loss: 0.1585 - accuracy: 0.9297\n",
      "Epoch 111/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9379\n",
      "Epoch 112/1500\n",
      "65/65 [==============================] - 0s 950us/step - loss: 0.1512 - accuracy: 0.9326\n",
      "Epoch 113/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.9331\n",
      "Epoch 114/1500\n",
      "65/65 [==============================] - 0s 979us/step - loss: 0.1577 - accuracy: 0.9331\n",
      "Epoch 115/1500\n",
      "65/65 [==============================] - 0s 943us/step - loss: 0.1469 - accuracy: 0.9384\n",
      "Epoch 116/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9394\n",
      "Epoch 117/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9326\n",
      "Epoch 118/1500\n",
      "65/65 [==============================] - 0s 1000us/step - loss: 0.1638 - accuracy: 0.9321\n",
      "Epoch 119/1500\n",
      "65/65 [==============================] - 0s 991us/step - loss: 0.1526 - accuracy: 0.9389\n",
      "Epoch 120/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9297\n",
      "Epoch 121/1500\n",
      "65/65 [==============================] - 0s 955us/step - loss: 0.1388 - accuracy: 0.9413\n",
      "Epoch 122/1500\n",
      "65/65 [==============================] - 0s 954us/step - loss: 0.1510 - accuracy: 0.9355\n",
      "Epoch 123/1500\n",
      "65/65 [==============================] - 0s 970us/step - loss: 0.1495 - accuracy: 0.9423\n",
      "Epoch 124/1500\n",
      "65/65 [==============================] - 0s 959us/step - loss: 0.1434 - accuracy: 0.9442\n",
      "Epoch 125/1500\n",
      "65/65 [==============================] - 0s 980us/step - loss: 0.1512 - accuracy: 0.9374\n",
      "Epoch 126/1500\n",
      "65/65 [==============================] - 0s 980us/step - loss: 0.1778 - accuracy: 0.9254\n",
      "Epoch 127/1500\n",
      "65/65 [==============================] - 0s 948us/step - loss: 0.1572 - accuracy: 0.9360\n",
      "Epoch 128/1500\n",
      "65/65 [==============================] - 0s 979us/step - loss: 0.1295 - accuracy: 0.9442\n",
      "Epoch 129/1500\n",
      "65/65 [==============================] - 0s 943us/step - loss: 0.1481 - accuracy: 0.9398\n",
      "Epoch 130/1500\n",
      "65/65 [==============================] - 0s 949us/step - loss: 0.1459 - accuracy: 0.9394\n",
      "Epoch 131/1500\n",
      "65/65 [==============================] - 0s 958us/step - loss: 0.1393 - accuracy: 0.9456\n",
      "Epoch 132/1500\n",
      "65/65 [==============================] - 0s 918us/step - loss: 0.1495 - accuracy: 0.9447\n",
      "Epoch 133/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9447\n",
      "Epoch 134/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9365\n",
      "Epoch 135/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9370\n",
      "Epoch 136/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9471\n",
      "Epoch 137/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9432\n",
      "Epoch 138/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9418\n",
      "Epoch 139/1500\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.1612 - accuracy: 0.9307\n",
      "Epoch 140/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9543\n",
      "Epoch 141/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9471\n",
      "Epoch 142/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9504\n",
      "Epoch 143/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9423\n",
      "Epoch 144/1500\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.1269 - accuracy: 0.9528\n",
      "Epoch 145/1500\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.1226 - accuracy: 0.9509\n",
      "Epoch 146/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.9557\n",
      "Epoch 147/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9500\n",
      "Epoch 148/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1111 - accuracy: 0.9601\n",
      "Epoch 149/1500\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1367 - accuracy: 0.9451\n",
      "Epoch 150/1500\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.1456 - accuracy: 0.9374\n",
      "Epoch 151/1500\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.1272 - accuracy: 0.9471\n",
      "Epoch 152/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9423\n",
      "Epoch 153/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.9475\n",
      "Epoch 154/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9408\n",
      "Epoch 155/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9451\n",
      "Epoch 156/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9538\n",
      "Epoch 157/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9567\n",
      "Epoch 158/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9475\n",
      "Epoch 159/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9552\n",
      "Epoch 160/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9562\n",
      "Epoch 161/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9581\n",
      "Epoch 162/1500\n",
      "65/65 [==============================] - 0s 989us/step - loss: 0.1173 - accuracy: 0.9562\n",
      "Epoch 163/1500\n",
      "65/65 [==============================] - 0s 992us/step - loss: 0.1431 - accuracy: 0.9379\n",
      "Epoch 164/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9519\n",
      "Epoch 165/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9552\n",
      "Epoch 166/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9567\n",
      "Epoch 167/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9427\n",
      "Epoch 168/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9495\n",
      "Epoch 169/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9480\n",
      "Epoch 170/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9442\n",
      "Epoch 171/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9538\n",
      "Epoch 172/1500\n",
      "65/65 [==============================] - 0s 967us/step - loss: 0.1345 - accuracy: 0.9413\n",
      "Epoch 173/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9567\n",
      "Epoch 174/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9543\n",
      "Epoch 175/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9586\n",
      "Epoch 176/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9509\n",
      "Epoch 177/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9639\n",
      "Epoch 178/1500\n",
      "65/65 [==============================] - 0s 999us/step - loss: 0.1119 - accuracy: 0.9548\n",
      "Epoch 179/1500\n",
      "65/65 [==============================] - 0s 975us/step - loss: 0.1233 - accuracy: 0.9543\n",
      "Epoch 180/1500\n",
      "65/65 [==============================] - 0s 980us/step - loss: 0.1164 - accuracy: 0.9500\n",
      "Epoch 181/1500\n",
      "65/65 [==============================] - 0s 962us/step - loss: 0.1185 - accuracy: 0.9552\n",
      "Epoch 182/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9610\n",
      "Epoch 183/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9548\n",
      "Epoch 184/1500\n",
      "65/65 [==============================] - 0s 977us/step - loss: 0.1189 - accuracy: 0.9538\n",
      "Epoch 185/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9500\n",
      "Epoch 186/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9610\n",
      "Epoch 187/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9538\n",
      "Epoch 188/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9485\n",
      "Epoch 189/1500\n",
      "65/65 [==============================] - 0s 964us/step - loss: 0.1099 - accuracy: 0.9543\n",
      "Epoch 190/1500\n",
      "65/65 [==============================] - 0s 966us/step - loss: 0.1177 - accuracy: 0.9495\n",
      "Epoch 191/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9601\n",
      "Epoch 192/1500\n",
      "65/65 [==============================] - 0s 957us/step - loss: 0.1240 - accuracy: 0.9475\n",
      "Epoch 193/1500\n",
      "65/65 [==============================] - 0s 936us/step - loss: 0.1135 - accuracy: 0.9552\n",
      "Epoch 194/1500\n",
      "65/65 [==============================] - 0s 959us/step - loss: 0.1019 - accuracy: 0.9605\n",
      "Epoch 195/1500\n",
      "65/65 [==============================] - 0s 979us/step - loss: 0.1111 - accuracy: 0.9601\n",
      "Epoch 196/1500\n",
      "65/65 [==============================] - 0s 996us/step - loss: 0.1138 - accuracy: 0.9586\n",
      "Epoch 197/1500\n",
      "65/65 [==============================] - 0s 950us/step - loss: 0.1017 - accuracy: 0.9548\n",
      "Epoch 198/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9466\n",
      "Epoch 199/1500\n",
      "65/65 [==============================] - 0s 955us/step - loss: 0.1154 - accuracy: 0.9543\n",
      "Epoch 200/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9437\n",
      "Epoch 201/1500\n",
      "65/65 [==============================] - 0s 959us/step - loss: 0.1009 - accuracy: 0.9577\n",
      "Epoch 202/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9601\n",
      "Epoch 203/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9591\n",
      "Epoch 204/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.9639\n",
      "Epoch 205/1500\n",
      "65/65 [==============================] - 0s 961us/step - loss: 0.1097 - accuracy: 0.9567\n",
      "Epoch 206/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9663\n",
      "Epoch 207/1500\n",
      "65/65 [==============================] - 0s 971us/step - loss: 0.0945 - accuracy: 0.9654\n",
      "Epoch 208/1500\n",
      "65/65 [==============================] - 0s 969us/step - loss: 0.1020 - accuracy: 0.9567\n",
      "Epoch 209/1500\n",
      "65/65 [==============================] - 0s 968us/step - loss: 0.1026 - accuracy: 0.9610\n",
      "Epoch 210/1500\n",
      "65/65 [==============================] - 0s 955us/step - loss: 0.0971 - accuracy: 0.9605\n",
      "Epoch 211/1500\n",
      "65/65 [==============================] - 0s 967us/step - loss: 0.1052 - accuracy: 0.9601\n",
      "Epoch 212/1500\n",
      "65/65 [==============================] - 0s 942us/step - loss: 0.1115 - accuracy: 0.9601\n",
      "Epoch 213/1500\n",
      "65/65 [==============================] - 0s 997us/step - loss: 0.0952 - accuracy: 0.9634\n",
      "Epoch 214/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9654\n",
      "Epoch 215/1500\n",
      "65/65 [==============================] - 0s 957us/step - loss: 0.1156 - accuracy: 0.9524\n",
      "Epoch 216/1500\n",
      "65/65 [==============================] - 0s 968us/step - loss: 0.1110 - accuracy: 0.9610\n",
      "Epoch 217/1500\n",
      "65/65 [==============================] - 0s 999us/step - loss: 0.0988 - accuracy: 0.9663\n",
      "Epoch 218/1500\n",
      "65/65 [==============================] - 0s 950us/step - loss: 0.0914 - accuracy: 0.9649\n",
      "Epoch 219/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.9658\n",
      "Epoch 220/1500\n",
      "65/65 [==============================] - 0s 999us/step - loss: 0.1040 - accuracy: 0.9629\n",
      "Epoch 221/1500\n",
      "65/65 [==============================] - 0s 975us/step - loss: 0.1106 - accuracy: 0.9581\n",
      "Epoch 222/1500\n",
      "65/65 [==============================] - 0s 971us/step - loss: 0.1178 - accuracy: 0.9524\n",
      "Epoch 223/1500\n",
      "65/65 [==============================] - 0s 964us/step - loss: 0.1036 - accuracy: 0.9586\n",
      "Epoch 224/1500\n",
      "65/65 [==============================] - 0s 945us/step - loss: 0.0932 - accuracy: 0.9682\n",
      "Epoch 225/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0956 - accuracy: 0.9610\n",
      "Epoch 226/1500\n",
      "65/65 [==============================] - 0s 970us/step - loss: 0.1039 - accuracy: 0.9581\n",
      "Epoch 227/1500\n",
      "65/65 [==============================] - 0s 989us/step - loss: 0.1136 - accuracy: 0.9519\n",
      "Epoch 228/1500\n",
      "65/65 [==============================] - 0s 960us/step - loss: 0.0979 - accuracy: 0.9577\n",
      "Epoch 229/1500\n",
      "65/65 [==============================] - 0s 993us/step - loss: 0.1201 - accuracy: 0.9581\n",
      "Epoch 230/1500\n",
      "65/65 [==============================] - 0s 982us/step - loss: 0.0976 - accuracy: 0.9625\n",
      "Epoch 231/1500\n",
      "65/65 [==============================] - 0s 953us/step - loss: 0.0944 - accuracy: 0.9639\n",
      "Epoch 232/1500\n",
      "65/65 [==============================] - 0s 956us/step - loss: 0.0943 - accuracy: 0.9586\n",
      "Epoch 233/1500\n",
      "65/65 [==============================] - 0s 965us/step - loss: 0.1042 - accuracy: 0.9596\n",
      "Epoch 234/1500\n",
      "65/65 [==============================] - 0s 943us/step - loss: 0.0921 - accuracy: 0.9639\n",
      "Epoch 235/1500\n",
      "65/65 [==============================] - 0s 969us/step - loss: 0.0899 - accuracy: 0.9654\n",
      "Epoch 236/1500\n",
      "65/65 [==============================] - 0s 949us/step - loss: 0.1094 - accuracy: 0.9601\n",
      "Epoch 237/1500\n",
      "65/65 [==============================] - 0s 967us/step - loss: 0.1092 - accuracy: 0.9577\n",
      "Epoch 238/1500\n",
      "65/65 [==============================] - 0s 969us/step - loss: 0.0965 - accuracy: 0.9629\n",
      "Epoch 239/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9591\n",
      "Epoch 240/1500\n",
      "65/65 [==============================] - 0s 973us/step - loss: 0.0748 - accuracy: 0.9735\n",
      "Epoch 241/1500\n",
      "65/65 [==============================] - 0s 988us/step - loss: 0.0981 - accuracy: 0.9596\n",
      "Epoch 242/1500\n",
      "65/65 [==============================] - 0s 952us/step - loss: 0.0907 - accuracy: 0.9658\n",
      "Epoch 243/1500\n",
      "65/65 [==============================] - 0s 955us/step - loss: 0.0881 - accuracy: 0.9697\n",
      "Epoch 244/1500\n",
      "65/65 [==============================] - 0s 965us/step - loss: 0.1161 - accuracy: 0.9538\n",
      "Epoch 245/1500\n",
      "65/65 [==============================] - 0s 950us/step - loss: 0.0871 - accuracy: 0.9706\n",
      "Epoch 246/1500\n",
      "65/65 [==============================] - 0s 951us/step - loss: 0.0962 - accuracy: 0.9634\n",
      "Epoch 247/1500\n",
      "65/65 [==============================] - 0s 966us/step - loss: 0.1004 - accuracy: 0.9639\n",
      "Epoch 248/1500\n",
      "65/65 [==============================] - 0s 942us/step - loss: 0.0736 - accuracy: 0.9740\n",
      "Epoch 249/1500\n",
      "65/65 [==============================] - 0s 988us/step - loss: 0.0991 - accuracy: 0.9610\n",
      "Epoch 250/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.9654\n",
      "Epoch 251/1500\n",
      "65/65 [==============================] - 0s 984us/step - loss: 0.1066 - accuracy: 0.9601\n",
      "Epoch 252/1500\n",
      "65/65 [==============================] - 0s 972us/step - loss: 0.0845 - accuracy: 0.9687\n",
      "Epoch 253/1500\n",
      "65/65 [==============================] - 0s 968us/step - loss: 0.0839 - accuracy: 0.9697\n",
      "Epoch 254/1500\n",
      "65/65 [==============================] - 0s 956us/step - loss: 0.0965 - accuracy: 0.9644\n",
      "Epoch 255/1500\n",
      "65/65 [==============================] - 0s 959us/step - loss: 0.0947 - accuracy: 0.9644\n",
      "Epoch 256/1500\n",
      "65/65 [==============================] - 0s 958us/step - loss: 0.0843 - accuracy: 0.9678\n",
      "Epoch 257/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.9731\n",
      "Epoch 258/1500\n",
      "65/65 [==============================] - 0s 996us/step - loss: 0.0745 - accuracy: 0.9726\n",
      "Epoch 259/1500\n",
      "65/65 [==============================] - 0s 984us/step - loss: 0.0693 - accuracy: 0.9769\n",
      "Epoch 260/1500\n",
      "65/65 [==============================] - 0s 969us/step - loss: 0.0920 - accuracy: 0.9620\n",
      "Epoch 261/1500\n",
      "65/65 [==============================] - 0s 961us/step - loss: 0.1053 - accuracy: 0.9548\n",
      "Epoch 262/1500\n",
      "65/65 [==============================] - 0s 925us/step - loss: 0.0873 - accuracy: 0.9682\n",
      "Epoch 263/1500\n",
      "65/65 [==============================] - 0s 947us/step - loss: 0.0849 - accuracy: 0.9668\n",
      "Epoch 264/1500\n",
      "65/65 [==============================] - 0s 948us/step - loss: 0.0872 - accuracy: 0.9644\n",
      "Epoch 265/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9634\n",
      "Epoch 266/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9692\n",
      "Epoch 267/1500\n",
      "65/65 [==============================] - 0s 973us/step - loss: 0.0733 - accuracy: 0.9745\n",
      "Epoch 268/1500\n",
      "65/65 [==============================] - 0s 983us/step - loss: 0.0800 - accuracy: 0.9663\n",
      "Epoch 269/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9605\n",
      "Epoch 270/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.9658\n",
      "Epoch 271/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9629\n",
      "Epoch 272/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9668\n",
      "Epoch 273/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9658\n",
      "Epoch 274/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9629\n",
      "Epoch 275/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.9740\n",
      "Epoch 276/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9769\n",
      "Epoch 277/1500\n",
      "65/65 [==============================] - 0s 971us/step - loss: 0.0724 - accuracy: 0.9750\n",
      "Epoch 278/1500\n",
      "65/65 [==============================] - 0s 959us/step - loss: 0.0789 - accuracy: 0.9716\n",
      "Epoch 279/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9654\n",
      "Epoch 280/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.9711\n",
      "Epoch 281/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9706\n",
      "Epoch 282/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.9610\n",
      "Epoch 283/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9687\n",
      "Epoch 284/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9702\n",
      "Epoch 285/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9798\n",
      "Epoch 286/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9682\n",
      "Epoch 287/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9702\n",
      "Epoch 288/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9649\n",
      "Epoch 289/1500\n",
      "65/65 [==============================] - 0s 941us/step - loss: 0.0841 - accuracy: 0.9721\n",
      "Epoch 290/1500\n",
      "65/65 [==============================] - 0s 967us/step - loss: 0.0840 - accuracy: 0.9673\n",
      "Epoch 291/1500\n",
      "65/65 [==============================] - 0s 970us/step - loss: 0.0844 - accuracy: 0.9678\n",
      "Epoch 292/1500\n",
      "65/65 [==============================] - 0s 962us/step - loss: 0.0684 - accuracy: 0.9774\n",
      "Epoch 293/1500\n",
      "65/65 [==============================] - 0s 990us/step - loss: 0.0815 - accuracy: 0.9716\n",
      "Epoch 294/1500\n",
      "65/65 [==============================] - 0s 971us/step - loss: 0.0862 - accuracy: 0.9644\n",
      "Epoch 295/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9697\n",
      "Epoch 296/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9755\n",
      "Epoch 297/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.9692\n",
      "Epoch 298/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.9716\n",
      "Epoch 299/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9697\n",
      "Epoch 300/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9706\n",
      "Epoch 301/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9687\n",
      "Epoch 302/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9654\n",
      "Epoch 303/1500\n",
      "65/65 [==============================] - 0s 952us/step - loss: 0.0796 - accuracy: 0.9673\n",
      "Epoch 304/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.9759\n",
      "Epoch 305/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9654\n",
      "Epoch 306/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9726\n",
      "Epoch 307/1500\n",
      "65/65 [==============================] - 0s 983us/step - loss: 0.0821 - accuracy: 0.9702\n",
      "Epoch 308/1500\n",
      "65/65 [==============================] - 0s 975us/step - loss: 0.0699 - accuracy: 0.9721\n",
      "Epoch 309/1500\n",
      "65/65 [==============================] - 0s 956us/step - loss: 0.0711 - accuracy: 0.9721\n",
      "Epoch 310/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9591\n",
      "Epoch 311/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0649 - accuracy: 0.9783\n",
      "Epoch 312/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.9735\n",
      "Epoch 313/1500\n",
      "65/65 [==============================] - 0s 980us/step - loss: 0.0911 - accuracy: 0.9658\n",
      "Epoch 314/1500\n",
      "65/65 [==============================] - 0s 986us/step - loss: 0.0699 - accuracy: 0.9721\n",
      "Epoch 315/1500\n",
      "65/65 [==============================] - 0s 972us/step - loss: 0.0832 - accuracy: 0.9692\n",
      "Epoch 316/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.9726\n",
      "Epoch 317/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9731\n",
      "Epoch 318/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.9774\n",
      "Epoch 319/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9706\n",
      "Epoch 320/1500\n",
      "65/65 [==============================] - 0s 965us/step - loss: 0.0709 - accuracy: 0.9726\n",
      "Epoch 321/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9721\n",
      "Epoch 322/1500\n",
      "65/65 [==============================] - 0s 982us/step - loss: 0.0700 - accuracy: 0.9721\n",
      "Epoch 323/1500\n",
      "65/65 [==============================] - 0s 965us/step - loss: 0.0766 - accuracy: 0.9721\n",
      "Epoch 324/1500\n",
      "65/65 [==============================] - 0s 960us/step - loss: 0.0734 - accuracy: 0.9721\n",
      "Epoch 325/1500\n",
      "65/65 [==============================] - 0s 951us/step - loss: 0.0795 - accuracy: 0.9668\n",
      "Epoch 326/1500\n",
      "65/65 [==============================] - 0s 969us/step - loss: 0.0711 - accuracy: 0.9716\n",
      "Epoch 327/1500\n",
      "65/65 [==============================] - 0s 980us/step - loss: 0.0783 - accuracy: 0.9678\n",
      "Epoch 328/1500\n",
      "65/65 [==============================] - 0s 969us/step - loss: 0.0598 - accuracy: 0.9832\n",
      "Epoch 329/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9596\n",
      "Epoch 330/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9726\n",
      "Epoch 331/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9668\n",
      "Epoch 332/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9740\n",
      "Epoch 333/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.9731\n",
      "Epoch 334/1500\n",
      "45/65 [===================>..........] - ETA: 0s - loss: 0.0729 - accuracy: 0.9708Restoring model weights from the end of the best epoch: 304.\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.9745\n",
      "Epoch 334: early stopping\n",
      "6/6 [==============================] - 0s 865us/step - loss: 1.0866 - accuracy: 0.6947\n",
      "6/6 [==============================] - 0s 667us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (16/21)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 190, Predictions: 190, Actuals: 190, Gender: 190\n",
      "Final Test Results - Loss: 1.0866035223007202, Accuracy: 0.6947368383407593, Precision: 0.6523616455928166, Recall: 0.7100077780658544, F1 Score: 0.6666666666666666\n",
      "Confusion Matrix:\n",
      " [[91  6 36]\n",
      " [ 2 26  0]\n",
      " [14  0 15]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "020A    23\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "028A    13\n",
      "111A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "025A    11\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "040A    10\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "014B    10\n",
      "045A     9\n",
      "051B     9\n",
      "065A     9\n",
      "033A     9\n",
      "015A     9\n",
      "095A     8\n",
      "013B     8\n",
      "094A     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "050A     7\n",
      "109A     6\n",
      "023A     6\n",
      "053A     6\n",
      "037A     6\n",
      "108A     6\n",
      "007A     6\n",
      "044A     5\n",
      "034A     5\n",
      "070A     5\n",
      "062A     4\n",
      "026A     4\n",
      "104A     4\n",
      "052A     4\n",
      "105A     4\n",
      "009A     4\n",
      "003A     4\n",
      "060A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "056A     3\n",
      "087A     2\n",
      "011A     2\n",
      "018A     2\n",
      "025B     2\n",
      "054A     2\n",
      "032A     2\n",
      "102A     2\n",
      "069A     2\n",
      "038A     2\n",
      "090A     1\n",
      "100A     1\n",
      "115A     1\n",
      "091A     1\n",
      "019B     1\n",
      "088A     1\n",
      "043A     1\n",
      "048A     1\n",
      "066A     1\n",
      "073A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "097B    14\n",
      "002A    13\n",
      "022A     9\n",
      "072A     9\n",
      "010A     8\n",
      "099A     7\n",
      "008A     6\n",
      "025C     5\n",
      "021A     5\n",
      "075A     5\n",
      "023B     5\n",
      "035A     4\n",
      "014A     3\n",
      "058A     3\n",
      "113A     3\n",
      "061A     2\n",
      "093A     2\n",
      "110A     1\n",
      "004A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    271\n",
      "M    223\n",
      "F    196\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    114\n",
      "X     77\n",
      "F     56\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 071A, 028A, 020...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [097A, 106A, 104A, 059A, 116A, 051B, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [103A, 097B, 019A, 074A, 067A, 022A, 072A, 002...\n",
      "kitten                                               [110A]\n",
      "senior                 [093A, 057A, 055A, 113A, 058A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 15, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 1, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '005A' '006A' '007A' '009A' '011A'\n",
      " '012A' '013B' '014B' '015A' '016A' '018A' '019B' '020A' '023A' '024A'\n",
      " '025A' '025B' '026A' '026C' '027A' '028A' '029A' '031A' '032A' '033A'\n",
      " '034A' '036A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051A' '051B' '052A' '053A'\n",
      " '054A' '056A' '059A' '060A' '062A' '063A' '064A' '065A' '066A' '068A'\n",
      " '069A' '070A' '071A' '073A' '076A' '087A' '088A' '090A' '091A' '092A'\n",
      " '094A' '095A' '096A' '097A' '100A' '101A' '102A' '104A' '105A' '106A'\n",
      " '108A' '109A' '111A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '004A' '008A' '010A' '014A' '019A' '021A' '022A' '023B' '025C'\n",
      " '026B' '035A' '055A' '057A' '058A' '061A' '067A' '072A' '074A' '075A'\n",
      " '093A' '097B' '099A' '103A' '110A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '005A' '006A' '007A' '009A' '011A'\n",
      " '012A' '013B' '014B' '015A' '016A' '018A' '019B' '020A' '023A' '024A'\n",
      " '025A' '025B' '026A' '026C' '027A' '028A' '029A' '031A' '032A' '033A'\n",
      " '034A' '036A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051A' '051B' '052A' '053A'\n",
      " '054A' '056A' '059A' '060A' '062A' '063A' '064A' '065A' '066A' '068A'\n",
      " '069A' '070A' '071A' '073A' '076A' '087A' '088A' '090A' '091A' '092A'\n",
      " '094A' '095A' '096A' '097A' '100A' '101A' '102A' '104A' '105A' '106A'\n",
      " '108A' '109A' '111A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '004A' '008A' '010A' '014A' '019A' '021A' '022A' '023B' '025C'\n",
      " '026B' '035A' '055A' '057A' '058A' '061A' '067A' '072A' '074A' '075A'\n",
      " '093A' '097B' '099A' '103A' '110A' '113A']\n",
      "Length of X_train_val:\n",
      "690\n",
      "Length of y_train_val:\n",
      "690\n",
      "Length of groups_train_val:\n",
      "690\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     399\n",
      "kitten    170\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     189\n",
      "senior     57\n",
      "kitten      1\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     399\n",
      "kitten    170\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     189\n",
      "senior     57\n",
      "kitten      1\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 798, 1: 680, 2: 484})\n",
      "Epoch 1/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 1.0000 - accuracy: 0.5617\n",
      "Epoch 2/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.7159 - accuracy: 0.6896\n",
      "Epoch 3/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6899 - accuracy: 0.7039\n",
      "Epoch 4/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6285 - accuracy: 0.7360\n",
      "Epoch 5/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.6063 - accuracy: 0.7594\n",
      "Epoch 6/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.5477 - accuracy: 0.7813\n",
      "Epoch 7/1500\n",
      "62/62 [==============================] - 0s 985us/step - loss: 0.5372 - accuracy: 0.7712\n",
      "Epoch 8/1500\n",
      "62/62 [==============================] - 0s 979us/step - loss: 0.5121 - accuracy: 0.7783\n",
      "Epoch 9/1500\n",
      "62/62 [==============================] - 0s 971us/step - loss: 0.5012 - accuracy: 0.7870\n",
      "Epoch 10/1500\n",
      "62/62 [==============================] - 0s 973us/step - loss: 0.4709 - accuracy: 0.8078\n",
      "Epoch 11/1500\n",
      "62/62 [==============================] - 0s 949us/step - loss: 0.4464 - accuracy: 0.8155\n",
      "Epoch 12/1500\n",
      "62/62 [==============================] - 0s 967us/step - loss: 0.4417 - accuracy: 0.8196\n",
      "Epoch 13/1500\n",
      "62/62 [==============================] - 0s 953us/step - loss: 0.4442 - accuracy: 0.8145\n",
      "Epoch 14/1500\n",
      "62/62 [==============================] - 0s 971us/step - loss: 0.4409 - accuracy: 0.8236\n",
      "Epoch 15/1500\n",
      "62/62 [==============================] - 0s 934us/step - loss: 0.4395 - accuracy: 0.8282\n",
      "Epoch 16/1500\n",
      "62/62 [==============================] - 0s 991us/step - loss: 0.4139 - accuracy: 0.8293\n",
      "Epoch 17/1500\n",
      "62/62 [==============================] - 0s 979us/step - loss: 0.4062 - accuracy: 0.8323\n",
      "Epoch 18/1500\n",
      "62/62 [==============================] - 0s 945us/step - loss: 0.3926 - accuracy: 0.8349\n",
      "Epoch 19/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4044 - accuracy: 0.8262\n",
      "Epoch 20/1500\n",
      "62/62 [==============================] - 0s 959us/step - loss: 0.3719 - accuracy: 0.8542\n",
      "Epoch 21/1500\n",
      "62/62 [==============================] - 0s 997us/step - loss: 0.3712 - accuracy: 0.8425\n",
      "Epoch 22/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.4007 - accuracy: 0.8303\n",
      "Epoch 23/1500\n",
      "62/62 [==============================] - 0s 918us/step - loss: 0.3740 - accuracy: 0.8507\n",
      "Epoch 24/1500\n",
      "62/62 [==============================] - 0s 984us/step - loss: 0.3543 - accuracy: 0.8609\n",
      "Epoch 25/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3639 - accuracy: 0.8502\n",
      "Epoch 26/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3387 - accuracy: 0.8583\n",
      "Epoch 27/1500\n",
      "62/62 [==============================] - 0s 986us/step - loss: 0.3555 - accuracy: 0.8466\n",
      "Epoch 28/1500\n",
      "62/62 [==============================] - 0s 974us/step - loss: 0.3195 - accuracy: 0.8639\n",
      "Epoch 29/1500\n",
      "62/62 [==============================] - 0s 965us/step - loss: 0.3408 - accuracy: 0.8593\n",
      "Epoch 30/1500\n",
      "62/62 [==============================] - 0s 983us/step - loss: 0.3414 - accuracy: 0.8644\n",
      "Epoch 31/1500\n",
      "62/62 [==============================] - 0s 966us/step - loss: 0.3395 - accuracy: 0.8598\n",
      "Epoch 32/1500\n",
      "62/62 [==============================] - 0s 994us/step - loss: 0.3189 - accuracy: 0.8761\n",
      "Epoch 33/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3188 - accuracy: 0.8731\n",
      "Epoch 34/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3155 - accuracy: 0.8741\n",
      "Epoch 35/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3148 - accuracy: 0.8761\n",
      "Epoch 36/1500\n",
      "62/62 [==============================] - 0s 995us/step - loss: 0.3203 - accuracy: 0.8619\n",
      "Epoch 37/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.3015 - accuracy: 0.8710\n",
      "Epoch 38/1500\n",
      "62/62 [==============================] - 0s 988us/step - loss: 0.3200 - accuracy: 0.8634\n",
      "Epoch 39/1500\n",
      "62/62 [==============================] - 0s 992us/step - loss: 0.3118 - accuracy: 0.8731\n",
      "Epoch 40/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2974 - accuracy: 0.8756\n",
      "Epoch 41/1500\n",
      "62/62 [==============================] - 0s 975us/step - loss: 0.2855 - accuracy: 0.8761\n",
      "Epoch 42/1500\n",
      "62/62 [==============================] - 0s 982us/step - loss: 0.2713 - accuracy: 0.8909\n",
      "Epoch 43/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.8792\n",
      "Epoch 44/1500\n",
      "62/62 [==============================] - 0s 966us/step - loss: 0.2872 - accuracy: 0.8812\n",
      "Epoch 45/1500\n",
      "62/62 [==============================] - 0s 991us/step - loss: 0.2911 - accuracy: 0.8818\n",
      "Epoch 46/1500\n",
      "62/62 [==============================] - 0s 969us/step - loss: 0.2817 - accuracy: 0.8884\n",
      "Epoch 47/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2752 - accuracy: 0.8909\n",
      "Epoch 48/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2715 - accuracy: 0.8996\n",
      "Epoch 49/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2844 - accuracy: 0.8787\n",
      "Epoch 50/1500\n",
      "62/62 [==============================] - 0s 989us/step - loss: 0.2803 - accuracy: 0.8782\n",
      "Epoch 51/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2550 - accuracy: 0.8940\n",
      "Epoch 52/1500\n",
      "62/62 [==============================] - 0s 959us/step - loss: 0.2490 - accuracy: 0.8950\n",
      "Epoch 53/1500\n",
      "62/62 [==============================] - 0s 1000us/step - loss: 0.2469 - accuracy: 0.9011\n",
      "Epoch 54/1500\n",
      "62/62 [==============================] - 0s 984us/step - loss: 0.2611 - accuracy: 0.8884\n",
      "Epoch 55/1500\n",
      "62/62 [==============================] - 0s 963us/step - loss: 0.2489 - accuracy: 0.8996\n",
      "Epoch 56/1500\n",
      "62/62 [==============================] - 0s 974us/step - loss: 0.2432 - accuracy: 0.9011\n",
      "Epoch 57/1500\n",
      "62/62 [==============================] - 0s 984us/step - loss: 0.2502 - accuracy: 0.8940\n",
      "Epoch 58/1500\n",
      "62/62 [==============================] - 0s 995us/step - loss: 0.2308 - accuracy: 0.9088\n",
      "Epoch 59/1500\n",
      "62/62 [==============================] - 0s 946us/step - loss: 0.2373 - accuracy: 0.9027\n",
      "Epoch 60/1500\n",
      "62/62 [==============================] - 0s 979us/step - loss: 0.2311 - accuracy: 0.9077\n",
      "Epoch 61/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2463 - accuracy: 0.9006\n",
      "Epoch 62/1500\n",
      "62/62 [==============================] - 0s 955us/step - loss: 0.2353 - accuracy: 0.9052\n",
      "Epoch 63/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2418 - accuracy: 0.9006\n",
      "Epoch 64/1500\n",
      "62/62 [==============================] - 0s 995us/step - loss: 0.2276 - accuracy: 0.9062\n",
      "Epoch 65/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2112 - accuracy: 0.9149\n",
      "Epoch 66/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2155 - accuracy: 0.9108\n",
      "Epoch 67/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2320 - accuracy: 0.9108\n",
      "Epoch 68/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9220\n",
      "Epoch 69/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9195\n",
      "Epoch 70/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2122 - accuracy: 0.9230\n",
      "Epoch 71/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2184 - accuracy: 0.9164\n",
      "Epoch 72/1500\n",
      "62/62 [==============================] - 0s 955us/step - loss: 0.2272 - accuracy: 0.9083\n",
      "Epoch 73/1500\n",
      "62/62 [==============================] - 0s 995us/step - loss: 0.2016 - accuracy: 0.9215\n",
      "Epoch 74/1500\n",
      "62/62 [==============================] - 0s 972us/step - loss: 0.2068 - accuracy: 0.9200\n",
      "Epoch 75/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9261\n",
      "Epoch 76/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2006 - accuracy: 0.9154\n",
      "Epoch 77/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2226 - accuracy: 0.9037\n",
      "Epoch 78/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9230\n",
      "Epoch 79/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2292 - accuracy: 0.9113\n",
      "Epoch 80/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.2079 - accuracy: 0.9246\n",
      "Epoch 81/1500\n",
      "62/62 [==============================] - 0s 987us/step - loss: 0.1981 - accuracy: 0.9205\n",
      "Epoch 82/1500\n",
      "62/62 [==============================] - 0s 985us/step - loss: 0.1979 - accuracy: 0.9246\n",
      "Epoch 83/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.9358\n",
      "Epoch 84/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.9281\n",
      "Epoch 85/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.9276\n",
      "Epoch 86/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9307\n",
      "Epoch 87/1500\n",
      "62/62 [==============================] - 0s 964us/step - loss: 0.1777 - accuracy: 0.9312\n",
      "Epoch 88/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.9230\n",
      "Epoch 89/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1900 - accuracy: 0.9286\n",
      "Epoch 90/1500\n",
      "62/62 [==============================] - 0s 990us/step - loss: 0.1791 - accuracy: 0.9337\n",
      "Epoch 91/1500\n",
      "62/62 [==============================] - 0s 994us/step - loss: 0.1757 - accuracy: 0.9332\n",
      "Epoch 92/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.9261\n",
      "Epoch 93/1500\n",
      "62/62 [==============================] - 0s 979us/step - loss: 0.1878 - accuracy: 0.9256\n",
      "Epoch 94/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9266\n",
      "Epoch 95/1500\n",
      "62/62 [==============================] - 0s 990us/step - loss: 0.1879 - accuracy: 0.9225\n",
      "Epoch 96/1500\n",
      "62/62 [==============================] - 0s 994us/step - loss: 0.1791 - accuracy: 0.9286\n",
      "Epoch 97/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9343\n",
      "Epoch 98/1500\n",
      "62/62 [==============================] - 0s 965us/step - loss: 0.1586 - accuracy: 0.9424\n",
      "Epoch 99/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9348\n",
      "Epoch 100/1500\n",
      "62/62 [==============================] - 0s 963us/step - loss: 0.1723 - accuracy: 0.9337\n",
      "Epoch 101/1500\n",
      "62/62 [==============================] - 0s 999us/step - loss: 0.1723 - accuracy: 0.9343\n",
      "Epoch 102/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.9409\n",
      "Epoch 103/1500\n",
      "62/62 [==============================] - 0s 964us/step - loss: 0.1682 - accuracy: 0.9317\n",
      "Epoch 104/1500\n",
      "62/62 [==============================] - 0s 995us/step - loss: 0.1787 - accuracy: 0.9368\n",
      "Epoch 105/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.9261\n",
      "Epoch 106/1500\n",
      "62/62 [==============================] - 0s 990us/step - loss: 0.1627 - accuracy: 0.9358\n",
      "Epoch 107/1500\n",
      "62/62 [==============================] - 0s 978us/step - loss: 0.1705 - accuracy: 0.9348\n",
      "Epoch 108/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1493 - accuracy: 0.9465\n",
      "Epoch 109/1500\n",
      "62/62 [==============================] - 0s 966us/step - loss: 0.1828 - accuracy: 0.9261\n",
      "Epoch 110/1500\n",
      "62/62 [==============================] - 0s 974us/step - loss: 0.1620 - accuracy: 0.9414\n",
      "Epoch 111/1500\n",
      "62/62 [==============================] - 0s 993us/step - loss: 0.1723 - accuracy: 0.9363\n",
      "Epoch 112/1500\n",
      "62/62 [==============================] - 0s 974us/step - loss: 0.1527 - accuracy: 0.9409\n",
      "Epoch 113/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9465\n",
      "Epoch 114/1500\n",
      "62/62 [==============================] - 0s 982us/step - loss: 0.1638 - accuracy: 0.9353\n",
      "Epoch 115/1500\n",
      "62/62 [==============================] - 0s 991us/step - loss: 0.1525 - accuracy: 0.9434\n",
      "Epoch 116/1500\n",
      "62/62 [==============================] - 0s 985us/step - loss: 0.1412 - accuracy: 0.9434\n",
      "Epoch 117/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9419\n",
      "Epoch 118/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9414\n",
      "Epoch 119/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9450\n",
      "Epoch 120/1500\n",
      "62/62 [==============================] - 0s 986us/step - loss: 0.1432 - accuracy: 0.9460\n",
      "Epoch 121/1500\n",
      "62/62 [==============================] - 0s 971us/step - loss: 0.1453 - accuracy: 0.9455\n",
      "Epoch 122/1500\n",
      "62/62 [==============================] - 0s 990us/step - loss: 0.1552 - accuracy: 0.9383\n",
      "Epoch 123/1500\n",
      "62/62 [==============================] - 0s 980us/step - loss: 0.1547 - accuracy: 0.9404\n",
      "Epoch 124/1500\n",
      "62/62 [==============================] - 0s 968us/step - loss: 0.1375 - accuracy: 0.9460\n",
      "Epoch 125/1500\n",
      "62/62 [==============================] - 0s 960us/step - loss: 0.1520 - accuracy: 0.9429\n",
      "Epoch 126/1500\n",
      "62/62 [==============================] - 0s 972us/step - loss: 0.1577 - accuracy: 0.9353\n",
      "Epoch 127/1500\n",
      "62/62 [==============================] - 0s 995us/step - loss: 0.1373 - accuracy: 0.9434\n",
      "Epoch 128/1500\n",
      "62/62 [==============================] - 0s 997us/step - loss: 0.1451 - accuracy: 0.9444\n",
      "Epoch 129/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.9434\n",
      "Epoch 130/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9455\n",
      "Epoch 131/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.9429\n",
      "Epoch 132/1500\n",
      "62/62 [==============================] - 0s 992us/step - loss: 0.1429 - accuracy: 0.9414\n",
      "Epoch 133/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.9414\n",
      "Epoch 134/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9506\n",
      "Epoch 135/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9465\n",
      "Epoch 136/1500\n",
      "62/62 [==============================] - 0s 999us/step - loss: 0.1487 - accuracy: 0.9511\n",
      "Epoch 137/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9541\n",
      "Epoch 138/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9404\n",
      "Epoch 139/1500\n",
      "62/62 [==============================] - 0s 986us/step - loss: 0.1390 - accuracy: 0.9490\n",
      "Epoch 140/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9506\n",
      "Epoch 141/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9516\n",
      "Epoch 142/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9572\n",
      "Epoch 143/1500\n",
      "62/62 [==============================] - 0s 999us/step - loss: 0.1531 - accuracy: 0.9434\n",
      "Epoch 144/1500\n",
      "62/62 [==============================] - 0s 987us/step - loss: 0.1332 - accuracy: 0.9460\n",
      "Epoch 145/1500\n",
      "62/62 [==============================] - 0s 981us/step - loss: 0.1294 - accuracy: 0.9485\n",
      "Epoch 146/1500\n",
      "62/62 [==============================] - 0s 965us/step - loss: 0.1459 - accuracy: 0.9414\n",
      "Epoch 147/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9536\n",
      "Epoch 148/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.9485\n",
      "Epoch 149/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1561 - accuracy: 0.9404\n",
      "Epoch 150/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9495\n",
      "Epoch 151/1500\n",
      "62/62 [==============================] - 0s 1000us/step - loss: 0.1191 - accuracy: 0.9546\n",
      "Epoch 152/1500\n",
      "62/62 [==============================] - 0s 995us/step - loss: 0.1211 - accuracy: 0.9501\n",
      "Epoch 153/1500\n",
      "62/62 [==============================] - 0s 980us/step - loss: 0.1209 - accuracy: 0.9562\n",
      "Epoch 154/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9546\n",
      "Epoch 155/1500\n",
      "62/62 [==============================] - 0s 976us/step - loss: 0.1325 - accuracy: 0.9470\n",
      "Epoch 156/1500\n",
      "62/62 [==============================] - 0s 994us/step - loss: 0.1427 - accuracy: 0.9444\n",
      "Epoch 157/1500\n",
      "62/62 [==============================] - 0s 993us/step - loss: 0.1415 - accuracy: 0.9465\n",
      "Epoch 158/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9546\n",
      "Epoch 159/1500\n",
      "62/62 [==============================] - 0s 971us/step - loss: 0.1329 - accuracy: 0.9475\n",
      "Epoch 160/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9587\n",
      "Epoch 161/1500\n",
      "62/62 [==============================] - 0s 998us/step - loss: 0.1180 - accuracy: 0.9546\n",
      "Epoch 162/1500\n",
      "62/62 [==============================] - 0s 942us/step - loss: 0.1190 - accuracy: 0.9587\n",
      "Epoch 163/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.9495\n",
      "Epoch 164/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9592\n",
      "Epoch 165/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9551\n",
      "Epoch 166/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9567\n",
      "Epoch 167/1500\n",
      "62/62 [==============================] - 0s 982us/step - loss: 0.1165 - accuracy: 0.9597\n",
      "Epoch 168/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9511\n",
      "Epoch 169/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9633\n",
      "Epoch 170/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9551\n",
      "Epoch 171/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9521\n",
      "Epoch 172/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9572\n",
      "Epoch 173/1500\n",
      "62/62 [==============================] - 0s 990us/step - loss: 0.1136 - accuracy: 0.9567\n",
      "Epoch 174/1500\n",
      "62/62 [==============================] - 0s 950us/step - loss: 0.1149 - accuracy: 0.9582\n",
      "Epoch 175/1500\n",
      "62/62 [==============================] - 0s 952us/step - loss: 0.1108 - accuracy: 0.9557\n",
      "Epoch 176/1500\n",
      "62/62 [==============================] - 0s 964us/step - loss: 0.1064 - accuracy: 0.9567\n",
      "Epoch 177/1500\n",
      "62/62 [==============================] - 0s 954us/step - loss: 0.1156 - accuracy: 0.9562\n",
      "Epoch 178/1500\n",
      "62/62 [==============================] - 0s 953us/step - loss: 0.1230 - accuracy: 0.9541\n",
      "Epoch 179/1500\n",
      "62/62 [==============================] - 0s 939us/step - loss: 0.1052 - accuracy: 0.9633\n",
      "Epoch 180/1500\n",
      "62/62 [==============================] - 0s 954us/step - loss: 0.1136 - accuracy: 0.9618\n",
      "Epoch 181/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9679\n",
      "Epoch 182/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9582\n",
      "Epoch 183/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0994 - accuracy: 0.9643\n",
      "Epoch 184/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9608\n",
      "Epoch 185/1500\n",
      "62/62 [==============================] - 0s 986us/step - loss: 0.1199 - accuracy: 0.9582\n",
      "Epoch 186/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9551\n",
      "Epoch 187/1500\n",
      "62/62 [==============================] - 0s 996us/step - loss: 0.1125 - accuracy: 0.9648\n",
      "Epoch 188/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9613\n",
      "Epoch 189/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9557\n",
      "Epoch 190/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9587\n",
      "Epoch 191/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9608\n",
      "Epoch 192/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9567\n",
      "Epoch 193/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9623\n",
      "Epoch 194/1500\n",
      "62/62 [==============================] - 0s 968us/step - loss: 0.1067 - accuracy: 0.9582\n",
      "Epoch 195/1500\n",
      "62/62 [==============================] - 0s 955us/step - loss: 0.1098 - accuracy: 0.9618\n",
      "Epoch 196/1500\n",
      "62/62 [==============================] - 0s 975us/step - loss: 0.1023 - accuracy: 0.9592\n",
      "Epoch 197/1500\n",
      "62/62 [==============================] - 0s 974us/step - loss: 0.0794 - accuracy: 0.9725\n",
      "Epoch 198/1500\n",
      "62/62 [==============================] - 0s 970us/step - loss: 0.1000 - accuracy: 0.9628\n",
      "Epoch 199/1500\n",
      "62/62 [==============================] - 0s 973us/step - loss: 0.1118 - accuracy: 0.9602\n",
      "Epoch 200/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1135 - accuracy: 0.9587\n",
      "Epoch 201/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9562\n",
      "Epoch 202/1500\n",
      "62/62 [==============================] - 0s 992us/step - loss: 0.0982 - accuracy: 0.9633\n",
      "Epoch 203/1500\n",
      "62/62 [==============================] - 0s 980us/step - loss: 0.0995 - accuracy: 0.9653\n",
      "Epoch 204/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.9577\n",
      "Epoch 205/1500\n",
      "62/62 [==============================] - 0s 987us/step - loss: 0.0971 - accuracy: 0.9669\n",
      "Epoch 206/1500\n",
      "62/62 [==============================] - 0s 956us/step - loss: 0.0948 - accuracy: 0.9653\n",
      "Epoch 207/1500\n",
      "62/62 [==============================] - 0s 980us/step - loss: 0.0941 - accuracy: 0.9674\n",
      "Epoch 208/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1017 - accuracy: 0.9602\n",
      "Epoch 209/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9602\n",
      "Epoch 210/1500\n",
      "62/62 [==============================] - 0s 966us/step - loss: 0.0975 - accuracy: 0.9628\n",
      "Epoch 211/1500\n",
      "62/62 [==============================] - 0s 997us/step - loss: 0.1242 - accuracy: 0.9572\n",
      "Epoch 212/1500\n",
      "62/62 [==============================] - 0s 981us/step - loss: 0.0959 - accuracy: 0.9659\n",
      "Epoch 213/1500\n",
      "62/62 [==============================] - 0s 963us/step - loss: 0.1110 - accuracy: 0.9613\n",
      "Epoch 214/1500\n",
      "62/62 [==============================] - 0s 966us/step - loss: 0.1082 - accuracy: 0.9608\n",
      "Epoch 215/1500\n",
      "62/62 [==============================] - 0s 972us/step - loss: 0.1109 - accuracy: 0.9618\n",
      "Epoch 216/1500\n",
      "62/62 [==============================] - 0s 963us/step - loss: 0.1101 - accuracy: 0.9531\n",
      "Epoch 217/1500\n",
      "62/62 [==============================] - 0s 990us/step - loss: 0.1005 - accuracy: 0.9628\n",
      "Epoch 218/1500\n",
      "62/62 [==============================] - 0s 961us/step - loss: 0.0911 - accuracy: 0.9679\n",
      "Epoch 219/1500\n",
      "62/62 [==============================] - 0s 972us/step - loss: 0.0986 - accuracy: 0.9689\n",
      "Epoch 220/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9557\n",
      "Epoch 221/1500\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9541\n",
      "Epoch 222/1500\n",
      "62/62 [==============================] - 0s 981us/step - loss: 0.1249 - accuracy: 0.9541\n",
      "Epoch 223/1500\n",
      "62/62 [==============================] - 0s 980us/step - loss: 0.1078 - accuracy: 0.9597\n",
      "Epoch 224/1500\n",
      "62/62 [==============================] - 0s 983us/step - loss: 0.1065 - accuracy: 0.9572\n",
      "Epoch 225/1500\n",
      "62/62 [==============================] - 0s 998us/step - loss: 0.0951 - accuracy: 0.9669\n",
      "Epoch 226/1500\n",
      "62/62 [==============================] - 0s 973us/step - loss: 0.0849 - accuracy: 0.9704\n",
      "Epoch 227/1500\n",
      "52/62 [========================>.....] - ETA: 0s - loss: 0.0916 - accuracy: 0.9681Restoring model weights from the end of the best epoch: 197.\n",
      "62/62 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9648\n",
      "Epoch 227: early stopping\n",
      "8/8 [==============================] - 0s 703us/step - loss: 0.9049 - accuracy: 0.6964\n",
      "8/8 [==============================] - 0s 604us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.77 (20/26)\n",
      "Before appending - Cat IDs: 190, Predictions: 190, Actuals: 190, Gender: 190\n",
      "After appending - Cat IDs: 437, Predictions: 437, Actuals: 437, Gender: 437\n",
      "Final Test Results - Loss: 0.9048540592193604, Accuracy: 0.6963562965393066, Precision: 0.4432133395667757, Recall: 0.7206906154274574, F1 Score: 0.44425566343042067\n",
      "Confusion Matrix:\n",
      " [[150  14  25]\n",
      " [  0   1   0]\n",
      " [ 36   0  21]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "097B    14\n",
      "002A    13\n",
      "028A    13\n",
      "111A    13\n",
      "116A    12\n",
      "068A    11\n",
      "036A    11\n",
      "025A    11\n",
      "014B    10\n",
      "005A    10\n",
      "040A    10\n",
      "022A     9\n",
      "051B     9\n",
      "072A     9\n",
      "065A     9\n",
      "015A     9\n",
      "033A     9\n",
      "095A     8\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "117A     7\n",
      "027A     7\n",
      "050A     7\n",
      "031A     7\n",
      "008A     6\n",
      "023A     6\n",
      "037A     6\n",
      "108A     6\n",
      "109A     6\n",
      "021A     5\n",
      "025C     5\n",
      "075A     5\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "035A     4\n",
      "009A     4\n",
      "026A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "003A     4\n",
      "064A     3\n",
      "060A     3\n",
      "056A     3\n",
      "058A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "087A     2\n",
      "093A     2\n",
      "025B     2\n",
      "038A     2\n",
      "018A     2\n",
      "011A     2\n",
      "061A     2\n",
      "043A     1\n",
      "019B     1\n",
      "090A     1\n",
      "110A     1\n",
      "091A     1\n",
      "066A     1\n",
      "088A     1\n",
      "004A     1\n",
      "048A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000B    19\n",
      "001A    14\n",
      "106A    14\n",
      "042A    14\n",
      "051A    12\n",
      "039A    12\n",
      "063A    11\n",
      "071A    10\n",
      "016A    10\n",
      "045A     9\n",
      "094A     8\n",
      "007A     6\n",
      "053A     6\n",
      "052A     4\n",
      "012A     3\n",
      "102A     2\n",
      "096A     1\n",
      "076A     1\n",
      "026C     1\n",
      "115A     1\n",
      "100A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    290\n",
      "M    277\n",
      "F    210\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    60\n",
      "X    58\n",
      "F    42\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 103A, 097B, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 109A, 050...\n",
      "senior    [093A, 097A, 057A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [001A, 071A, 039A, 063A, 007A, 000B, 076A, 096...\n",
      "kitten                                   [042A, 045A, 115A]\n",
      "senior                       [106A, 051A, 016A, 094A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 13, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 3, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '005A' '006A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025A' '025B' '025C' '026A' '026B' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A'\n",
      " '041A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051B' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '060A' '061A' '062A' '064A' '065A'\n",
      " '066A' '067A' '068A' '069A' '070A' '072A' '073A' '074A' '075A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '095A' '097A' '097B' '099A' '101A'\n",
      " '103A' '104A' '105A' '108A' '109A' '110A' '111A' '113A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '007A' '012A' '016A' '024A' '026C' '039A' '042A' '045A'\n",
      " '051A' '052A' '053A' '063A' '071A' '076A' '094A' '096A' '100A' '102A'\n",
      " '106A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '005A' '006A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025A' '025B' '025C' '026A' '026B' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A'\n",
      " '041A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051B' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '060A' '061A' '062A' '064A' '065A'\n",
      " '066A' '067A' '068A' '069A' '070A' '072A' '073A' '074A' '075A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '095A' '097A' '097B' '099A' '101A'\n",
      " '103A' '104A' '105A' '108A' '109A' '110A' '111A' '113A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '007A' '012A' '016A' '024A' '026C' '039A' '042A' '045A'\n",
      " '051A' '052A' '053A' '063A' '071A' '076A' '094A' '096A' '100A' '102A'\n",
      " '106A' '115A']\n",
      "Length of X_train_val:\n",
      "777\n",
      "Length of y_train_val:\n",
      "777\n",
      "Length of groups_train_val:\n",
      "777\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     497\n",
      "kitten    147\n",
      "senior    133\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     91\n",
      "senior    45\n",
      "kitten    24\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     497\n",
      "kitten    147\n",
      "senior    133\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     91\n",
      "senior    45\n",
      "kitten    24\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 994, 1: 588, 2: 532})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.8655 - accuracy: 0.5932\n",
      "Epoch 2/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.6644 - accuracy: 0.6949\n",
      "Epoch 3/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.6093 - accuracy: 0.7228\n",
      "Epoch 4/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.5707 - accuracy: 0.7389\n",
      "Epoch 5/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.5488 - accuracy: 0.7592\n",
      "Epoch 6/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.5197 - accuracy: 0.7687\n",
      "Epoch 7/1500\n",
      "67/67 [==============================] - 0s 935us/step - loss: 0.5182 - accuracy: 0.7640\n",
      "Epoch 8/1500\n",
      "67/67 [==============================] - 0s 931us/step - loss: 0.4825 - accuracy: 0.7862\n",
      "Epoch 9/1500\n",
      "67/67 [==============================] - 0s 980us/step - loss: 0.4617 - accuracy: 0.7999\n",
      "Epoch 10/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4523 - accuracy: 0.8018\n",
      "Epoch 11/1500\n",
      "67/67 [==============================] - 0s 987us/step - loss: 0.4364 - accuracy: 0.7985\n",
      "Epoch 12/1500\n",
      "67/67 [==============================] - 0s 947us/step - loss: 0.3974 - accuracy: 0.8236\n",
      "Epoch 13/1500\n",
      "67/67 [==============================] - 0s 981us/step - loss: 0.4201 - accuracy: 0.8094\n",
      "Epoch 14/1500\n",
      "67/67 [==============================] - 0s 992us/step - loss: 0.4102 - accuracy: 0.8127\n",
      "Epoch 15/1500\n",
      "67/67 [==============================] - 0s 923us/step - loss: 0.4037 - accuracy: 0.8169\n",
      "Epoch 16/1500\n",
      "67/67 [==============================] - 0s 948us/step - loss: 0.3783 - accuracy: 0.8288\n",
      "Epoch 17/1500\n",
      "67/67 [==============================] - 0s 938us/step - loss: 0.3973 - accuracy: 0.8259\n",
      "Epoch 18/1500\n",
      "67/67 [==============================] - 0s 951us/step - loss: 0.3895 - accuracy: 0.8245\n",
      "Epoch 19/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3586 - accuracy: 0.8382\n",
      "Epoch 20/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3745 - accuracy: 0.8307\n",
      "Epoch 21/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3719 - accuracy: 0.8316\n",
      "Epoch 22/1500\n",
      "67/67 [==============================] - 0s 984us/step - loss: 0.3547 - accuracy: 0.8448\n",
      "Epoch 23/1500\n",
      "67/67 [==============================] - 0s 922us/step - loss: 0.3405 - accuracy: 0.8434\n",
      "Epoch 24/1500\n",
      "67/67 [==============================] - 0s 923us/step - loss: 0.3390 - accuracy: 0.8477\n",
      "Epoch 25/1500\n",
      "67/67 [==============================] - 0s 927us/step - loss: 0.3546 - accuracy: 0.8467\n",
      "Epoch 26/1500\n",
      "67/67 [==============================] - 0s 926us/step - loss: 0.3472 - accuracy: 0.8500\n",
      "Epoch 27/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3374 - accuracy: 0.8458\n",
      "Epoch 28/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3322 - accuracy: 0.8548\n",
      "Epoch 29/1500\n",
      "67/67 [==============================] - 0s 985us/step - loss: 0.3259 - accuracy: 0.8496\n",
      "Epoch 30/1500\n",
      "67/67 [==============================] - 0s 921us/step - loss: 0.3182 - accuracy: 0.8614\n",
      "Epoch 31/1500\n",
      "67/67 [==============================] - 0s 954us/step - loss: 0.3073 - accuracy: 0.8609\n",
      "Epoch 32/1500\n",
      "67/67 [==============================] - 0s 960us/step - loss: 0.3199 - accuracy: 0.8581\n",
      "Epoch 33/1500\n",
      "67/67 [==============================] - 0s 954us/step - loss: 0.3011 - accuracy: 0.8633\n",
      "Epoch 34/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.3050 - accuracy: 0.8619\n",
      "Epoch 35/1500\n",
      "67/67 [==============================] - 0s 918us/step - loss: 0.3140 - accuracy: 0.8657\n",
      "Epoch 36/1500\n",
      "67/67 [==============================] - 0s 917us/step - loss: 0.2880 - accuracy: 0.8780\n",
      "Epoch 37/1500\n",
      "67/67 [==============================] - 0s 918us/step - loss: 0.2946 - accuracy: 0.8661\n",
      "Epoch 38/1500\n",
      "67/67 [==============================] - 0s 929us/step - loss: 0.2994 - accuracy: 0.8680\n",
      "Epoch 39/1500\n",
      "67/67 [==============================] - 0s 922us/step - loss: 0.2896 - accuracy: 0.8756\n",
      "Epoch 40/1500\n",
      "67/67 [==============================] - 0s 948us/step - loss: 0.2795 - accuracy: 0.8709\n",
      "Epoch 41/1500\n",
      "67/67 [==============================] - 0s 986us/step - loss: 0.2861 - accuracy: 0.8709\n",
      "Epoch 42/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2924 - accuracy: 0.8685\n",
      "Epoch 43/1500\n",
      "67/67 [==============================] - 0s 958us/step - loss: 0.2700 - accuracy: 0.8789\n",
      "Epoch 44/1500\n",
      "67/67 [==============================] - 0s 944us/step - loss: 0.2823 - accuracy: 0.8694\n",
      "Epoch 45/1500\n",
      "67/67 [==============================] - 0s 917us/step - loss: 0.2726 - accuracy: 0.8827\n",
      "Epoch 46/1500\n",
      "67/67 [==============================] - 0s 954us/step - loss: 0.2682 - accuracy: 0.8836\n",
      "Epoch 47/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2830 - accuracy: 0.8732\n",
      "Epoch 48/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2613 - accuracy: 0.8841\n",
      "Epoch 49/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2538 - accuracy: 0.8940\n",
      "Epoch 50/1500\n",
      "67/67 [==============================] - 0s 929us/step - loss: 0.2577 - accuracy: 0.8903\n",
      "Epoch 51/1500\n",
      "67/67 [==============================] - 0s 986us/step - loss: 0.2460 - accuracy: 0.8945\n",
      "Epoch 52/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.8898\n",
      "Epoch 53/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2542 - accuracy: 0.8912\n",
      "Epoch 54/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2336 - accuracy: 0.9016\n",
      "Epoch 55/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2489 - accuracy: 0.8879\n",
      "Epoch 56/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2450 - accuracy: 0.8992\n",
      "Epoch 57/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2287 - accuracy: 0.8969\n",
      "Epoch 58/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2528 - accuracy: 0.8879\n",
      "Epoch 59/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.8997\n",
      "Epoch 60/1500\n",
      "67/67 [==============================] - 0s 993us/step - loss: 0.2456 - accuracy: 0.8907\n",
      "Epoch 61/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.2334 - accuracy: 0.8964\n",
      "Epoch 62/1500\n",
      "67/67 [==============================] - 0s 976us/step - loss: 0.2400 - accuracy: 0.8945\n",
      "Epoch 63/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2245 - accuracy: 0.9054\n",
      "Epoch 64/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2260 - accuracy: 0.9007\n",
      "Epoch 65/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2469 - accuracy: 0.8879\n",
      "Epoch 66/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2241 - accuracy: 0.9026\n",
      "Epoch 67/1500\n",
      "67/67 [==============================] - 0s 974us/step - loss: 0.2248 - accuracy: 0.8997\n",
      "Epoch 68/1500\n",
      "67/67 [==============================] - 0s 987us/step - loss: 0.2267 - accuracy: 0.9068\n",
      "Epoch 69/1500\n",
      "67/67 [==============================] - 0s 943us/step - loss: 0.2240 - accuracy: 0.9044\n",
      "Epoch 70/1500\n",
      "67/67 [==============================] - 0s 929us/step - loss: 0.2248 - accuracy: 0.9002\n",
      "Epoch 71/1500\n",
      "67/67 [==============================] - 0s 958us/step - loss: 0.2226 - accuracy: 0.9082\n",
      "Epoch 72/1500\n",
      "67/67 [==============================] - 0s 973us/step - loss: 0.2365 - accuracy: 0.8921\n",
      "Epoch 73/1500\n",
      "67/67 [==============================] - 0s 961us/step - loss: 0.2185 - accuracy: 0.9016\n",
      "Epoch 74/1500\n",
      "67/67 [==============================] - 0s 959us/step - loss: 0.2115 - accuracy: 0.9106\n",
      "Epoch 75/1500\n",
      "67/67 [==============================] - 0s 943us/step - loss: 0.2049 - accuracy: 0.9120\n",
      "Epoch 76/1500\n",
      "67/67 [==============================] - 0s 920us/step - loss: 0.2202 - accuracy: 0.9030\n",
      "Epoch 77/1500\n",
      "67/67 [==============================] - 0s 933us/step - loss: 0.2335 - accuracy: 0.8955\n",
      "Epoch 78/1500\n",
      "67/67 [==============================] - 0s 900us/step - loss: 0.2306 - accuracy: 0.9040\n",
      "Epoch 79/1500\n",
      "67/67 [==============================] - 0s 932us/step - loss: 0.2106 - accuracy: 0.9068\n",
      "Epoch 80/1500\n",
      "67/67 [==============================] - 0s 952us/step - loss: 0.2105 - accuracy: 0.9144\n",
      "Epoch 81/1500\n",
      "67/67 [==============================] - 0s 946us/step - loss: 0.2221 - accuracy: 0.9120\n",
      "Epoch 82/1500\n",
      "67/67 [==============================] - 0s 917us/step - loss: 0.1998 - accuracy: 0.9186\n",
      "Epoch 83/1500\n",
      "67/67 [==============================] - 0s 971us/step - loss: 0.2006 - accuracy: 0.9149\n",
      "Epoch 84/1500\n",
      "67/67 [==============================] - 0s 953us/step - loss: 0.2025 - accuracy: 0.9106\n",
      "Epoch 85/1500\n",
      "67/67 [==============================] - 0s 951us/step - loss: 0.1886 - accuracy: 0.9248\n",
      "Epoch 86/1500\n",
      "67/67 [==============================] - 0s 983us/step - loss: 0.1999 - accuracy: 0.9120\n",
      "Epoch 87/1500\n",
      "67/67 [==============================] - 0s 995us/step - loss: 0.1961 - accuracy: 0.9182\n",
      "Epoch 88/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9167\n",
      "Epoch 89/1500\n",
      "67/67 [==============================] - 0s 947us/step - loss: 0.1851 - accuracy: 0.9219\n",
      "Epoch 90/1500\n",
      "67/67 [==============================] - 0s 962us/step - loss: 0.2022 - accuracy: 0.9087\n",
      "Epoch 91/1500\n",
      "67/67 [==============================] - 0s 979us/step - loss: 0.1990 - accuracy: 0.9201\n",
      "Epoch 92/1500\n",
      "67/67 [==============================] - 0s 966us/step - loss: 0.2073 - accuracy: 0.9068\n",
      "Epoch 93/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2163 - accuracy: 0.9073\n",
      "Epoch 94/1500\n",
      "67/67 [==============================] - 0s 935us/step - loss: 0.1969 - accuracy: 0.9101\n",
      "Epoch 95/1500\n",
      "67/67 [==============================] - 0s 940us/step - loss: 0.1752 - accuracy: 0.9219\n",
      "Epoch 96/1500\n",
      "67/67 [==============================] - 0s 986us/step - loss: 0.1843 - accuracy: 0.9333\n",
      "Epoch 97/1500\n",
      "67/67 [==============================] - 0s 966us/step - loss: 0.1844 - accuracy: 0.9196\n",
      "Epoch 98/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9101\n",
      "Epoch 99/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9191\n",
      "Epoch 100/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.9158\n",
      "Epoch 101/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9201\n",
      "Epoch 102/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1932 - accuracy: 0.9158\n",
      "Epoch 103/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9229\n",
      "Epoch 104/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1883 - accuracy: 0.9238\n",
      "Epoch 105/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9111\n",
      "Epoch 106/1500\n",
      "67/67 [==============================] - 0s 999us/step - loss: 0.1646 - accuracy: 0.9376\n",
      "Epoch 107/1500\n",
      "67/67 [==============================] - 0s 975us/step - loss: 0.1635 - accuracy: 0.9281\n",
      "Epoch 108/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9248\n",
      "Epoch 109/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9272\n",
      "Epoch 110/1500\n",
      "67/67 [==============================] - 0s 990us/step - loss: 0.1842 - accuracy: 0.9234\n",
      "Epoch 111/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9281\n",
      "Epoch 112/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9309\n",
      "Epoch 113/1500\n",
      "67/67 [==============================] - 0s 941us/step - loss: 0.1638 - accuracy: 0.9314\n",
      "Epoch 114/1500\n",
      "67/67 [==============================] - 0s 958us/step - loss: 0.1916 - accuracy: 0.9182\n",
      "Epoch 115/1500\n",
      "67/67 [==============================] - 0s 976us/step - loss: 0.1647 - accuracy: 0.9324\n",
      "Epoch 116/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.1756 - accuracy: 0.9186\n",
      "Epoch 117/1500\n",
      "67/67 [==============================] - 0s 984us/step - loss: 0.1656 - accuracy: 0.9328\n",
      "Epoch 118/1500\n",
      "67/67 [==============================] - 0s 936us/step - loss: 0.1816 - accuracy: 0.9224\n",
      "Epoch 119/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1710 - accuracy: 0.9281\n",
      "Epoch 120/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1722 - accuracy: 0.9272\n",
      "Epoch 121/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9328\n",
      "Epoch 122/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9309\n",
      "Epoch 123/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9314\n",
      "Epoch 124/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9324\n",
      "Epoch 125/1500\n",
      "67/67 [==============================] - 0s 980us/step - loss: 0.1627 - accuracy: 0.9342\n",
      "Epoch 126/1500\n",
      "67/67 [==============================] - 0s 994us/step - loss: 0.1559 - accuracy: 0.9314\n",
      "Epoch 127/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9380\n",
      "Epoch 128/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9300\n",
      "Epoch 129/1500\n",
      "67/67 [==============================] - 0s 978us/step - loss: 0.1595 - accuracy: 0.9281\n",
      "Epoch 130/1500\n",
      "67/67 [==============================] - 0s 971us/step - loss: 0.1641 - accuracy: 0.9395\n",
      "Epoch 131/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9428\n",
      "Epoch 132/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9319\n",
      "Epoch 133/1500\n",
      "67/67 [==============================] - 0s 998us/step - loss: 0.1587 - accuracy: 0.9309\n",
      "Epoch 134/1500\n",
      "67/67 [==============================] - 0s 998us/step - loss: 0.1425 - accuracy: 0.9418\n",
      "Epoch 135/1500\n",
      "67/67 [==============================] - 0s 943us/step - loss: 0.1455 - accuracy: 0.9385\n",
      "Epoch 136/1500\n",
      "67/67 [==============================] - 0s 967us/step - loss: 0.1346 - accuracy: 0.9461\n",
      "Epoch 137/1500\n",
      "67/67 [==============================] - 0s 986us/step - loss: 0.1463 - accuracy: 0.9347\n",
      "Epoch 138/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.9253\n",
      "Epoch 139/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9456\n",
      "Epoch 140/1500\n",
      "67/67 [==============================] - 0s 987us/step - loss: 0.1560 - accuracy: 0.9404\n",
      "Epoch 141/1500\n",
      "67/67 [==============================] - 0s 972us/step - loss: 0.1347 - accuracy: 0.9480\n",
      "Epoch 142/1500\n",
      "67/67 [==============================] - 0s 949us/step - loss: 0.1514 - accuracy: 0.9305\n",
      "Epoch 143/1500\n",
      "67/67 [==============================] - 0s 971us/step - loss: 0.1391 - accuracy: 0.9428\n",
      "Epoch 144/1500\n",
      "67/67 [==============================] - 0s 957us/step - loss: 0.1571 - accuracy: 0.9357\n",
      "Epoch 145/1500\n",
      "67/67 [==============================] - 0s 983us/step - loss: 0.1483 - accuracy: 0.9390\n",
      "Epoch 146/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.9371\n",
      "Epoch 147/1500\n",
      "67/67 [==============================] - 0s 995us/step - loss: 0.1516 - accuracy: 0.9399\n",
      "Epoch 148/1500\n",
      "67/67 [==============================] - 0s 990us/step - loss: 0.1485 - accuracy: 0.9423\n",
      "Epoch 149/1500\n",
      "67/67 [==============================] - 0s 955us/step - loss: 0.1309 - accuracy: 0.9432\n",
      "Epoch 150/1500\n",
      "67/67 [==============================] - 0s 934us/step - loss: 0.1317 - accuracy: 0.9437\n",
      "Epoch 151/1500\n",
      "67/67 [==============================] - 0s 956us/step - loss: 0.1394 - accuracy: 0.9465\n",
      "Epoch 152/1500\n",
      "67/67 [==============================] - 0s 973us/step - loss: 0.1386 - accuracy: 0.9470\n",
      "Epoch 153/1500\n",
      "67/67 [==============================] - 0s 976us/step - loss: 0.1300 - accuracy: 0.9484\n",
      "Epoch 154/1500\n",
      "67/67 [==============================] - 0s 981us/step - loss: 0.1338 - accuracy: 0.9428\n",
      "Epoch 155/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9503\n",
      "Epoch 156/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9413\n",
      "Epoch 157/1500\n",
      "67/67 [==============================] - 0s 940us/step - loss: 0.1439 - accuracy: 0.9395\n",
      "Epoch 158/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9418\n",
      "Epoch 159/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9409\n",
      "Epoch 160/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9409\n",
      "Epoch 161/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9437\n",
      "Epoch 162/1500\n",
      "67/67 [==============================] - 0s 951us/step - loss: 0.1228 - accuracy: 0.9527\n",
      "Epoch 163/1500\n",
      "67/67 [==============================] - 0s 972us/step - loss: 0.1296 - accuracy: 0.9475\n",
      "Epoch 164/1500\n",
      "67/67 [==============================] - 0s 976us/step - loss: 0.1365 - accuracy: 0.9395\n",
      "Epoch 165/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9475\n",
      "Epoch 166/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9527\n",
      "Epoch 167/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9489\n",
      "Epoch 168/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9484\n",
      "Epoch 169/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9527\n",
      "Epoch 170/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9518\n",
      "Epoch 171/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1351 - accuracy: 0.9475\n",
      "Epoch 172/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9432\n",
      "Epoch 173/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9442\n",
      "Epoch 174/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9437\n",
      "Epoch 175/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9551\n",
      "Epoch 176/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9551\n",
      "Epoch 177/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9612\n",
      "Epoch 178/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9499\n",
      "Epoch 179/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9470\n",
      "Epoch 180/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.9536\n",
      "Epoch 181/1500\n",
      "67/67 [==============================] - 0s 978us/step - loss: 0.1237 - accuracy: 0.9494\n",
      "Epoch 182/1500\n",
      "67/67 [==============================] - 0s 954us/step - loss: 0.1300 - accuracy: 0.9484\n",
      "Epoch 183/1500\n",
      "67/67 [==============================] - 0s 938us/step - loss: 0.1141 - accuracy: 0.9560\n",
      "Epoch 184/1500\n",
      "67/67 [==============================] - 0s 960us/step - loss: 0.1137 - accuracy: 0.9588\n",
      "Epoch 185/1500\n",
      "67/67 [==============================] - 0s 966us/step - loss: 0.1197 - accuracy: 0.9541\n",
      "Epoch 186/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.1119 - accuracy: 0.9570\n",
      "Epoch 187/1500\n",
      "67/67 [==============================] - 0s 973us/step - loss: 0.1237 - accuracy: 0.9489\n",
      "Epoch 188/1500\n",
      "67/67 [==============================] - 0s 937us/step - loss: 0.1207 - accuracy: 0.9518\n",
      "Epoch 189/1500\n",
      "67/67 [==============================] - 0s 948us/step - loss: 0.1225 - accuracy: 0.9570\n",
      "Epoch 190/1500\n",
      "67/67 [==============================] - 0s 939us/step - loss: 0.1102 - accuracy: 0.9579\n",
      "Epoch 191/1500\n",
      "67/67 [==============================] - 0s 958us/step - loss: 0.1011 - accuracy: 0.9645\n",
      "Epoch 192/1500\n",
      "67/67 [==============================] - 0s 950us/step - loss: 0.1166 - accuracy: 0.9541\n",
      "Epoch 193/1500\n",
      "67/67 [==============================] - 0s 970us/step - loss: 0.1230 - accuracy: 0.9447\n",
      "Epoch 194/1500\n",
      "67/67 [==============================] - 0s 972us/step - loss: 0.1112 - accuracy: 0.9574\n",
      "Epoch 195/1500\n",
      "67/67 [==============================] - 0s 958us/step - loss: 0.1061 - accuracy: 0.9565\n",
      "Epoch 196/1500\n",
      "67/67 [==============================] - 0s 979us/step - loss: 0.1054 - accuracy: 0.9607\n",
      "Epoch 197/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9603\n",
      "Epoch 198/1500\n",
      "67/67 [==============================] - 0s 972us/step - loss: 0.1170 - accuracy: 0.9574\n",
      "Epoch 199/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9503\n",
      "Epoch 200/1500\n",
      "67/67 [==============================] - 0s 993us/step - loss: 0.1115 - accuracy: 0.9574\n",
      "Epoch 201/1500\n",
      "67/67 [==============================] - 0s 992us/step - loss: 0.1228 - accuracy: 0.9508\n",
      "Epoch 202/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9527\n",
      "Epoch 203/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9617\n",
      "Epoch 204/1500\n",
      "67/67 [==============================] - 0s 970us/step - loss: 0.1075 - accuracy: 0.9555\n",
      "Epoch 205/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.1163 - accuracy: 0.9565\n",
      "Epoch 206/1500\n",
      "67/67 [==============================] - 0s 950us/step - loss: 0.1163 - accuracy: 0.9527\n",
      "Epoch 207/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.1287 - accuracy: 0.9451\n",
      "Epoch 208/1500\n",
      "67/67 [==============================] - 0s 976us/step - loss: 0.1167 - accuracy: 0.9518\n",
      "Epoch 209/1500\n",
      "67/67 [==============================] - 0s 936us/step - loss: 0.1149 - accuracy: 0.9588\n",
      "Epoch 210/1500\n",
      "67/67 [==============================] - 0s 978us/step - loss: 0.1245 - accuracy: 0.9546\n",
      "Epoch 211/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.1228 - accuracy: 0.9527\n",
      "Epoch 212/1500\n",
      "67/67 [==============================] - 0s 935us/step - loss: 0.1177 - accuracy: 0.9513\n",
      "Epoch 213/1500\n",
      "67/67 [==============================] - 0s 955us/step - loss: 0.1256 - accuracy: 0.9518\n",
      "Epoch 214/1500\n",
      "67/67 [==============================] - 0s 948us/step - loss: 0.1223 - accuracy: 0.9541\n",
      "Epoch 215/1500\n",
      "67/67 [==============================] - 0s 944us/step - loss: 0.1061 - accuracy: 0.9598\n",
      "Epoch 216/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.1115 - accuracy: 0.9527\n",
      "Epoch 217/1500\n",
      "67/67 [==============================] - 0s 975us/step - loss: 0.0974 - accuracy: 0.9588\n",
      "Epoch 218/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.1150 - accuracy: 0.9555\n",
      "Epoch 219/1500\n",
      "67/67 [==============================] - 0s 951us/step - loss: 0.0968 - accuracy: 0.9674\n",
      "Epoch 220/1500\n",
      "67/67 [==============================] - 0s 978us/step - loss: 0.1149 - accuracy: 0.9555\n",
      "Epoch 221/1500\n",
      "67/67 [==============================] - 0s 953us/step - loss: 0.1099 - accuracy: 0.9522\n",
      "Epoch 222/1500\n",
      "67/67 [==============================] - 0s 972us/step - loss: 0.1038 - accuracy: 0.9622\n",
      "Epoch 223/1500\n",
      "67/67 [==============================] - 0s 938us/step - loss: 0.1082 - accuracy: 0.9607\n",
      "Epoch 224/1500\n",
      "67/67 [==============================] - 0s 991us/step - loss: 0.0986 - accuracy: 0.9598\n",
      "Epoch 225/1500\n",
      "67/67 [==============================] - 0s 975us/step - loss: 0.1193 - accuracy: 0.9480\n",
      "Epoch 226/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.0986 - accuracy: 0.9631\n",
      "Epoch 227/1500\n",
      "67/67 [==============================] - 0s 962us/step - loss: 0.0947 - accuracy: 0.9636\n",
      "Epoch 228/1500\n",
      "67/67 [==============================] - 0s 985us/step - loss: 0.1145 - accuracy: 0.9518\n",
      "Epoch 229/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9626\n",
      "Epoch 230/1500\n",
      "67/67 [==============================] - 0s 998us/step - loss: 0.1081 - accuracy: 0.9617\n",
      "Epoch 231/1500\n",
      "67/67 [==============================] - 0s 975us/step - loss: 0.0908 - accuracy: 0.9650\n",
      "Epoch 232/1500\n",
      "67/67 [==============================] - 0s 960us/step - loss: 0.1061 - accuracy: 0.9560\n",
      "Epoch 233/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9603\n",
      "Epoch 234/1500\n",
      "67/67 [==============================] - 0s 937us/step - loss: 0.1099 - accuracy: 0.9546\n",
      "Epoch 235/1500\n",
      "67/67 [==============================] - 0s 972us/step - loss: 0.1128 - accuracy: 0.9532\n",
      "Epoch 236/1500\n",
      "67/67 [==============================] - 0s 981us/step - loss: 0.0985 - accuracy: 0.9640\n",
      "Epoch 237/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.0975 - accuracy: 0.9598\n",
      "Epoch 238/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.1006 - accuracy: 0.9626\n",
      "Epoch 239/1500\n",
      "67/67 [==============================] - 0s 954us/step - loss: 0.1025 - accuracy: 0.9622\n",
      "Epoch 240/1500\n",
      "67/67 [==============================] - 0s 949us/step - loss: 0.0975 - accuracy: 0.9650\n",
      "Epoch 241/1500\n",
      "67/67 [==============================] - 0s 934us/step - loss: 0.1077 - accuracy: 0.9565\n",
      "Epoch 242/1500\n",
      "67/67 [==============================] - 0s 935us/step - loss: 0.0918 - accuracy: 0.9674\n",
      "Epoch 243/1500\n",
      "67/67 [==============================] - 0s 948us/step - loss: 0.0964 - accuracy: 0.9631\n",
      "Epoch 244/1500\n",
      "67/67 [==============================] - 0s 959us/step - loss: 0.0977 - accuracy: 0.9593\n",
      "Epoch 245/1500\n",
      "67/67 [==============================] - 0s 995us/step - loss: 0.1106 - accuracy: 0.9565\n",
      "Epoch 246/1500\n",
      "67/67 [==============================] - 0s 983us/step - loss: 0.1031 - accuracy: 0.9555\n",
      "Epoch 247/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9645\n",
      "Epoch 248/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9622\n",
      "Epoch 249/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9593\n",
      "Epoch 250/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9655\n",
      "Epoch 251/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9570\n",
      "Epoch 252/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9527\n",
      "Epoch 253/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9570\n",
      "Epoch 254/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9636\n",
      "Epoch 255/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9579\n",
      "Epoch 256/1500\n",
      "67/67 [==============================] - 0s 993us/step - loss: 0.0964 - accuracy: 0.9584\n",
      "Epoch 257/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9607\n",
      "Epoch 258/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.9683\n",
      "Epoch 259/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.9669\n",
      "Epoch 260/1500\n",
      "67/67 [==============================] - 0s 952us/step - loss: 0.1005 - accuracy: 0.9588\n",
      "Epoch 261/1500\n",
      "67/67 [==============================] - 0s 984us/step - loss: 0.0930 - accuracy: 0.9626\n",
      "Epoch 262/1500\n",
      "67/67 [==============================] - 0s 959us/step - loss: 0.0818 - accuracy: 0.9688\n",
      "Epoch 263/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.0965 - accuracy: 0.9593\n",
      "Epoch 264/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9579\n",
      "Epoch 265/1500\n",
      "67/67 [==============================] - 0s 951us/step - loss: 0.0893 - accuracy: 0.9655\n",
      "Epoch 266/1500\n",
      "67/67 [==============================] - 0s 990us/step - loss: 0.0953 - accuracy: 0.9574\n",
      "Epoch 267/1500\n",
      "67/67 [==============================] - 0s 963us/step - loss: 0.1021 - accuracy: 0.9598\n",
      "Epoch 268/1500\n",
      "67/67 [==============================] - 0s 970us/step - loss: 0.0932 - accuracy: 0.9659\n",
      "Epoch 269/1500\n",
      "67/67 [==============================] - 0s 975us/step - loss: 0.0961 - accuracy: 0.9683\n",
      "Epoch 270/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9560\n",
      "Epoch 271/1500\n",
      "67/67 [==============================] - 0s 992us/step - loss: 0.1007 - accuracy: 0.9612\n",
      "Epoch 272/1500\n",
      "67/67 [==============================] - 0s 956us/step - loss: 0.0883 - accuracy: 0.9678\n",
      "Epoch 273/1500\n",
      "67/67 [==============================] - 0s 972us/step - loss: 0.0852 - accuracy: 0.9678\n",
      "Epoch 274/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9664\n",
      "Epoch 275/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9640\n",
      "Epoch 276/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.0848 - accuracy: 0.9640\n",
      "Epoch 277/1500\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.0835 - accuracy: 0.9683\n",
      "Epoch 278/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9650\n",
      "Epoch 279/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9588\n",
      "Epoch 280/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.0929 - accuracy: 0.9626\n",
      "Epoch 281/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9659\n",
      "Epoch 282/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.1023 - accuracy: 0.9598\n",
      "Epoch 283/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.0930 - accuracy: 0.9626\n",
      "Epoch 284/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.0918 - accuracy: 0.9683\n",
      "Epoch 285/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9574\n",
      "Epoch 286/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.9716\n",
      "Epoch 287/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.9650\n",
      "Epoch 288/1500\n",
      "45/67 [===================>..........] - ETA: 0s - loss: 0.0968 - accuracy: 0.9576Restoring model weights from the end of the best epoch: 258.\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9579\n",
      "Epoch 288: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 1.6138 - accuracy: 0.6938\n",
      "5/5 [==============================] - 0s 882us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.82 (18/22)\n",
      "Before appending - Cat IDs: 437, Predictions: 437, Actuals: 437, Gender: 437\n",
      "After appending - Cat IDs: 597, Predictions: 597, Actuals: 597, Gender: 597\n",
      "Final Test Results - Loss: 1.6137901544570923, Accuracy: 0.6937500238418579, Precision: 0.680293501048218, Recall: 0.6904660154660154, F1 Score: 0.6748310717405971\n",
      "Confusion Matrix:\n",
      " [[75  2 14]\n",
      " [ 1 23  0]\n",
      " [30  2 13]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "106A    14\n",
      "097B    14\n",
      "001A    14\n",
      "059A    14\n",
      "042A    14\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "051A    12\n",
      "036A    11\n",
      "063A    11\n",
      "068A    11\n",
      "025A    11\n",
      "040A    10\n",
      "016A    10\n",
      "014B    10\n",
      "071A    10\n",
      "072A     9\n",
      "033A     9\n",
      "022A     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "010A     8\n",
      "031A     7\n",
      "050A     7\n",
      "099A     7\n",
      "117A     7\n",
      "037A     6\n",
      "007A     6\n",
      "008A     6\n",
      "109A     6\n",
      "053A     6\n",
      "108A     6\n",
      "023A     6\n",
      "075A     5\n",
      "023B     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "025C     5\n",
      "070A     5\n",
      "035A     4\n",
      "105A     4\n",
      "009A     4\n",
      "052A     4\n",
      "003A     4\n",
      "113A     3\n",
      "014A     3\n",
      "060A     3\n",
      "012A     3\n",
      "058A     3\n",
      "102A     2\n",
      "025B     2\n",
      "061A     2\n",
      "054A     2\n",
      "032A     2\n",
      "018A     2\n",
      "093A     2\n",
      "069A     2\n",
      "041A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "115A     1\n",
      "076A     1\n",
      "026C     1\n",
      "019B     1\n",
      "088A     1\n",
      "004A     1\n",
      "066A     1\n",
      "096A     1\n",
      "049A     1\n",
      "092A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "020A    23\n",
      "101A    15\n",
      "111A    13\n",
      "028A    13\n",
      "005A    10\n",
      "065A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "026A     4\n",
      "062A     4\n",
      "104A     4\n",
      "064A     3\n",
      "056A     3\n",
      "006A     3\n",
      "087A     2\n",
      "038A     2\n",
      "011A     2\n",
      "043A     1\n",
      "073A     1\n",
      "048A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    322\n",
      "M    275\n",
      "F    156\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    96\n",
      "M    62\n",
      "X    26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 001A, 103A, 071A, 097B, 019A, 074...\n",
      "kitten    [044A, 014B, 040A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 028A, 020A, 062A, 101A, 095A, 005...\n",
      "kitten                                   [111A, 043A, 048A]\n",
      "senior                             [104A, 051B, 056A, 011A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 58, 'kitten': 13, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 16, 'kitten': 3, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '004A' '007A' '008A' '009A' '010A'\n",
      " '012A' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '022A'\n",
      " '023A' '023B' '024A' '025A' '025B' '025C' '026B' '026C' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '039A' '040A' '041A' '042A'\n",
      " '044A' '045A' '046A' '047A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '063A' '066A' '067A' '068A'\n",
      " '069A' '070A' '071A' '072A' '074A' '075A' '076A' '088A' '090A' '091A'\n",
      " '092A' '093A' '094A' '096A' '097A' '097B' '099A' '100A' '102A' '103A'\n",
      " '105A' '106A' '108A' '109A' '110A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '005A' '006A' '011A' '013B' '020A' '026A' '027A' '028A' '038A'\n",
      " '043A' '048A' '051B' '056A' '062A' '064A' '065A' '073A' '087A' '095A'\n",
      " '101A' '104A' '111A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'068A'}\n",
      "Moved to Test Set:\n",
      "{'068A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '007A' '008A' '009A'\n",
      " '010A' '012A' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '021A'\n",
      " '022A' '023A' '023B' '024A' '025A' '025B' '025C' '026B' '026C' '029A'\n",
      " '031A' '032A' '033A' '034A' '035A' '036A' '037A' '039A' '040A' '041A'\n",
      " '042A' '044A' '045A' '046A' '047A' '049A' '050A' '051A' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '061A' '063A' '066A' '067A'\n",
      " '069A' '070A' '071A' '072A' '074A' '075A' '076A' '088A' '090A' '091A'\n",
      " '092A' '093A' '094A' '096A' '097A' '097B' '099A' '100A' '102A' '103A'\n",
      " '105A' '106A' '108A' '109A' '110A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['005A' '006A' '011A' '013B' '020A' '026A' '027A' '028A' '038A' '043A'\n",
      " '048A' '051B' '056A' '062A' '064A' '065A' '068A' '073A' '087A' '095A'\n",
      " '101A' '104A' '111A']\n",
      "Length of X_train_val:\n",
      "781\n",
      "Length of y_train_val:\n",
      "781\n",
      "Length of groups_train_val:\n",
      "781\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     437\n",
      "senior    160\n",
      "kitten    156\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     151\n",
      "senior     18\n",
      "kitten     15\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     465\n",
      "senior    160\n",
      "kitten    156\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     123\n",
      "senior     18\n",
      "kitten     15\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 930, 2: 640, 1: 624})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.9418 - accuracy: 0.5752\n",
      "Epoch 2/1500\n",
      "69/69 [==============================] - 0s 961us/step - loss: 0.7159 - accuracy: 0.6873\n",
      "Epoch 3/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.6518 - accuracy: 0.7170\n",
      "Epoch 4/1500\n",
      "69/69 [==============================] - 0s 936us/step - loss: 0.6175 - accuracy: 0.7375\n",
      "Epoch 5/1500\n",
      "69/69 [==============================] - 0s 959us/step - loss: 0.5758 - accuracy: 0.7562\n",
      "Epoch 6/1500\n",
      "69/69 [==============================] - 0s 929us/step - loss: 0.5688 - accuracy: 0.7671\n",
      "Epoch 7/1500\n",
      "69/69 [==============================] - 0s 908us/step - loss: 0.5478 - accuracy: 0.7648\n",
      "Epoch 8/1500\n",
      "69/69 [==============================] - 0s 926us/step - loss: 0.5020 - accuracy: 0.7835\n",
      "Epoch 9/1500\n",
      "69/69 [==============================] - 0s 898us/step - loss: 0.5167 - accuracy: 0.7881\n",
      "Epoch 10/1500\n",
      "69/69 [==============================] - 0s 909us/step - loss: 0.4952 - accuracy: 0.7890\n",
      "Epoch 11/1500\n",
      "69/69 [==============================] - 0s 949us/step - loss: 0.4812 - accuracy: 0.7954\n",
      "Epoch 12/1500\n",
      "69/69 [==============================] - 0s 921us/step - loss: 0.4666 - accuracy: 0.7903\n",
      "Epoch 13/1500\n",
      "69/69 [==============================] - 0s 913us/step - loss: 0.4461 - accuracy: 0.8136\n",
      "Epoch 14/1500\n",
      "69/69 [==============================] - 0s 908us/step - loss: 0.4332 - accuracy: 0.8163\n",
      "Epoch 15/1500\n",
      "69/69 [==============================] - 0s 935us/step - loss: 0.4335 - accuracy: 0.8118\n",
      "Epoch 16/1500\n",
      "69/69 [==============================] - 0s 890us/step - loss: 0.4486 - accuracy: 0.8145\n",
      "Epoch 17/1500\n",
      "69/69 [==============================] - 0s 966us/step - loss: 0.3982 - accuracy: 0.8277\n",
      "Epoch 18/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.4284 - accuracy: 0.8090\n",
      "Epoch 19/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.4230 - accuracy: 0.8209\n",
      "Epoch 20/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3930 - accuracy: 0.8268\n",
      "Epoch 21/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3896 - accuracy: 0.8254\n",
      "Epoch 22/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3670 - accuracy: 0.8373\n",
      "Epoch 23/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3673 - accuracy: 0.8418\n",
      "Epoch 24/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3602 - accuracy: 0.8510\n",
      "Epoch 25/1500\n",
      "69/69 [==============================] - 0s 988us/step - loss: 0.3566 - accuracy: 0.8469\n",
      "Epoch 26/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.8537\n",
      "Epoch 27/1500\n",
      "69/69 [==============================] - 0s 969us/step - loss: 0.3496 - accuracy: 0.8473\n",
      "Epoch 28/1500\n",
      "69/69 [==============================] - 0s 953us/step - loss: 0.3313 - accuracy: 0.8605\n",
      "Epoch 29/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3330 - accuracy: 0.8541\n",
      "Epoch 30/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3373 - accuracy: 0.8532\n",
      "Epoch 31/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3232 - accuracy: 0.8582\n",
      "Epoch 32/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3086 - accuracy: 0.8637\n",
      "Epoch 33/1500\n",
      "69/69 [==============================] - 0s 944us/step - loss: 0.3392 - accuracy: 0.8528\n",
      "Epoch 34/1500\n",
      "69/69 [==============================] - 0s 945us/step - loss: 0.3135 - accuracy: 0.8596\n",
      "Epoch 35/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2936 - accuracy: 0.8706\n",
      "Epoch 36/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3080 - accuracy: 0.8665\n",
      "Epoch 37/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.3257 - accuracy: 0.8683\n",
      "Epoch 38/1500\n",
      "69/69 [==============================] - 0s 991us/step - loss: 0.3210 - accuracy: 0.8665\n",
      "Epoch 39/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3069 - accuracy: 0.8778\n",
      "Epoch 40/1500\n",
      "69/69 [==============================] - 0s 999us/step - loss: 0.2965 - accuracy: 0.8719\n",
      "Epoch 41/1500\n",
      "69/69 [==============================] - 0s 946us/step - loss: 0.2735 - accuracy: 0.8870\n",
      "Epoch 42/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3000 - accuracy: 0.8674\n",
      "Epoch 43/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2675 - accuracy: 0.8810\n",
      "Epoch 44/1500\n",
      "69/69 [==============================] - 0s 912us/step - loss: 0.2727 - accuracy: 0.8861\n",
      "Epoch 45/1500\n",
      "69/69 [==============================] - 0s 912us/step - loss: 0.2755 - accuracy: 0.8842\n",
      "Epoch 46/1500\n",
      "69/69 [==============================] - 0s 930us/step - loss: 0.2660 - accuracy: 0.8897\n",
      "Epoch 47/1500\n",
      "69/69 [==============================] - 0s 950us/step - loss: 0.2781 - accuracy: 0.8833\n",
      "Epoch 48/1500\n",
      "69/69 [==============================] - 0s 908us/step - loss: 0.2673 - accuracy: 0.8897\n",
      "Epoch 49/1500\n",
      "69/69 [==============================] - 0s 923us/step - loss: 0.2412 - accuracy: 0.9002\n",
      "Epoch 50/1500\n",
      "69/69 [==============================] - 0s 943us/step - loss: 0.2505 - accuracy: 0.8979\n",
      "Epoch 51/1500\n",
      "69/69 [==============================] - 0s 908us/step - loss: 0.2509 - accuracy: 0.8974\n",
      "Epoch 52/1500\n",
      "69/69 [==============================] - 0s 968us/step - loss: 0.2443 - accuracy: 0.9002\n",
      "Epoch 53/1500\n",
      "69/69 [==============================] - 0s 935us/step - loss: 0.2490 - accuracy: 0.8933\n",
      "Epoch 54/1500\n",
      "69/69 [==============================] - 0s 931us/step - loss: 0.2517 - accuracy: 0.8911\n",
      "Epoch 55/1500\n",
      "69/69 [==============================] - 0s 921us/step - loss: 0.2569 - accuracy: 0.8947\n",
      "Epoch 56/1500\n",
      "69/69 [==============================] - 0s 929us/step - loss: 0.2363 - accuracy: 0.9047\n",
      "Epoch 57/1500\n",
      "69/69 [==============================] - 0s 930us/step - loss: 0.2215 - accuracy: 0.9084\n",
      "Epoch 58/1500\n",
      "69/69 [==============================] - 0s 925us/step - loss: 0.2511 - accuracy: 0.8979\n",
      "Epoch 59/1500\n",
      "69/69 [==============================] - 0s 966us/step - loss: 0.2416 - accuracy: 0.9061\n",
      "Epoch 60/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2431 - accuracy: 0.9034\n",
      "Epoch 61/1500\n",
      "69/69 [==============================] - 0s 937us/step - loss: 0.2102 - accuracy: 0.9116\n",
      "Epoch 62/1500\n",
      "69/69 [==============================] - 0s 934us/step - loss: 0.2269 - accuracy: 0.9043\n",
      "Epoch 63/1500\n",
      "69/69 [==============================] - 0s 917us/step - loss: 0.2434 - accuracy: 0.8984\n",
      "Epoch 64/1500\n",
      "69/69 [==============================] - 0s 942us/step - loss: 0.2333 - accuracy: 0.9015\n",
      "Epoch 65/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2235 - accuracy: 0.9079\n",
      "Epoch 66/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2229 - accuracy: 0.9079\n",
      "Epoch 67/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2163 - accuracy: 0.9047\n",
      "Epoch 68/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9139\n",
      "Epoch 69/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2155 - accuracy: 0.9129\n",
      "Epoch 70/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2253 - accuracy: 0.9120\n",
      "Epoch 71/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2156 - accuracy: 0.9088\n",
      "Epoch 72/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9170\n",
      "Epoch 73/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2267 - accuracy: 0.9029\n",
      "Epoch 74/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2243 - accuracy: 0.9015\n",
      "Epoch 75/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9198\n",
      "Epoch 76/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2086 - accuracy: 0.9125\n",
      "Epoch 77/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9335\n",
      "Epoch 78/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1988 - accuracy: 0.9239\n",
      "Epoch 79/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9280\n",
      "Epoch 80/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1869 - accuracy: 0.9243\n",
      "Epoch 81/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2005 - accuracy: 0.9221\n",
      "Epoch 82/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.9257\n",
      "Epoch 83/1500\n",
      "69/69 [==============================] - 0s 983us/step - loss: 0.1991 - accuracy: 0.9189\n",
      "Epoch 84/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9248\n",
      "Epoch 85/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.9298\n",
      "Epoch 86/1500\n",
      "69/69 [==============================] - 0s 991us/step - loss: 0.1889 - accuracy: 0.9211\n",
      "Epoch 87/1500\n",
      "69/69 [==============================] - 0s 950us/step - loss: 0.1861 - accuracy: 0.9198\n",
      "Epoch 88/1500\n",
      "69/69 [==============================] - 0s 957us/step - loss: 0.1876 - accuracy: 0.9239\n",
      "Epoch 89/1500\n",
      "69/69 [==============================] - 0s 938us/step - loss: 0.1875 - accuracy: 0.9271\n",
      "Epoch 90/1500\n",
      "69/69 [==============================] - 0s 956us/step - loss: 0.1901 - accuracy: 0.9216\n",
      "Epoch 91/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9230\n",
      "Epoch 92/1500\n",
      "69/69 [==============================] - 0s 942us/step - loss: 0.1715 - accuracy: 0.9339\n",
      "Epoch 93/1500\n",
      "69/69 [==============================] - 0s 951us/step - loss: 0.1846 - accuracy: 0.9221\n",
      "Epoch 94/1500\n",
      "69/69 [==============================] - 0s 933us/step - loss: 0.1841 - accuracy: 0.9271\n",
      "Epoch 95/1500\n",
      "69/69 [==============================] - 0s 946us/step - loss: 0.1752 - accuracy: 0.9321\n",
      "Epoch 96/1500\n",
      "69/69 [==============================] - 0s 929us/step - loss: 0.1752 - accuracy: 0.9325\n",
      "Epoch 97/1500\n",
      "69/69 [==============================] - 0s 936us/step - loss: 0.1627 - accuracy: 0.9398\n",
      "Epoch 98/1500\n",
      "69/69 [==============================] - 0s 951us/step - loss: 0.1663 - accuracy: 0.9303\n",
      "Epoch 99/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.1579 - accuracy: 0.9407\n",
      "Epoch 100/1500\n",
      "69/69 [==============================] - 0s 928us/step - loss: 0.1745 - accuracy: 0.9271\n",
      "Epoch 101/1500\n",
      "69/69 [==============================] - 0s 946us/step - loss: 0.1664 - accuracy: 0.9339\n",
      "Epoch 102/1500\n",
      "69/69 [==============================] - 0s 956us/step - loss: 0.1941 - accuracy: 0.9248\n",
      "Epoch 103/1500\n",
      "69/69 [==============================] - 0s 909us/step - loss: 0.1807 - accuracy: 0.9275\n",
      "Epoch 104/1500\n",
      "69/69 [==============================] - 0s 941us/step - loss: 0.1495 - accuracy: 0.9407\n",
      "Epoch 105/1500\n",
      "69/69 [==============================] - 0s 912us/step - loss: 0.1636 - accuracy: 0.9335\n",
      "Epoch 106/1500\n",
      "69/69 [==============================] - 0s 928us/step - loss: 0.1735 - accuracy: 0.9298\n",
      "Epoch 107/1500\n",
      "69/69 [==============================] - 0s 959us/step - loss: 0.1556 - accuracy: 0.9366\n",
      "Epoch 108/1500\n",
      "69/69 [==============================] - 0s 939us/step - loss: 0.1594 - accuracy: 0.9380\n",
      "Epoch 109/1500\n",
      "69/69 [==============================] - 0s 936us/step - loss: 0.1612 - accuracy: 0.9380\n",
      "Epoch 110/1500\n",
      "69/69 [==============================] - 0s 925us/step - loss: 0.1531 - accuracy: 0.9339\n",
      "Epoch 111/1500\n",
      "69/69 [==============================] - 0s 931us/step - loss: 0.1514 - accuracy: 0.9430\n",
      "Epoch 112/1500\n",
      "69/69 [==============================] - 0s 976us/step - loss: 0.1591 - accuracy: 0.9366\n",
      "Epoch 113/1500\n",
      "69/69 [==============================] - 0s 943us/step - loss: 0.1659 - accuracy: 0.9339\n",
      "Epoch 114/1500\n",
      "69/69 [==============================] - 0s 931us/step - loss: 0.1745 - accuracy: 0.9280\n",
      "Epoch 115/1500\n",
      "69/69 [==============================] - 0s 944us/step - loss: 0.1553 - accuracy: 0.9389\n",
      "Epoch 116/1500\n",
      "69/69 [==============================] - 0s 966us/step - loss: 0.1578 - accuracy: 0.9439\n",
      "Epoch 117/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9366\n",
      "Epoch 118/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9398\n",
      "Epoch 119/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9353\n",
      "Epoch 120/1500\n",
      "69/69 [==============================] - 0s 942us/step - loss: 0.1591 - accuracy: 0.9385\n",
      "Epoch 121/1500\n",
      "69/69 [==============================] - 0s 942us/step - loss: 0.1494 - accuracy: 0.9417\n",
      "Epoch 122/1500\n",
      "69/69 [==============================] - 0s 934us/step - loss: 0.1554 - accuracy: 0.9453\n",
      "Epoch 123/1500\n",
      "69/69 [==============================] - 0s 939us/step - loss: 0.1440 - accuracy: 0.9398\n",
      "Epoch 124/1500\n",
      "69/69 [==============================] - 0s 960us/step - loss: 0.1463 - accuracy: 0.9417\n",
      "Epoch 125/1500\n",
      "69/69 [==============================] - 0s 951us/step - loss: 0.1433 - accuracy: 0.9444\n",
      "Epoch 126/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1602 - accuracy: 0.9307\n",
      "Epoch 127/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1568 - accuracy: 0.9394\n",
      "Epoch 128/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9371\n",
      "Epoch 129/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9467\n",
      "Epoch 130/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9453\n",
      "Epoch 131/1500\n",
      "69/69 [==============================] - 0s 968us/step - loss: 0.1402 - accuracy: 0.9444\n",
      "Epoch 132/1500\n",
      "69/69 [==============================] - 0s 993us/step - loss: 0.1579 - accuracy: 0.9353\n",
      "Epoch 133/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9412\n",
      "Epoch 134/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9467\n",
      "Epoch 135/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.9439\n",
      "Epoch 136/1500\n",
      "69/69 [==============================] - 0s 977us/step - loss: 0.1424 - accuracy: 0.9435\n",
      "Epoch 137/1500\n",
      "69/69 [==============================] - 0s 953us/step - loss: 0.1404 - accuracy: 0.9394\n",
      "Epoch 138/1500\n",
      "69/69 [==============================] - 0s 976us/step - loss: 0.1240 - accuracy: 0.9526\n",
      "Epoch 139/1500\n",
      "69/69 [==============================] - 0s 946us/step - loss: 0.1313 - accuracy: 0.9462\n",
      "Epoch 140/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9494\n",
      "Epoch 141/1500\n",
      "69/69 [==============================] - 0s 976us/step - loss: 0.1275 - accuracy: 0.9544\n",
      "Epoch 142/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9467\n",
      "Epoch 143/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1404 - accuracy: 0.9403\n",
      "Epoch 144/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9412\n",
      "Epoch 145/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9535\n",
      "Epoch 146/1500\n",
      "69/69 [==============================] - 0s 939us/step - loss: 0.1361 - accuracy: 0.9435\n",
      "Epoch 147/1500\n",
      "69/69 [==============================] - 0s 959us/step - loss: 0.1080 - accuracy: 0.9558\n",
      "Epoch 148/1500\n",
      "69/69 [==============================] - 0s 945us/step - loss: 0.1442 - accuracy: 0.9430\n",
      "Epoch 149/1500\n",
      "69/69 [==============================] - 0s 928us/step - loss: 0.1370 - accuracy: 0.9471\n",
      "Epoch 150/1500\n",
      "69/69 [==============================] - 0s 960us/step - loss: 0.1383 - accuracy: 0.9458\n",
      "Epoch 151/1500\n",
      "69/69 [==============================] - 0s 962us/step - loss: 0.1389 - accuracy: 0.9467\n",
      "Epoch 152/1500\n",
      "69/69 [==============================] - 0s 948us/step - loss: 0.1298 - accuracy: 0.9499\n",
      "Epoch 153/1500\n",
      "69/69 [==============================] - 0s 935us/step - loss: 0.1144 - accuracy: 0.9599\n",
      "Epoch 154/1500\n",
      "69/69 [==============================] - 0s 949us/step - loss: 0.1246 - accuracy: 0.9512\n",
      "Epoch 155/1500\n",
      "69/69 [==============================] - 0s 942us/step - loss: 0.1130 - accuracy: 0.9517\n",
      "Epoch 156/1500\n",
      "69/69 [==============================] - 0s 986us/step - loss: 0.1231 - accuracy: 0.9499\n",
      "Epoch 157/1500\n",
      "69/69 [==============================] - 0s 941us/step - loss: 0.1411 - accuracy: 0.9412\n",
      "Epoch 158/1500\n",
      "69/69 [==============================] - 0s 943us/step - loss: 0.1438 - accuracy: 0.9439\n",
      "Epoch 159/1500\n",
      "69/69 [==============================] - 0s 943us/step - loss: 0.1274 - accuracy: 0.9503\n",
      "Epoch 160/1500\n",
      "69/69 [==============================] - 0s 928us/step - loss: 0.1136 - accuracy: 0.9622\n",
      "Epoch 161/1500\n",
      "69/69 [==============================] - 0s 921us/step - loss: 0.1083 - accuracy: 0.9567\n",
      "Epoch 162/1500\n",
      "69/69 [==============================] - 0s 944us/step - loss: 0.1270 - accuracy: 0.9508\n",
      "Epoch 163/1500\n",
      "69/69 [==============================] - 0s 936us/step - loss: 0.1111 - accuracy: 0.9581\n",
      "Epoch 164/1500\n",
      "69/69 [==============================] - 0s 942us/step - loss: 0.1184 - accuracy: 0.9544\n",
      "Epoch 165/1500\n",
      "69/69 [==============================] - 0s 931us/step - loss: 0.1340 - accuracy: 0.9439\n",
      "Epoch 166/1500\n",
      "69/69 [==============================] - 0s 948us/step - loss: 0.1065 - accuracy: 0.9558\n",
      "Epoch 167/1500\n",
      "69/69 [==============================] - 0s 937us/step - loss: 0.1204 - accuracy: 0.9544\n",
      "Epoch 168/1500\n",
      "69/69 [==============================] - 0s 921us/step - loss: 0.1221 - accuracy: 0.9485\n",
      "Epoch 169/1500\n",
      "69/69 [==============================] - 0s 943us/step - loss: 0.1073 - accuracy: 0.9558\n",
      "Epoch 170/1500\n",
      "69/69 [==============================] - 0s 957us/step - loss: 0.1131 - accuracy: 0.9603\n",
      "Epoch 171/1500\n",
      "69/69 [==============================] - 0s 956us/step - loss: 0.1102 - accuracy: 0.9599\n",
      "Epoch 172/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.1199 - accuracy: 0.9531\n",
      "Epoch 173/1500\n",
      "69/69 [==============================] - 0s 946us/step - loss: 0.1133 - accuracy: 0.9576\n",
      "Epoch 174/1500\n",
      "69/69 [==============================] - 0s 948us/step - loss: 0.1116 - accuracy: 0.9562\n",
      "Epoch 175/1500\n",
      "69/69 [==============================] - 0s 949us/step - loss: 0.1177 - accuracy: 0.9535\n",
      "Epoch 176/1500\n",
      "69/69 [==============================] - 0s 952us/step - loss: 0.1126 - accuracy: 0.9535\n",
      "Epoch 177/1500\n",
      "69/69 [==============================] - 0s 941us/step - loss: 0.1165 - accuracy: 0.9562\n",
      "Epoch 178/1500\n",
      "69/69 [==============================] - 0s 920us/step - loss: 0.1125 - accuracy: 0.9608\n",
      "Epoch 179/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9658\n",
      "Epoch 180/1500\n",
      "69/69 [==============================] - 0s 959us/step - loss: 0.1068 - accuracy: 0.9544\n",
      "Epoch 181/1500\n",
      "69/69 [==============================] - 0s 962us/step - loss: 0.1058 - accuracy: 0.9572\n",
      "Epoch 182/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9686\n",
      "Epoch 183/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9517\n",
      "Epoch 184/1500\n",
      "69/69 [==============================] - 0s 953us/step - loss: 0.1296 - accuracy: 0.9458\n",
      "Epoch 185/1500\n",
      "69/69 [==============================] - 0s 959us/step - loss: 0.1203 - accuracy: 0.9549\n",
      "Epoch 186/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9608\n",
      "Epoch 187/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9567\n",
      "Epoch 188/1500\n",
      "69/69 [==============================] - 0s 942us/step - loss: 0.1085 - accuracy: 0.9594\n",
      "Epoch 189/1500\n",
      "69/69 [==============================] - 0s 950us/step - loss: 0.1071 - accuracy: 0.9576\n",
      "Epoch 190/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.1178 - accuracy: 0.9531\n",
      "Epoch 191/1500\n",
      "69/69 [==============================] - 0s 960us/step - loss: 0.1242 - accuracy: 0.9517\n",
      "Epoch 192/1500\n",
      "69/69 [==============================] - 0s 949us/step - loss: 0.1153 - accuracy: 0.9544\n",
      "Epoch 193/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.1010 - accuracy: 0.9585\n",
      "Epoch 194/1500\n",
      "69/69 [==============================] - 0s 941us/step - loss: 0.1083 - accuracy: 0.9590\n",
      "Epoch 195/1500\n",
      "69/69 [==============================] - 0s 952us/step - loss: 0.1130 - accuracy: 0.9553\n",
      "Epoch 196/1500\n",
      "69/69 [==============================] - 0s 957us/step - loss: 0.1070 - accuracy: 0.9558\n",
      "Epoch 197/1500\n",
      "69/69 [==============================] - 0s 951us/step - loss: 0.0975 - accuracy: 0.9649\n",
      "Epoch 198/1500\n",
      "69/69 [==============================] - 0s 970us/step - loss: 0.0761 - accuracy: 0.9731\n",
      "Epoch 199/1500\n",
      "69/69 [==============================] - 0s 972us/step - loss: 0.0862 - accuracy: 0.9681\n",
      "Epoch 200/1500\n",
      "69/69 [==============================] - 0s 957us/step - loss: 0.1038 - accuracy: 0.9594\n",
      "Epoch 201/1500\n",
      "69/69 [==============================] - 0s 950us/step - loss: 0.0932 - accuracy: 0.9644\n",
      "Epoch 202/1500\n",
      "69/69 [==============================] - 0s 937us/step - loss: 0.1008 - accuracy: 0.9608\n",
      "Epoch 203/1500\n",
      "69/69 [==============================] - 0s 938us/step - loss: 0.1059 - accuracy: 0.9535\n",
      "Epoch 204/1500\n",
      "69/69 [==============================] - 0s 971us/step - loss: 0.1161 - accuracy: 0.9526\n",
      "Epoch 205/1500\n",
      "69/69 [==============================] - 0s 971us/step - loss: 0.1161 - accuracy: 0.9590\n",
      "Epoch 206/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9622\n",
      "Epoch 207/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9590\n",
      "Epoch 208/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9549\n",
      "Epoch 209/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9581\n",
      "Epoch 210/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9640\n",
      "Epoch 211/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9654\n",
      "Epoch 212/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9676\n",
      "Epoch 213/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9681\n",
      "Epoch 214/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9654\n",
      "Epoch 215/1500\n",
      "69/69 [==============================] - 0s 947us/step - loss: 0.1038 - accuracy: 0.9558\n",
      "Epoch 216/1500\n",
      "69/69 [==============================] - 0s 944us/step - loss: 0.1121 - accuracy: 0.9558\n",
      "Epoch 217/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.1064 - accuracy: 0.9631\n",
      "Epoch 218/1500\n",
      "69/69 [==============================] - 0s 953us/step - loss: 0.0963 - accuracy: 0.9649\n",
      "Epoch 219/1500\n",
      "69/69 [==============================] - 0s 946us/step - loss: 0.0936 - accuracy: 0.9635\n",
      "Epoch 220/1500\n",
      "69/69 [==============================] - 0s 951us/step - loss: 0.0972 - accuracy: 0.9635\n",
      "Epoch 221/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.1099 - accuracy: 0.9562\n",
      "Epoch 222/1500\n",
      "69/69 [==============================] - 0s 952us/step - loss: 0.0899 - accuracy: 0.9676\n",
      "Epoch 223/1500\n",
      "69/69 [==============================] - 0s 939us/step - loss: 0.0937 - accuracy: 0.9631\n",
      "Epoch 224/1500\n",
      "69/69 [==============================] - 0s 945us/step - loss: 0.0956 - accuracy: 0.9635\n",
      "Epoch 225/1500\n",
      "69/69 [==============================] - 0s 952us/step - loss: 0.1043 - accuracy: 0.9594\n",
      "Epoch 226/1500\n",
      "69/69 [==============================] - 0s 928us/step - loss: 0.1002 - accuracy: 0.9608\n",
      "Epoch 227/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9585\n",
      "Epoch 228/1500\n",
      "51/69 [=====================>........] - ETA: 0s - loss: 0.0952 - accuracy: 0.9608Restoring model weights from the end of the best epoch: 198.\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9603\n",
      "Epoch 228: early stopping\n",
      "5/5 [==============================] - 0s 814us/step - loss: 0.8035 - accuracy: 0.7115\n",
      "5/5 [==============================] - 0s 719us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.78 (18/23)\n",
      "Before appending - Cat IDs: 597, Predictions: 597, Actuals: 597, Gender: 597\n",
      "After appending - Cat IDs: 753, Predictions: 753, Actuals: 753, Gender: 753\n",
      "Final Test Results - Loss: 0.8034568428993225, Accuracy: 0.7115384340286255, Precision: 0.6788383838383839, Recall: 0.7990063233965673, F1 Score: 0.6927245693624283\n",
      "Confusion Matrix:\n",
      " [[83  3 37]\n",
      " [ 0 15  0]\n",
      " [ 5  0 13]]\n",
      "outer_fold 5\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "097B    14\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "022A     9\n",
      "033A     9\n",
      "045A     9\n",
      "065A     9\n",
      "015A     9\n",
      "072A     9\n",
      "051B     9\n",
      "010A     8\n",
      "094A     8\n",
      "013B     8\n",
      "095A     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "117A     7\n",
      "023A     6\n",
      "008A     6\n",
      "053A     6\n",
      "108A     6\n",
      "007A     6\n",
      "037A     6\n",
      "034A     5\n",
      "023B     5\n",
      "075A     5\n",
      "025C     5\n",
      "021A     5\n",
      "070A     5\n",
      "105A     4\n",
      "035A     4\n",
      "026A     4\n",
      "003A     4\n",
      "052A     4\n",
      "104A     4\n",
      "062A     4\n",
      "060A     3\n",
      "012A     3\n",
      "006A     3\n",
      "064A     3\n",
      "014A     3\n",
      "058A     3\n",
      "056A     3\n",
      "113A     3\n",
      "093A     2\n",
      "025B     2\n",
      "038A     2\n",
      "102A     2\n",
      "061A     2\n",
      "011A     2\n",
      "087A     2\n",
      "048A     1\n",
      "073A     1\n",
      "004A     1\n",
      "019B     1\n",
      "076A     1\n",
      "026C     1\n",
      "096A     1\n",
      "115A     1\n",
      "092A     1\n",
      "110A     1\n",
      "043A     1\n",
      "100A     1\n",
      "024A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "059A    14\n",
      "116A    12\n",
      "025A    11\n",
      "014B    10\n",
      "040A    10\n",
      "050A     7\n",
      "109A     6\n",
      "044A     5\n",
      "009A     4\n",
      "032A     2\n",
      "069A     2\n",
      "054A     2\n",
      "018A     2\n",
      "091A     1\n",
      "049A     1\n",
      "041A     1\n",
      "066A     1\n",
      "088A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    318\n",
      "X    239\n",
      "F    224\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    109\n",
      "F     28\n",
      "M     19\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten     [111A, 047A, 042A, 043A, 045A, 048A, 115A, 110A]\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 113A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [091A, 009A, 025A, 069A, 032A, 018A, 066A, 088A]\n",
      "kitten    [044A, 014B, 040A, 046A, 109A, 050A, 049A, 041A]\n",
      "senior                            [059A, 116A, 054A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 66, 'kitten': 8, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 8, 'kitten': 8, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '008A' '010A' '011A' '012A' '013B' '014A' '015A' '016A' '019A' '019B'\n",
      " '020A' '021A' '022A' '023A' '023B' '024A' '025B' '025C' '026A' '026B'\n",
      " '026C' '027A' '028A' '029A' '031A' '033A' '034A' '035A' '036A' '037A'\n",
      " '038A' '039A' '042A' '043A' '045A' '047A' '048A' '051A' '051B' '052A'\n",
      " '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A' '064A'\n",
      " '065A' '067A' '068A' '070A' '071A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '092A' '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['009A' '014B' '018A' '025A' '032A' '040A' '041A' '044A' '046A' '049A'\n",
      " '050A' '054A' '059A' '066A' '069A' '088A' '090A' '091A' '109A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'042A'}\n",
      "Moved to Test Set:\n",
      "{'042A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '008A' '010A' '011A' '012A' '013B' '014A' '015A' '016A' '019A' '019B'\n",
      " '020A' '021A' '022A' '023A' '023B' '024A' '025B' '025C' '026A' '026B'\n",
      " '026C' '027A' '028A' '029A' '031A' '033A' '034A' '035A' '036A' '037A'\n",
      " '038A' '039A' '043A' '045A' '046A' '047A' '048A' '051A' '051B' '052A'\n",
      " '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A' '064A'\n",
      " '065A' '067A' '068A' '070A' '071A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '092A' '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['009A' '014B' '018A' '025A' '032A' '040A' '041A' '042A' '044A' '049A'\n",
      " '050A' '054A' '059A' '066A' '069A' '088A' '090A' '091A' '109A' '116A']\n",
      "Length of X_train_val:\n",
      "830\n",
      "Length of y_train_val:\n",
      "830\n",
      "Length of groups_train_val:\n",
      "830\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     564\n",
      "senior    149\n",
      "kitten     68\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "kitten    103\n",
      "senior     29\n",
      "adult      24\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     564\n",
      "senior    149\n",
      "kitten    117\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "kitten    54\n",
      "senior    29\n",
      "adult     24\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1128, 2: 596, 1: 468})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.9307 - accuracy: 0.5479\n",
      "Epoch 2/1500\n",
      "69/69 [==============================] - 0s 972us/step - loss: 0.7228 - accuracy: 0.6245\n",
      "Epoch 3/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.6515 - accuracy: 0.6702\n",
      "Epoch 4/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.6074 - accuracy: 0.6975\n",
      "Epoch 5/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.5712 - accuracy: 0.7185\n",
      "Epoch 6/1500\n",
      "69/69 [==============================] - 0s 995us/step - loss: 0.5376 - accuracy: 0.7331\n",
      "Epoch 7/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.5167 - accuracy: 0.7477\n",
      "Epoch 8/1500\n",
      "69/69 [==============================] - 0s 992us/step - loss: 0.4941 - accuracy: 0.7505\n",
      "Epoch 9/1500\n",
      "69/69 [==============================] - 0s 957us/step - loss: 0.4845 - accuracy: 0.7619\n",
      "Epoch 10/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.4655 - accuracy: 0.7769\n",
      "Epoch 11/1500\n",
      "69/69 [==============================] - 0s 958us/step - loss: 0.4608 - accuracy: 0.7742\n",
      "Epoch 12/1500\n",
      "69/69 [==============================] - 0s 970us/step - loss: 0.4228 - accuracy: 0.7911\n",
      "Epoch 13/1500\n",
      "69/69 [==============================] - 0s 913us/step - loss: 0.4242 - accuracy: 0.7970\n",
      "Epoch 14/1500\n",
      "69/69 [==============================] - 0s 926us/step - loss: 0.4362 - accuracy: 0.7742\n",
      "Epoch 15/1500\n",
      "69/69 [==============================] - 0s 919us/step - loss: 0.4135 - accuracy: 0.8084\n",
      "Epoch 16/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.4191 - accuracy: 0.7847\n",
      "Epoch 17/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.4033 - accuracy: 0.8002\n",
      "Epoch 18/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3859 - accuracy: 0.8130\n",
      "Epoch 19/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3849 - accuracy: 0.8175\n",
      "Epoch 20/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3867 - accuracy: 0.8038\n",
      "Epoch 21/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3727 - accuracy: 0.8189\n",
      "Epoch 22/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3615 - accuracy: 0.8125\n",
      "Epoch 23/1500\n",
      "69/69 [==============================] - 0s 936us/step - loss: 0.3421 - accuracy: 0.8294\n",
      "Epoch 24/1500\n",
      "69/69 [==============================] - 0s 948us/step - loss: 0.3531 - accuracy: 0.8198\n",
      "Epoch 25/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3396 - accuracy: 0.8317\n",
      "Epoch 26/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3148 - accuracy: 0.8444\n",
      "Epoch 27/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3168 - accuracy: 0.8440\n",
      "Epoch 28/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3303 - accuracy: 0.8403\n",
      "Epoch 29/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3396 - accuracy: 0.8271\n",
      "Epoch 30/1500\n",
      "69/69 [==============================] - 0s 948us/step - loss: 0.3091 - accuracy: 0.8554\n",
      "Epoch 31/1500\n",
      "69/69 [==============================] - 0s 971us/step - loss: 0.3142 - accuracy: 0.8449\n",
      "Epoch 32/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3140 - accuracy: 0.8463\n",
      "Epoch 33/1500\n",
      "69/69 [==============================] - 0s 994us/step - loss: 0.3013 - accuracy: 0.8490\n",
      "Epoch 34/1500\n",
      "69/69 [==============================] - 0s 950us/step - loss: 0.2970 - accuracy: 0.8572\n",
      "Epoch 35/1500\n",
      "69/69 [==============================] - 0s 944us/step - loss: 0.3081 - accuracy: 0.8517\n",
      "Epoch 36/1500\n",
      "69/69 [==============================] - 0s 980us/step - loss: 0.2898 - accuracy: 0.8572\n",
      "Epoch 37/1500\n",
      "69/69 [==============================] - 0s 962us/step - loss: 0.3057 - accuracy: 0.8499\n",
      "Epoch 38/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.2960 - accuracy: 0.8558\n",
      "Epoch 39/1500\n",
      "69/69 [==============================] - 0s 979us/step - loss: 0.2977 - accuracy: 0.8554\n",
      "Epoch 40/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2804 - accuracy: 0.8654\n",
      "Epoch 41/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.2792 - accuracy: 0.8677\n",
      "Epoch 42/1500\n",
      "69/69 [==============================] - 0s 934us/step - loss: 0.2719 - accuracy: 0.8691\n",
      "Epoch 43/1500\n",
      "69/69 [==============================] - 0s 946us/step - loss: 0.2772 - accuracy: 0.8636\n",
      "Epoch 44/1500\n",
      "69/69 [==============================] - 0s 962us/step - loss: 0.2539 - accuracy: 0.8828\n",
      "Epoch 45/1500\n",
      "69/69 [==============================] - 0s 966us/step - loss: 0.2589 - accuracy: 0.8691\n",
      "Epoch 46/1500\n",
      "69/69 [==============================] - 0s 968us/step - loss: 0.2761 - accuracy: 0.8714\n",
      "Epoch 47/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.2654 - accuracy: 0.8718\n",
      "Epoch 48/1500\n",
      "69/69 [==============================] - 0s 972us/step - loss: 0.2507 - accuracy: 0.8741\n",
      "Epoch 49/1500\n",
      "69/69 [==============================] - 0s 985us/step - loss: 0.2564 - accuracy: 0.8800\n",
      "Epoch 50/1500\n",
      "69/69 [==============================] - 0s 937us/step - loss: 0.2691 - accuracy: 0.8768\n",
      "Epoch 51/1500\n",
      "69/69 [==============================] - 0s 973us/step - loss: 0.2497 - accuracy: 0.8832\n",
      "Epoch 52/1500\n",
      "69/69 [==============================] - 0s 973us/step - loss: 0.2586 - accuracy: 0.8695\n",
      "Epoch 53/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2551 - accuracy: 0.8809\n",
      "Epoch 54/1500\n",
      "69/69 [==============================] - 0s 982us/step - loss: 0.2548 - accuracy: 0.8745\n",
      "Epoch 55/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.2423 - accuracy: 0.8910\n",
      "Epoch 56/1500\n",
      "69/69 [==============================] - 0s 958us/step - loss: 0.2474 - accuracy: 0.8837\n",
      "Epoch 57/1500\n",
      "69/69 [==============================] - 0s 939us/step - loss: 0.2375 - accuracy: 0.8809\n",
      "Epoch 58/1500\n",
      "69/69 [==============================] - 0s 979us/step - loss: 0.2455 - accuracy: 0.8841\n",
      "Epoch 59/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2274 - accuracy: 0.8923\n",
      "Epoch 60/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.8768\n",
      "Epoch 61/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2446 - accuracy: 0.8796\n",
      "Epoch 62/1500\n",
      "69/69 [==============================] - 0s 976us/step - loss: 0.2448 - accuracy: 0.8823\n",
      "Epoch 63/1500\n",
      "69/69 [==============================] - 0s 996us/step - loss: 0.2201 - accuracy: 0.8983\n",
      "Epoch 64/1500\n",
      "69/69 [==============================] - 0s 980us/step - loss: 0.2355 - accuracy: 0.8905\n",
      "Epoch 65/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2271 - accuracy: 0.8919\n",
      "Epoch 66/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.9001\n",
      "Epoch 67/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2240 - accuracy: 0.8937\n",
      "Epoch 68/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2132 - accuracy: 0.8996\n",
      "Epoch 69/1500\n",
      "69/69 [==============================] - 0s 982us/step - loss: 0.2135 - accuracy: 0.9060\n",
      "Epoch 70/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.8978\n",
      "Epoch 71/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.2188 - accuracy: 0.8937\n",
      "Epoch 72/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2162 - accuracy: 0.8955\n",
      "Epoch 73/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2103 - accuracy: 0.9042\n",
      "Epoch 74/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2157 - accuracy: 0.9069\n",
      "Epoch 75/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2054 - accuracy: 0.9078\n",
      "Epoch 76/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2108 - accuracy: 0.8987\n",
      "Epoch 77/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2001 - accuracy: 0.8992\n",
      "Epoch 78/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2029 - accuracy: 0.9015\n",
      "Epoch 79/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9051\n",
      "Epoch 80/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.2075 - accuracy: 0.9051\n",
      "Epoch 81/1500\n",
      "69/69 [==============================] - 0s 979us/step - loss: 0.2014 - accuracy: 0.9028\n",
      "Epoch 82/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2082 - accuracy: 0.8960\n",
      "Epoch 83/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2036 - accuracy: 0.9129\n",
      "Epoch 84/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2148 - accuracy: 0.8969\n",
      "Epoch 85/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.9083\n",
      "Epoch 86/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9015\n",
      "Epoch 87/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1942 - accuracy: 0.9092\n",
      "Epoch 88/1500\n",
      "69/69 [==============================] - 0s 964us/step - loss: 0.2079 - accuracy: 0.9001\n",
      "Epoch 89/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1896 - accuracy: 0.9165\n",
      "Epoch 90/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9165\n",
      "Epoch 91/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2000 - accuracy: 0.9060\n",
      "Epoch 92/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9037\n",
      "Epoch 93/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 0.9047\n",
      "Epoch 94/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9101\n",
      "Epoch 95/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2150 - accuracy: 0.8937\n",
      "Epoch 96/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9188\n",
      "Epoch 97/1500\n",
      "69/69 [==============================] - 0s 968us/step - loss: 0.1776 - accuracy: 0.9156\n",
      "Epoch 98/1500\n",
      "69/69 [==============================] - 0s 959us/step - loss: 0.1959 - accuracy: 0.9056\n",
      "Epoch 99/1500\n",
      "69/69 [==============================] - 0s 947us/step - loss: 0.1778 - accuracy: 0.9129\n",
      "Epoch 100/1500\n",
      "69/69 [==============================] - 0s 964us/step - loss: 0.1902 - accuracy: 0.9101\n",
      "Epoch 101/1500\n",
      "69/69 [==============================] - 0s 994us/step - loss: 0.1750 - accuracy: 0.9197\n",
      "Epoch 102/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9138\n",
      "Epoch 103/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1729 - accuracy: 0.9211\n",
      "Epoch 104/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9101\n",
      "Epoch 105/1500\n",
      "69/69 [==============================] - 0s 967us/step - loss: 0.1912 - accuracy: 0.9129\n",
      "Epoch 106/1500\n",
      "69/69 [==============================] - 0s 940us/step - loss: 0.1776 - accuracy: 0.9170\n",
      "Epoch 107/1500\n",
      "69/69 [==============================] - 0s 945us/step - loss: 0.1802 - accuracy: 0.9156\n",
      "Epoch 108/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9293\n",
      "Epoch 109/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9243\n",
      "Epoch 110/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9206\n",
      "Epoch 111/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.9174\n",
      "Epoch 112/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9197\n",
      "Epoch 113/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9275\n",
      "Epoch 114/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9243\n",
      "Epoch 115/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1678 - accuracy: 0.9243\n",
      "Epoch 116/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9238\n",
      "Epoch 117/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9206\n",
      "Epoch 118/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9302\n",
      "Epoch 119/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1683 - accuracy: 0.9211\n",
      "Epoch 120/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9224\n",
      "Epoch 121/1500\n",
      "69/69 [==============================] - 0s 975us/step - loss: 0.1639 - accuracy: 0.9256\n",
      "Epoch 122/1500\n",
      "69/69 [==============================] - 0s 980us/step - loss: 0.1740 - accuracy: 0.9183\n",
      "Epoch 123/1500\n",
      "69/69 [==============================] - 0s 961us/step - loss: 0.1718 - accuracy: 0.9193\n",
      "Epoch 124/1500\n",
      "69/69 [==============================] - 0s 972us/step - loss: 0.1729 - accuracy: 0.9206\n",
      "Epoch 125/1500\n",
      "69/69 [==============================] - 0s 944us/step - loss: 0.1711 - accuracy: 0.9252\n",
      "Epoch 126/1500\n",
      "69/69 [==============================] - 0s 940us/step - loss: 0.1523 - accuracy: 0.9297\n",
      "Epoch 127/1500\n",
      "69/69 [==============================] - 0s 961us/step - loss: 0.1450 - accuracy: 0.9339\n",
      "Epoch 128/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.1620 - accuracy: 0.9261\n",
      "Epoch 129/1500\n",
      "69/69 [==============================] - 0s 970us/step - loss: 0.1587 - accuracy: 0.9252\n",
      "Epoch 130/1500\n",
      "69/69 [==============================] - 0s 966us/step - loss: 0.1516 - accuracy: 0.9284\n",
      "Epoch 131/1500\n",
      "69/69 [==============================] - 0s 983us/step - loss: 0.1763 - accuracy: 0.9165\n",
      "Epoch 132/1500\n",
      "69/69 [==============================] - 0s 979us/step - loss: 0.1433 - accuracy: 0.9357\n",
      "Epoch 133/1500\n",
      "69/69 [==============================] - 0s 999us/step - loss: 0.1574 - accuracy: 0.9325\n",
      "Epoch 134/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9311\n",
      "Epoch 135/1500\n",
      "69/69 [==============================] - 0s 989us/step - loss: 0.1424 - accuracy: 0.9366\n",
      "Epoch 136/1500\n",
      "69/69 [==============================] - 0s 967us/step - loss: 0.1544 - accuracy: 0.9279\n",
      "Epoch 137/1500\n",
      "69/69 [==============================] - 0s 932us/step - loss: 0.1524 - accuracy: 0.9316\n",
      "Epoch 138/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.1549 - accuracy: 0.9329\n",
      "Epoch 139/1500\n",
      "69/69 [==============================] - 0s 923us/step - loss: 0.1491 - accuracy: 0.9302\n",
      "Epoch 140/1500\n",
      "69/69 [==============================] - 0s 956us/step - loss: 0.1460 - accuracy: 0.9325\n",
      "Epoch 141/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.1485 - accuracy: 0.9325\n",
      "Epoch 142/1500\n",
      "69/69 [==============================] - 0s 958us/step - loss: 0.1542 - accuracy: 0.9316\n",
      "Epoch 143/1500\n",
      "69/69 [==============================] - 0s 945us/step - loss: 0.1419 - accuracy: 0.9398\n",
      "Epoch 144/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9366\n",
      "Epoch 145/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.9370\n",
      "Epoch 146/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9352\n",
      "Epoch 147/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9329\n",
      "Epoch 148/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.9380\n",
      "Epoch 149/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9316\n",
      "Epoch 150/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1422 - accuracy: 0.9352\n",
      "Epoch 151/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9370\n",
      "Epoch 152/1500\n",
      "69/69 [==============================] - 0s 981us/step - loss: 0.1446 - accuracy: 0.9261\n",
      "Epoch 153/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1623 - accuracy: 0.9325\n",
      "Epoch 154/1500\n",
      "69/69 [==============================] - 0s 993us/step - loss: 0.1198 - accuracy: 0.9489\n",
      "Epoch 155/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9475\n",
      "Epoch 156/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9361\n",
      "Epoch 157/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9339\n",
      "Epoch 158/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9398\n",
      "Epoch 159/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9380\n",
      "Epoch 160/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9375\n",
      "Epoch 161/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9434\n",
      "Epoch 162/1500\n",
      "69/69 [==============================] - 0s 953us/step - loss: 0.1365 - accuracy: 0.9430\n",
      "Epoch 163/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9457\n",
      "Epoch 164/1500\n",
      "69/69 [==============================] - 0s 936us/step - loss: 0.1199 - accuracy: 0.9466\n",
      "Epoch 165/1500\n",
      "69/69 [==============================] - 0s 953us/step - loss: 0.1197 - accuracy: 0.9443\n",
      "Epoch 166/1500\n",
      "69/69 [==============================] - 0s 945us/step - loss: 0.1431 - accuracy: 0.9416\n",
      "Epoch 167/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9320\n",
      "Epoch 168/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9407\n",
      "Epoch 169/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9411\n",
      "Epoch 170/1500\n",
      "69/69 [==============================] - 0s 984us/step - loss: 0.1365 - accuracy: 0.9411\n",
      "Epoch 171/1500\n",
      "69/69 [==============================] - 0s 976us/step - loss: 0.1302 - accuracy: 0.9416\n",
      "Epoch 172/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9316\n",
      "Epoch 173/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9389\n",
      "Epoch 174/1500\n",
      "69/69 [==============================] - 0s 944us/step - loss: 0.1358 - accuracy: 0.9343\n",
      "Epoch 175/1500\n",
      "69/69 [==============================] - 0s 2ms/step - loss: 0.1357 - accuracy: 0.9430\n",
      "Epoch 176/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9398\n",
      "Epoch 177/1500\n",
      "69/69 [==============================] - 0s 991us/step - loss: 0.1288 - accuracy: 0.9462\n",
      "Epoch 178/1500\n",
      "69/69 [==============================] - 0s 992us/step - loss: 0.1444 - accuracy: 0.9311\n",
      "Epoch 179/1500\n",
      "69/69 [==============================] - 0s 988us/step - loss: 0.1326 - accuracy: 0.9434\n",
      "Epoch 180/1500\n",
      "69/69 [==============================] - 0s 959us/step - loss: 0.1232 - accuracy: 0.9416\n",
      "Epoch 181/1500\n",
      "69/69 [==============================] - 0s 987us/step - loss: 0.1237 - accuracy: 0.9416\n",
      "Epoch 182/1500\n",
      "69/69 [==============================] - 0s 986us/step - loss: 0.1187 - accuracy: 0.9448\n",
      "Epoch 183/1500\n",
      "69/69 [==============================] - 0s 972us/step - loss: 0.1205 - accuracy: 0.9503\n",
      "Epoch 184/1500\n",
      "69/69 [==============================] - 0s 951us/step - loss: 0.1159 - accuracy: 0.9498\n",
      "Epoch 185/1500\n",
      "69/69 [==============================] - 0s 965us/step - loss: 0.1279 - accuracy: 0.9421\n",
      "Epoch 186/1500\n",
      "69/69 [==============================] - 0s 976us/step - loss: 0.1201 - accuracy: 0.9489\n",
      "Epoch 187/1500\n",
      "69/69 [==============================] - 0s 948us/step - loss: 0.1160 - accuracy: 0.9516\n",
      "Epoch 188/1500\n",
      "69/69 [==============================] - 0s 957us/step - loss: 0.1307 - accuracy: 0.9430\n",
      "Epoch 189/1500\n",
      "69/69 [==============================] - 0s 962us/step - loss: 0.1239 - accuracy: 0.9443\n",
      "Epoch 190/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.1228 - accuracy: 0.9453\n",
      "Epoch 191/1500\n",
      "69/69 [==============================] - 0s 976us/step - loss: 0.1177 - accuracy: 0.9462\n",
      "Epoch 192/1500\n",
      "69/69 [==============================] - 0s 948us/step - loss: 0.1269 - accuracy: 0.9448\n",
      "Epoch 193/1500\n",
      "69/69 [==============================] - 0s 957us/step - loss: 0.1381 - accuracy: 0.9448\n",
      "Epoch 194/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9384\n",
      "Epoch 195/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9466\n",
      "Epoch 196/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9462\n",
      "Epoch 197/1500\n",
      "69/69 [==============================] - 0s 966us/step - loss: 0.1172 - accuracy: 0.9494\n",
      "Epoch 198/1500\n",
      "69/69 [==============================] - 0s 973us/step - loss: 0.1177 - accuracy: 0.9516\n",
      "Epoch 199/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9457\n",
      "Epoch 200/1500\n",
      "69/69 [==============================] - 0s 970us/step - loss: 0.1309 - accuracy: 0.9421\n",
      "Epoch 201/1500\n",
      "69/69 [==============================] - 0s 998us/step - loss: 0.1232 - accuracy: 0.9471\n",
      "Epoch 202/1500\n",
      "69/69 [==============================] - 0s 991us/step - loss: 0.1317 - accuracy: 0.9416\n",
      "Epoch 203/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9498\n",
      "Epoch 204/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9526\n",
      "Epoch 205/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9443\n",
      "Epoch 206/1500\n",
      "69/69 [==============================] - 0s 987us/step - loss: 0.1378 - accuracy: 0.9402\n",
      "Epoch 207/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9384\n",
      "Epoch 208/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9503\n",
      "Epoch 209/1500\n",
      "69/69 [==============================] - 0s 990us/step - loss: 0.1141 - accuracy: 0.9503\n",
      "Epoch 210/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9457\n",
      "Epoch 211/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9453\n",
      "Epoch 212/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9535\n",
      "Epoch 213/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9503\n",
      "Epoch 214/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9539\n",
      "Epoch 215/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9466\n",
      "Epoch 216/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9439\n",
      "Epoch 217/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9407\n",
      "Epoch 218/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9503\n",
      "Epoch 219/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9421\n",
      "Epoch 220/1500\n",
      "69/69 [==============================] - 0s 6ms/step - loss: 0.0849 - accuracy: 0.9662\n",
      "Epoch 221/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9425\n",
      "Epoch 222/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9411\n",
      "Epoch 223/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9516\n",
      "Epoch 224/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9462\n",
      "Epoch 225/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9580\n",
      "Epoch 226/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9535\n",
      "Epoch 227/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9489\n",
      "Epoch 228/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9548\n",
      "Epoch 229/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9557\n",
      "Epoch 230/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9507\n",
      "Epoch 231/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9553\n",
      "Epoch 232/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9521\n",
      "Epoch 233/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9489\n",
      "Epoch 234/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9370\n",
      "Epoch 235/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9471\n",
      "Epoch 236/1500\n",
      "69/69 [==============================] - 0s 927us/step - loss: 0.0991 - accuracy: 0.9567\n",
      "Epoch 237/1500\n",
      "69/69 [==============================] - 0s 958us/step - loss: 0.1085 - accuracy: 0.9535\n",
      "Epoch 238/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9585\n",
      "Epoch 239/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9521\n",
      "Epoch 240/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9626\n",
      "Epoch 241/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9503\n",
      "Epoch 242/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.1230 - accuracy: 0.9503\n",
      "Epoch 243/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9494\n",
      "Epoch 244/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9475\n",
      "Epoch 245/1500\n",
      "69/69 [==============================] - 0s 975us/step - loss: 0.1077 - accuracy: 0.9557\n",
      "Epoch 246/1500\n",
      "69/69 [==============================] - 0s 970us/step - loss: 0.1051 - accuracy: 0.9557\n",
      "Epoch 247/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9544\n",
      "Epoch 248/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9603\n",
      "Epoch 249/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9530\n",
      "Epoch 250/1500\n",
      "40/69 [================>.............] - ETA: 0s - loss: 0.1121 - accuracy: 0.9453Restoring model weights from the end of the best epoch: 220.\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9443\n",
      "Epoch 250: early stopping\n",
      "4/4 [==============================] - 0s 1ms/step - loss: 0.7662 - accuracy: 0.7383\n",
      "4/4 [==============================] - 0s 934us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.65 (13/20)\n",
      "Before appending - Cat IDs: 753, Predictions: 753, Actuals: 753, Gender: 753\n",
      "After appending - Cat IDs: 860, Predictions: 860, Actuals: 860, Gender: 860\n",
      "Final Test Results - Loss: 0.7662308812141418, Accuracy: 0.7383177280426025, Precision: 0.7581090407177363, Recall: 0.7406875266070668, F1 Score: 0.7261072261072261\n",
      "Confusion Matrix:\n",
      " [[19  2  3]\n",
      " [14 40  0]\n",
      " [ 9  0 20]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6409170394614678\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 1.0349870920181274\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7069398641586304\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6425631821527862\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7321716517925922\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=int(random_seeds[2]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train_val),\n",
    "        y=y_train_val\n",
    "    )\n",
    "    weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping], class_weight=weight_dict)\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "# Append averages to total lists\n",
    "all_f1.append(unseen_set_avg_f1)\n",
    "all_losses.append(unseen_set_avg_loss)\n",
    "all_accuracies.append(unseen_set_avg_acc)\n",
    "all_precisions.append(unseen_set_avg_precision)\n",
    "all_recalls.append(unseen_set_avg_recall)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5950d851-e643-4a64-bc36-aa4b364446b0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed_3_folds_values = (unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1)\n",
    "\n",
    "# Plotting all fold metrics\n",
    "seed_3_folds_plot = plot_all_metrics(unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1, \"Folds\", \"Fold Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ffe035-936f-4a5e-883e-d1c2c29daa16",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b0b159f9-802f-414e-8ebd-f9e08943d5d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 860, Predictions: 860, Actuals: 860, Gender: 860\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f958e5ec-f83f-4653-a738-3804f295385e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "895d62be-0c46-43cf-a33f-b0165e06d747",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.76 (84/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "# store for final evaluation \n",
    "all_majority_vote_accuracies.append(majority_vote_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e45cd330-51a8-4fdc-8038-601fbf7684bf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbc0f1a9-d45e-4174-95f7-407c3132959e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, kitten, kitten, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[senior, adult, senior, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, adult, a...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, adult, kitten, kitten, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, kit...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, kit...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, senior, adult, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, senior, kitten, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, kitten, kitten, senior, adult, adult, ...         adult            adult                   True\n",
       "69    063A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "81    075A               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "65    059A  [senior, adult, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "59    053A      [senior, adult, senior, senior, adult, adult]         adult            adult                   True\n",
       "58    052A                     [adult, senior, senior, adult]         adult            adult                   True\n",
       "57    051B  [adult, adult, senior, adult, senior, adult, s...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "53    048A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "97    102A                                    [senior, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [senior, senior, adult]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A    [senior, senior, adult, senior, senior, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, adult, adult, a...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, adult, adult, senior, senior, senior,...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, adult, kitten, kitten, kitten,...        kitten           kitten                   True\n",
       "26    023B              [adult, adult, adult, kitten, kitten]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "29    025B                                    [senior, adult]         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, kit...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "15    014A                             [adult, adult, kitten]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, senior, senior, senior, ...         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, kit...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "4     003A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "40    034A               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "17    015A  [senior, senior, adult, adult, senior, adult, ...         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "101   106A  [adult, senior, adult, senior, adult, adult, a...         adult           senior                  False\n",
       "12    011A                                    [senior, adult]         adult           senior                  False\n",
       "90    095A  [senior, senior, adult, senior, senior, senior...        senior            adult                  False\n",
       "47    041A                                            [adult]         adult           kitten                  False\n",
       "18    016A  [adult, adult, adult, senior, senior, senior, ...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "50    044A              [adult, adult, adult, kitten, kitten]         adult           kitten                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "54    049A                                            [adult]         adult           kitten                  False\n",
       "42    036A  [senior, senior, senior, senior, adult, senior...        senior            adult                  False\n",
       "56    051A  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "61    055A  [adult, adult, senior, adult, adult, adult, se...         adult           senior                  False\n",
       "38    032A                                   [kitten, kitten]        kitten            adult                  False\n",
       "63    057A  [adult, adult, adult, senior, senior, senior, ...         adult           senior                  False\n",
       "64    058A                             [senior, adult, adult]         adult           senior                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "74    068A  [adult, senior, kitten, senior, senior, adult,...        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3f44da1b-8279-4fa3-87b2-53b5f63babb0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     63\n",
      "kitten    11\n",
      "senior    10\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3a335feb-6a0f-456f-8402-2b8dd5efb686",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             63  86.301370\n",
      "1           kitten           15             11  73.333333\n",
      "2           senior           22             10  45.454545\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# store for final evaluation \n",
    "all_majority_vote_details.append(class_stats)\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "052c6081-0864-4006-9536-1868bc003d5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnXUlEQVR4nO3dd3iN9//H8edJZMgQESJib03VHilae9ZqqWq/VaWCr92qr1atFl1UrRql1KrV2qsUNRNqUxGrIcQuIQMZ5/dHrty/HEmIJCRxXo/rcl05932f+37fx7nPeZ3P/bk/t8lsNpsREREREbESNpldgIiIiIjIs6QALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhHJxmJiYjK7hAz3PO6TiGQtOTK7AJHUioqKolmzZkRERABQtmxZFi5cmMlVSXqcPXuWH374gSNHjhAREUGePHmoW7cugwcPTvE51apVs3icK1cu/vjjD2xsLH/Pf/PNNyxbtsxi2ogRI2jVqlWaat2/fz89e/YEoECBAqxZsyZN63kSI0eOZO3atQD4+fnRo0cPi/mbNm1i2bJlzJw5M0O3++DBA5o2bcrdu3cBeP/99+nTp0+Ky7ds2ZIrV64A0K1bN+N1elJ3797lxx9/JHfu3HzwwQdpWkdGW7NmDZ9//jkAVapU4ccff8zUej7//HOL996iRYsoXbp0JlaUemFhYaxbt45t27Zx6dIlbt26RY4cOciXLx/ly5enZcuW1KhRI7PLFCuhFmDJNjZv3myEX4CgoCD+/vvvTKxI0iM6OppevXqxY8cOwsLCiImJ4dq1a1y9evWJ1nPnzh0CAwOTTN+3b19GlZrl3LhxAz8/P4YMGWIEz4xkb29Pw4YNjcebN29Ocdnjx49b1NC8efM0bXPbtm288cYbLFq0SC3AKYiIiOCPP/6wmLZ8+fJMqubJ7Nq1iw4dOjB+/HgOHTrEtWvXiI6OJioqigsXLrB+/Xp69erFkCFDePDgQWaXK1ZALcCSbaxatSrJtBUrVvDiiy9mQjWSXmfPnuXmzZvG4+bNm5M7d24qVKjwxOvat2+fxfvg2rVrnD9/PkPqTODl5UXnzp0BcHV1zdB1p6ROnTp4eHgAUKlSJWN6cHAwhw4deqrbbtasGStXrgTg0qVL/P3338kea1u2bDH+9vHxoWjRomna3vbt27l161aanmstNm/eTFRUlMW0DRs20L9/fxwdHTOpqsfbunUr//vf/4zHTk5O1KxZkwIFCnD79m327t1rfBZs2rQJZ2dnPvvss8wqV6yEArBkC8HBwRw5cgSIP+V9584dIP7D8sMPP8TZ2Tkzy5M0SNya7+npyahRo554HY6Ojty7d499+/bRpUsXY3ri1t+cOXMmCQ1pUahQIfr27Zvu9TyJRo0a0ahRo2e6zQRVq1Ylf/78Rov85s2bkw3AW7duNf5u1qzZM6vPGiVuBEj4HAwPD2fTpk20bt06EytL2cWLF40uJAA1atRgzJgxuLu7G9MePHjAqFGj2LBhAwArV67k3XffTfOPKZHUUACWbCHxB/+bb75JQEAAf//9N5GRkWzcuJF27dql+NyTJ08yf/58Dh48yO3bt8mTJw8lS5akY8eO1KpVK8ny4eHhLFy4kG3btnHx4kXs7Ozw9vamSZMmvPnmmzg5ORnLPqqP5qP6jCb0Y/Xw8GDmzJmMHDmSwMBAcuXKxf/+9z8aNmzIgwcPWLhwIZs3byYkJIT79+/j7OxM8eLFadeuHa+99lqaa+/atStHjx4FYMCAAbz77rsW61m0aBHfffcdEN8KOWHChBRf3wQxMTGsWbOG9evX888//xAVFUX+/PmpXbs2nTp1wtPT01i2VatWXL582Xh87do14zVZvXo13t7ej90eQIUKFdi3bx9Hjx7l/v37ODg4APDXX38Zy1SsWJGAgIBkn3/jxg1++ukn/P39uXbtGrGxseTOnRsfHx+6dOli0Rqdmj7AmzZtYvXq1Zw+fZq7d+/i4eFBjRo16NSpE8WKFbNYdsaMGUbf3U8++YQ7d+7wyy+/EBUVhY+Pj/G+ePj9lXgawOXLl6lWrRoFChTgs88+M/rqurm58fvvv5Mjx/9/zMfExNCsWTNu374NwLx58/Dx8Un2tTGZTDRt2pR58+YB8QG4f//+mEwmY5nAwEAuXboEgK2tLU2aNDHm3b59m2XLlrF161ZCQ0Mxm80ULVqUxo0b06FDB4sWy4f7dc+cOZOZM2cmOab++OMPli5dSlBQELGxsRQuXJjGjRvzzjvvJGkBjYyMZP78+Wzfvp2QkBAePHiAi4sLpUuXpk2bNmnuqnHjxg0mTZrErl27iI6OpmzZsnTu3JlXXnkFgLi4OFq1amX8cPjmm28supMAfPfddyxatAiI/zx7VJ/3BGfPnuXYsWPA/5+N+Oabb4D4M2GPCsAXL15k+vTpBAQEEBUVRbly5fDz88PR0ZFu3boB8f24R44cafG8J3m9UzJ37lzjx26BAgUYN26cxWcoxHe5+eyzz/j333/x9PSkZMmS2NnZGfNTc6wkOHbsGEuXLuXw4cPcuHEDV1dXypcvT4cOHfD19bXY7uOO6cSfU9OnTzfep4mPwe+//x5XV1d+/PFHjh8/jp2dHTVq1KB3794UKlQoVa+RZA4FYMnyYmJiWLdunfG4VatWeHl5Gf1/V6xYkWIAXrt2LaNGjSI2NtaYdvXqVa5evcqePXvo06cP77//vjHvypUr/Pe//yUkJMSYdu/ePYKCgggKCmLLli1Mnz49yQd4Wt27d48+ffoQGhoKwM2bNylTpgxxcXF89tlnbNu2zWL5u3fvcvToUY4ePcrFixctwsGT1N66dWsjAG/atClJAE7c57Nly5aP3Y/bt28zcOBAo5U+wYULF7hw4QJr165l7NixSYJOelWtWpV9+/Zx//59Dh06ZHzB7d+/H4AiRYqQN2/eZJ9769YtunfvzoULFyym37x5k507d7Jnzx4mTZpEzZo1H1vH/fv3GTJkCNu3b7eYfvnyZVatWsWGDRsYMWIETZs2Tfb5y5cv59SpU8ZjLy+vx24zOTVq1MDLy4srV64QFhZGQEAAderUMebv37/fCL8lSpRIMfwmaN68uRGAr169ytGjR6lYsaIxP3H3h+rVqxuvdWBgIAMHDuTatWsW6wsMDCQwMJC1a9cyefJk8ufPn+p9S+6ixtOnT3P69Gn++OMPpk2bhpubGxD/vu/WrZvFawrxF2Ht37+f/fv3c/HiRfz8/FK9fYh/b3Tu3Nmin/rhw4c5fPgwH330Ee+88w42Nja0bNmSn376CYg/vhIHYLPZbPG6pfaizMSNAC1btqR58+ZMmDCB+/fvc+zYMc6cOUOpUqWSPO/kyZP897//NS5oBDhy5Ah9+/bl9ddfT3F7T/J6pyQuLs7iDEG7du1S/Ox0dHTkhx9+eOT64NHHyuzZs5k+fTpxcXHGtH///ZcdO3awY8cO3n77bQYOHPjYbTyJHTt2sHr1aovvmM2bN7N3716mT59OmTJlMnR7knF0EZxkeTt37uTff/8FoHLlyhQqVIgmTZqQM2dOIP4DPrmLoM6dO8eYMWOMD6bSpUvz5ptvWrQCTJkyhaCgIOPxZ599ZgRIFxcXWrZsSZs2bYwuFidOnGDatGkZtm8RERGEhobyyiuv8Prrr1OzZk0KFy7Mrl27jPDr7OxMmzZt6Nixo8WH6S+//ILZbE5T7U2aNDG+iE6cOMHFixeN9Vy5csVoacqVKxevvvrqY/fj888/N8Jvjhw5qF+/Pq+//roRcO7evcvHH39sbKddu3YWYdDZ2ZnOnTvTuXNnXFxcUv36Va1a1fg7odX3/PnzRkBJPP9hP//8sxF+CxYsSMeOHXnjjTeMEBcbG8vixYtTVcekSZOM8GsymahVqxbt2rUzTuE+ePCAESNGGK/rw06dOkXevHnp0KEDVapUSTEoQ3yLfHKvXbt27bCxsbEIVJs2bbJ47pP+sCldujQlS5ZM9vmQfPeHu3fvMmjQICP85s6dm1atWtG0aVPjPXfu3Dk++ugj42K3zp07W2ynYsWKdO7c2ej3vG7dOiOMmUwmXn31Vdq1a2ecVTh16hTffvut8fz169cbIcnd3Z3WrVvzzjvvWIwwMHPmTIv3fWokvLfq1KnDG2+8YRHgJ06cSHBwMBAfahNaynft2kVkZKSx3JEjR4zXJjU/QiD+gtH169cb+9+yZUtcXFwsgnVyF8PFxcUxbNgwI/w6ODjQvHlzWrRogZOTU4oX0D3p652S0NBQwsLCjMeJ+7GnVUrHytatW5k6daoRfsuVK8ebb75JlSpVjOcuWrSIBQsWpLuGxFasWIGdnR3NmzenefPmxlmoO3fuMHToUIvPaMla1AIsWV7ilo+EL3dnZ2caNWpknLJavnx5kosmFi1aRHR0NAD16tXj66+/Nk4Hjx49mpUrV+Ls7My+ffsoW7YsR44cMUKcs7MzCxYsME5htWrVim7dumFra8vff/9NXFxckmG30qp+/fqMHTvWYpq9vT1t27bl9OnT9OzZk5dffhmIb9lq3LgxUVFRREREcPv2bdzd3Z+4dicnJxo1asTq1auB+KDUtWtXIP60Z8KHdpMmTbC3t39k/UeOHGHnzp1A/GnwadOmUblyZSC+S0avXr04ceIE4eHhzJo1i5EjR/L++++zf/9+fv/9dyA+aKelf2358uUt+gGDZfeHqlWrptj9oXDhwjRt2pQLFy4wceJE8uTJA8S3eia0DCac3n+UK1euWLSUjRo1ygiDDx48YPDgwezcuZOYmBgmT56c4jBakydPTtVwVo0aNSJ37twpvnatW7dm1qxZmM1mtm/fbnQNiYmJ4c8//wTi/59atGjx2G1B/OsxZcoUIP698dFHH2FjY8OpU6eMHxAODg7Ur18fgGXLlhmjQnh7ezN79mzjR0VwcDCdO3cmIiKCoKAgNmzYQKtWrejbty83b97k7NmzQHxLduKzG3PnzjX+/uSTT4wzPr1796Zjx45cu3aNzZs307dvX7y8vCz+33r37k3btm2Nxz/88ANXrlyhePHiFq12qfW///2PDh06APEhp2vXrgQHBxMbG8uqVavo378/hQoVolq1avz111/cv3+fHTt2GO+JxD8ikuvGlJzt27cbLfcJjQAAbdq0MYLxhg0b6Nevn0XXhP379/PPP/8A8f/nP/74o9GPOzg4mP/85z/cv38/yfae9PVOSeKLXAHjGEuwd+9eevfunexzk+uSkSC5YyXhPQrxP7AHDx5sfEbPmTPHaF2eOXMmbdu2faIf2o9ia2vLrFmzKFeuHADt27enW7dumM1mzp07x759+1J1FkmePbUAS5Z27do1/P39gfiLmRJfENSmTRvj702bNlm0ssD/nwYH6NChg0VfyN69e7Ny5Ur+/PNPOnXqlGT5V1991aL/VqVKlViwYAE7duxg9uzZGRZ+gWRb+3x9fRk6dChz587l5Zdf5v79+xw+fJj58+dbtCgkfHmlpfaHX78EiYdZSk0rYeLlmzRpYoRfiG+JTjx+7Pbt2y1OT6ZXjhw5jH66QUFBhIWFWVwA96guF+3bt2fMmDHMnz+fPHnyEBYWxq5duyy62yQXDh62detWY58qVapkcSGYvb29xSnXQ4cOGUEmsRIlSmTYWK4FChQwWjojIiLYvXs3EH9hYEJrXM2aNVPsGvKwZs2aGa2ZN27c4ODBg4Bl94dXX33VONOQ+P3QtWtXi+0UK1aMjh07Go8f7uKTnBs3bnDu3DkA7OzsLMJsrly5qFu3LhDf2pnw4ychjACMHTuWjz/+mCVLlhjdAUaNGkXXrl2f+CIrNzc3i+5WuXLl4o033jAeHz9+3Pg78fGV8GMlcZcAW1vbVAfgh7s/JKhSpQqFCxcG4lveHx4iLXGXpJdfftniIsZixYol+yMoLa93ShJaQxOk5QfHw5I7VoKCgowfY46OjvTr18/iM/q9996jQIECQPwx8bi6n0T9+vUt3m8VK1Y0GiyAJN3CJOtQC7BkaWvWrDE+NG1tbfn4448t5ptMJsxmMxEREfz+++8WfdoS9z9M+PBL4O7ubnEV8uOWB8sv1dRI7amv5LYF8S2Ly5cvJyAgwLgI5WEJwSsttVesWJFixYoRHBzMmTNn+Oeff8iZM6fxJV6sWDHKly//2PoT9zlObjuJp929e5ewsLAkr316JPQDTvhCPnDgAABFixZ9bMg7fvw4q1at4sCBA0n6AgOpCuuP2/9ChQrh7OxMREQEZrOZS5cukTt3botlUnoPpFWbNm3Yu3cvEN/i2KBBgyfu/pDAy8uLypUrG8F38+bNVKtWzaL7Q+Ig9STvh9R0QUg8xnB0dPQjW9MSWjsbNWpk/Ji5f/8+f/75p9H6nStXLurVq0enTp0oXrz4Y7efWMGCBbG1tbWYlvjixsQtnvXr18fV1ZW7d+8SEBDA3bt3OX36NNevXwdS/yPkypUrxv8lxI+QsHHjRuPxvXv3jL+XL19u8X+bsC0g2bCf3P6n5fVOycN9vK9evWqxTW9vb2NoQYjvLpJwFiAlyR0rid9zhQsXTjIqkK2tLaVLlzYuaEu8/KOk5vhP7nUtVqwYe/bsAZK2gkvWoQAsWZbZbDZO0UP86fRH3dxgxYoVKV7U8aQtD2lpqXg48CZ0v3ic5IZwS7hIJTIyEpPJRKVKlahSpQoVKlRg9OjRFl9sD3uS2tu0acPEiROB+FbgxBeopDYkJW5ZT87Dr0viUQQyQuJ+vgsWLDBaOR/V/xfiu8iMHz8es9mMo6MjdevWpVKlSnh5efHpp5+mevuP2/+HJbf/GT2MX7169XBzcyMsLIydO3dy584do4+yq6ur0YqXWs2aNTMC8NatW2nXrp0Rftzc3CxavJ70/fA4iUOIjY3NI388JazbZDLx+eef8/rrr7Nhwwb8/f2NC03v3LnD6tWr2bBhA9OnT7e4qO9xkrtBR+LjLfG+Ozg40KxZM5YtW0Z0dDTbtm2zuFYhta2/a9assXgNEi5eTc7Ro0c5e/as0Z868Wud2jMvaXm9U+Lu7k7BggWNLin79++3uAajcOHCFt13EneDSUlyx0pqjsHEtSZ3DCb3+qTmhizJ3bQj8QgWGf15JxlHAViyrAMHDqSqD2aCEydOEBQURNmyZYH4sWUTfukHBwdbtNRcuHCB3377jRIlSlC2bFnKlStnMUxXcjdRmDZtGq6urpQsWZLKlSvj6OhocZotcUsMkOyp7uQk/rBMMH78eKNLR+I+pZD8h3Jaaof4L+EffviBmJgYYwB6iP/iS20f0cQtMokvKExuWq5cuR575fiTevHFF41+wIlPQT8qAN+5c4fJkydjNpuxs7Nj6dKlxtBrCad/U+tx+3/x4kVjGCgbGxsKFiyYZJnk3gPpYW9vT/PmzVm8eDH37t1j7NixxtjZjRs3TnJq+nEaNWrE2LFjiY6O5tatWxYXQDVu3NgigBQoUMC46CooKChJK3Di16hIkSKP3Xbi97adnR0bNmywOO5iY2OTtMomKFasGIMGDSJHjhxcuXKFw4cP8+uvv3L48GGio6OZNWsWkydPfmwNCS5evMi9e/cs+tkmPnPwcItumzZtjP7hGzduNMKdi4sL9erVe+z2zGbzE99ye8WKFcaZsnz58iVbZ4IzZ84kmZae1zs5zZo1M0bESBjf9+EzIAlSE9KTO1YSH4MhISFERERYBOXY2FiLfU3oNpJ4Px7+/I6LizOOmUdJ7jVM/Fon/j+QrEV9gCXLSrgLFUDHjh2N4Yse/pf4yu7EVzUnDkBLly61aJFdunQpCxcuZNSoUcaHc+Ll/f39LVoiTp48yU8//cSECRMYMGCA8as/V65cxjIPB6fEfSQfJbkWgtOnTxt/J/6y8Pf3t7hbVsIXRlpqh/iLUhLGLz1//jwnTpwA4i9CSvxF+CiJR4n4/fffOXz4sPE4IiLCYmijevXqZXiLiJ2dXbJ3j3tUAD5//rzxOtja2lrc2S3hoiJI3Rdy4v0/dOiQRVeD6Ohovv/+e4uakvsB8KSvSeIv7pRaqRL3QU24wQA8WfeHBLly5aJ27drG48T/xw/f/CLx6zF79mxu3LhhPD5//jxLliwxHidcOAdYhKzE++Tl5WX8aLh//z6//fabMS8qKoq2bdvSpk0bPvzwQyOMDBs2jCZNmtCoUSPjM8HLy4tmzZrRvn174/lPetvthLGFE4SHh1tcAPnwKAflypUzfpDv27fPOB2e2h8he/fuNVqu3dzcCAgISPYzMPFNZNavX2/0XU/cH9/f3984viF+NIXEXSkSpOX1fpQOHToYn2G3b9/mww8/TDI83oMHD5gzZ06SUUuSk9yxUqZMGSME37t3jylTpli0+M6fP9/o/uDi4kL16tUByzs63rlzx+K9un379lSdxUv4P0lw5swZo/sDWP4fSNaiFmDJku7evWtxgcyj7obVtGlTo2vExo0bGTBgADlz5qRjx46sXbuWmJgY9u3bx9tvv0316tW5dOmSxQfUW2+9BcR/eVWoUMG4qUKXLl2oW7cujo6OFqGmRYsWRvBNfDHGnj17+Oqrryhbtizbt283Lj5Ki7x58xpffEOGDKFJkybcvHmTHTt2WCyX8EWXltoTtGnTJsnFSE8SkqpWrUrlypU5dOgQsbGx9OzZk1dffRU3Nzf8/f2NPoWurq5PPO5qalWpUsWie8zj+v8mnnfv3j26dOlCzZo1CQwMtDjFnJqL4AoVKkTz5s2NkDlkyBDWrl1LgQIF2L9/vzE0lp2dncUFgemRuHXr+vXrjBgxAsDijlulS5fGx8fHIvQUKVIkTbeahvigm9CPNkHBggWThL727dvz22+/cevWLS5dusTbb79NnTp1iImJYfv27caZDR8fH4vwnHifVq9eTXh4OKVLl+aNN97gnXfeMUZK+eabb9i5cydFihRh7969RrCJiYkx+mOWKlXK+P/47rvv8Pf3p3DhwsaYsAmepPtDghkzZnD06FEKFSrEnj17jLNUDg4Oyd6Mok2bNkmGDEvt8ZX44rd69eqleKq/bt26ODg4cP/+fe7cucMff/zBa6+9RtWqVSlRogTnzp0jLi6O7t2706BBA8xmM9u2bUv29D3wxK/3o3h4eDB06FAGDx5MbGwsx44d4/XXX6dWrVoUKFCAW7du4e/vn+SM2ZN0CzKZTHzwwQeMHj0aiB+J5Pjx45QvX56zZ88a3XcAevToYay7SJEixutmNpsZMGAAr7/+OqGhoakeAtFsNtO3b1/q1auHo6MjW7duNT43ypQpYzEMm2QtagGWLGnDhg3Gh0i+fPke+UXVoEED47RYwsVwEP8l+OmnnxqtZcHBwSxbtswi/Hbp0sVipIDRo0cbrR+RkZFs2LCBFStWEB4eDsRfgTxgwACLbSc+pf3bb7/x5Zdfsnv3bt58880073/CyBQQ3zLx66+/sm3bNmJjYy2G70l8MceT1p7g5ZdftjhN5+zsnKrTswlsbGz46quveOGFF4D4L8atW7eyYsUKI/zmypWL7777LsMv9krw8GgPj+v/W6BAAYsfVcHBwSxZsoSjR4+SI0cO4xR3WFhYqk6Dfvrpp0bfRrPZzO7du/n111+N8Ovg4MCoUaOSvZVwWhQvXtyiJXndunVs2LAhSWvww4EsLa2/CV555ZUkoSS5EUzy5s3Lt99+i4eHBxB/w5E1a9awYcMGI/yWKlWKcePGWbRkJw7SN2/eZNmyZcYV9G+++abFtvbs2cPixYuNfsguLi588803xufAu+++S+PGjYH40987d+7kl19+YePGjUYNxYoVo1evXk/0GjRu3BgPDw/8/f1ZtmyZEX5tbGz45JNPkh0SLPHYsBAfulITvMPCwixurPKoRgAnJyeLlvcVK1YYdY0aNcr4f7t37x7r169nw4YNxMXFGa8RWLasPunr/Tj16tXjhx9+MN4T9+/fZ9u2bfzyyy9s2LDBIvy6urrSo0cPPvzww1StO0Hbtm15//33jf0IDAxk2bJlFuH3P//5D2+//bbx2N7e3mgAgfizZV999RVz584lf/78FmcXU1KtWjVsbGzYvHkza9asMbo7ubm5pen27vLsKABLlpS45aNBgwaPPEXs6upqcUvjhA9/iG99mTNnjvHFZWtrS65cuahZsybjxo1LMgalt7c38+fPp2vXrhQvXhwHBwccHBwoWbIk3bt3Z+7cuRbBI2fOnMyaNYvmzZuTO3duHB0dKV++PKNHj042bKbWm2++yddff42Pjw9OTk7kzJmT8uXLM2rUKIv1Ju5m8aS1J7C1tbUIZo0aNUr1bU4T5M2blzlz5vDpp59SpUoV3NzcsLe3p3Dhwrz99tssWbLkqbaEJPQDTvC4AAzwxRdf0KtXL4oVK4a9vT1ubm7UqVOHWbNmGafmzWazMdrBwxcHJebk5MTkyZMZPXo0tWrVwsPDAzs7O7y8vGjTpg2//PLLIwPMk7Kzs2Ps2LH4+PhgZ2dHrly5qFatWpIW68StvSaTKdX9upPj4OBAgwYNLKaldDvhypUrs3jxYvz8/ChTpozxHn7hhRfo378/P//8c5IuNg0aNKBHjx54enqSI0cO8ufPb7Qw2tjYMHr0aEaNGkX16tUt3l9vvPEGCxcutBixxNbWljFjxvDtt9/i6+tLgQIFyJEjB87Ozrzwwgv07NmTefPmPfFoJN7e3ixcuJBWrVoZx3uVKlWYMmVKind0c3V1tWgpTe3/wYYNG4wWWjc3N+O0fUoSB9bDhw8bYbVs2bLMnTuX+vXrkytXLnLmzEnNmjWZPXu2RRBPuLEQPPnrnRrVqlXjt99+Y+DAgdSoUYM8efJga2uLs7MzRYoUoVmzZowcOZL169fj5+f3xBeXAvTp04dZs2bRokULChQogJ2dHe7u7rz66qtMnTo12VDdt29fBgwYQNGiRbG3t6dAgQJ06tSJefPmpep6hcqVK/PTTz9RvXp1HB0dcXNzM24hnvjmLpL1mMy6TYmIVbtw4QIdO3Y0vmxnzJiRqgBpbX7++WdjsP2SJUta9GXNqr744gtjJJWqVasyY8aMTK7I+hw8eJDu3bsD8T9CVq1aZVxw+bRduXKFDRs2kDt3btzc3KhcubJF6P/888+Ni+wGDBiQ5JbokryRI0eydu1aAPz8/Cxu2iLZh/oAi1ihy5cvs3TpUmJjY9m4caMRfkuWLKnw+5CNGzcyduxYi1u6Pq2uHBnh119/5dq1a5w8edKiu096uuTIkzl58iSbN28mMjLS4sYqtWvXfmbhF+LPYCS+CLVw4cLUqlULGxsbzpw5Y9wQwmQyUadOnWdWl0hWkGUD8NWrV3nrrbcYN26cRf++kJAQxo8fz6FDh7C1taVRo0b07dvXol9kZGQkkydPZuvWrURGRlK5cmU++ugji2GwRKyZyWSyuJod4k+rDxo0KJMqyrr+/vtvi/AL8Xe8y6pOnDhhMX42xN9ZsGHDhplUkfWJioqyuJ0wxPeb7d+//zOto0CBArz++utGt7CQkJBkz1y88847+n4Uq5MlA/CVK1fo27evcfFOgrt379KzZ088PDwYOXIkt27dYtKkSYSGhlqM5fjZZ59x/Phx+vXrh7OzMzNnzqRnz54sXbo0yRXwItYoX758FC5cmGvXruHo6EjZsmXp2rXrI28dbM3c3NyIjIzE29ubt956K119aZ+2MmXKkDt3bqKiosiXLx+NGjWiW7duGpD/GfL29sbLy4t///0XV1dXypcvT/fu3Z/4znMZYciQIVSsWJHff/+d06dPGxecubm5UbZsWdq2bZukb7eINchSfYDj4uJYt24dEyZMAOKvgp0+fbrxpTxnzhx++ukn1q5da4wruHv3bvr378+sWbOoVKkSR48epWvXrkycONEYt/LWrVu0bt2a999/nw8++CAzdk1EREREsogsNQrE6dOn+eqrr3jttdcsxrNM4O/vT+XKlS1uDODr64uzs7Mx5qq/vz85c+a0uN2iu7s7VapUSde4rCIiIiLyfMhSAdjLy4sVK1bw0UcfJTsMU3BwcJJbZ9ra2uLt7W3c/jU4OJiCBQsmuVVj4cKFk71FrIiIiIhYlyzVB9jNze2R4+6Fh4cne3cYJycnY/Dp1CzzpIKCgoznpnbgbxERERF5tqKjozGZTI+9DXWWCsCPk3gg+oclDEyfmmXSIqGrdEq3jhQRERGR7CFbBWAXFxfjNpaJRUREGHcVcnFx4d9//012mcRDpT2JsmXLcuzYMcxmM6VKlUrTOkRERETk6Tpz5kyqRr3JVgG4aNGihISEWEyLjY0lNDTUuHVp0aJFCQgIIC4uzqLFNyQkJN3jHJpMJpycnNK1DhERERF5OlI75GOWugjucXx9fTl48CC3bt0ypgUEBBAZGWmM+uDr60tERAT+/v7GMrdu3eLQoUMWI0OIiIiIiHXKVgG4ffv2ODg40Lt3b7Zt28bKlSsZNmwYtWrVomLFigBUqVKFqlWrMmzYMFauXMm2bdvo1asXrq6utG/fPpP3QEREREQyW7bqAuHu7s706dMZP348Q4cOxdnZmYYNGzJgwACL5caOHcv333/PxIkTiYuLo2LFinz11Ve6C5yIiIiIZK07wWVlx44dA+Cll17K5EpEREREJDmpzWvZqguEiIiIiEh6KQCLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlVyZHYBIomtWLGCRYsWERoaipeXFx06dODNN9/EZDIBcO3aNSZNmoS/vz8xMTG8+OKL9OvXj3Llyj1yvWvWrGH+/PlcunSJ/Pnz06FDB9566y1jvSIiImI9FIAly1i5ciVjxozhrbfeom7duhw6dIixY8fy4MED3n33XSIiIvDz88Pe3p5PP/0UBwcHZs2aRe/evVmyZAl58+ZNcb2jR4/mvffew9fXl+PHj/P9998TGRlJ165dn/FeioiISGZTAJYsY/Xq1VSqVIlBgwYBUKNGDc6fP8/SpUt59913WbRoEWFhYfz6669G2H3hhRfo1KkT+/fvp1mzZsmud86cOTRs2JB+/foZ671w4QJLlixRABYREbFCCsCSZdy/fz9JK66bmxthYWEAbNmyhYYNG1oskzdvXjZs2PDI9U6YMAEHBweLaXZ2djx48CCDKhcREZHsRBfBSZbx9ttvExAQwPr16wkPD8ff359169bRokULYmJiOHfuHEWLFmXatGk0bdqUmjVr0qNHD86ePfvI9RYvXhxvb2/MZjNhYWGsXLmSdevW0b59+2e0ZyIiIpKVqAVYsoymTZty4MABhg8fbkx7+eWXGThwIHfu3CE2NpZffvmFggULMmzYMB48eMD06dPp3r07ixcvJl++fI9c/7Fjx4wuDz4+Prz77rtPdX9EREQka1ILsGQZAwcOZMuWLfTr148ZM2YwaNAgTpw4weDBgy26K0yePJk6derQoEEDJk2aRGRkJEuXLn3s+gsUKMCMGTMYMWIEN27coGvXrty7d+9p7pKIiIhkQWoBlizhyJEj7Nmzh6FDh9K2bVsAqlatSsGCBRkwYACtWrUypjk5ORnP8/Lyonjx4gQFBT12G/ny5SNfvnzGert3784ff/xBy5Ytn8o+iYiISNakFmDJEi5fvgxAxYoVLaZXqVIFgODgYNzd3ZO9cC0mJibJRW4JIiMj2bhxIyEhIRbTE8YNvnHjRrprFxERkexFAViyhGLFigFw6NAhi+lHjhwBoFChQtSuXZt9+/Zx+/ZtY35wcDDnz5+nUqVKya7X1taWUaNGMW/ePIvpAQEBAJQqVSpjdkBERESyDXWBkCyhXLlyNGjQgO+//547d+5Qvnx5zp07x48//sgLL7xAvXr1KFeuHH/++Se9e/fGz8+P6Ohopk6dSv78+Y1uExB/sZu7uzuFChXCwcGBLl26MGPGDPLkyUO1atU4deoUM2fOpEaNGtSuXTvzdlpEREQyhclsNpszu4js4NixYwC89NJLmVzJ8ys6OpqffvqJ9evXc/36dby8vKhXrx5+fn5Gv99z584xefJkDhw4gI2NDTVr1uSjjz4if/78xnqqVatGy5YtGTlyJABms5nffvuNpUuXcunSJXLnzk2zZs3o3r17il0nREREJPtJbV5TAE4lBWARERGRrC21eU19gEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWArFafR77I0/f+IiIg8PboTnJWyMZlYHHCKa3ciM7sUeYhnLic6+pbJ7DJERESeWwrAVuzanUhCb0VkdhkiIiIiz5S6QIiIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiViVHZheQFitWrGDRokWEhobi5eVFhw4dePPNNzGZTACEhIQwfvx4Dh06hK2tLY0aNaJv3764uLhkcuUiIiIiktmyXQBeuXIlY8aM4a233qJu3bocOnSIsWPH8uDBA959913u3r1Lz5498fDwYOTIkdy6dYtJkyYRGhrK5MmTM7t8EREREclk2S4Ar169mkqVKjFo0CAAatSowfnz51m6dCnvvvsuv/76K2FhYSxcuJDcuXMD4OnpSf/+/Tl8+DCVKlXKvOJFREREJNNluz7A9+/fx9nZ2WKam5sbYWFhAPj7+1O5cmUj/AL4+vri7OzM7t27n2WpIiIiIpIFZbsA/PbbbxMQEMD69esJDw/H39+fdevW0aJFCwCCg4MpUqSIxXNsbW3x9vbm/PnzmVGyiIiIiGQh2a4LRNOmTTlw4ADDhw83pr388ssMHDgQgPDw8CQtxABOTk5ERESka9tms5nIyMh0rSMrMJlM5MyZM7PLkMeIiorCbDZndhkiIiLZhtlsNgZFeJRsF4AHDhzI4cOH6devHy+++CJnzpzhxx9/ZPDgwYwbN464uLgUn2tjk74G7+joaAIDA9O1jqwgZ86c+Pj4ZHYZ8hj//PMPUVFRmV2GiIhItmJvb//YZbJVAD5y5Ah79uxh6NChtG3bFoCqVatSsGBBBgwYwK5du3BxcUm2lTYiIgJPT890bd/Ozo5SpUqlax1ZQWp+GUnmK168uFqA5bEOHTpE//79U5zfpUsXunTpgr+/P3PmzCE4OBg3NzeaN29Op06dsLOzS/G5cXFxLFmyhNWrV3P9+nUKFy7M22+/TZMmTZ7GroiIpNuZM2dStVy2CsCXL18GoGLFihbTq1SpAsDZs2cpWrQoISEhFvNjY2MJDQ2lfv366dq+yWTCyckpXesQSS11U5HUqFixInPmzEkyfdq0afz999+0bNmSo0eP8umnn/Laa6/Rt29fgoOD+eGHHwgLC+Ozzz5Lcd1Tp05l3rx59OzZEx8fH3bv3s3o0aNxdHSkWbNmT3O3RETSJLWNfNkqABcrVgyIb/EoXry4Mf3IkSMAFCpUCF9fX+bNm8etW7dwd3cHICAggMjISHx9fZ95zSIiT5OLiwsvvfSSxbTt27ezb98+vv76a4oWLcqXX35JuXLlGDFiBAA1a9bk9u3bzJ49m48++ijZH1v37t1j0aJFvP3227z//vtA/LCTgYGBLFmyRAFYRLK1bBWAy5UrR4MGDfj++++5c+cO5cuX59y5c/z444+88MIL1KtXj6pVq7JkyRJ69+6Nn58fYWFhTJo0iVq1aiVpORYRed7cu3ePsWPHUqdOHRo1agTAsGHDiImJsVjOzs6OuLi4JNMTz589e7bRkJB4enh4+NMpXkTkGclWARhgzJgx/PTTTyxfvpwZM2bg5eVFq1at8PPzI0eOHLi7uzN9+nTGjx/P0KFDcXZ2pmHDhgwYMCCzSxcReeoWL17M9evXmTZtmjGtUKFCxt/h4eHs27ePBQsW0LRpU1xdXZNdj62tLaVLlwbir6r+999/WbNmDfv27WPIkCFPdydERJ6ybBeA7ezs6NmzJz179kxxmVKlSjF16tRnWJWISOaLjo5m0aJFNGnShMKFCyeZf+PGDaPrQsGCBenVq1eq1vv7778zdOhQAOrUqUPz5s0zrmgRkUyQ7W6EISIiyduyZQs3b96kU6dOyc53cHBg2rRpfP3119jb29OlSxeuXbv22PWWL1+eH3/8kUGDBnHkyBH69eunEUpEJFvLdi3AIiKSvC1btlCiRAnKlCmT7HxXV1eqV68OgI+PD23atGHVqlX4+fk9cr2FChWiUKFCVKlSBWdnZ0aOHMmhQ4eMEXhERLIbtQCLiDwHYmJi8Pf3p3HjxhbTY2Nj2bx5MydPnrSY7u3tTa5cubh+/Xqy67t16xZr167l33//tZherlw5gBSfJyKSHSgAi4g8B86cOcO9e/eSjHZja2vLlClTmDJlisX0kydPEhYWZlzo9rD79+8zcuRIVq1aZTE9ICAAIMXniYhkB+oCISLyHEi4+1GJEiWSzPPz82PkyJF89dVXNGzYkEuXLjFjxgxKlixJq1atAHjw4AFBQUF4enqSP39+vLy8aN26NbNmzSJHjhyULVuWQ4cOMXfuXNq0aZPsdkREsgsFYBGR58DNmzcBkh3WrGXLljg6OjJ37lzWrVuHk5MT9erVo0+fPjg6OgLxI0R06dIFPz8/evToAcCnn35KwYIFWbFiBZcvXyZ//vz06NEjxYvsRESyC5NZl/KmyrFjxwCS3HEpO5u06TChtyIyuwx5iLe7M/2aVMrsMkRERLKd1OY19QEWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARUSeQJxGjsyy9H8jIqmlG2GIiDwBG5OJxQGnuHYnMrNLkUQ8cznR0bdMZpchItmEArCIyBO6didSN5EREcnG1AVCRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErEq67gR38eJFrl69yq1bt8iRIwe5c+emRIkS5MqVK6PqExERERHJUE8cgI8fP86KFSsICAjg+vXryS5TpEgRXnnlFVq1akWJEiXSXaSIiIiISEZJdQA+fPgwkyZN4vjx4wCYzeYUlz1//jwXLlxg4cKFVKpUiQEDBuDj45P+akVERERE0ilVAXjMmDGsXr2auLg4AIoVK8ZLL71E6dKlyZcvH87OzgDcuXOH69evc/r0aU6ePMm5c+c4dOgQXbp0oUWLFowYMeLp7YmIiIiISCqkKgCvXLkST09P3njjDRo1akTRokVTtfKbN2/yxx9/sHz5ctatW6cALCIiIiKZLlUB+Ntvv6Vu3brY2DzZoBEeHh689dZbvPXWWwQEBKSpQBERERGRjJSqAFy/fv10b8jX1zfd6xARERERSa90DYMGEB4ezrRp09i1axc3b97E09OTZs2a0aVLF+zs7DKiRhERERGRDJPuAPzFF1+wbds243FISAizZs0iKiqK/v37p3f1IiIiIiIZKl0BODo6mu3bt9OgQQM6depE7ty5CQ8PZ9WqVfz+++8KwCIiIiKS5aTqqrYxY8Zw48aNJNPv379PXFwcJUqU4MUXX6RQoUKUK1eOF198kfv372d4sSIiIiIi6ZXqYdA2bNhAhw4deP/9941bHbu4uFC6dGl++uknFi5ciKurK5GRkURERFC3bt2nWriIiIiISFqkqgX4888/x8PDg/nz59OmTRvmzJnDvXv3jHnFihUjKiqKa9euER4eToUKFRg0aNBTLVxEREREJC1S1QLcokULmjRpwvLly5k9ezZTp05lyZIldOvWjddff50lS5Zw+fJl/v33Xzw9PfH09HzadYuIiIiIpEmq72yRI0cOOnTowMqVK/nvf//LgwcP+Pbbb2nfvj2///473t7elC9fXuFXRERERLK0J7u1G+Do6EjXrl1ZtWoVnTp14vr16wwfPpx33nmH3bt3P40aRUREREQyTKoD8M2bN1m3bh3z58/n999/x2Qy0bdvX1auXMnrr7/OP//8w4cffkj37t05evTo06xZRERERCTNUtUHeP/+/QwcOJCoqChjmru7OzNmzKBYsWJ8+umndOrUiWnTprF582a6detGnTp1GD9+/FMrXEREREQkLVLVAjxp0iRy5MhB7dq1adq0KXXr1iVHjhxMnTrVWKZQoUKMGTOGBQsW8PLLL7Nr166nVrSIiIiISFqlqgU4ODiYSZMmUalSJWPa3bt36datW5Jly5Qpw8SJEzl8+HBG1SgiIiIikmFSFYC9vLwYNWoUtWrVwsXFhaioKA4fPkyBAgVSfE7isCwiIiIiklWkKgB37dqVESNGsHjxYkwmE2azGTs7O4suECIiIiIi2UGqAnCzZs0oXrw427dvN2520aRJEwoVKvS06xMRERERyVCpCsAAZcuWpWzZsk+zFhERERGRpy5Vo0AMHDiQffv2pXkjJ06cYOjQoWl+/sOOHTtGjx49qFOnDk2aNGHEiBH8+++/xvyQkBA+/PBD6tWrR8OGDfnqq68IDw/PsO2LiIiISPaVqhbgnTt3snPnTgoVKkTDhg2pV68eL7zwAjY2yefnmJgYjhw5wr59+9i5cydnzpwBYPTo0ekuODAwkJ49e1KjRg3GjRvH9evXmTJlCiEhIcyePZu7d+/Ss2dPPDw8GDlyJLdu3WLSpEmEhoYyefLkdG9fRERERLK3VAXgmTNn8s0333D69Gnmzp3L3LlzsbOzo3jx4uTLlw9nZ2dMJhORkZFcuXKFCxcucP/+fQDMZjPlypVj4MCBGVLwpEmTKFu2LN99950RwJ2dnfnuu++4dOkSmzZtIiwsjIULF5I7d24APD096d+/P4cPH9boFCIiIiJWLlUBuGLFiixYsIAtW7Ywf/58AgMDefDgAUFBQZw6dcpiWbPZDIDJZKJGjRq0a9eOevXqYTKZ0l3s7du3OXDgACNHjrRofW7QoAENGjQAwN/fn8qVKxvhF8DX1xdnZ2d2796tACwiIiJi5VJ9EZyNjQ2NGzemcePGhIaGsmfPHo4cOcL169eN/rd58uShUKFCVKpUierVq5M/f/4MLfbMmTPExcXh7u7O0KFD2bFjB2azmfr16zNo0CBcXV0JDg6mcePGFs+ztbXF29ub8+fPp2v7ZrOZyMjIdK0jKzCZTOTMmTOzy5DHiIqKMn5QStagYyfr03EjYt3MZnOqGl1THYAT8/b2pn379rRv3z4tT0+zW7duAfDFF19Qq1Ytxo0bx4ULF/jhhx+4dOkSs2bNIjw8HGdn5yTPdXJyIiIiIl3bj46OJjAwMF3ryApy5syJj49PZpchj/HPP/8QFRWV2WVIIjp2sj4dNyJib2//2GXSFIAzS3R0NADlypVj2LBhANSoUQNXV1c+++wz9u7dS1xcXIrPT+mivdSys7OjVKlS6VpHVpAR3VHk6StevLhasrIYHTtZn44bEeuWMPDC42SrAOzk5ATAK6+8YjG9Vq1aAJw8eRIXF5dkuylERETg6emZru2bTCajBpGnTafaRZ6cjhsR65bahor0NYk+Y0WKFAHgwYMHFtNjYmIAcHR0pGjRooSEhFjMj42NJTQ0lGLFij2TOkVEREQk68pWAbh48eJ4e3uzadMmi1Nc27dvB6BSpUr4+vpy8OBBo78wQEBAAJGRkfj6+j7zmkVEREQka8lWAdhkMtGvXz+OHTvGkCFD2Lt3L4sXL2b8+PE0aNCAcuXK0b59exwcHOjduzfbtm1j5cqVDBs2jFq1alGxYsXM3gURERERyWRp6gN8/Phxypcvn9G1pEqjRo1wcHBg5syZfPjhh+TKlYt27drx3//+FwB3d3emT5/O+PHjGTp0KM7OzjRs2JABAwZkSr0iIiIikrWkKQB36dKF4sWL89prr9GiRQvy5cuX0XU90iuvvJLkQrjESpUqxdSpU59hRSIiIiKSXaS5C0RwcDA//PADLVu2pE+fPvz+++/G7Y9FRERERLKqNLUAd+7cmS1btnDx4kXMZjP79u1j3759ODk50bhxY1577TXdclhEREREsqQ0BeA+ffrQp08fgoKC+OOPP9iyZQshISFERESwatUqVq1ahbe3Ny1btqRly5Z4eXlldN0iIiIiImmSrlEgypYtS+/evVm+fDkLFy6kTZs2mM1mzGYzoaGh/Pjjj7Rt25axY8c+8g5tIiIiIiLPSrrvBHf37l22bNnC5s2bOXDgACaTyQjBEH8TimXLlpErVy569OiR7oJFRERERNIjTQE4MjKSP//8k02bNrFv3z7jTmxmsxkbGxtq1qxJ69atMZlMTJ48mdDQUDZu3KgALCIiIiKZLk0BuHHjxkRHRwMYLb3e3t60atUqSZ9fT09PPvjgA65du5YB5YqIiIiIpE+aAvCDBw8AsLe3p0GDBrRp04Zq1aolu6y3tzcArq6uaSxRRERERCTjpCkAv/DCC7Ru3ZpmzZrh4uLyyGVz5szJDz/8QMGCBdNUoIiIiIhIRkpTAJ43bx4Q3xc4OjoaOzs7AM6fP0/evHlxdnY2lnV2dqZGjRoZUKqIiIiISPqleRi0VatW0bJlS44dO2ZMW7BgAc2bN2f16tUZUpyIiIiISEZLUwDevXs3o0ePJjw8nDNnzhjTg4ODiYqKYvTo0ezbty/DihQRERERyShpCsALFy4EoECBApQsWdKY/p///IfChQtjNpuZP39+xlQoIiIiIpKB0tQH+OzZs5hMJoYPH07VqlWN6fXq1cPNzY3u3btz+vTpDCtSRERERCSjpKkFODw8HAB3d/ck8xKGO7t79246yhIREREReTrSFIDz588PwPLlyy2mm81mFi9ebLGMiIiIiEhWkqYuEPXq1WP+/PksXbqUgIAASpcuTUxMDKdOneLy5cuYTCbq1q2b0bWKiIiIiKRbmgJw165d+fPPPwkJCeHChQtcuHDBmGc2mylcuDAffPBBhhUpIiIi8jQNGjSIkydPsmbNGmPaBx98wJEjR5IsO2/ePHx8fJJdz/3793n11VeJjY21mJ4zZ0527tyZsUVLmqUpALu4uDBnzhymTJnCli1bjP6+Li4uNGrUiN69ez/2DnEiIiIiWcH69evZtm0bBQoUMKaZzWbOnDnDf/7zHxo1amSxfPHixVNc19mzZ4mNjWXUqFEUKlTImG5jk+ZbL8hTkKYADODm5sZnn33GkCFDuH37NmazGXd3d0wmU0bWJyIiIvLUXL9+nXHjxiW5dunixYtERERQu3ZtXnrppVSv79SpU9ja2tKwYUPs7e0zulzJIOn+OWIymXB3dydPnjxG+I2Li2PPnj3pLk5ERETkaRo1ahQ1a9akevXqFtODgoIAKFOmzBOtLygoiGLFiin8ZnFpagE2m83Mnj2bHTt2cOfOHeLi4ox5MTEx3L59m5iYGPbu3ZthhYqIiIhkpJUrV3Ly5EmWLl3KhAkTLOadOnUKJycnJk6cyI4dO4iKiqJatWp89NFHFCtWLMV1JrQA9+7dmyNHjmBvb0/Dhg0ZMGAAzs7OT3eHJNXSFICXLFnC9OnTMZlMmM1mi3kJ09QVQkRERLKqy5cv8/333zN8+HBy586dZP6pU6eIjIzE1dWVcePGcfnyZWbOnImfnx+//PIL+fLlS/KchH7DZrOZtm3b8sEHH3DixAlmzpzJP//8w48//qi+wFlEmgLwunXrgPgrGj08PLh48SI+Pj5ERkbyzz//YDKZGDx4cIYWKiIiIpIRzGYzX3zxBbVq1aJhw4bJLtOrVy/ee+89qlSpAkDlypWpUKECb775JosWLaJfv37Jrve7777D3d2dkiVLAlClShU8PDwYNmwY/v7+1K5d++ntmKRamn6GXLx4EZPJxDfffMNXX32F2WymR48eLF26lHfeeQez2UxwcHAGlyoiIiKSfkuXLuX06dMMHDiQmJgYYmJijDPaMTExxMXFUaZMGSP8JihUqBDFixfn9OnTya7XxsaGatWqGeE3QZ06dQBSfJ48e2kKwPfv3wegSJEilClTBicnJ44fPw7A66+/DsDu3bszqEQRERGRjLNlyxZu375Ns2bN8PX1xdfXl3Xr1nH58mV8fX2ZPn06a9eu5ejRo0mee+/evWS7TED8iBIrVqzgypUrFtMTclNKz5NnL01dIPLkycO1a9cICgrC29ub0qVLs3v3bvz8/Lh48SIA165dy9BCRURERDLCkCFDiIyMtJg2c+ZMAgMDGT9+PPny5aNbt27kzZuXn376yVjm5MmTXLx4kc6dOye73tjYWMaMGUOXLl3o3bu3MX3Tpk3Y2tpSuXLlp7ND8sTSFIArVqzIpk2bGDZsGIsWLaJy5crMnTuXDh06GL968uTJk6GFioiIiGSE5EZxcHNzw87OzrjDm5+fHyNHjmT48OG0aNGCK1euMH36dMqUKUPLli0BePDgAUFBQXh6epI/f368vLxo1aoV8+fPx8HBgQoVKnD48GHmzJlDhw4dKFq06LPcTXmENAXgbt26ERAQQHh4OPny5aNp06bMmzeP4OBgYwSIh++aIiIiIpJdtGzZEgcHB+bNm8fHH39Mzpw5qVevHn369MHW1haAGzdu0KVLF/z8/OjRowcAn376KQULFmT9+vXMnj0bT09PevTowXvvvZeZuyMPMZkfHscslUJDQ1m/fj3dunUD4m8jOG3aNCIjI2nQoAEff/wxDg4OGVpsZjp27BjAE90NJqubtOkwobciMrsMeYi3uzP9mlTK7DLkEXTsZD06bkQEUp/X0tQCvHv3bipUqGCEX4AWLVrQokWLtKxOREREROSZSdMoEMOHD6dZs2bs2LEjo+sREREREXmq0hSA7927R3R09CNvBSgiIiIikhWlKQAn3DVl27ZtGVqMiIiIiMjTlqY+wGXKlGHXrl388MMPLF++nBIlSuDi4kKOHP+/OpPJxPDhwzOsUBERERGRjJCmADxx4kRMJhMAly9f5vLly8kupwAsIiIiIllNmgIwwONGT0sIyCIiIiIiWUmaAvDq1aszug4RERF5jsWZzdiocSxLssb/mzQF4AIFCmR0HSIiIvIcszGZWBxwimt3IjO7FEnEM5cTHX3LZHYZz1yaAvDBgwdTtVyVKlXSsnoRERF5Dl27E6m7KEqWkKYA3KNHj8f28TWZTOzduzdNRYmIiIiIPC1P7SI4EREREZGsKE0B2M/Pz+Kx2WzmwYMHXLlyhW3btlGuXDm6du2aIQWKiIiIiGSkNAXg7t27pzjvjz/+YMiQIdy9ezfNRYmIiIiIPC1puhXyozRo0ACARYsWZfSqRURERETSLcMD8F9//YXZbObs2bMZvWoRERERkXRLUxeInj17JpkWFxdHeHg4586dAyBPnjzpq0xERERE5ClIUwA+cOBAisOgJYwO0bJly7RXJSIiIiLylGToMGh2dnbky5ePpk2b0q1bt3QVllqDBg3i5MmTrFmzxpgWEhLC+PHjOXToELa2tjRq1Ii+ffvi4uLyTGoSERERkawrTQH4r7/+yug60mT9+vVs27bN4tbMd+/epWfPnnh4eDBy5Ehu3brFpEmTCA0NZfLkyZlYrYiIiIhkBWluAU5OdHQ0dnZ2GbnKFF2/fp1x48aRP39+i+m//vorYWFhLFy4kNy5cwPg6elJ//79OXz4MJUqVXom9YmIiIhI1pTmUSCCgoLo1asXJ0+eNKZNmjSJbt26cfr06Qwp7lFGjRpFzZo1qV69usV0f39/KleubIRfAF9fX5ydndm9e/dTr0tEREREsrY0BeBz587Ro0cP9u/fbxF2g4ODOXLkCN27dyc4ODijakxi5cqVnDx5ksGDByeZFxwcTJEiRSym2dra4u3tzfnz559aTSIiIiKSPaSpC8Ts2bOJiIjA3t7eYjSIF154gYMHDxIREcHPP//MyJEjM6pOw+XLl/n+++8ZPny4RStvgvDwcJydnZNMd3JyIiIiIl3bNpvNREZGpmsdWYHJZCJnzpyZXYY8RlRUVLIXm0rm0bGT9em4yZp07GR9z8uxYzabUxypLLE0BeDDhw9jMpkYOnQozZs3N6b36tWLUqVK8dlnn3Ho0KG0rPqRzGYzX3zxBbVq1aJhw4bJLhMXF5fi821s0nffj+joaAIDA9O1jqwgZ86c+Pj4ZHYZ8hj//PMPUVFRmV2GJKJjJ+vTcZM16djJ+p6nY8fe3v6xy6QpAP/7778AlC9fPsm8smXLAnDjxo20rPqRli5dyunTp1m8eDExMTHA/w/HFhMTg42NDS4uLsm20kZERODp6Zmu7dvZ2VGqVKl0rSMrSM0vI8l8xYsXfy5+jT9PdOxkfTpusiYdO1nf83LsnDlzJlXLpSkAu7m5cfPmTf766y8KFy5sMW/Pnj0AuLq6pmXVj7RlyxZu375Ns2bNkszz9fXFz8+PokWLEhISYjEvNjaW0NBQ6tevn67tm0wmnJyc0rUOkdTS6UKRJ6fjRiRtnpdjJ7U/ttIUgKtVq8bGjRv57rvvCAwMpGzZssTExHDixAk2b96MyWRKMjpDRhgyZEiS1t2ZM2cSGBjI+PHjyZcvHzY2NsybN49bt27h7u4OQEBAAJGRkfj6+mZ4TSIiIiKSvaQpAHfr1o0dO3YQFRXFqlWrLOaZzWZy5szJBx98kCEFJlasWLEk09zc3LCzszP6FrVv354lS5bQu3dv/Pz8CAsLY9KkSdSqVYuKFStmeE0iIiIikr2k6aqwokWLMnnyZIoUKYLZbLb4V6RIESZPnpxsWH0W3N3dmT59Orlz52bo0KFMnTqVhg0b8tVXX2VKPSIiIiKStaT5TnAVKlTg119/JSgoiJCQEMxmM4ULF6Zs2bLPtLN7ckOtlSpViqlTpz6zGkREREQk+0jXrZAjIyMpUaKEMfLD+fPniYyMTHYcXhERERGRrCDNA+OuWrWKli1bcuzYMWPaggULaN68OatXr86Q4kREREREMlqaAvDu3bsZPXo04eHhFuOtBQcHExUVxejRo9m3b1+GFSkiIiIiklHSFIAXLlwIQIECBShZsqQx/T//+Q+FCxfGbDYzf/78jKlQRERERCQDpakP8NmzZzGZTAwfPpyqVasa0+vVq4ebmxvdu3fn9OnTGVakiIiIiEhGSVMLcHh4OIBxo4nEEu4Ad/fu3XSUJSIiIiLydKQpAOfPnx+A5cuXW0w3m80sXrzYYhkRERERkawkTV0g6tWrx/z581m6dCkBAQGULl2amJgYTp06xeXLlzGZTNStWzejaxURERERSbc0BeCuXbvy559/EhISwoULF7hw4YIxL+GGGE/jVsgiIiIiIumVpi4QLi4uzJkzh7Zt2+Li4mLcBtnZ2Zm2bdsye/ZsXFxcMrpWEREREZF0S/Od4Nzc3Pjss88YMmQIt2/fxmw24+7u/kxvgywiIiIi8qTSfCe4BCaTCXd3d/LkyYPJZCIqKooVK1bw3nvvZUR9IiIiIiIZKs0twA8LDAxk+fLlbNq0iaioqIxarYiIiIhIhkpXAI6MjGTDhg2sXLmSoKAgY7rZbFZXCBERERHJktIUgP/++29WrFjB5s2bjdZes9kMgK2tLXXr1qVdu3YZV6WIiIiISAZJdQCOiIhgw4YNrFixwrjNcULoTWAymVi7di158+bN2CpFRERERDJIqgLwF198wR9//MG9e/csQq+TkxMNGjTAy8uLWbNmASj8ioiIiEiWlqoAvGbNGkwmE2azmRw5cuDr60vz5s2pW7cuDg4O+Pv7P+06RUREREQyxBMNg2YymfD09KR8+fL4+Pjg4ODwtOoSEREREXkqUtUCXKlSJQ4fPgzA5cuXmTFjBjNmzMDHx4dmzZrprm8iIiIikm2kKgDPnDmTCxcusHLlStavX8/NmzcBOHHiBCdOnLBYNjY2Fltb24yvVEREREQkA6S6C0SRIkXo168f69atY+zYsdSpU8foF5x43N9mzZoxYcIEzp49+9SKFhERERFJqyceB9jW1pZ69epRr149bty4werVq1mzZg0XL14EICwsjF9++YVFixaxd+/eDC9YRERERCQ9nugiuIflzZuXrl27smLFCqZNm0azZs2ws7MzWoVFRERERLKadN0KObFq1apRrVo1Bg8ezPr161m9enVGrVpEREREJMNkWABO4OLiQocOHejQoUNGr1pEREREJN3S1QVCRERERCS7UQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlVyZHYBTyouLo7ly5fz66+/cunSJfLkycOrr75Kjx49cHFxASAkJITx48dz6NAhbG1tadSoEX379jXmi4iIiIj1ynYBeN68eUybNo1OnTpRvXp1Lly4wPTp0zl79iw//PAD4eHh9OzZEw8PD0aOHMmtW7eYNGkSoaGhTJ48ObPLFxEREZFMlq0CcFxcHHPnzuWNN96gT58+ANSsWRM3NzeGDBlCYGAge/fuJSwsjIULF5I7d24APD096d+/P4cPH6ZSpUqZtwMiIiIikumyVR/giIgIWrRoQdOmTS2mFytWDICLFy/i7+9P5cqVjfAL4Ovri7OzM7t3736G1YqIiIhIVpStWoBdXV0ZNGhQkul//vknACVKlCA4OJjGjRtbzLe1tcXb25vz588/izJFREREJAvLVgE4OcePH2fu3Lm88sorlCpVivDwcJydnZMs5+TkRERERLq2ZTabiYyMTNc6sgKTyUTOnDkzuwx5jKioKMxmc2aXIYno2Mn6dNxkTTp2sr7n5dgxm82YTKbHLpetA/Dhw4f58MMP8fb2ZsSIEUB8P+GU2Nikr8dHdHQ0gYGB6VpHVpAzZ058fHwyuwx5jH/++YeoqKjMLkMS0bGT9em4yZp07GR9z9OxY29v/9hlsm0A3rRpE59//jlFihRh8uTJRp9fFxeXZFtpIyIi8PT0TNc27ezsKFWqVLrWkRWk5peRZL7ixYs/F7/Gnyc6drI+HTdZk46drO95OXbOnDmTquWyZQCeP38+kyZNomrVqowbN85ifN+iRYsSEhJisXxsbCyhoaHUr18/Xds1mUw4OTmlax0iqaXThSJPTseNSNo8L8dOan9sZatRIAB+++03Jk6cSKNGjZg8eXKSm1v4+vpy8OBBbt26ZUwLCAggMjISX1/fZ12uiIiIiGQx2aoF+MaNG4wfPx5vb2/eeustTp48aTG/UKFCtG/fniVLltC7d2/8/PwICwtj0qRJ1KpVi4oVK2ZS5SIiIiKSVWSrALx7927u379PaGgo3bp1SzJ/xIgRtGrViunTpzN+/HiGDh2Ks7MzDRs2ZMCAAc++YBERERHJcrJVAG7Tpg1t2rR57HKlSpVi6tSpz6AiEREREclusl0fYBERERGR9FAAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKo81wE4ICCA9957j9q1a9O6dWvmz5+P2WzO7LJEREREJBM9twH42LFjDBgwgKJFizJ27FiaNWvGpEmTmDt3bmaXJiIiIiKZKEdmF/C0zJgxg7JlyzJq1CgAatWqRUxMDHPmzKFjx444OjpmcoUiIiIikhmeyxbgBw8ecODAAerXr28xvWHDhkRERHD48OHMKUxEREREMt1zGYAvXbpEdHQ0RYoUsZheuHBhAM6fP58ZZYmIiIhIFvBcdoEIDw8HwNnZ2WK6k5MTABEREU+0vqCgIB48eADA0aNHM6DCzGcymaiRJ47Y3OoKktXY2sRx7NgxXbCZRenYyZp03GR9Onaypuft2ImOjsZkMj12uecyAMfFxT1yvo3Nkzd8J7yYqXlRswtnB7vMLkEe4Xl6rz1vdOxkXTpusjYdO1nX83LsmEwm6w3ALi4uAERGRlpMT2j5TZifWmXLls2YwkREREQk0z2XfYALFSqEra0tISEhFtMTHhcrViwTqhIRERGRrOC5DMAODg5UrlyZbdu2WfRp2bp1Ky4uLpQvXz4TqxMRERGRzPRcBmCADz74gOPHj/PJJ5+we/dupk2bxvz58+nSpYvGABYRERGxYibz83LZXzK2bdvGjBkzOH/+PJ6enrz55pu8++67mV2WiIiIiGSi5zoAi4iIiIg87LntAiEiIiIikhwFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsVk8jAcrzLrn3uN73ImLNFIAlWwoNDaVatWqsWbMmzc+5e/cuw4cP59ChQ0+rTJGnolWrVowcOTLZeTNmzKBatWrG48OHD9O/f3+LZWbNmsX8+fOfZokiViUt30mSuRSAxWoFBQWxfv164uLiMrsUkQzTtm1b5syZYzxeuXIl//zzj8Uy06dPJyoq6lmXJvLcyps3L3PmzKFOnTqZXYqkUo7MLkBERDJO/vz5yZ8/f2aXIWJV7O3teemllzK7DHkCagGWTHfv3j2mTJnC66+/zssvv0zdunXp1asXQUFBxjJbt27l7bffpnbt2vznP//h1KlTFutYs2YN1apVIzQ01GJ6SqeK9+/fT8+ePQHo2bMn3bt3z/gdE3lGVq1aRfXq1Zk1a5ZFF4iRI0eydu1aLl++bJyeTZg3c+ZMi64SZ86cYcCAAdStW5e6devy8ccfc/HiRWP+/v37qVatGvv27aN3797Url2bpk2bMmnSJGJjY5/tDos8gcDAQP773/9St25dXn31VXr16sWxY8eM+YcOHaJ79+7Url2bBg0aMGLECG7dumXMX7NmDTVr1uT48eN06dKFWrVq0bJlS4tuRMl1gbhw4QL/+9//aNq0KXXq1KFHjx4cPnw4yXMWLFhAu3btqF27NqtXr366L4YYFIAl040YMYLVq1fz/vvvM2XKFD788EPOnTvH0KFDMZvN7Nixg8GDB1OqVCnGjRtH48aNGTZsWLq2Wa5cOQYPHgzA4MGD+eSTTzJiV0SeuU2bNjFmzBi6detGt27dLOZ169aN2rVr4+HhYZyeTege0aZNG+Pv8+fP88EHH/Dvv/8ycuRIhg0bxqVLl4xpiQ0bNozKlSszYcIEmjZtyrx581i5cuUz2VeRJxUeHk7fvn3JnTs33377LV9++SVRUVH06dOH8PBwDh48yH//+18cHR35+uuv+eijjzhw4AA9evTg3r17xnri4uL45JNPaNKkCRMnTqRSpUpMnDgRf3//ZLd77tw5OnXqxOXLlxk0aBCjR4/GZDLRs2dPDhw4YLHszJkz6dy5M1988QU1a9Z8qq+H/D91gZBMFR0dTWRkJIMGDaJx48YAVK1alfDwcCZMmMDNmzeZNWsWL774IqNGjQLg5ZdfBmDKlClp3q6LiwvFixcHoHjx4pQoUSKdeyLy7O3cuZPhw4fz/vvv06NHjyTzCxUqhLu7u8XpWXd3dwA8PT2NaTNnzsTR0ZGpU6fi4uICQPXq1WnTpg3z58+3uIiubdu2RtCuXr0627dvZ9euXbRr1+6p7qtIWvzzzz/cvn2bjh07UrFiRQCKFSvG8uXLiYiIYMqUKRQtWpTvv/8eW1tbAF566SU6dOjA6tWr6dChAxA/akq3bt1o27YtABUrVmTbtm3s3LnT+E5KbObMmdjZ2TF9+nScnZ0BqFOnDm+99RYTJ05k3rx5xrKNGjWidevWT/NlkGSoBVgylZ2dHZMnT6Zx48Zcu3aN/fv389tvv7Fr1y4gPiAHBgbyyiuvWDwvISyLWKvAwEA++eQTPD09je48afXXX39RpUoVHB0diYmJISYmBmdnZypXrszevXstln24n6Onp6cuqJMsq2TJkri7u/Phhx/y5Zdfsm3bNjw8POjXrx9ubm4cP36cOnXqYDabjfd+wYIFKVasWJL3foUKFYy/7e3tyZ07d4rv/QMHDvDKK68Y4RcgR44cNGnShMDAQCIjI43pZcqUyeC9ltRQC7BkOn9/f7777juCg4NxdnamdOnSODk5AXDt2jXMZjO5c+e2eE7evHkzoVKRrOPs2bPUqVOHXbt2sXTpUjp27Jjmdd2+fZvNmzezefPmJPMSWowTODo6Wjw2mUwaSUWyLCcnJ2bOnMlPP/3E5s2bWb58OQ4ODrz22mt06dKFuLg45s6dy9y5c5M818HBweLxw+99GxubFMfTDgsLw8PDI8l0Dw8PzGYzERERFjXKs6cALJnq4sWLfPzxx9StW5cJEyZQsGBBTCYTy5YtY8+ePbi5uWFjY5OkH2JYWJjFY5PJBJDkizjxr2yR50mtWrWYMGECn376KVOnTqVevXp4eXmlaV2urq7UqFGDd999N8m8hNPCItlVsWLFGDVqFLGxsfz999+sX7+eX3/9FU9PT0wmE++88w5NmzZN8ryHA++TcHNz4+bNm0mmJ0xzc3Pjxo0baV6/pJ+6QEimCgwM5P79+7z//vsUKlTICLJ79uwB4k8ZVahQga1bt1r80t6xY4fFehJOM129etWYFhwcnCQoJ6YvdsnO8uTJA8DAgQOxsbHh66+/TnY5G5ukH/MPT6tSpQr//PMPZcqUwcfHBx8fH1544QUWLlzIn3/+meG1izwrf/zxB40aNeLGjRvY2tpSoUIFPvnkE1xdXbl58yblypUjODjYeN/7+PhQokQJZsyYkeRitSdRpUoVdu7cadHSGxsby++//46Pjw/29vYZsXuSDgrAkqnKlSuHra0tkydPJiAggJ07dzJo0CCjD/C9e/fo3bs3586dY9CgQezZs4dFixYxY8YMi/VUq1YNBwcHJkyYwO7du9m0aRMDBw7Ezc0txW27uroCsHv37iTDqolkF3nz5qV3797s2rWLjRs3Jpnv6urKv//+y+7du40WJ1dXV44cOcLBgwcxm834+fkREhLChx9+yJ9//om/vz//+9//2LRpE6VLl37WuySSYSpVqkRcXBwff/wxf/75J3/99RdjxowhPDychg0b0rt3bwICAhg6dCi7du1ix44d9OvXj7/++oty5cqlebt+fn7cv3+fnj178scff7B9+3b69u3LpUuX6N27dwbuoaSVArBkqsKFCzNmzBiuXr3KwIED+fLLL4H427maTCYOHTpE5cqVmTRpEteuXWPQoEEsX76c4cOHW6zH1dWVsWPHEhsby8cff8z06dPx8/PDx8cnxW2XKFGCpk2bsnTpUoYOHfpU91PkaWrXrh0vvvgi3333XZKzHq1ataJAgQIMHDiQtWvXAtClSxcCAwPp168fV69epXTp0syaNQuTycSIESMYPHgwN27cYNy4cTRo0CAzdkkkQ+TNm5fJkyfj4uLCqFGjGDBgAEFBQXz77bdUq1YNX19fJk+ezNWrVxk8eDDDhw/H1taWqVOnpuvGFiVLlmTWrFm4u7vzxRdfGN9ZM2bM0FBnWYTJnFIPbhERERGR55BagEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSo5MrsAEZHngZ+fH4cOHQLibz4xYsSITK4oqTNnzvDbb7+xb98+bty4wYMHD3B3d+eFF16gdevW1K1bN7NLFBF5JnQjDBGRdDp//jzt2rUzHjs6OrJx40ZcXFwysSpLP//8M9OnTycmJibFZZo3b87nn3+OjY1ODorI802fciIi6bRq1SqLx/fu3WP9+vWZVE1SS5cuZcqUKcTExJA/f36GDBnCsmXLWLx4MQMGDMDZ2RmADRs28Msvv2RytSIiT59agEVE0iEmJobXXnuNmzdv4u3tzdWrV4mNjaVMmTJZIkzeuHGDVq1aER0dTf78+Zk3bx4eHh4Wy+zevZv+/fsDkC9fPtavX4/JZMqMckVEngn1ARYRSYddu3Zx8+ZNAFq3bs3x48fZtWsXp06d4vjx45QvXz7Jc0JDQ5kyZQoBAQFER0dTuXJlPvroI7788ksOHjxIlSpV+PHHH43lg4ODmTFjBn/99ReRkZEUKFCA5s2b06lTJxwcHB5Z39q1a4mOjgagW7duScIvQO3atRkwYADe3t74+PgY4XfNmjV8/vnnAIwfP565c+dy4sQJ3N3dmT9/Ph4eHkRHR7N48WI2btxISEgIACVLlqRt27a0bt3aIkh3796dgwcPArB//35j+v79++nZsycQ35e6R48eFsuXKVOGb775hokTJ/LXX39hMpl4+eWX6du3L97e3o/cfxGR5CgAi4ikQ+LuD02bNqVw4cLs2rULgOXLlycJwJcvX6Zz587cunXLmLZnzx5OnDiRbJ/hv//+m169ehEREWFMO3/+PNOnT2ffvn1MnTqVHDlS/ihPCJwAvr6+KS737rvvPmIvYcSIEdy9excADw8PPDw8iIyMpHv37pw8edJi2WPHjnHs2DF2797NV199ha2t7SPX/Ti3bt2iS5cu3L5925i2efNmDh48yNy5c/Hy8krX+kXE+qgPsIhIGl2/fp09e/YA4OPjQ+HChalbt67Rp3bz5s2Eh4dbPGfKlClG+G3evDmLFi1i2rRp5MmTh4sXL1osazab+eKLL4iIiCB37tyMHTuW3377jUGDBmFjY8PBgwdZsmTJI2u8evWq8Xe+fPks5t24cYOrV68m+ffgwYMk64mOjmb8+PH88ssvfPTRRwBMmDDBCL9NmjRhwYIFzJ49m5o1awKwdetW5s+f/+gXMRWuX79Orly5mDJlCosWLaJ58+YA3Lx5k8mTJ6d7/SJifRSARUTSaM2aNcTGxgLQrFkzIH4EiPr16wMQFRXFxo0bjeXj4uKM1uH8+fMzYsQISpcuTfXq1RkzZkyS9Z8+fZqzZ88C0LJlS3x8fHB0dKRevXpUqVIFgHXr1j2yxsQjOjw8AsR7773Ha6+9luTf0aNHk6ynUaNGvPrqq5QpU4bKlSsTERFhbLtkyZKMGjWKcuXKUaFCBcaNG2d0tXhcQE+tYcOG4evrS+nSpRkxYgQFChQAYOfOncb/gYhIaikAi4ikgdlsZvXq1cZjFxcX9uzZw549eyxOya9YscL4+9atW0ZXBh8fH4uuC6VLlzZajhNcuHDB+HvBggUWITWhD+3Zs2eTbbFNkD9/fuPv0NDQJ91NQ8mSJZPUdv/+fQCqVatm0c0hZ86cVKhQAYhvvU3cdSEtTCaTRVeSHDly4OPjA0BkZGS61y8i1kd9gEVE0uDAgQMWXRa++OKLZJcLCgri77//5sUXX8TOzs6YnpoBeFLTdzY2NpY7d+6QN2/eZOfXqFHDaHXetWsXJUqUMOYlHqpt5MiRrF27NsXtPNw/+XG1PW7/YmNjjXUkBOlHrSsmJibF108jVojIk1ILsIhIGjw89u+jJLQC58qVC1dXVwACAwMtuiScPHnS4kI3gMKFCxt/9+rVi/379xv/FixYwMaNG9m/f3+K4Rfi++Y6OjoCMHfu3BRbgR/e9sMevtCuYMGC2NvbA/GjOMTFxRnzoqKiOHbsGBDfAp07d24AY/mHt3flypVHbhvif3AkiI2NJSgoCIgP5gnrFxFJLQVgEZEndPfuXbZu3QqAm5sb/v7+FuF0//79bNy40Wjh3LRpkxH4mjZtCsRfnPb5559z5swZAgIC+Oyzz5Jsp2TJkpQpUwaI7wLx+++/c/HiRdavX0/nzp1p1qwZgwYNemStefPm5cMPPwQgLCyMLl26sGzZMoKDgwkODmbjxo306NGDbdu2PdFr4OzsTMOGDYH4bhjDhw/n5MmTHDt2jP/973/G0HAdOnQwnpP4IrxFixYRFxdHUFAQc+fOfez2vv76a3bu3MmZM2f4+uuvuXTpEgD16tXTnetE5ImpC4SIyBPasGGDcdq+RYsWFqfmE+TNm5e6deuydetWIiMj2bhxI+3ataNr165s27aNmzdvsmHDBjZs2ACAl5cXOXPmJCoqyjilbzKZGDhwIP369ePOnTtJQrKbm5sxZu6jtGvXjujoaCZOnMjNmzf55ptvkl3O1taWNm3aGP1rH2fQoEGcOnWKs2fPsnHjRosL/gAaNGhgMbxa06ZNWbNmDQAzZ85k1qxZmM1mXnrppcf2TzabzUaQT5AvXz769OmTqlpFRBLTz2YRkSeUuPtDmzZtUlyuXbt2xt8J3SA8PT356aefqF+/Ps7Ozjg7O9OgQQNmzZpldBFI3FWgatWq/PzzzzRu3BgPDw/s7OzInz8/rVq14ueff6ZUqVKpqrljx44sW7aMLl26ULZsWdzc3LCzsyNv3rzUqFGDPn36sGbNGoYMGYKTk1Oq1pkrVy7mz59P//79eeGFF3BycsLR0ZHy5cszdOhQvvnmG4u+wr6+vowaNYqSJUtib29PgQIF8PPz4/vvv3/sthJes5w5c+Li4kKTJk2YM2fOI7t/iIikRLdCFhF5hgICArC3t8fT0xMvLy+jb21cXByvvPIK9+/fp0mTJnz55ZeZXGnmS+nOcSIi6aUuECIiz9CSJUvYuXMnAG3btqVz5848ePCAtWvXGt0qUtsFQURE0kYBWETkGXrrrbfYvXs3cXFxrFy5kpUrV1rMz58/P61bt86c4kRErIT6AIuIPEO+vr5MnTqVV155BQ8PD2xtbbG3t6dQoUK0a9eOn3/+mVy5cmV2mSIizzX1ARYRERERq6IWYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEq/wcj5gMmO1vZcAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311f21a9-ee1b-4db6-a205-f364b57d2ea3",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7290376c-ca36-42aa-a5db-42f331c80807",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          560            418  74.642857\n",
      "1           kitten          122            105  86.065574\n",
      "2           senior          178             82  46.067416\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# store for final evaluation \n",
    "all_class_stats.append(class_stats)\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9d8f2586-1474-4b34-ad2a-90bc3b145fa2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2f2d5-3695-4003-a95b-6efe341ae7ca",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "586904ce-5e3c-40a8-bc2b-21c5b173fce5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      149     69.95\n",
      "1          M    337      235     69.73\n",
      "2          X    310      221     71.29\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)\n",
    "\n",
    "# store for final evaluation \n",
    "all_gender_stats.append(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6a4f541f-419d-41cb-994a-c95aaa3c08f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec851758-1567-4dd0-afc9-ae726430658a",
   "metadata": {},
   "source": [
    "### Log current total CV Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "04b4bec3-8366-4c78-8b97-fbe73a437c57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Majority Vote Accuracy so far: [0.7636363636363637, 0.7636363636363637, 0.7636363636363637]\n",
      "Total Class Statistics so far:\n",
      " [  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          551            443  80.399274\n",
      "1           kitten          118            102  86.440678\n",
      "2           senior          178             76  42.696629,   actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          556            438  78.776978\n",
      "1           kitten          118            101  85.593220\n",
      "2           senior          178             89  50.000000,   actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          560            418  74.642857\n",
      "1           kitten          122            105  86.065574\n",
      "2           senior          178             82  46.067416]\n",
      "Total Gender Accuracy so far:\n",
      " [  all_gender  count  correct  accuracy\n",
      "0          F    213      165     77.46\n",
      "1          M    337      234     69.44\n",
      "2          X    297      222     74.75,   all_gender  count  correct  accuracy\n",
      "0          F    213      158     74.18\n",
      "1          M    344      254     73.84\n",
      "2          X    295      216     73.22,   all_gender  count  correct  accuracy\n",
      "0          F    213      149     69.95\n",
      "1          M    337      235     69.73\n",
      "2          X    310      221     71.29]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Majority Vote Accuracy so far:\", all_majority_vote_accuracies)\n",
    "print(\"Total Class Statistics so far:\\n\", all_class_stats)\n",
    "print(\"Total Gender Accuracy so far:\\n\", all_gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b5d44-f489-4de0-8785-4bfa52d09797",
   "metadata": {},
   "source": [
    "# RANDOM SEED 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c4439fc4-670c-4009-8ca9-23c419ee99ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "adult     588\n",
      "senior    534\n",
      "kitten    513\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[3])) \n",
    "np.random.seed(int(random_seeds[3]))\n",
    "tf.random.set_seed(int(random_seeds[3]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_19.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5fe04e3d-fd90-43c1-97af-5eac75e35f85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ed5261d-2aeb-412e-a017-65b461a2cf5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70aa6-1716-400a-be48-cccb3511542e",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb9d0248-481e-4807-85dd-407fa88963f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "097B    14\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "028A    13\n",
      "111A    13\n",
      "051A    12\n",
      "039A    12\n",
      "068A    11\n",
      "025A    11\n",
      "036A    11\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "005A    10\n",
      "014B    10\n",
      "022A     9\n",
      "051B     9\n",
      "045A     9\n",
      "065A     9\n",
      "072A     9\n",
      "015A     9\n",
      "033A     9\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "008A     6\n",
      "109A     6\n",
      "108A     6\n",
      "037A     6\n",
      "023A     6\n",
      "007A     6\n",
      "023B     5\n",
      "044A     5\n",
      "070A     5\n",
      "025C     5\n",
      "021A     5\n",
      "009A     4\n",
      "062A     4\n",
      "035A     4\n",
      "104A     4\n",
      "003A     4\n",
      "052A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "060A     3\n",
      "012A     3\n",
      "014A     3\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "032A     2\n",
      "069A     2\n",
      "087A     2\n",
      "093A     2\n",
      "018A     2\n",
      "038A     2\n",
      "024A     1\n",
      "090A     1\n",
      "110A     1\n",
      "091A     1\n",
      "115A     1\n",
      "049A     1\n",
      "041A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "043A     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "019A    17\n",
      "001A    14\n",
      "002A    13\n",
      "116A    12\n",
      "063A    11\n",
      "095A     8\n",
      "013B     8\n",
      "099A     7\n",
      "053A     6\n",
      "034A     5\n",
      "075A     5\n",
      "026A     4\n",
      "105A     4\n",
      "061A     2\n",
      "054A     2\n",
      "004A     1\n",
      "088A     1\n",
      "019B     1\n",
      "092A     1\n",
      "100A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    312\n",
      "M    301\n",
      "F    162\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    90\n",
      "M    36\n",
      "X    36\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 103A, 071A, 097B, 028A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 113...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 001A, 019A, 095A, 034A, 002A, 063A, 013...\n",
      "senior                                   [116A, 054A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 16, 'senior': 19}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'senior': 3}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '002B' '003A' '005A' '006A' '007A' '008A' '009A' '010A' '011A'\n",
      " '012A' '014A' '014B' '015A' '016A' '018A' '020A' '021A' '022A' '023A'\n",
      " '023B' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A' '029A'\n",
      " '031A' '032A' '033A' '035A' '036A' '037A' '038A' '039A' '040A' '041A'\n",
      " '042A' '043A' '044A' '045A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '055A' '056A' '057A' '058A' '059A' '060A' '062A' '064A'\n",
      " '065A' '066A' '067A' '068A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '087A' '090A' '091A' '093A' '094A' '096A' '097A' '097B' '101A'\n",
      " '102A' '103A' '104A' '106A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '001A' '002A' '004A' '013B' '019A' '019B' '026A' '034A' '053A'\n",
      " '054A' '061A' '063A' '075A' '088A' '092A' '095A' '099A' '100A' '105A'\n",
      " '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'023A'}\n",
      "Moved to Test Set:\n",
      "{'023A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002B' '003A' '005A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '014A' '014B' '015A' '016A' '018A' '020A' '021A' '022A'\n",
      " '023B' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A' '029A'\n",
      " '031A' '032A' '033A' '035A' '036A' '037A' '038A' '039A' '040A' '041A'\n",
      " '042A' '043A' '044A' '045A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '055A' '056A' '057A' '058A' '059A' '060A' '062A' '064A'\n",
      " '065A' '066A' '067A' '068A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '087A' '090A' '091A' '093A' '094A' '096A' '097A' '097B' '101A'\n",
      " '102A' '103A' '104A' '106A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '002A' '004A' '013B' '019A' '019B' '023A' '026A' '034A' '053A'\n",
      " '054A' '061A' '063A' '075A' '088A' '092A' '095A' '099A' '100A' '105A'\n",
      " '116A']\n",
      "Length of X_train_val:\n",
      "808\n",
      "Length of y_train_val:\n",
      "808\n",
      "Length of groups_train_val:\n",
      "808\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     442\n",
      "kitten    171\n",
      "senior    162\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     146\n",
      "senior     16\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     475\n",
      "kitten    171\n",
      "senior    162\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     113\n",
      "senior     16\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 950, 1: 684, 2: 648})\n",
      "Epoch 1/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.9217 - accuracy: 0.5964\n",
      "Epoch 2/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.7244 - accuracy: 0.6770\n",
      "Epoch 3/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.6541 - accuracy: 0.7134\n",
      "Epoch 4/1500\n",
      "72/72 [==============================] - 0s 969us/step - loss: 0.6020 - accuracy: 0.7493\n",
      "Epoch 5/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5757 - accuracy: 0.7528\n",
      "Epoch 6/1500\n",
      "72/72 [==============================] - 0s 973us/step - loss: 0.5746 - accuracy: 0.7590\n",
      "Epoch 7/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5292 - accuracy: 0.7660\n",
      "Epoch 8/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5126 - accuracy: 0.7813\n",
      "Epoch 9/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4886 - accuracy: 0.7870\n",
      "Epoch 10/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5052 - accuracy: 0.7879\n",
      "Epoch 11/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4637 - accuracy: 0.8041\n",
      "Epoch 12/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.8037\n",
      "Epoch 13/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4435 - accuracy: 0.8032\n",
      "Epoch 14/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4223 - accuracy: 0.8177\n",
      "Epoch 15/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4361 - accuracy: 0.8212\n",
      "Epoch 16/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4312 - accuracy: 0.8173\n",
      "Epoch 17/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4140 - accuracy: 0.8173\n",
      "Epoch 18/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4000 - accuracy: 0.8335\n",
      "Epoch 19/1500\n",
      "72/72 [==============================] - 0s 975us/step - loss: 0.4129 - accuracy: 0.8287\n",
      "Epoch 20/1500\n",
      "72/72 [==============================] - 0s 969us/step - loss: 0.3840 - accuracy: 0.8374\n",
      "Epoch 21/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3862 - accuracy: 0.8304\n",
      "Epoch 22/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3942 - accuracy: 0.8330\n",
      "Epoch 23/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8488\n",
      "Epoch 24/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3592 - accuracy: 0.8510\n",
      "Epoch 25/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3479 - accuracy: 0.8506\n",
      "Epoch 26/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.8405\n",
      "Epoch 27/1500\n",
      "72/72 [==============================] - 0s 979us/step - loss: 0.3520 - accuracy: 0.8462\n",
      "Epoch 28/1500\n",
      "72/72 [==============================] - 0s 977us/step - loss: 0.3379 - accuracy: 0.8563\n",
      "Epoch 29/1500\n",
      "72/72 [==============================] - 0s 974us/step - loss: 0.3271 - accuracy: 0.8598\n",
      "Epoch 30/1500\n",
      "72/72 [==============================] - 0s 968us/step - loss: 0.3287 - accuracy: 0.8580\n",
      "Epoch 31/1500\n",
      "72/72 [==============================] - 0s 983us/step - loss: 0.3252 - accuracy: 0.8585\n",
      "Epoch 32/1500\n",
      "72/72 [==============================] - 0s 979us/step - loss: 0.3252 - accuracy: 0.8593\n",
      "Epoch 33/1500\n",
      "72/72 [==============================] - 0s 975us/step - loss: 0.3151 - accuracy: 0.8663\n",
      "Epoch 34/1500\n",
      "72/72 [==============================] - 0s 955us/step - loss: 0.3260 - accuracy: 0.8598\n",
      "Epoch 35/1500\n",
      "72/72 [==============================] - 0s 984us/step - loss: 0.3115 - accuracy: 0.8699\n",
      "Epoch 36/1500\n",
      "72/72 [==============================] - 0s 986us/step - loss: 0.3059 - accuracy: 0.8734\n",
      "Epoch 37/1500\n",
      "72/72 [==============================] - 0s 987us/step - loss: 0.3029 - accuracy: 0.8694\n",
      "Epoch 38/1500\n",
      "72/72 [==============================] - 0s 989us/step - loss: 0.3236 - accuracy: 0.8620\n",
      "Epoch 39/1500\n",
      "72/72 [==============================] - 0s 963us/step - loss: 0.2898 - accuracy: 0.8716\n",
      "Epoch 40/1500\n",
      "72/72 [==============================] - 0s 977us/step - loss: 0.3008 - accuracy: 0.8712\n",
      "Epoch 41/1500\n",
      "72/72 [==============================] - 0s 966us/step - loss: 0.2872 - accuracy: 0.8747\n",
      "Epoch 42/1500\n",
      "72/72 [==============================] - 0s 961us/step - loss: 0.2988 - accuracy: 0.8751\n",
      "Epoch 43/1500\n",
      "72/72 [==============================] - 0s 958us/step - loss: 0.3068 - accuracy: 0.8764\n",
      "Epoch 44/1500\n",
      "72/72 [==============================] - 0s 999us/step - loss: 0.2835 - accuracy: 0.8804\n",
      "Epoch 45/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2743 - accuracy: 0.8874\n",
      "Epoch 46/1500\n",
      "72/72 [==============================] - 0s 958us/step - loss: 0.2846 - accuracy: 0.8817\n",
      "Epoch 47/1500\n",
      "72/72 [==============================] - 0s 984us/step - loss: 0.2706 - accuracy: 0.8904\n",
      "Epoch 48/1500\n",
      "72/72 [==============================] - 0s 958us/step - loss: 0.2953 - accuracy: 0.8769\n",
      "Epoch 49/1500\n",
      "72/72 [==============================] - 0s 955us/step - loss: 0.2780 - accuracy: 0.8918\n",
      "Epoch 50/1500\n",
      "72/72 [==============================] - 0s 952us/step - loss: 0.2456 - accuracy: 0.9005\n",
      "Epoch 51/1500\n",
      "72/72 [==============================] - 0s 949us/step - loss: 0.2558 - accuracy: 0.8940\n",
      "Epoch 52/1500\n",
      "72/72 [==============================] - 0s 955us/step - loss: 0.2404 - accuracy: 0.9071\n",
      "Epoch 53/1500\n",
      "72/72 [==============================] - 0s 957us/step - loss: 0.2647 - accuracy: 0.8931\n",
      "Epoch 54/1500\n",
      "72/72 [==============================] - 0s 962us/step - loss: 0.2715 - accuracy: 0.8891\n",
      "Epoch 55/1500\n",
      "72/72 [==============================] - 0s 962us/step - loss: 0.2578 - accuracy: 0.8953\n",
      "Epoch 56/1500\n",
      "72/72 [==============================] - 0s 946us/step - loss: 0.2619 - accuracy: 0.8909\n",
      "Epoch 57/1500\n",
      "72/72 [==============================] - 0s 962us/step - loss: 0.2609 - accuracy: 0.8878\n",
      "Epoch 58/1500\n",
      "72/72 [==============================] - 0s 961us/step - loss: 0.2475 - accuracy: 0.8900\n",
      "Epoch 59/1500\n",
      "72/72 [==============================] - 0s 958us/step - loss: 0.2542 - accuracy: 0.8931\n",
      "Epoch 60/1500\n",
      "72/72 [==============================] - 0s 989us/step - loss: 0.2350 - accuracy: 0.8996\n",
      "Epoch 61/1500\n",
      "72/72 [==============================] - 0s 979us/step - loss: 0.2473 - accuracy: 0.9001\n",
      "Epoch 62/1500\n",
      "72/72 [==============================] - 0s 964us/step - loss: 0.2452 - accuracy: 0.8992\n",
      "Epoch 63/1500\n",
      "72/72 [==============================] - 0s 971us/step - loss: 0.2548 - accuracy: 0.8940\n",
      "Epoch 64/1500\n",
      "72/72 [==============================] - 0s 960us/step - loss: 0.2471 - accuracy: 0.8975\n",
      "Epoch 65/1500\n",
      "72/72 [==============================] - 0s 965us/step - loss: 0.2478 - accuracy: 0.9005\n",
      "Epoch 66/1500\n",
      "72/72 [==============================] - 0s 967us/step - loss: 0.2492 - accuracy: 0.8948\n",
      "Epoch 67/1500\n",
      "72/72 [==============================] - 0s 942us/step - loss: 0.2404 - accuracy: 0.9062\n",
      "Epoch 68/1500\n",
      "72/72 [==============================] - 0s 950us/step - loss: 0.2286 - accuracy: 0.9067\n",
      "Epoch 69/1500\n",
      "72/72 [==============================] - 0s 964us/step - loss: 0.2328 - accuracy: 0.9001\n",
      "Epoch 70/1500\n",
      "72/72 [==============================] - 0s 962us/step - loss: 0.2421 - accuracy: 0.8996\n",
      "Epoch 71/1500\n",
      "72/72 [==============================] - 0s 948us/step - loss: 0.2334 - accuracy: 0.9014\n",
      "Epoch 72/1500\n",
      "72/72 [==============================] - 0s 954us/step - loss: 0.2359 - accuracy: 0.9062\n",
      "Epoch 73/1500\n",
      "72/72 [==============================] - 0s 990us/step - loss: 0.2316 - accuracy: 0.9027\n",
      "Epoch 74/1500\n",
      "72/72 [==============================] - 0s 970us/step - loss: 0.2333 - accuracy: 0.9023\n",
      "Epoch 75/1500\n",
      "72/72 [==============================] - 0s 935us/step - loss: 0.2127 - accuracy: 0.9110\n",
      "Epoch 76/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2235 - accuracy: 0.9110\n",
      "Epoch 77/1500\n",
      "72/72 [==============================] - 0s 985us/step - loss: 0.2316 - accuracy: 0.9062\n",
      "Epoch 78/1500\n",
      "72/72 [==============================] - 0s 999us/step - loss: 0.1945 - accuracy: 0.9281\n",
      "Epoch 79/1500\n",
      "72/72 [==============================] - 0s 979us/step - loss: 0.2130 - accuracy: 0.9097\n",
      "Epoch 80/1500\n",
      "72/72 [==============================] - 0s 986us/step - loss: 0.2288 - accuracy: 0.9067\n",
      "Epoch 81/1500\n",
      "72/72 [==============================] - 0s 946us/step - loss: 0.2094 - accuracy: 0.9181\n",
      "Epoch 82/1500\n",
      "72/72 [==============================] - 0s 931us/step - loss: 0.2131 - accuracy: 0.9115\n",
      "Epoch 83/1500\n",
      "72/72 [==============================] - 0s 969us/step - loss: 0.1994 - accuracy: 0.9181\n",
      "Epoch 84/1500\n",
      "72/72 [==============================] - 0s 972us/step - loss: 0.2145 - accuracy: 0.9150\n",
      "Epoch 85/1500\n",
      "72/72 [==============================] - 0s 940us/step - loss: 0.2205 - accuracy: 0.9145\n",
      "Epoch 86/1500\n",
      "72/72 [==============================] - 0s 970us/step - loss: 0.2019 - accuracy: 0.9181\n",
      "Epoch 87/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9176\n",
      "Epoch 88/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1931 - accuracy: 0.9242\n",
      "Epoch 89/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9198\n",
      "Epoch 90/1500\n",
      "72/72 [==============================] - 0s 988us/step - loss: 0.1953 - accuracy: 0.9181\n",
      "Epoch 91/1500\n",
      "72/72 [==============================] - 0s 998us/step - loss: 0.1914 - accuracy: 0.9229\n",
      "Epoch 92/1500\n",
      "72/72 [==============================] - 0s 954us/step - loss: 0.2011 - accuracy: 0.9211\n",
      "Epoch 93/1500\n",
      "72/72 [==============================] - 0s 958us/step - loss: 0.1965 - accuracy: 0.9211\n",
      "Epoch 94/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.9259\n",
      "Epoch 95/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.9264\n",
      "Epoch 96/1500\n",
      "72/72 [==============================] - 0s 981us/step - loss: 0.1863 - accuracy: 0.9281\n",
      "Epoch 97/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9273\n",
      "Epoch 98/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1879 - accuracy: 0.9229\n",
      "Epoch 99/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1824 - accuracy: 0.9233\n",
      "Epoch 100/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1871 - accuracy: 0.9290\n",
      "Epoch 101/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1940 - accuracy: 0.9224\n",
      "Epoch 102/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9202\n",
      "Epoch 103/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9259\n",
      "Epoch 104/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9246\n",
      "Epoch 105/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9264\n",
      "Epoch 106/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9277\n",
      "Epoch 107/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.9330\n",
      "Epoch 108/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9338\n",
      "Epoch 109/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1912 - accuracy: 0.9233\n",
      "Epoch 110/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9273\n",
      "Epoch 111/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9330\n",
      "Epoch 112/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1689 - accuracy: 0.9330\n",
      "Epoch 113/1500\n",
      "72/72 [==============================] - 0s 973us/step - loss: 0.1840 - accuracy: 0.9198\n",
      "Epoch 114/1500\n",
      "72/72 [==============================] - 0s 972us/step - loss: 0.1847 - accuracy: 0.9273\n",
      "Epoch 115/1500\n",
      "72/72 [==============================] - 0s 978us/step - loss: 0.1783 - accuracy: 0.9334\n",
      "Epoch 116/1500\n",
      "72/72 [==============================] - 0s 954us/step - loss: 0.1664 - accuracy: 0.9351\n",
      "Epoch 117/1500\n",
      "72/72 [==============================] - 0s 972us/step - loss: 0.1667 - accuracy: 0.9312\n",
      "Epoch 118/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9312\n",
      "Epoch 119/1500\n",
      "72/72 [==============================] - 0s 952us/step - loss: 0.1730 - accuracy: 0.9360\n",
      "Epoch 120/1500\n",
      "72/72 [==============================] - 0s 963us/step - loss: 0.1720 - accuracy: 0.9303\n",
      "Epoch 121/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9343\n",
      "Epoch 122/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9338\n",
      "Epoch 123/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9382\n",
      "Epoch 124/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1627 - accuracy: 0.9312\n",
      "Epoch 125/1500\n",
      "72/72 [==============================] - 0s 999us/step - loss: 0.1733 - accuracy: 0.9308\n",
      "Epoch 126/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9347\n",
      "Epoch 127/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1558 - accuracy: 0.9387\n",
      "Epoch 128/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9378\n",
      "Epoch 129/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9343\n",
      "Epoch 130/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1605 - accuracy: 0.9360\n",
      "Epoch 131/1500\n",
      "72/72 [==============================] - 0s 1000us/step - loss: 0.1691 - accuracy: 0.9325\n",
      "Epoch 132/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9426\n",
      "Epoch 133/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9395\n",
      "Epoch 134/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9290\n",
      "Epoch 135/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9351\n",
      "Epoch 136/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9312\n",
      "Epoch 137/1500\n",
      "72/72 [==============================] - 0s 969us/step - loss: 0.1729 - accuracy: 0.9303\n",
      "Epoch 138/1500\n",
      "72/72 [==============================] - 0s 974us/step - loss: 0.1549 - accuracy: 0.9391\n",
      "Epoch 139/1500\n",
      "72/72 [==============================] - 0s 989us/step - loss: 0.1405 - accuracy: 0.9496\n",
      "Epoch 140/1500\n",
      "72/72 [==============================] - 0s 975us/step - loss: 0.1441 - accuracy: 0.9382\n",
      "Epoch 141/1500\n",
      "72/72 [==============================] - 0s 973us/step - loss: 0.1572 - accuracy: 0.9413\n",
      "Epoch 142/1500\n",
      "72/72 [==============================] - 0s 964us/step - loss: 0.1672 - accuracy: 0.9325\n",
      "Epoch 143/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9325\n",
      "Epoch 144/1500\n",
      "72/72 [==============================] - 0s 964us/step - loss: 0.1545 - accuracy: 0.9465\n",
      "Epoch 145/1500\n",
      "72/72 [==============================] - 0s 978us/step - loss: 0.1536 - accuracy: 0.9387\n",
      "Epoch 146/1500\n",
      "72/72 [==============================] - 0s 974us/step - loss: 0.1505 - accuracy: 0.9422\n",
      "Epoch 147/1500\n",
      "72/72 [==============================] - 0s 987us/step - loss: 0.1607 - accuracy: 0.9400\n",
      "Epoch 148/1500\n",
      "72/72 [==============================] - 0s 956us/step - loss: 0.1460 - accuracy: 0.9443\n",
      "Epoch 149/1500\n",
      "72/72 [==============================] - 0s 978us/step - loss: 0.1482 - accuracy: 0.9417\n",
      "Epoch 150/1500\n",
      "72/72 [==============================] - 0s 971us/step - loss: 0.1490 - accuracy: 0.9400\n",
      "Epoch 151/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9483\n",
      "Epoch 152/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9395\n",
      "Epoch 153/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1388 - accuracy: 0.9465\n",
      "Epoch 154/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9470\n",
      "Epoch 155/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9553\n",
      "Epoch 156/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1477 - accuracy: 0.9435\n",
      "Epoch 157/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1683 - accuracy: 0.9286\n",
      "Epoch 158/1500\n",
      "72/72 [==============================] - 0s 974us/step - loss: 0.1514 - accuracy: 0.9417\n",
      "Epoch 159/1500\n",
      "72/72 [==============================] - 0s 960us/step - loss: 0.1547 - accuracy: 0.9347\n",
      "Epoch 160/1500\n",
      "72/72 [==============================] - 0s 965us/step - loss: 0.1652 - accuracy: 0.9334\n",
      "Epoch 161/1500\n",
      "72/72 [==============================] - 0s 980us/step - loss: 0.1406 - accuracy: 0.9452\n",
      "Epoch 162/1500\n",
      "72/72 [==============================] - 0s 964us/step - loss: 0.1492 - accuracy: 0.9435\n",
      "Epoch 163/1500\n",
      "72/72 [==============================] - 0s 993us/step - loss: 0.1205 - accuracy: 0.9535\n",
      "Epoch 164/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9452\n",
      "Epoch 165/1500\n",
      "72/72 [==============================] - 0s 997us/step - loss: 0.1472 - accuracy: 0.9378\n",
      "Epoch 166/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9426\n",
      "Epoch 167/1500\n",
      "72/72 [==============================] - 0s 979us/step - loss: 0.1396 - accuracy: 0.9500\n",
      "Epoch 168/1500\n",
      "72/72 [==============================] - 0s 975us/step - loss: 0.1362 - accuracy: 0.9509\n",
      "Epoch 169/1500\n",
      "72/72 [==============================] - 0s 977us/step - loss: 0.1378 - accuracy: 0.9479\n",
      "Epoch 170/1500\n",
      "72/72 [==============================] - 0s 986us/step - loss: 0.1436 - accuracy: 0.9465\n",
      "Epoch 171/1500\n",
      "72/72 [==============================] - 0s 973us/step - loss: 0.1319 - accuracy: 0.9549\n",
      "Epoch 172/1500\n",
      "72/72 [==============================] - 0s 976us/step - loss: 0.1334 - accuracy: 0.9430\n",
      "Epoch 173/1500\n",
      "72/72 [==============================] - 0s 969us/step - loss: 0.1358 - accuracy: 0.9474\n",
      "Epoch 174/1500\n",
      "72/72 [==============================] - 0s 999us/step - loss: 0.1206 - accuracy: 0.9522\n",
      "Epoch 175/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9439\n",
      "Epoch 176/1500\n",
      "72/72 [==============================] - 0s 995us/step - loss: 0.1580 - accuracy: 0.9351\n",
      "Epoch 177/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9448\n",
      "Epoch 178/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9496\n",
      "Epoch 179/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9483\n",
      "Epoch 180/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.9496\n",
      "Epoch 181/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9435\n",
      "Epoch 182/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9540\n",
      "Epoch 183/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9518\n",
      "Epoch 184/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9514\n",
      "Epoch 185/1500\n",
      "72/72 [==============================] - 0s 984us/step - loss: 0.1412 - accuracy: 0.9452\n",
      "Epoch 186/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9571\n",
      "Epoch 187/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9544\n",
      "Epoch 188/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9487\n",
      "Epoch 189/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9505\n",
      "Epoch 190/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1351 - accuracy: 0.9479\n",
      "Epoch 191/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9514\n",
      "Epoch 192/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9452\n",
      "Epoch 193/1500\n",
      "52/72 [====================>.........] - ETA: 0s - loss: 0.1362 - accuracy: 0.9477Restoring model weights from the end of the best epoch: 163.\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9514\n",
      "Epoch 193: early stopping\n",
      "5/5 [==============================] - 0s 939us/step - loss: 0.6747 - accuracy: 0.7442\n",
      "5/5 [==============================] - 0s 781us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.67 (14/21)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 129, Predictions: 129, Actuals: 129, Gender: 129\n",
      "Final Test Results - Loss: 0.6747045516967773, Accuracy: 0.7441860437393188, Precision: 0.40226063829787234, Recall: 0.44413716814159293, F1 Score: 0.4051932367149759\n",
      "Confusion Matrix:\n",
      " [[87  3 23]\n",
      " [ 0  0  0]\n",
      " [ 7  0  9]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "042A    14\n",
      "059A    14\n",
      "097B    14\n",
      "001A    14\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "063A    11\n",
      "068A    11\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "014B    10\n",
      "040A    10\n",
      "033A     9\n",
      "022A     9\n",
      "045A     9\n",
      "065A     9\n",
      "072A     9\n",
      "015A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "099A     7\n",
      "050A     7\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "108A     6\n",
      "037A     6\n",
      "007A     6\n",
      "053A     6\n",
      "025C     5\n",
      "044A     5\n",
      "034A     5\n",
      "023B     5\n",
      "075A     5\n",
      "070A     5\n",
      "021A     5\n",
      "003A     4\n",
      "105A     4\n",
      "026A     4\n",
      "052A     4\n",
      "009A     4\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "061A     2\n",
      "054A     2\n",
      "025B     2\n",
      "069A     2\n",
      "087A     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "090A     1\n",
      "100A     1\n",
      "091A     1\n",
      "110A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "066A     1\n",
      "026C     1\n",
      "076A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "074A    25\n",
      "101A    15\n",
      "106A    14\n",
      "028A    13\n",
      "036A    11\n",
      "025A    11\n",
      "031A     7\n",
      "117A     7\n",
      "027A     7\n",
      "104A     4\n",
      "062A     4\n",
      "035A     4\n",
      "058A     3\n",
      "014A     3\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "093A     2\n",
      "018A     2\n",
      "032A     2\n",
      "038A     2\n",
      "073A     1\n",
      "096A     1\n",
      "048A     1\n",
      "115A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    315\n",
      "M    285\n",
      "F    187\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    65\n",
      "M    52\n",
      "X    33\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [097A, 057A, 055A, 059A, 113A, 116A, 051B, 054...\n",
      "Name: cat_id, dtype: object"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [028A, 074A, 062A, 101A, 027A, 038A, 025A, 014...\n",
      "kitten                                         [048A, 115A]\n",
      "senior                 [093A, 106A, 104A, 117A, 058A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 14, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 2, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '008A' '009A' '010A' '011A' '013B' '014B' '015A' '016A' '019A' '019B'\n",
      " '020A' '021A' '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C'\n",
      " '029A' '033A' '034A' '037A' '039A' '040A' '041A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '049A' '050A' '051A' '051B' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '059A' '061A' '063A' '065A' '066A' '067A' '068A'\n",
      " '069A' '070A' '071A' '072A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '092A' '094A' '095A' '097A' '097B' '099A' '100A' '102A' '103A' '105A'\n",
      " '108A' '109A' '110A' '111A' '113A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['012A' '014A' '018A' '024A' '025A' '027A' '028A' '031A' '032A' '035A'\n",
      " '036A' '038A' '048A' '058A' '060A' '062A' '064A' '073A' '074A' '093A'\n",
      " '096A' '101A' '104A' '106A' '115A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '008A' '009A' '010A' '011A' '013B' '014B' '015A' '016A' '019A' '019B'\n",
      " '020A' '021A' '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C'\n",
      " '029A' '033A' '034A' '037A' '039A' '040A' '041A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '049A' '050A' '051A' '051B' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '059A' '061A' '063A' '065A' '066A' '067A' '068A'\n",
      " '069A' '070A' '071A' '072A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '092A' '094A' '095A' '097A' '097B' '099A' '100A' '102A' '103A' '105A'\n",
      " '108A' '109A' '110A' '111A' '113A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['012A' '014A' '018A' '024A' '025A' '027A' '028A' '031A' '032A' '035A'\n",
      " '036A' '038A' '048A' '058A' '060A' '062A' '064A' '073A' '074A' '093A'\n",
      " '096A' '101A' '104A' '106A' '115A' '117A']\n",
      "Length of X_train_val:\n",
      "787\n",
      "Length of y_train_val:\n",
      "787\n",
      "Length of groups_train_val:\n",
      "787\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     471\n",
      "kitten    169\n",
      "senior    147\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     117\n",
      "senior     31\n",
      "kitten      2\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     471\n",
      "kitten    169\n",
      "senior    147\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     117\n",
      "senior     31\n",
      "kitten      2\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 942, 1: 676, 2: 588})\n",
      "Epoch 1/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.8919 - accuracy: 0.6074\n",
      "Epoch 2/1500\n",
      "69/69 [==============================] - 0s 992us/step - loss: 0.6925 - accuracy: 0.6818\n",
      "Epoch 3/1500\n",
      "69/69 [==============================] - 0s 989us/step - loss: 0.6263 - accuracy: 0.7262\n",
      "Epoch 4/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.5669 - accuracy: 0.7575\n",
      "Epoch 5/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.5522 - accuracy: 0.7588\n",
      "Epoch 6/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.5088 - accuracy: 0.7761\n",
      "Epoch 7/1500\n",
      "69/69 [==============================] - 0s 983us/step - loss: 0.4844 - accuracy: 0.7897\n",
      "Epoch 8/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.4746 - accuracy: 0.7901\n",
      "Epoch 9/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.4824 - accuracy: 0.7915\n",
      "Epoch 10/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.4501 - accuracy: 0.8015\n",
      "Epoch 11/1500\n",
      "69/69 [==============================] - 0s 968us/step - loss: 0.4089 - accuracy: 0.8246\n",
      "Epoch 12/1500\n",
      "69/69 [==============================] - 0s 945us/step - loss: 0.4239 - accuracy: 0.8323\n",
      "Epoch 13/1500\n",
      "69/69 [==============================] - 0s 953us/step - loss: 0.3872 - accuracy: 0.8277\n",
      "Epoch 14/1500\n",
      "69/69 [==============================] - 0s 994us/step - loss: 0.3978 - accuracy: 0.8309\n",
      "Epoch 15/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.3824 - accuracy: 0.8404\n",
      "Epoch 16/1500\n",
      "69/69 [==============================] - 0s 941us/step - loss: 0.3678 - accuracy: 0.8404\n",
      "Epoch 17/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3544 - accuracy: 0.8463\n",
      "Epoch 18/1500\n",
      "69/69 [==============================] - 0s 990us/step - loss: 0.3661 - accuracy: 0.8409\n",
      "Epoch 19/1500\n",
      "69/69 [==============================] - 0s 945us/step - loss: 0.3674 - accuracy: 0.8386\n",
      "Epoch 20/1500\n",
      "69/69 [==============================] - 0s 952us/step - loss: 0.3307 - accuracy: 0.8568\n",
      "Epoch 21/1500\n",
      "69/69 [==============================] - 0s 951us/step - loss: 0.3288 - accuracy: 0.8572\n",
      "Epoch 22/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3250 - accuracy: 0.8617\n",
      "Epoch 23/1500\n",
      "69/69 [==============================] - 0s 986us/step - loss: 0.3329 - accuracy: 0.8513\n",
      "Epoch 24/1500\n",
      "69/69 [==============================] - 0s 979us/step - loss: 0.3283 - accuracy: 0.8531\n",
      "Epoch 25/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3111 - accuracy: 0.8667\n",
      "Epoch 26/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.8708\n",
      "Epoch 27/1500\n",
      "69/69 [==============================] - 0s 989us/step - loss: 0.3121 - accuracy: 0.8581\n",
      "Epoch 28/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.3017 - accuracy: 0.8704\n",
      "Epoch 29/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.3013 - accuracy: 0.8658\n",
      "Epoch 30/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2906 - accuracy: 0.8790\n",
      "Epoch 31/1500\n",
      "69/69 [==============================] - 0s 993us/step - loss: 0.3024 - accuracy: 0.8617\n",
      "Epoch 32/1500\n",
      "69/69 [==============================] - 0s 952us/step - loss: 0.2579 - accuracy: 0.8903\n",
      "Epoch 33/1500\n",
      "69/69 [==============================] - 0s 964us/step - loss: 0.2650 - accuracy: 0.8930\n",
      "Epoch 34/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2748 - accuracy: 0.8858\n",
      "Epoch 35/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2865 - accuracy: 0.8781\n",
      "Epoch 36/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2975 - accuracy: 0.8735\n",
      "Epoch 37/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2559 - accuracy: 0.8908\n",
      "Epoch 38/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2775 - accuracy: 0.8767\n",
      "Epoch 39/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2597 - accuracy: 0.8844\n",
      "Epoch 40/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2628 - accuracy: 0.8876\n",
      "Epoch 41/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2632 - accuracy: 0.8930\n",
      "Epoch 42/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2514 - accuracy: 0.8962\n",
      "Epoch 43/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2499 - accuracy: 0.8908\n",
      "Epoch 44/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2431 - accuracy: 0.8930\n",
      "Epoch 45/1500\n",
      "69/69 [==============================] - 0s 999us/step - loss: 0.2425 - accuracy: 0.8930\n",
      "Epoch 46/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2375 - accuracy: 0.9048\n",
      "Epoch 47/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2280 - accuracy: 0.9012\n",
      "Epoch 48/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2481 - accuracy: 0.8944\n",
      "Epoch 49/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.2331 - accuracy: 0.8976\n",
      "Epoch 50/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.2162 - accuracy: 0.9093\n",
      "Epoch 51/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2076 - accuracy: 0.9121\n",
      "Epoch 52/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2190 - accuracy: 0.9089\n",
      "Epoch 53/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2213 - accuracy: 0.9116\n",
      "Epoch 54/1500\n",
      "69/69 [==============================] - 0s 995us/step - loss: 0.2207 - accuracy: 0.9093\n",
      "Epoch 55/1500\n",
      "69/69 [==============================] - 0s 988us/step - loss: 0.2034 - accuracy: 0.9180\n",
      "Epoch 56/1500\n",
      "69/69 [==============================] - 0s 948us/step - loss: 0.2191 - accuracy: 0.9080\n",
      "Epoch 57/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.2126 - accuracy: 0.9157\n",
      "Epoch 58/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.1975 - accuracy: 0.9130\n",
      "Epoch 59/1500\n",
      "69/69 [==============================] - 0s 969us/step - loss: 0.1842 - accuracy: 0.9229\n",
      "Epoch 60/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9093\n",
      "Epoch 61/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.2088 - accuracy: 0.9161\n",
      "Epoch 62/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9252\n",
      "Epoch 63/1500\n",
      "69/69 [==============================] - 0s 997us/step - loss: 0.1864 - accuracy: 0.9229\n",
      "Epoch 64/1500\n",
      "69/69 [==============================] - 0s 983us/step - loss: 0.2137 - accuracy: 0.9093\n",
      "Epoch 65/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9275\n",
      "Epoch 66/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9148\n",
      "Epoch 67/1500\n",
      "69/69 [==============================] - 0s 939us/step - loss: 0.1940 - accuracy: 0.9180\n",
      "Epoch 68/1500\n",
      "69/69 [==============================] - 0s 953us/step - loss: 0.1772 - accuracy: 0.9220\n",
      "Epoch 69/1500\n",
      "69/69 [==============================] - 0s 998us/step - loss: 0.1695 - accuracy: 0.9320\n",
      "Epoch 70/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1870 - accuracy: 0.9220\n",
      "Epoch 71/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.9252\n",
      "Epoch 72/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9229\n",
      "Epoch 73/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.9225\n",
      "Epoch 74/1500\n",
      "69/69 [==============================] - 0s 961us/step - loss: 0.1831 - accuracy: 0.9275\n",
      "Epoch 75/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.9211\n",
      "Epoch 76/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.9302\n",
      "Epoch 77/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9211\n",
      "Epoch 78/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9334\n",
      "Epoch 79/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9284\n",
      "Epoch 80/1500\n",
      "69/69 [==============================] - 0s 988us/step - loss: 0.1771 - accuracy: 0.9243\n",
      "Epoch 81/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9270\n",
      "Epoch 82/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1747 - accuracy: 0.9261\n",
      "Epoch 83/1500\n",
      "69/69 [==============================] - 0s 986us/step - loss: 0.1551 - accuracy: 0.9320\n",
      "Epoch 84/1500\n",
      "69/69 [==============================] - 0s 968us/step - loss: 0.1642 - accuracy: 0.9316\n",
      "Epoch 85/1500\n",
      "69/69 [==============================] - 0s 943us/step - loss: 0.1647 - accuracy: 0.9338\n",
      "Epoch 86/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.1620 - accuracy: 0.9361\n",
      "Epoch 87/1500\n",
      "69/69 [==============================] - 0s 977us/step - loss: 0.1386 - accuracy: 0.9492\n",
      "Epoch 88/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.1686 - accuracy: 0.9302\n",
      "Epoch 89/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9379\n",
      "Epoch 90/1500\n",
      "69/69 [==============================] - 0s 998us/step - loss: 0.1552 - accuracy: 0.9347\n",
      "Epoch 91/1500\n",
      "69/69 [==============================] - 0s 971us/step - loss: 0.1653 - accuracy: 0.9311\n",
      "Epoch 92/1500\n",
      "69/69 [==============================] - 0s 996us/step - loss: 0.1773 - accuracy: 0.9293\n",
      "Epoch 93/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.1615 - accuracy: 0.9270\n",
      "Epoch 94/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9433\n",
      "Epoch 95/1500\n",
      "69/69 [==============================] - 0s 968us/step - loss: 0.1610 - accuracy: 0.9334\n",
      "Epoch 96/1500\n",
      "69/69 [==============================] - 0s 960us/step - loss: 0.1592 - accuracy: 0.9383\n",
      "Epoch 97/1500\n",
      "69/69 [==============================] - 0s 946us/step - loss: 0.1486 - accuracy: 0.9379\n",
      "Epoch 98/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9465\n",
      "Epoch 99/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9393\n",
      "Epoch 100/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1590 - accuracy: 0.9365\n",
      "Epoch 101/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.9488\n",
      "Epoch 102/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9415\n",
      "Epoch 103/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.1496 - accuracy: 0.9388\n",
      "Epoch 104/1500\n",
      "69/69 [==============================] - 0s 970us/step - loss: 0.1633 - accuracy: 0.9379\n",
      "Epoch 105/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.1437 - accuracy: 0.9451\n",
      "Epoch 106/1500\n",
      "69/69 [==============================] - 0s 971us/step - loss: 0.1577 - accuracy: 0.9338\n",
      "Epoch 107/1500\n",
      "69/69 [==============================] - 0s 969us/step - loss: 0.1316 - accuracy: 0.9465\n",
      "Epoch 108/1500\n",
      "69/69 [==============================] - 0s 980us/step - loss: 0.1327 - accuracy: 0.9506\n",
      "Epoch 109/1500\n",
      "69/69 [==============================] - 0s 996us/step - loss: 0.1376 - accuracy: 0.9456\n",
      "Epoch 110/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.1500 - accuracy: 0.9383\n",
      "Epoch 111/1500\n",
      "69/69 [==============================] - 0s 993us/step - loss: 0.1493 - accuracy: 0.9411\n",
      "Epoch 112/1500\n",
      "69/69 [==============================] - 0s 985us/step - loss: 0.1389 - accuracy: 0.9438\n",
      "Epoch 113/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.1511 - accuracy: 0.9424\n",
      "Epoch 114/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9492\n",
      "Epoch 115/1500\n",
      "69/69 [==============================] - 0s 974us/step - loss: 0.1378 - accuracy: 0.9479\n",
      "Epoch 116/1500\n",
      "69/69 [==============================] - 0s 958us/step - loss: 0.1299 - accuracy: 0.9479\n",
      "Epoch 117/1500\n",
      "69/69 [==============================] - 0s 974us/step - loss: 0.1496 - accuracy: 0.9370\n",
      "Epoch 118/1500\n",
      "69/69 [==============================] - 0s 989us/step - loss: 0.1349 - accuracy: 0.9461\n",
      "Epoch 119/1500\n",
      "69/69 [==============================] - 0s 970us/step - loss: 0.1312 - accuracy: 0.9492\n",
      "Epoch 120/1500\n",
      "69/69 [==============================] - 0s 987us/step - loss: 0.1397 - accuracy: 0.9442\n",
      "Epoch 121/1500\n",
      "69/69 [==============================] - 0s 969us/step - loss: 0.1246 - accuracy: 0.9501\n",
      "Epoch 122/1500\n",
      "69/69 [==============================] - 0s 949us/step - loss: 0.1367 - accuracy: 0.9433\n",
      "Epoch 123/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9438\n",
      "Epoch 124/1500\n",
      "69/69 [==============================] - 0s 974us/step - loss: 0.1384 - accuracy: 0.9447\n",
      "Epoch 125/1500\n",
      "69/69 [==============================] - 0s 998us/step - loss: 0.1297 - accuracy: 0.9510\n",
      "Epoch 126/1500\n",
      "69/69 [==============================] - 0s 971us/step - loss: 0.1302 - accuracy: 0.9497\n",
      "Epoch 127/1500\n",
      "69/69 [==============================] - 0s 957us/step - loss: 0.1197 - accuracy: 0.9547\n",
      "Epoch 128/1500\n",
      "69/69 [==============================] - 0s 959us/step - loss: 0.1242 - accuracy: 0.9519\n",
      "Epoch 129/1500\n",
      "69/69 [==============================] - 0s 954us/step - loss: 0.1275 - accuracy: 0.9551\n",
      "Epoch 130/1500\n",
      "69/69 [==============================] - 0s 970us/step - loss: 0.1144 - accuracy: 0.9569\n",
      "Epoch 131/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.1204 - accuracy: 0.9492\n",
      "Epoch 132/1500\n",
      "69/69 [==============================] - 0s 965us/step - loss: 0.1083 - accuracy: 0.9574\n",
      "Epoch 133/1500\n",
      "69/69 [==============================] - 0s 949us/step - loss: 0.1148 - accuracy: 0.9578\n",
      "Epoch 134/1500\n",
      "69/69 [==============================] - 0s 974us/step - loss: 0.1165 - accuracy: 0.9519\n",
      "Epoch 135/1500\n",
      "69/69 [==============================] - 0s 966us/step - loss: 0.1653 - accuracy: 0.9356\n",
      "Epoch 136/1500\n",
      "69/69 [==============================] - 0s 966us/step - loss: 0.1170 - accuracy: 0.9529\n",
      "Epoch 137/1500\n",
      "69/69 [==============================] - 0s 982us/step - loss: 0.1158 - accuracy: 0.9574\n",
      "Epoch 138/1500\n",
      "69/69 [==============================] - 0s 961us/step - loss: 0.1095 - accuracy: 0.9533\n",
      "Epoch 139/1500\n",
      "69/69 [==============================] - 0s 959us/step - loss: 0.1119 - accuracy: 0.9569\n",
      "Epoch 140/1500\n",
      "69/69 [==============================] - 0s 3ms/step - loss: 0.1177 - accuracy: 0.9538\n",
      "Epoch 141/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9451\n",
      "Epoch 142/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9492\n",
      "Epoch 143/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9519\n",
      "Epoch 144/1500\n",
      "69/69 [==============================] - 0s 947us/step - loss: 0.1201 - accuracy: 0.9529\n",
      "Epoch 145/1500\n",
      "69/69 [==============================] - 0s 994us/step - loss: 0.1108 - accuracy: 0.9597\n",
      "Epoch 146/1500\n",
      "69/69 [==============================] - 0s 981us/step - loss: 0.1184 - accuracy: 0.9519\n",
      "Epoch 147/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9569\n",
      "Epoch 148/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9574\n",
      "Epoch 149/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9538\n",
      "Epoch 150/1500\n",
      "69/69 [==============================] - 0s 981us/step - loss: 0.1039 - accuracy: 0.9610\n",
      "Epoch 151/1500\n",
      "69/69 [==============================] - 0s 983us/step - loss: 0.1065 - accuracy: 0.9569\n",
      "Epoch 152/1500\n",
      "69/69 [==============================] - 0s 967us/step - loss: 0.1238 - accuracy: 0.9547\n",
      "Epoch 153/1500\n",
      "69/69 [==============================] - 0s 969us/step - loss: 0.1089 - accuracy: 0.9592\n",
      "Epoch 154/1500\n",
      "69/69 [==============================] - 0s 960us/step - loss: 0.1076 - accuracy: 0.9578\n",
      "Epoch 155/1500\n",
      "69/69 [==============================] - 0s 973us/step - loss: 0.1011 - accuracy: 0.9642\n",
      "Epoch 156/1500\n",
      "69/69 [==============================] - 0s 978us/step - loss: 0.0957 - accuracy: 0.9646\n",
      "Epoch 157/1500\n",
      "69/69 [==============================] - 0s 960us/step - loss: 0.1115 - accuracy: 0.9556\n",
      "Epoch 158/1500\n",
      "69/69 [==============================] - 0s 959us/step - loss: 0.0986 - accuracy: 0.9637\n",
      "Epoch 159/1500\n",
      "69/69 [==============================] - 0s 945us/step - loss: 0.1019 - accuracy: 0.9597\n",
      "Epoch 160/1500\n",
      "69/69 [==============================] - 0s 971us/step - loss: 0.0974 - accuracy: 0.9642\n",
      "Epoch 161/1500\n",
      "69/69 [==============================] - 0s 958us/step - loss: 0.1127 - accuracy: 0.9597\n",
      "Epoch 162/1500\n",
      "69/69 [==============================] - 0s 957us/step - loss: 0.1202 - accuracy: 0.9515\n",
      "Epoch 163/1500\n",
      "69/69 [==============================] - 0s 956us/step - loss: 0.0958 - accuracy: 0.9678\n",
      "Epoch 164/1500\n",
      "69/69 [==============================] - 0s 952us/step - loss: 0.1189 - accuracy: 0.9529\n",
      "Epoch 165/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9529\n",
      "Epoch 166/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9642\n",
      "Epoch 167/1500\n",
      "69/69 [==============================] - 0s 986us/step - loss: 0.1099 - accuracy: 0.9510\n",
      "Epoch 168/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.0995 - accuracy: 0.9606\n",
      "Epoch 169/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.1021 - accuracy: 0.9597\n",
      "Epoch 170/1500\n",
      "69/69 [==============================] - 0s 955us/step - loss: 0.0917 - accuracy: 0.9651\n",
      "Epoch 171/1500\n",
      "69/69 [==============================] - 0s 945us/step - loss: 0.0978 - accuracy: 0.9624\n",
      "Epoch 172/1500\n",
      "69/69 [==============================] - 0s 947us/step - loss: 0.1044 - accuracy: 0.9583\n",
      "Epoch 173/1500\n",
      "69/69 [==============================] - 0s 963us/step - loss: 0.1166 - accuracy: 0.9515\n",
      "Epoch 174/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9674\n",
      "Epoch 175/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0887 - accuracy: 0.9710\n",
      "Epoch 176/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9642\n",
      "Epoch 177/1500\n",
      "69/69 [==============================] - 0s 1000us/step - loss: 0.0977 - accuracy: 0.9651\n",
      "Epoch 178/1500\n",
      "69/69 [==============================] - 0s 986us/step - loss: 0.0890 - accuracy: 0.9674\n",
      "Epoch 179/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9678\n",
      "Epoch 180/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9651\n",
      "Epoch 181/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9665\n",
      "Epoch 182/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9578\n",
      "Epoch 183/1500\n",
      "69/69 [==============================] - 0s 977us/step - loss: 0.0923 - accuracy: 0.9678\n",
      "Epoch 184/1500\n",
      "69/69 [==============================] - 0s 952us/step - loss: 0.0874 - accuracy: 0.9660\n",
      "Epoch 185/1500\n",
      "69/69 [==============================] - 0s 981us/step - loss: 0.1101 - accuracy: 0.9551\n",
      "Epoch 186/1500\n",
      "69/69 [==============================] - 0s 992us/step - loss: 0.1002 - accuracy: 0.9651\n",
      "Epoch 187/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9646\n",
      "Epoch 188/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.9660\n",
      "Epoch 189/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9542\n",
      "Epoch 190/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9637\n",
      "Epoch 191/1500\n",
      "69/69 [==============================] - 0s 977us/step - loss: 0.1013 - accuracy: 0.9615\n",
      "Epoch 192/1500\n",
      "69/69 [==============================] - 0s 1000us/step - loss: 0.0918 - accuracy: 0.9674\n",
      "Epoch 193/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9642\n",
      "Epoch 194/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9678\n",
      "Epoch 195/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9719\n",
      "Epoch 196/1500\n",
      "69/69 [==============================] - 0s 979us/step - loss: 0.0956 - accuracy: 0.9592\n",
      "Epoch 197/1500\n",
      "69/69 [==============================] - 0s 961us/step - loss: 0.0850 - accuracy: 0.9669\n",
      "Epoch 198/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9660\n",
      "Epoch 199/1500\n",
      "69/69 [==============================] - 0s 983us/step - loss: 0.0796 - accuracy: 0.9714\n",
      "Epoch 200/1500\n",
      "69/69 [==============================] - 0s 990us/step - loss: 0.0884 - accuracy: 0.9669\n",
      "Epoch 201/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9660\n",
      "Epoch 202/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9606\n",
      "Epoch 203/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.9633\n",
      "Epoch 204/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9637\n",
      "Epoch 205/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9633\n",
      "Epoch 206/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9646\n",
      "Epoch 207/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9665\n",
      "Epoch 208/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9642\n",
      "Epoch 209/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9723\n",
      "Epoch 210/1500\n",
      "69/69 [==============================] - 0s 983us/step - loss: 0.0823 - accuracy: 0.9687\n",
      "Epoch 211/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9642\n",
      "Epoch 212/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.9696\n",
      "Epoch 213/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9642\n",
      "Epoch 214/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9683\n",
      "Epoch 215/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9701\n",
      "Epoch 216/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.9692\n",
      "Epoch 217/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9674\n",
      "Epoch 218/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9606\n",
      "Epoch 219/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9669\n",
      "Epoch 220/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9692\n",
      "Epoch 221/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9773\n",
      "Epoch 222/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9733\n",
      "Epoch 223/1500\n",
      "69/69 [==============================] - 0s 975us/step - loss: 0.0944 - accuracy: 0.9628\n",
      "Epoch 224/1500\n",
      "69/69 [==============================] - 0s 971us/step - loss: 0.0869 - accuracy: 0.9660\n",
      "Epoch 225/1500\n",
      "69/69 [==============================] - 0s 1000us/step - loss: 0.0906 - accuracy: 0.9637\n",
      "Epoch 226/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9687\n",
      "Epoch 227/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9728\n",
      "Epoch 228/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9710\n",
      "Epoch 229/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9687\n",
      "Epoch 230/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.9651\n",
      "Epoch 231/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9665\n",
      "Epoch 232/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9642\n",
      "Epoch 233/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9687\n",
      "Epoch 234/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.9773\n",
      "Epoch 235/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9692\n",
      "Epoch 236/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.9737\n",
      "Epoch 237/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9742\n",
      "Epoch 238/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9714\n",
      "Epoch 239/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9665\n",
      "Epoch 240/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9760\n",
      "Epoch 241/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.9728\n",
      "Epoch 242/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9669\n",
      "Epoch 243/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9637\n",
      "Epoch 244/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9696\n",
      "Epoch 245/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.9737\n",
      "Epoch 246/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9674\n",
      "Epoch 247/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.9701\n",
      "Epoch 248/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9606\n",
      "Epoch 249/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9692\n",
      "Epoch 250/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9719\n",
      "Epoch 251/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.9746\n",
      "Epoch 252/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9692\n",
      "Epoch 253/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9751\n",
      "Epoch 254/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9701\n",
      "Epoch 255/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.9737\n",
      "Epoch 256/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9742\n",
      "Epoch 257/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.9728\n",
      "Epoch 258/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.9742\n",
      "Epoch 259/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9710\n",
      "Epoch 260/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.9773\n",
      "Epoch 261/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.9746\n",
      "Epoch 262/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9714\n",
      "Epoch 263/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.9755\n",
      "Epoch 264/1500\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9628\n",
      "Epoch 265/1500\n",
      "69/69 [==============================] - 0s 994us/step - loss: 0.0849 - accuracy: 0.9696\n",
      "Epoch 266/1500\n",
      "51/69 [=====================>........] - ETA: 0s - loss: 0.0751 - accuracy: 0.9718Restoring model weights from the end of the best epoch: 236.\n",
      "69/69 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9728\n",
      "Epoch 266: early stopping\n",
      "5/5 [==============================] - 0s 811us/step - loss: 1.3008 - accuracy: 0.7200\n",
      "5/5 [==============================] - 0s 746us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.65 (17/26)\n",
      "Before appending - Cat IDs: 129, Predictions: 129, Actuals: 129, Gender: 129\n",
      "After appending - Cat IDs: 279, Predictions: 279, Actuals: 279, Gender: 279\n",
      "Final Test Results - Loss: 1.300822138786316, Accuracy: 0.7200000286102295, Precision: 0.45821661998132585, Recall: 0.5505468247403732, F1 Score: 0.46929749472122356\n",
      "Confusion Matrix:\n",
      " [[97  6 14]\n",
      " [ 1  1  0]\n",
      " [21  0 10]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "101A    15\n",
      "106A    14\n",
      "001A    14\n",
      "059A    14\n",
      "042A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "036A    11\n",
      "025A    11\n",
      "063A    11\n",
      "005A    10\n",
      "040A    10\n",
      "014B    10\n",
      "045A     9\n",
      "051B     9\n",
      "022A     9\n",
      "065A     9\n",
      "015A     9\n",
      "013B     8\n",
      "095A     8\n",
      "010A     8\n",
      "027A     7\n",
      "117A     7\n",
      "099A     7\n",
      "031A     7\n",
      "053A     6\n",
      "007A     6\n",
      "037A     6\n",
      "023A     6\n",
      "108A     6\n",
      "034A     5\n",
      "021A     5\n",
      "075A     5\n",
      "025C     5\n",
      "044A     5\n",
      "070A     5\n",
      "052A     4\n",
      "035A     4\n",
      "062A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "003A     4\n",
      "058A     3\n",
      "064A     3\n",
      "012A     3\n",
      "060A     3\n",
      "006A     3\n",
      "014A     3\n",
      "054A     2\n",
      "032A     2\n",
      "011A     2\n",
      "102A     2\n",
      "038A     2\n",
      "069A     2\n",
      "061A     2\n",
      "025B     2\n",
      "093A     2\n",
      "018A     2\n",
      "019B     1\n",
      "091A     1\n",
      "115A     1\n",
      "100A     1\n",
      "024A     1\n",
      "043A     1\n",
      "048A     1\n",
      "088A     1\n",
      "004A     1\n",
      "073A     1\n",
      "026C     1\n",
      "076A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Group Distribution:\n",
      "057A    27\n",
      "097A    16\n",
      "097B    14\n",
      "111A    13\n",
      "039A    12\n",
      "068A    11\n",
      "071A    10\n",
      "016A    10\n",
      "033A     9\n",
      "072A     9\n",
      "094A     8\n",
      "050A     7\n",
      "008A     6\n",
      "109A     6\n",
      "023B     5\n",
      "009A     4\n",
      "056A     3\n",
      "113A     3\n",
      "087A     2\n",
      "066A     1\n",
      "110A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    296\n",
      "F    244\n",
      "X    219\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    129\n",
      "M     41\n",
      "F      8\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 015A, 001A, 103A, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 040A, 046A, 047A, 042A, 043A, 049...\n",
      "senior    [093A, 106A, 104A, 055A, 059A, 116A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 071A, 097B, 072A, 039A, 009A, 087A, 068...\n",
      "kitten                             [111A, 109A, 050A, 110A]\n",
      "senior           [097A, 057A, 113A, 056A, 016A, 094A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 63, 'kitten': 12, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 11, 'kitten': 4, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B'\n",
      " '020A' '021A' '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B'\n",
      " '026C' '027A' '028A' '029A' '031A' '032A' '034A' '035A' '036A' '037A'\n",
      " '038A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '047A' '048A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '058A' '059A' '060A'\n",
      " '061A' '062A' '063A' '064A' '065A' '067A' '069A' '070A' '073A' '074A'\n",
      " '075A' '076A' '088A' '091A' '092A' '093A' '095A' '096A' '099A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['008A' '009A' '016A' '023B' '033A' '039A' '050A' '056A' '057A' '066A'\n",
      " '068A' '071A' '072A' '087A' '090A' '094A' '097A' '097B' '109A' '110A'\n",
      " '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B'\n",
      " '020A' '021A' '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B'\n",
      " '026C' '027A' '028A' '029A' '031A' '032A' '034A' '035A' '036A' '037A'\n",
      " '038A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '047A' '048A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '058A' '059A' '060A'\n",
      " '061A' '062A' '063A' '064A' '065A' '067A' '069A' '070A' '073A' '074A'\n",
      " '075A' '076A' '088A' '091A' '092A' '093A' '095A' '096A' '099A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['008A' '009A' '016A' '023B' '033A' '039A' '050A' '056A' '057A' '066A'\n",
      " '068A' '071A' '072A' '087A' '090A' '094A' '097A' '097B' '109A' '110A'\n",
      " '111A' '113A']\n",
      "Length of X_train_val:\n",
      "759\n",
      "Length of y_train_val:\n",
      "759\n",
      "Length of groups_train_val:\n",
      "759\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     505\n",
      "kitten    144\n",
      "senior    110\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     83\n",
      "senior    68\n",
      "kitten    27\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     505\n",
      "kitten    144\n",
      "senior    110\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     83\n",
      "senior    68\n",
      "kitten    27\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1010, 1: 576, 2: 440})\n",
      "Epoch 1/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.0048 - accuracy: 0.5355\n",
      "Epoch 2/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.7188 - accuracy: 0.6510\n",
      "Epoch 3/1500\n",
      "64/64 [==============================] - 0s 976us/step - loss: 0.6722 - accuracy: 0.6910\n",
      "Epoch 4/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.6140 - accuracy: 0.7098\n",
      "Epoch 5/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5728 - accuracy: 0.7300\n",
      "Epoch 6/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5613 - accuracy: 0.7428\n",
      "Epoch 7/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5287 - accuracy: 0.7468\n",
      "Epoch 8/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5226 - accuracy: 0.7665\n",
      "Epoch 9/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4834 - accuracy: 0.7808\n",
      "Epoch 10/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4737 - accuracy: 0.7759\n",
      "Epoch 11/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4701 - accuracy: 0.7883\n",
      "Epoch 12/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.7912\n",
      "Epoch 13/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4428 - accuracy: 0.7887\n",
      "Epoch 14/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4423 - accuracy: 0.7858\n",
      "Epoch 15/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8036\n",
      "Epoch 16/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4371 - accuracy: 0.8021\n",
      "Epoch 17/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4073 - accuracy: 0.8159\n",
      "Epoch 18/1500\n",
      "64/64 [==============================] - 0s 958us/step - loss: 0.3910 - accuracy: 0.8268\n",
      "Epoch 19/1500\n",
      "64/64 [==============================] - 0s 996us/step - loss: 0.4042 - accuracy: 0.8174\n",
      "Epoch 20/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4295 - accuracy: 0.8050\n",
      "Epoch 21/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8272\n",
      "Epoch 22/1500\n",
      "64/64 [==============================] - 0s 996us/step - loss: 0.3877 - accuracy: 0.8223\n",
      "Epoch 23/1500\n",
      "64/64 [==============================] - 0s 968us/step - loss: 0.3832 - accuracy: 0.8361\n",
      "Epoch 24/1500\n",
      "64/64 [==============================] - 0s 980us/step - loss: 0.3613 - accuracy: 0.8386\n",
      "Epoch 25/1500\n",
      "64/64 [==============================] - 0s 981us/step - loss: 0.3431 - accuracy: 0.8411\n",
      "Epoch 26/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.3715 - accuracy: 0.8243\n",
      "Epoch 27/1500\n",
      "64/64 [==============================] - 0s 948us/step - loss: 0.3536 - accuracy: 0.8430\n",
      "Epoch 28/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3297 - accuracy: 0.8396\n",
      "Epoch 29/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.3467 - accuracy: 0.8401\n",
      "Epoch 30/1500\n",
      "64/64 [==============================] - 0s 953us/step - loss: 0.3365 - accuracy: 0.8470\n",
      "Epoch 31/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8460\n",
      "Epoch 32/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3035 - accuracy: 0.8682\n",
      "Epoch 33/1500\n",
      "64/64 [==============================] - 0s 999us/step - loss: 0.3300 - accuracy: 0.8435\n",
      "Epoch 34/1500\n",
      "64/64 [==============================] - 0s 973us/step - loss: 0.3123 - accuracy: 0.8500\n",
      "Epoch 35/1500\n",
      "64/64 [==============================] - 0s 996us/step - loss: 0.3270 - accuracy: 0.8445\n",
      "Epoch 36/1500\n",
      "64/64 [==============================] - 0s 993us/step - loss: 0.3160 - accuracy: 0.8643\n",
      "Epoch 37/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2918 - accuracy: 0.8657\n",
      "Epoch 38/1500\n",
      "64/64 [==============================] - 0s 993us/step - loss: 0.3007 - accuracy: 0.8667\n",
      "Epoch 39/1500\n",
      "64/64 [==============================] - 0s 989us/step - loss: 0.3136 - accuracy: 0.8667\n",
      "Epoch 40/1500\n",
      "64/64 [==============================] - 0s 979us/step - loss: 0.3094 - accuracy: 0.8539\n",
      "Epoch 41/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2943 - accuracy: 0.8643\n",
      "Epoch 42/1500\n",
      "64/64 [==============================] - 0s 999us/step - loss: 0.2957 - accuracy: 0.8677\n",
      "Epoch 43/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2778 - accuracy: 0.8761\n",
      "Epoch 44/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2837 - accuracy: 0.8662\n",
      "Epoch 45/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2714 - accuracy: 0.8746\n",
      "Epoch 46/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2762 - accuracy: 0.8791\n",
      "Epoch 47/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2717 - accuracy: 0.8786\n",
      "Epoch 48/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2673 - accuracy: 0.8781\n",
      "Epoch 49/1500\n",
      "64/64 [==============================] - 0s 953us/step - loss: 0.2709 - accuracy: 0.8717\n",
      "Epoch 50/1500\n",
      "64/64 [==============================] - 0s 969us/step - loss: 0.2655 - accuracy: 0.8756\n",
      "Epoch 51/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.2500 - accuracy: 0.9008\n",
      "Epoch 52/1500\n",
      "64/64 [==============================] - 0s 974us/step - loss: 0.2636 - accuracy: 0.8840\n",
      "Epoch 53/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2680 - accuracy: 0.8850\n",
      "Epoch 54/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2493 - accuracy: 0.8919\n",
      "Epoch 55/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2414 - accuracy: 0.8934\n",
      "Epoch 56/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2408 - accuracy: 0.8963\n",
      "Epoch 57/1500\n",
      "64/64 [==============================] - 0s 992us/step - loss: 0.2476 - accuracy: 0.8934\n",
      "Epoch 58/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2420 - accuracy: 0.8954\n",
      "Epoch 59/1500\n",
      "64/64 [==============================] - 0s 996us/step - loss: 0.2504 - accuracy: 0.8850\n",
      "Epoch 60/1500\n",
      "64/64 [==============================] - 0s 969us/step - loss: 0.2373 - accuracy: 0.8998\n",
      "Epoch 61/1500\n",
      "64/64 [==============================] - 0s 985us/step - loss: 0.2408 - accuracy: 0.8973\n",
      "Epoch 62/1500\n",
      "64/64 [==============================] - 0s 983us/step - loss: 0.2299 - accuracy: 0.9018\n",
      "Epoch 63/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.2395 - accuracy: 0.8978\n",
      "Epoch 64/1500\n",
      "64/64 [==============================] - 0s 962us/step - loss: 0.2364 - accuracy: 0.8959\n",
      "Epoch 65/1500\n",
      "64/64 [==============================] - 0s 943us/step - loss: 0.2478 - accuracy: 0.8899\n",
      "Epoch 66/1500\n",
      "64/64 [==============================] - 0s 963us/step - loss: 0.2376 - accuracy: 0.8944\n",
      "Epoch 67/1500\n",
      "64/64 [==============================] - 0s 963us/step - loss: 0.2267 - accuracy: 0.9087\n",
      "Epoch 68/1500\n",
      "64/64 [==============================] - 0s 966us/step - loss: 0.2350 - accuracy: 0.8949\n",
      "Epoch 69/1500\n",
      "64/64 [==============================] - 0s 938us/step - loss: 0.2326 - accuracy: 0.8978\n",
      "Epoch 70/1500\n",
      "64/64 [==============================] - 0s 995us/step - loss: 0.2278 - accuracy: 0.8959\n",
      "Epoch 71/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2099 - accuracy: 0.9087\n",
      "Epoch 72/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9161\n",
      "Epoch 73/1500\n",
      "64/64 [==============================] - 0s 974us/step - loss: 0.2190 - accuracy: 0.9107\n",
      "Epoch 74/1500\n",
      "64/64 [==============================] - 0s 952us/step - loss: 0.2262 - accuracy: 0.9087\n",
      "Epoch 75/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.2231 - accuracy: 0.9062\n",
      "Epoch 76/1500\n",
      "64/64 [==============================] - 0s 996us/step - loss: 0.2110 - accuracy: 0.9072\n",
      "Epoch 77/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.2236 - accuracy: 0.8968\n",
      "Epoch 78/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2177 - accuracy: 0.9082\n",
      "Epoch 79/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9195\n",
      "Epoch 80/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9052\n",
      "Epoch 81/1500\n",
      "64/64 [==============================] - 0s 979us/step - loss: 0.2087 - accuracy: 0.9057\n",
      "Epoch 82/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2135 - accuracy: 0.9033\n",
      "Epoch 83/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9195\n",
      "Epoch 84/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9116\n",
      "Epoch 85/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2171 - accuracy: 0.9121\n",
      "Epoch 86/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9146\n",
      "Epoch 87/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9107\n",
      "Epoch 88/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1892 - accuracy: 0.9181\n",
      "Epoch 89/1500\n",
      "64/64 [==============================] - 0s 950us/step - loss: 0.1887 - accuracy: 0.9205\n",
      "Epoch 90/1500\n",
      "64/64 [==============================] - 0s 988us/step - loss: 0.2046 - accuracy: 0.9186\n",
      "Epoch 91/1500\n",
      "64/64 [==============================] - 0s 986us/step - loss: 0.1909 - accuracy: 0.9171\n",
      "Epoch 92/1500\n",
      "64/64 [==============================] - 0s 974us/step - loss: 0.2101 - accuracy: 0.9136\n",
      "Epoch 93/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1908 - accuracy: 0.9269\n",
      "Epoch 94/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9210\n",
      "Epoch 95/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9131\n",
      "Epoch 96/1500\n",
      "64/64 [==============================] - 0s 994us/step - loss: 0.1857 - accuracy: 0.9215\n",
      "Epoch 97/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.1884 - accuracy: 0.9210\n",
      "Epoch 98/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9235\n",
      "Epoch 99/1500\n",
      "64/64 [==============================] - 0s 977us/step - loss: 0.1909 - accuracy: 0.9210\n",
      "Epoch 100/1500\n",
      "64/64 [==============================] - 0s 968us/step - loss: 0.1736 - accuracy: 0.9191\n",
      "Epoch 101/1500\n",
      "64/64 [==============================] - 0s 930us/step - loss: 0.1789 - accuracy: 0.9309\n",
      "Epoch 102/1500\n",
      "64/64 [==============================] - 0s 964us/step - loss: 0.1968 - accuracy: 0.9146\n",
      "Epoch 103/1500\n",
      "64/64 [==============================] - 0s 967us/step - loss: 0.1705 - accuracy: 0.9324\n",
      "Epoch 104/1500\n",
      "64/64 [==============================] - 0s 969us/step - loss: 0.1815 - accuracy: 0.9284\n",
      "Epoch 105/1500\n",
      "64/64 [==============================] - 0s 964us/step - loss: 0.1570 - accuracy: 0.9284\n",
      "Epoch 106/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.9348\n",
      "Epoch 107/1500\n",
      "64/64 [==============================] - 0s 963us/step - loss: 0.1575 - accuracy: 0.9304\n",
      "Epoch 108/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9230\n",
      "Epoch 109/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9314\n",
      "Epoch 110/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9383\n",
      "Epoch 111/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.9235\n",
      "Epoch 112/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9383\n",
      "Epoch 113/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1602 - accuracy: 0.9344\n",
      "Epoch 114/1500\n",
      "64/64 [==============================] - 0s 975us/step - loss: 0.1654 - accuracy: 0.9210\n",
      "Epoch 115/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.9265\n",
      "Epoch 116/1500\n",
      "64/64 [==============================] - 0s 972us/step - loss: 0.1586 - accuracy: 0.9348\n",
      "Epoch 117/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.9314\n",
      "Epoch 118/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9289\n",
      "Epoch 119/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9304\n",
      "Epoch 120/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9368\n",
      "Epoch 121/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1627 - accuracy: 0.9373\n",
      "Epoch 122/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9304\n",
      "Epoch 123/1500\n",
      "64/64 [==============================] - 0s 977us/step - loss: 0.1517 - accuracy: 0.9363\n",
      "Epoch 124/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.9358\n",
      "Epoch 125/1500\n",
      "64/64 [==============================] - 0s 976us/step - loss: 0.1369 - accuracy: 0.9423\n",
      "Epoch 126/1500\n",
      "64/64 [==============================] - 0s 979us/step - loss: 0.1546 - accuracy: 0.9363\n",
      "Epoch 127/1500\n",
      "64/64 [==============================] - 0s 949us/step - loss: 0.1610 - accuracy: 0.9309\n",
      "Epoch 128/1500\n",
      "64/64 [==============================] - 0s 991us/step - loss: 0.1374 - accuracy: 0.9432\n",
      "Epoch 129/1500\n",
      "64/64 [==============================] - 0s 998us/step - loss: 0.1576 - accuracy: 0.9304\n",
      "Epoch 130/1500\n",
      "64/64 [==============================] - 0s 993us/step - loss: 0.1386 - accuracy: 0.9418\n",
      "Epoch 131/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.9398\n",
      "Epoch 132/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9393\n",
      "Epoch 133/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9393\n",
      "Epoch 134/1500\n",
      "64/64 [==============================] - 0s 985us/step - loss: 0.1340 - accuracy: 0.9408\n",
      "Epoch 135/1500\n",
      "64/64 [==============================] - 0s 990us/step - loss: 0.1400 - accuracy: 0.9408\n",
      "Epoch 136/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9457\n",
      "Epoch 137/1500\n",
      "64/64 [==============================] - 0s 954us/step - loss: 0.1420 - accuracy: 0.9398\n",
      "Epoch 138/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.1377 - accuracy: 0.9437\n",
      "Epoch 139/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9432\n",
      "Epoch 140/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9373\n",
      "Epoch 141/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9348\n",
      "Epoch 142/1500\n",
      "64/64 [==============================] - 0s 963us/step - loss: 0.1439 - accuracy: 0.9467\n",
      "Epoch 143/1500\n",
      "64/64 [==============================] - 0s 996us/step - loss: 0.1358 - accuracy: 0.9393\n",
      "Epoch 144/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9427\n",
      "Epoch 145/1500\n",
      "64/64 [==============================] - 0s 984us/step - loss: 0.1232 - accuracy: 0.9501\n",
      "Epoch 146/1500\n",
      "64/64 [==============================] - 0s 964us/step - loss: 0.1408 - accuracy: 0.9432\n",
      "Epoch 147/1500\n",
      "64/64 [==============================] - 0s 968us/step - loss: 0.1290 - accuracy: 0.9497\n",
      "Epoch 148/1500\n",
      "64/64 [==============================] - 0s 923us/step - loss: 0.1540 - accuracy: 0.9368\n",
      "Epoch 149/1500\n",
      "64/64 [==============================] - 0s 962us/step - loss: 0.1391 - accuracy: 0.9423\n",
      "Epoch 150/1500\n",
      "64/64 [==============================] - 0s 971us/step - loss: 0.1278 - accuracy: 0.9472\n",
      "Epoch 151/1500\n",
      "64/64 [==============================] - 0s 967us/step - loss: 0.1531 - accuracy: 0.9427\n",
      "Epoch 152/1500\n",
      "64/64 [==============================] - 0s 966us/step - loss: 0.1235 - accuracy: 0.9521\n",
      "Epoch 153/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9393\n",
      "Epoch 154/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1429 - accuracy: 0.9383\n",
      "Epoch 155/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9442\n",
      "Epoch 156/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9452\n",
      "Epoch 157/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9452\n",
      "Epoch 158/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9492\n",
      "Epoch 159/1500\n",
      "64/64 [==============================] - 0s 994us/step - loss: 0.1325 - accuracy: 0.9516\n",
      "Epoch 160/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.1302 - accuracy: 0.9492\n",
      "Epoch 161/1500\n",
      "64/64 [==============================] - 0s 992us/step - loss: 0.1392 - accuracy: 0.9442\n",
      "Epoch 162/1500\n",
      "64/64 [==============================] - 0s 968us/step - loss: 0.1294 - accuracy: 0.9497\n",
      "Epoch 163/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9521\n",
      "Epoch 164/1500\n",
      "64/64 [==============================] - 0s 944us/step - loss: 0.1206 - accuracy: 0.9506\n",
      "Epoch 165/1500\n",
      "64/64 [==============================] - 0s 966us/step - loss: 0.1176 - accuracy: 0.9511\n",
      "Epoch 166/1500\n",
      "64/64 [==============================] - 0s 988us/step - loss: 0.1259 - accuracy: 0.9487\n",
      "Epoch 167/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9536\n",
      "Epoch 168/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9462\n",
      "Epoch 169/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.9501\n",
      "Epoch 170/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.9467\n",
      "Epoch 171/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9492\n",
      "Epoch 172/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9556\n",
      "Epoch 173/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9472\n",
      "Epoch 174/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9556\n",
      "Epoch 175/1500\n",
      "64/64 [==============================] - 0s 993us/step - loss: 0.1104 - accuracy: 0.9580\n",
      "Epoch 176/1500\n",
      "64/64 [==============================] - 0s 985us/step - loss: 0.1205 - accuracy: 0.9497\n",
      "Epoch 177/1500\n",
      "64/64 [==============================] - 0s 964us/step - loss: 0.1229 - accuracy: 0.9492\n",
      "Epoch 178/1500\n",
      "64/64 [==============================] - 0s 960us/step - loss: 0.1197 - accuracy: 0.9516\n",
      "Epoch 179/1500\n",
      "64/64 [==============================] - 0s 946us/step - loss: 0.1127 - accuracy: 0.9467\n",
      "Epoch 180/1500\n",
      "64/64 [==============================] - 0s 971us/step - loss: 0.1358 - accuracy: 0.9418\n",
      "Epoch 181/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9531\n",
      "Epoch 182/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9610\n",
      "Epoch 183/1500\n",
      "64/64 [==============================] - 0s 988us/step - loss: 0.1149 - accuracy: 0.9541\n",
      "Epoch 184/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9457\n",
      "Epoch 185/1500\n",
      "64/64 [==============================] - 0s 966us/step - loss: 0.1240 - accuracy: 0.9526\n",
      "Epoch 186/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9556\n",
      "Epoch 187/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9546\n",
      "Epoch 188/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.9650\n",
      "Epoch 189/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9600\n",
      "Epoch 190/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9595\n",
      "Epoch 191/1500\n",
      "64/64 [==============================] - 0s 985us/step - loss: 0.1041 - accuracy: 0.9605\n",
      "Epoch 192/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9571\n",
      "Epoch 193/1500\n",
      "64/64 [==============================] - 0s 979us/step - loss: 0.1133 - accuracy: 0.9511\n",
      "Epoch 194/1500\n",
      "64/64 [==============================] - 0s 979us/step - loss: 0.1340 - accuracy: 0.9492\n",
      "Epoch 195/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9610\n",
      "Epoch 196/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9531\n",
      "Epoch 197/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9585\n",
      "Epoch 198/1500\n",
      "64/64 [==============================] - 0s 960us/step - loss: 0.1024 - accuracy: 0.9615\n",
      "Epoch 199/1500\n",
      "64/64 [==============================] - 0s 964us/step - loss: 0.1040 - accuracy: 0.9590\n",
      "Epoch 200/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9516\n",
      "Epoch 201/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9556\n",
      "Epoch 202/1500\n",
      "64/64 [==============================] - 0s 995us/step - loss: 0.1030 - accuracy: 0.9605\n",
      "Epoch 203/1500\n",
      "64/64 [==============================] - 0s 951us/step - loss: 0.1101 - accuracy: 0.9571\n",
      "Epoch 204/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9585\n",
      "Epoch 205/1500\n",
      "64/64 [==============================] - 0s 991us/step - loss: 0.1003 - accuracy: 0.9561\n",
      "Epoch 206/1500\n",
      "64/64 [==============================] - 0s 951us/step - loss: 0.1062 - accuracy: 0.9600\n",
      "Epoch 207/1500\n",
      "64/64 [==============================] - 0s 974us/step - loss: 0.0945 - accuracy: 0.9620\n",
      "Epoch 208/1500\n",
      "64/64 [==============================] - 0s 975us/step - loss: 0.1293 - accuracy: 0.9487\n",
      "Epoch 209/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9536\n",
      "Epoch 210/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9625\n",
      "Epoch 211/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9551\n",
      "Epoch 212/1500\n",
      "64/64 [==============================] - 0s 958us/step - loss: 0.0857 - accuracy: 0.9669\n",
      "Epoch 213/1500\n",
      "64/64 [==============================] - 0s 944us/step - loss: 0.0959 - accuracy: 0.9610\n",
      "Epoch 214/1500\n",
      "64/64 [==============================] - 0s 946us/step - loss: 0.1072 - accuracy: 0.9566\n",
      "Epoch 215/1500\n",
      "64/64 [==============================] - 0s 976us/step - loss: 0.1023 - accuracy: 0.9541\n",
      "Epoch 216/1500\n",
      "64/64 [==============================] - 0s 958us/step - loss: 0.1011 - accuracy: 0.9595\n",
      "Epoch 217/1500\n",
      "64/64 [==============================] - 0s 959us/step - loss: 0.1025 - accuracy: 0.9590\n",
      "Epoch 218/1500\n",
      "64/64 [==============================] - 0s 967us/step - loss: 0.0917 - accuracy: 0.9600\n",
      "Epoch 219/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9580\n",
      "Epoch 220/1500\n",
      "64/64 [==============================] - 0s 998us/step - loss: 0.0976 - accuracy: 0.9576\n",
      "Epoch 221/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9605\n",
      "Epoch 222/1500\n",
      "64/64 [==============================] - 0s 991us/step - loss: 0.1063 - accuracy: 0.9551\n",
      "Epoch 223/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9645\n",
      "Epoch 224/1500\n",
      "64/64 [==============================] - 0s 967us/step - loss: 0.0971 - accuracy: 0.9600\n",
      "Epoch 225/1500\n",
      "64/64 [==============================] - 0s 962us/step - loss: 0.0929 - accuracy: 0.9620\n",
      "Epoch 226/1500\n",
      "64/64 [==============================] - 0s 979us/step - loss: 0.1038 - accuracy: 0.9571\n",
      "Epoch 227/1500\n",
      "64/64 [==============================] - 0s 981us/step - loss: 0.0947 - accuracy: 0.9659\n",
      "Epoch 228/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9620\n",
      "Epoch 229/1500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9610\n",
      "Epoch 230/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9610\n",
      "Epoch 231/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.9615\n",
      "Epoch 232/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9654\n",
      "Epoch 233/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9664\n",
      "Epoch 234/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9620\n",
      "Epoch 235/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9679\n",
      "Epoch 236/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9714\n",
      "Epoch 237/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9714\n",
      "Epoch 238/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9610\n",
      "Epoch 239/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9669\n",
      "Epoch 240/1500\n",
      "64/64 [==============================] - 0s 979us/step - loss: 0.0916 - accuracy: 0.9610\n",
      "Epoch 241/1500\n",
      "64/64 [==============================] - 0s 958us/step - loss: 0.0756 - accuracy: 0.9724\n",
      "Epoch 242/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9659\n",
      "Epoch 243/1500\n",
      "64/64 [==============================] - 0s 964us/step - loss: 0.0891 - accuracy: 0.9709\n",
      "Epoch 244/1500\n",
      "64/64 [==============================] - 0s 938us/step - loss: 0.0884 - accuracy: 0.9650\n",
      "Epoch 245/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.0955 - accuracy: 0.9674\n",
      "Epoch 246/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9576\n",
      "Epoch 247/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9580\n",
      "Epoch 248/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.9689\n",
      "Epoch 249/1500\n",
      "64/64 [==============================] - 0s 989us/step - loss: 0.0930 - accuracy: 0.9645\n",
      "Epoch 250/1500\n",
      "64/64 [==============================] - 0s 963us/step - loss: 0.0949 - accuracy: 0.9585\n",
      "Epoch 251/1500\n",
      "64/64 [==============================] - 0s 990us/step - loss: 0.0805 - accuracy: 0.9679\n",
      "Epoch 252/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9600\n",
      "Epoch 253/1500\n",
      "64/64 [==============================] - 0s 988us/step - loss: 0.0863 - accuracy: 0.9650\n",
      "Epoch 254/1500\n",
      "64/64 [==============================] - 0s 991us/step - loss: 0.0784 - accuracy: 0.9654\n",
      "Epoch 255/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.9640\n",
      "Epoch 256/1500\n",
      "64/64 [==============================] - 0s 961us/step - loss: 0.0884 - accuracy: 0.9645\n",
      "Epoch 257/1500\n",
      "64/64 [==============================] - 0s 978us/step - loss: 0.0801 - accuracy: 0.9679\n",
      "Epoch 258/1500\n",
      "64/64 [==============================] - 0s 956us/step - loss: 0.0752 - accuracy: 0.9729\n",
      "Epoch 259/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9664\n",
      "Epoch 260/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9640\n",
      "Epoch 261/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9679\n",
      "Epoch 262/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9659\n",
      "Epoch 263/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9645\n",
      "Epoch 264/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9654\n",
      "Epoch 265/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9694\n",
      "Epoch 266/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9674\n",
      "Epoch 267/1500\n",
      "52/64 [=======================>......] - ETA: 0s - loss: 0.0930 - accuracy: 0.9639Restoring model weights from the end of the best epoch: 237.\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9630\n",
      "Epoch 267: early stopping\n",
      "6/6 [==============================] - 0s 783us/step - loss: 0.8692 - accuracy: 0.6798\n",
      "6/6 [==============================] - 0s 678us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.73 (16/22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before appending - Cat IDs: 279, Predictions: 279, Actuals: 279, Gender: 279\n",
      "After appending - Cat IDs: 457, Predictions: 457, Actuals: 457, Gender: 457\n",
      "Final Test Results - Loss: 0.869245707988739, Accuracy: 0.6797752976417542, Precision: 0.7397602397602397, Recall: 0.6931145059540996, F1 Score: 0.6848844884488449\n",
      "Confusion Matrix:\n",
      " [[72  6  5]\n",
      " [ 4 22  1]\n",
      " [41  0 27]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "002A    13\n",
      "111A    13\n",
      "028A    13\n",
      "116A    12\n",
      "039A    12\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "068A    11\n",
      "071A    10\n",
      "016A    10\n",
      "014B    10\n",
      "005A    10\n",
      "072A     9\n",
      "065A     9\n",
      "033A     9\n",
      "022A     9\n",
      "015A     9\n",
      "095A     8\n",
      "013B     8\n",
      "094A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "099A     7\n",
      "007A     6\n",
      "109A     6\n",
      "053A     6\n",
      "037A     6\n",
      "008A     6\n",
      "075A     5\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "070A     5\n",
      "021A     5\n",
      "023B     5\n",
      "105A     4\n",
      "104A     4\n",
      "026A     4\n",
      "052A     4\n",
      "009A     4\n",
      "003A     4\n",
      "062A     4\n",
      "035A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "014A     3\n",
      "058A     3\n",
      "102A     2\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "061A     2\n",
      "087A     2\n",
      "038A     2\n",
      "110A     1\n",
      "115A     1\n",
      "073A     1\n",
      "090A     1\n",
      "024A     1\n",
      "100A     1\n",
      "066A     1\n",
      "019B     1\n",
      "088A     1\n",
      "004A     1\n",
      "048A     1\n",
      "026C     1\n",
      "092A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "042A    14\n",
      "059A    14\n",
      "051A    12\n",
      "040A    10\n",
      "045A     9\n",
      "051B     9\n",
      "010A     8\n",
      "023A     6\n",
      "108A     6\n",
      "069A     2\n",
      "011A     2\n",
      "025B     2\n",
      "076A     1\n",
      "043A     1\n",
      "049A     1\n",
      "041A     1\n",
      "091A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    282\n",
      "M    214\n",
      "F    191\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    123\n",
      "X     66\n",
      "F     61\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 071A, 097B, 028...\n",
      "kitten    [044A, 014B, 111A, 046A, 109A, 050A, 048A, 115...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 113A, 116A, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [103A, 067A, 002B, 091A, 023A, 069A, 000B, 076...\n",
      "kitten           [040A, 047A, 042A, 043A, 049A, 041A, 045A]\n",
      "senior                 [055A, 059A, 051B, 051A, 108A, 011A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 64, 'kitten': 9, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 10, 'kitten': 7, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '003A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '021A' '022A' '023B' '024A' '025A' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '029A' '031A' '032A' '033A' '034A' '035A' '036A' '037A' '038A'\n",
      " '039A' '044A' '046A' '048A' '050A' '052A' '053A' '054A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '064A' '065A' '066A' '068A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '087A' '088A' '090A' '092A' '093A'\n",
      " '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '104A'\n",
      " '105A' '106A' '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002B' '010A' '011A' '023A' '025B' '040A' '041A' '042A' '043A'\n",
      " '045A' '047A' '049A' '051A' '051B' '055A' '059A' '067A' '069A' '076A'\n",
      " '091A' '103A' '108A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '003A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '021A' '022A' '023B' '024A' '025A' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '029A' '031A' '032A' '033A' '034A' '035A' '036A' '037A' '038A'\n",
      " '039A' '044A' '046A' '048A' '050A' '052A' '053A' '054A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '064A' '065A' '066A' '068A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '087A' '088A' '090A' '092A' '093A'\n",
      " '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '104A'\n",
      " '105A' '106A' '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002B' '010A' '011A' '023A' '025B' '040A' '041A' '042A' '043A'\n",
      " '045A' '047A' '049A' '051A' '051B' '055A' '059A' '067A' '069A' '076A'\n",
      " '091A' '103A' '108A']\n",
      "Length of X_train_val:\n",
      "687\n",
      "Length of y_train_val:\n",
      "687\n",
      "Length of groups_train_val:\n",
      "687\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     465\n",
      "senior    115\n",
      "kitten    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     123\n",
      "kitten     64\n",
      "senior     63\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     465\n",
      "senior    115\n",
      "kitten    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     123\n",
      "kitten     64\n",
      "senior     63\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 930, 2: 460, 1: 428})\n",
      "Epoch 1/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.9603 - accuracy: 0.5171\n",
      "Epoch 2/1500\n",
      "57/57 [==============================] - 0s 985us/step - loss: 0.7808 - accuracy: 0.5952\n",
      "Epoch 3/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.7077 - accuracy: 0.6425\n",
      "Epoch 4/1500\n",
      "57/57 [==============================] - 0s 976us/step - loss: 0.6655 - accuracy: 0.6656\n",
      "Epoch 5/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.6098 - accuracy: 0.6848\n",
      "Epoch 6/1500\n",
      "57/57 [==============================] - 0s 984us/step - loss: 0.6040 - accuracy: 0.6997\n",
      "Epoch 7/1500\n",
      "57/57 [==============================] - 0s 970us/step - loss: 0.5990 - accuracy: 0.7112\n",
      "Epoch 8/1500\n",
      "57/57 [==============================] - 0s 965us/step - loss: 0.5267 - accuracy: 0.7360\n",
      "Epoch 9/1500\n",
      "57/57 [==============================] - 0s 974us/step - loss: 0.5270 - accuracy: 0.7371\n",
      "Epoch 10/1500\n",
      "57/57 [==============================] - 0s 979us/step - loss: 0.4986 - accuracy: 0.7563\n",
      "Epoch 11/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.4876 - accuracy: 0.7679\n",
      "Epoch 12/1500\n",
      "57/57 [==============================] - 0s 1000us/step - loss: 0.4675 - accuracy: 0.7668\n",
      "Epoch 13/1500\n",
      "57/57 [==============================] - 0s 975us/step - loss: 0.4860 - accuracy: 0.7706\n",
      "Epoch 14/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.4718 - accuracy: 0.7805\n",
      "Epoch 15/1500\n",
      "57/57 [==============================] - 0s 987us/step - loss: 0.4560 - accuracy: 0.7849\n",
      "Epoch 16/1500\n",
      "57/57 [==============================] - 0s 970us/step - loss: 0.4532 - accuracy: 0.7871\n",
      "Epoch 17/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.4351 - accuracy: 0.7899\n",
      "Epoch 18/1500\n",
      "57/57 [==============================] - 0s 969us/step - loss: 0.4305 - accuracy: 0.8058\n",
      "Epoch 19/1500\n",
      "57/57 [==============================] - 0s 950us/step - loss: 0.4191 - accuracy: 0.7987\n",
      "Epoch 20/1500\n",
      "57/57 [==============================] - 0s 954us/step - loss: 0.4193 - accuracy: 0.7998\n",
      "Epoch 21/1500\n",
      "57/57 [==============================] - 0s 972us/step - loss: 0.4022 - accuracy: 0.8113\n",
      "Epoch 22/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.4008 - accuracy: 0.8069\n",
      "Epoch 23/1500\n",
      "57/57 [==============================] - 0s 989us/step - loss: 0.3947 - accuracy: 0.8119\n",
      "Epoch 24/1500\n",
      "57/57 [==============================] - 0s 995us/step - loss: 0.3916 - accuracy: 0.8130\n",
      "Epoch 25/1500\n",
      "57/57 [==============================] - 0s 993us/step - loss: 0.3701 - accuracy: 0.8201\n",
      "Epoch 26/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.3846 - accuracy: 0.8102\n",
      "Epoch 27/1500\n",
      "57/57 [==============================] - 0s 973us/step - loss: 0.3615 - accuracy: 0.8355\n",
      "Epoch 28/1500\n",
      "57/57 [==============================] - 0s 969us/step - loss: 0.3674 - accuracy: 0.8311\n",
      "Epoch 29/1500\n",
      "57/57 [==============================] - 0s 991us/step - loss: 0.3599 - accuracy: 0.8377\n",
      "Epoch 30/1500\n",
      "57/57 [==============================] - 0s 969us/step - loss: 0.3262 - accuracy: 0.8482\n",
      "Epoch 31/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8454\n",
      "Epoch 32/1500\n",
      "57/57 [==============================] - 0s 972us/step - loss: 0.3562 - accuracy: 0.8317\n",
      "Epoch 33/1500\n",
      "57/57 [==============================] - 0s 999us/step - loss: 0.3337 - accuracy: 0.8416\n",
      "Epoch 34/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.3148 - accuracy: 0.8641\n",
      "Epoch 35/1500\n",
      "57/57 [==============================] - 0s 981us/step - loss: 0.3473 - accuracy: 0.8443\n",
      "Epoch 36/1500\n",
      "57/57 [==============================] - 0s 981us/step - loss: 0.3283 - accuracy: 0.8399\n",
      "Epoch 37/1500\n",
      "57/57 [==============================] - 0s 968us/step - loss: 0.3161 - accuracy: 0.8504\n",
      "Epoch 38/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.3091 - accuracy: 0.8465\n",
      "Epoch 39/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2891 - accuracy: 0.8702\n",
      "Epoch 40/1500\n",
      "57/57 [==============================] - 0s 962us/step - loss: 0.3031 - accuracy: 0.8625\n",
      "Epoch 41/1500\n",
      "57/57 [==============================] - 0s 964us/step - loss: 0.3333 - accuracy: 0.8410\n",
      "Epoch 42/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.3084 - accuracy: 0.8537\n",
      "Epoch 43/1500\n",
      "57/57 [==============================] - 0s 971us/step - loss: 0.2934 - accuracy: 0.8570\n",
      "Epoch 44/1500\n",
      "57/57 [==============================] - 0s 979us/step - loss: 0.3032 - accuracy: 0.8586\n",
      "Epoch 45/1500\n",
      "57/57 [==============================] - 0s 979us/step - loss: 0.3045 - accuracy: 0.8586\n",
      "Epoch 46/1500\n",
      "57/57 [==============================] - 0s 964us/step - loss: 0.2918 - accuracy: 0.8592\n",
      "Epoch 47/1500\n",
      "57/57 [==============================] - 0s 966us/step - loss: 0.2819 - accuracy: 0.8652\n",
      "Epoch 48/1500\n",
      "57/57 [==============================] - 0s 961us/step - loss: 0.3024 - accuracy: 0.8641\n",
      "Epoch 49/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.3158 - accuracy: 0.8520\n",
      "Epoch 50/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2799 - accuracy: 0.8768\n",
      "Epoch 51/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2758 - accuracy: 0.8696\n",
      "Epoch 52/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2714 - accuracy: 0.8707\n",
      "Epoch 53/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2561 - accuracy: 0.8861\n",
      "Epoch 54/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2839 - accuracy: 0.8757\n",
      "Epoch 55/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2638 - accuracy: 0.8740\n",
      "Epoch 56/1500\n",
      "57/57 [==============================] - 0s 984us/step - loss: 0.2786 - accuracy: 0.8630\n",
      "Epoch 57/1500\n",
      "57/57 [==============================] - 0s 986us/step - loss: 0.2629 - accuracy: 0.8878\n",
      "Epoch 58/1500\n",
      "57/57 [==============================] - 0s 979us/step - loss: 0.2615 - accuracy: 0.8746\n",
      "Epoch 59/1500\n",
      "57/57 [==============================] - 0s 983us/step - loss: 0.2446 - accuracy: 0.8828\n",
      "Epoch 60/1500\n",
      "57/57 [==============================] - 0s 965us/step - loss: 0.2651 - accuracy: 0.8740\n",
      "Epoch 61/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2492 - accuracy: 0.8784\n",
      "Epoch 62/1500\n",
      "57/57 [==============================] - 0s 999us/step - loss: 0.2725 - accuracy: 0.8779\n",
      "Epoch 63/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.8795\n",
      "Epoch 64/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2587 - accuracy: 0.8823\n",
      "Epoch 65/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2476 - accuracy: 0.8861\n",
      "Epoch 66/1500\n",
      "57/57 [==============================] - 0s 962us/step - loss: 0.2507 - accuracy: 0.8905\n",
      "Epoch 67/1500\n",
      "57/57 [==============================] - 0s 995us/step - loss: 0.2388 - accuracy: 0.8878\n",
      "Epoch 68/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2455 - accuracy: 0.8773\n",
      "Epoch 69/1500\n",
      "57/57 [==============================] - 0s 966us/step - loss: 0.2540 - accuracy: 0.8839\n",
      "Epoch 70/1500\n",
      "57/57 [==============================] - 0s 978us/step - loss: 0.2323 - accuracy: 0.8966\n",
      "Epoch 71/1500\n",
      "57/57 [==============================] - 0s 962us/step - loss: 0.2271 - accuracy: 0.8883\n",
      "Epoch 72/1500\n",
      "57/57 [==============================] - 0s 963us/step - loss: 0.2250 - accuracy: 0.8927\n",
      "Epoch 73/1500\n",
      "57/57 [==============================] - 0s 967us/step - loss: 0.2193 - accuracy: 0.8938\n",
      "Epoch 74/1500\n",
      "57/57 [==============================] - 0s 956us/step - loss: 0.2319 - accuracy: 0.8938\n",
      "Epoch 75/1500\n",
      "57/57 [==============================] - 0s 968us/step - loss: 0.2273 - accuracy: 0.8949\n",
      "Epoch 76/1500\n",
      "57/57 [==============================] - 0s 947us/step - loss: 0.2193 - accuracy: 0.8966\n",
      "Epoch 77/1500\n",
      "57/57 [==============================] - 0s 965us/step - loss: 0.2315 - accuracy: 0.8894\n",
      "Epoch 78/1500\n",
      "57/57 [==============================] - 0s 969us/step - loss: 0.2326 - accuracy: 0.9048\n",
      "Epoch 79/1500\n",
      "57/57 [==============================] - 0s 968us/step - loss: 0.2277 - accuracy: 0.8944\n",
      "Epoch 80/1500\n",
      "57/57 [==============================] - 0s 970us/step - loss: 0.2066 - accuracy: 0.9054\n",
      "Epoch 81/1500\n",
      "57/57 [==============================] - 0s 976us/step - loss: 0.2211 - accuracy: 0.9021\n",
      "Epoch 82/1500\n",
      "57/57 [==============================] - 0s 966us/step - loss: 0.1875 - accuracy: 0.9070\n",
      "Epoch 83/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2127 - accuracy: 0.9076\n",
      "Epoch 84/1500\n",
      "57/57 [==============================] - 0s 967us/step - loss: 0.2110 - accuracy: 0.9010\n",
      "Epoch 85/1500\n",
      "57/57 [==============================] - 0s 961us/step - loss: 0.2164 - accuracy: 0.8993\n",
      "Epoch 86/1500\n",
      "57/57 [==============================] - 0s 971us/step - loss: 0.2004 - accuracy: 0.9076\n",
      "Epoch 87/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2157 - accuracy: 0.8993\n",
      "Epoch 88/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9180\n",
      "Epoch 89/1500\n",
      "57/57 [==============================] - 0s 976us/step - loss: 0.2070 - accuracy: 0.9010\n",
      "Epoch 90/1500\n",
      "57/57 [==============================] - 0s 967us/step - loss: 0.2072 - accuracy: 0.9087\n",
      "Epoch 91/1500\n",
      "57/57 [==============================] - 0s 977us/step - loss: 0.2111 - accuracy: 0.9054\n",
      "Epoch 92/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9004\n",
      "Epoch 93/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2239 - accuracy: 0.8911\n",
      "Epoch 94/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.9098\n",
      "Epoch 95/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2136 - accuracy: 0.9015\n",
      "Epoch 96/1500\n",
      "57/57 [==============================] - 0s 972us/step - loss: 0.2092 - accuracy: 0.8988\n",
      "Epoch 97/1500\n",
      "57/57 [==============================] - 0s 980us/step - loss: 0.1775 - accuracy: 0.9169\n",
      "Epoch 98/1500\n",
      "57/57 [==============================] - 0s 971us/step - loss: 0.1820 - accuracy: 0.9125\n",
      "Epoch 99/1500\n",
      "57/57 [==============================] - 0s 982us/step - loss: 0.1759 - accuracy: 0.9257\n",
      "Epoch 100/1500\n",
      "57/57 [==============================] - 0s 992us/step - loss: 0.1917 - accuracy: 0.9197\n",
      "Epoch 101/1500\n",
      "57/57 [==============================] - 0s 973us/step - loss: 0.1793 - accuracy: 0.9219\n",
      "Epoch 102/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9081\n",
      "Epoch 103/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.8982\n",
      "Epoch 104/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1692 - accuracy: 0.9246\n",
      "Epoch 105/1500\n",
      "57/57 [==============================] - 0s 979us/step - loss: 0.1722 - accuracy: 0.9197\n",
      "Epoch 106/1500\n",
      "57/57 [==============================] - 0s 952us/step - loss: 0.1731 - accuracy: 0.9263\n",
      "Epoch 107/1500\n",
      "57/57 [==============================] - 0s 939us/step - loss: 0.1834 - accuracy: 0.9175\n",
      "Epoch 108/1500\n",
      "57/57 [==============================] - 0s 968us/step - loss: 0.1793 - accuracy: 0.9208\n",
      "Epoch 109/1500\n",
      "57/57 [==============================] - 0s 950us/step - loss: 0.1734 - accuracy: 0.9175\n",
      "Epoch 110/1500\n",
      "57/57 [==============================] - 0s 969us/step - loss: 0.1833 - accuracy: 0.9175\n",
      "Epoch 111/1500\n",
      "57/57 [==============================] - 0s 969us/step - loss: 0.1693 - accuracy: 0.9235\n",
      "Epoch 112/1500\n",
      "57/57 [==============================] - 0s 984us/step - loss: 0.1846 - accuracy: 0.9202\n",
      "Epoch 113/1500\n",
      "57/57 [==============================] - 0s 967us/step - loss: 0.1799 - accuracy: 0.9147\n",
      "Epoch 114/1500\n",
      "57/57 [==============================] - 0s 969us/step - loss: 0.1855 - accuracy: 0.9186\n",
      "Epoch 115/1500\n",
      "57/57 [==============================] - 0s 973us/step - loss: 0.1802 - accuracy: 0.9120\n",
      "Epoch 116/1500\n",
      "57/57 [==============================] - 0s 967us/step - loss: 0.1612 - accuracy: 0.9318\n",
      "Epoch 117/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9197\n",
      "Epoch 118/1500\n",
      "57/57 [==============================] - 0s 978us/step - loss: 0.1771 - accuracy: 0.9235\n",
      "Epoch 119/1500\n",
      "57/57 [==============================] - 0s 957us/step - loss: 0.1768 - accuracy: 0.9246\n",
      "Epoch 120/1500\n",
      "57/57 [==============================] - 0s 938us/step - loss: 0.1698 - accuracy: 0.9136\n",
      "Epoch 121/1500\n",
      "57/57 [==============================] - 0s 987us/step - loss: 0.1806 - accuracy: 0.9120\n",
      "Epoch 122/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9301\n",
      "Epoch 123/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9263\n",
      "Epoch 124/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.9389\n",
      "Epoch 125/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9329\n",
      "Epoch 126/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9312\n",
      "Epoch 127/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9334\n",
      "Epoch 128/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9318\n",
      "Epoch 129/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9373\n",
      "Epoch 130/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9268\n",
      "Epoch 131/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9340\n",
      "Epoch 132/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9290\n",
      "Epoch 133/1500\n",
      "57/57 [==============================] - 0s 992us/step - loss: 0.1552 - accuracy: 0.9367\n",
      "Epoch 134/1500\n",
      "57/57 [==============================] - 0s 972us/step - loss: 0.1486 - accuracy: 0.9367\n",
      "Epoch 135/1500\n",
      "57/57 [==============================] - 0s 995us/step - loss: 0.1549 - accuracy: 0.9318\n",
      "Epoch 136/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9263\n",
      "Epoch 137/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9296\n",
      "Epoch 138/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9400\n",
      "Epoch 139/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9362\n",
      "Epoch 140/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9329\n",
      "Epoch 141/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1477 - accuracy: 0.9373\n",
      "Epoch 142/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9428\n",
      "Epoch 143/1500\n",
      "57/57 [==============================] - 0s 980us/step - loss: 0.1561 - accuracy: 0.9356\n",
      "Epoch 144/1500\n",
      "57/57 [==============================] - 0s 986us/step - loss: 0.1373 - accuracy: 0.9461\n",
      "Epoch 145/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.9307\n",
      "Epoch 146/1500\n",
      "57/57 [==============================] - 0s 985us/step - loss: 0.1420 - accuracy: 0.9428\n",
      "Epoch 147/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9362\n",
      "Epoch 148/1500\n",
      "57/57 [==============================] - 0s 989us/step - loss: 0.1512 - accuracy: 0.9296\n",
      "Epoch 149/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1534 - accuracy: 0.9285\n",
      "Epoch 150/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9422\n",
      "Epoch 151/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9444\n",
      "Epoch 152/1500\n",
      "57/57 [==============================] - 0s 971us/step - loss: 0.1407 - accuracy: 0.9411\n",
      "Epoch 153/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9450\n",
      "Epoch 154/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9433\n",
      "Epoch 155/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9406\n",
      "Epoch 156/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9450\n",
      "Epoch 157/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9488\n",
      "Epoch 158/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9450\n",
      "Epoch 159/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9477\n",
      "Epoch 160/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9395\n",
      "Epoch 161/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9455\n",
      "Epoch 162/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9373\n",
      "Epoch 163/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9450\n",
      "Epoch 164/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.9444\n",
      "Epoch 165/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9439\n",
      "Epoch 166/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.9444\n",
      "Epoch 167/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9571\n",
      "Epoch 168/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9472\n",
      "Epoch 169/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9417\n",
      "Epoch 170/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9428\n",
      "Epoch 171/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9505\n",
      "Epoch 172/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9521\n",
      "Epoch 173/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9367\n",
      "Epoch 174/1500\n",
      "57/57 [==============================] - 0s 970us/step - loss: 0.1310 - accuracy: 0.9450\n",
      "Epoch 175/1500\n",
      "57/57 [==============================] - 0s 977us/step - loss: 0.1113 - accuracy: 0.9593\n",
      "Epoch 176/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9466\n",
      "Epoch 177/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9488\n",
      "Epoch 178/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9510\n",
      "Epoch 179/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.9505\n",
      "Epoch 180/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9428\n",
      "Epoch 181/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9538\n",
      "Epoch 182/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9439\n",
      "Epoch 183/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9543\n",
      "Epoch 184/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9455\n",
      "Epoch 185/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9598\n",
      "Epoch 186/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9400\n",
      "Epoch 187/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9543\n",
      "Epoch 188/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9560\n",
      "Epoch 189/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9543\n",
      "Epoch 190/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9538\n",
      "Epoch 191/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9450\n",
      "Epoch 192/1500\n",
      "57/57 [==============================] - 0s 994us/step - loss: 0.1421 - accuracy: 0.9461\n",
      "Epoch 193/1500\n",
      "57/57 [==============================] - 0s 985us/step - loss: 0.1238 - accuracy: 0.9428\n",
      "Epoch 194/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9499\n",
      "Epoch 195/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9532\n",
      "Epoch 196/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9560\n",
      "Epoch 197/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9549\n",
      "Epoch 198/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9505\n",
      "Epoch 199/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9554\n",
      "Epoch 200/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9510\n",
      "Epoch 201/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9560\n",
      "Epoch 202/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9571\n",
      "Epoch 203/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9505\n",
      "Epoch 204/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9461\n",
      "Epoch 205/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9367\n",
      "Epoch 206/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9499\n",
      "Epoch 207/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9532\n",
      "Epoch 208/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9527\n",
      "Epoch 209/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9527\n",
      "Epoch 210/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9587\n",
      "Epoch 211/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9642\n",
      "Epoch 212/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9477\n",
      "Epoch 213/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9389\n",
      "Epoch 214/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9439\n",
      "Epoch 215/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9642\n",
      "Epoch 216/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9609\n",
      "Epoch 217/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9494\n",
      "Epoch 218/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9560\n",
      "Epoch 219/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9483\n",
      "Epoch 220/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9466\n",
      "Epoch 221/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9488\n",
      "Epoch 222/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9604\n",
      "Epoch 223/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9620\n",
      "Epoch 224/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9631\n",
      "Epoch 225/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9560\n",
      "Epoch 226/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9653\n",
      "Epoch 227/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9565\n",
      "Epoch 228/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9648\n",
      "Epoch 229/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9587\n",
      "Epoch 230/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9587\n",
      "Epoch 231/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9565\n",
      "Epoch 232/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9593\n",
      "Epoch 233/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9576\n",
      "Epoch 234/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.9620\n",
      "Epoch 235/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9582\n",
      "Epoch 236/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9565\n",
      "Epoch 237/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9488\n",
      "Epoch 238/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9571\n",
      "Epoch 239/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9466\n",
      "Epoch 240/1500\n",
      "57/57 [==============================] - 0s 996us/step - loss: 0.0985 - accuracy: 0.9642\n",
      "Epoch 241/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9521\n",
      "Epoch 242/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9521\n",
      "Epoch 243/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9510\n",
      "Epoch 244/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9620\n",
      "Epoch 245/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.9598\n",
      "Epoch 246/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9565\n",
      "Epoch 247/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9516\n",
      "Epoch 248/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9626\n",
      "Epoch 249/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9576\n",
      "Epoch 250/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9461\n",
      "Epoch 251/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.9637\n",
      "Epoch 252/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9609\n",
      "Epoch 253/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9604\n",
      "Epoch 254/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9499\n",
      "Epoch 255/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9560\n",
      "Epoch 256/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9538\n",
      "Epoch 257/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9604\n",
      "Epoch 258/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9593\n",
      "Epoch 259/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9521\n",
      "Epoch 260/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9532\n",
      "Epoch 261/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9598\n",
      "Epoch 262/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.9648\n",
      "Epoch 263/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9675\n",
      "Epoch 264/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9659\n",
      "Epoch 265/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9598\n",
      "Epoch 266/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9686\n",
      "Epoch 267/1500\n",
      "57/57 [==============================] - 0s 4ms/step - loss: 0.0891 - accuracy: 0.9659\n",
      "Epoch 268/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9554\n",
      "Epoch 269/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9560\n",
      "Epoch 270/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9598\n",
      "Epoch 271/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9648\n",
      "Epoch 272/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9576\n",
      "Epoch 273/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9604\n",
      "Epoch 274/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9620\n",
      "Epoch 275/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9664\n",
      "Epoch 276/1500\n",
      "57/57 [==============================] - 0s 990us/step - loss: 0.0844 - accuracy: 0.9664\n",
      "Epoch 277/1500\n",
      "57/57 [==============================] - 0s 979us/step - loss: 0.1026 - accuracy: 0.9554\n",
      "Epoch 278/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9653\n",
      "Epoch 279/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9593\n",
      "Epoch 280/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9615\n",
      "Epoch 281/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9598\n",
      "Epoch 282/1500\n",
      "57/57 [==============================] - 0s 997us/step - loss: 0.0852 - accuracy: 0.9664\n",
      "Epoch 283/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9670\n",
      "Epoch 284/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9626\n",
      "Epoch 285/1500\n",
      "57/57 [==============================] - 0s 984us/step - loss: 0.0969 - accuracy: 0.9582\n",
      "Epoch 286/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9598\n",
      "Epoch 287/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9664\n",
      "Epoch 288/1500\n",
      "57/57 [==============================] - 0s 985us/step - loss: 0.0946 - accuracy: 0.9648\n",
      "Epoch 289/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9565\n",
      "Epoch 290/1500\n",
      "57/57 [==============================] - 0s 991us/step - loss: 0.0882 - accuracy: 0.9604\n",
      "Epoch 291/1500\n",
      "57/57 [==============================] - 0s 982us/step - loss: 0.0991 - accuracy: 0.9593\n",
      "Epoch 292/1500\n",
      "57/57 [==============================] - 0s 996us/step - loss: 0.0991 - accuracy: 0.9554\n",
      "Epoch 293/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9626\n",
      "Epoch 294/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9571\n",
      "Epoch 295/1500\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9642\n",
      "Epoch 296/1500\n",
      "51/57 [=========================>....] - ETA: 0s - loss: 0.0870 - accuracy: 0.9651Restoring model weights from the end of the best epoch: 266.\n",
      "57/57 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.9653\n",
      "Epoch 296: early stopping\n",
      "8/8 [==============================] - 0s 737us/step - loss: 0.8249 - accuracy: 0.6840\n",
      "8/8 [==============================] - 0s 550us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.74 (17/23)\n",
      "Before appending - Cat IDs: 457, Predictions: 457, Actuals: 457, Gender: 457\n",
      "After appending - Cat IDs: 707, Predictions: 707, Actuals: 707, Gender: 707\n",
      "Final Test Results - Loss: 0.824876070022583, Accuracy: 0.6840000152587891, Precision: 0.7378778692411134, Recall: 0.648932523551426, F1 Score: 0.6728999351548058\n",
      "Confusion Matrix:\n",
      " [[98  2 23]\n",
      " [29 35  0]\n",
      " [25  0 38]]\n",
      "outer_fold 5\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "097B    14\n",
      "001A    14\n",
      "059A    14\n",
      "042A    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "016A    10\n",
      "040A    10\n",
      "071A    10\n",
      "033A     9\n",
      "051B     9\n",
      "072A     9\n",
      "045A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "094A     8\n",
      "050A     7\n",
      "031A     7\n",
      "117A     7\n",
      "027A     7\n",
      "099A     7\n",
      "008A     6\n",
      "109A     6\n",
      "053A     6\n",
      "023A     6\n",
      "108A     6\n",
      "023B     5\n",
      "034A     5\n",
      "075A     5\n",
      "009A     4\n",
      "104A     4\n",
      "105A     4\n",
      "035A     4\n",
      "026A     4\n",
      "062A     4\n",
      "056A     3\n",
      "014A     3\n",
      "012A     3\n",
      "058A     3\n",
      "113A     3\n",
      "064A     3\n",
      "060A     3\n",
      "011A     2\n",
      "061A     2\n",
      "025B     2\n",
      "069A     2\n",
      "093A     2\n",
      "087A     2\n",
      "054A     2\n",
      "018A     2\n",
      "038A     2\n",
      "032A     2\n",
      "088A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "076A     1\n",
      "019B     1\n",
      "096A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "091A     1\n",
      "043A     1\n",
      "073A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "020A    23\n",
      "029A    17\n",
      "014B    10\n",
      "005A    10\n",
      "015A     9\n",
      "022A     9\n",
      "065A     9\n",
      "037A     6\n",
      "007A     6\n",
      "070A     5\n",
      "044A     5\n",
      "025C     5\n",
      "021A     5\n",
      "003A     4\n",
      "052A     4\n",
      "006A     3\n",
      "102A     2\n",
      "026C     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    264\n",
      "M    252\n",
      "F    224\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    85\n",
      "X    84\n",
      "F    28\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 001A, 103A, 071A, 097B, 028A, 019...\n",
      "kitten    [111A, 040A, 047A, 042A, 109A, 050A, 043A, 049...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 113...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 015A, 020A, 022A, 029A, 005A, 065A, 070...\n",
      "kitten                                   [044A, 014B, 046A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 13, 'senior': 22}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 3}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '016A' '018A' '019A' '019B' '023A' '023B' '024A'\n",
      " '025A' '025B' '026A' '027A' '028A' '031A' '032A' '033A' '034A' '035A'\n",
      " '036A' '038A' '039A' '040A' '041A' '042A' '043A' '045A' '047A' '048A'\n",
      " '049A' '050A' '051A' '051B' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '064A' '066A' '067A' '068A' '069A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '092A' '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A'\n",
      " '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '005A' '006A' '007A' '014B' '015A' '020A' '021A' '022A' '025C'\n",
      " '026B' '026C' '029A' '037A' '044A' '046A' '052A' '065A' '070A' '102A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'047A'}\n",
      "Moved to Test Set:\n",
      "{'047A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '016A' '018A' '019A' '019B' '023A' '023B' '024A'\n",
      " '025A' '025B' '026A' '027A' '028A' '031A' '032A' '033A' '034A' '035A'\n",
      " '036A' '038A' '039A' '040A' '041A' '042A' '043A' '045A' '046A' '048A'\n",
      " '049A' '050A' '051A' '051B' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '064A' '066A' '067A' '068A' '069A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '092A' '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A'\n",
      " '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '005A' '006A' '007A' '014B' '015A' '020A' '021A' '022A' '025C'\n",
      " '026B' '026C' '029A' '037A' '044A' '047A' '052A' '065A' '070A' '102A']\n",
      "Length of X_train_val:\n",
      "775\n",
      "Length of y_train_val:\n",
      "775\n",
      "Length of groups_train_val:\n",
      "775\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     469\n",
      "senior    178\n",
      "kitten     93\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     119\n",
      "kitten     78\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     469\n",
      "senior    178\n",
      "kitten    128\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     119\n",
      "kitten     43\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 938, 2: 712, 1: 512})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "68/68 [==============================] - 0s 993us/step - loss: 0.8581 - accuracy: 0.6064\n",
      "Epoch 2/1500\n",
      "68/68 [==============================] - 0s 950us/step - loss: 0.6916 - accuracy: 0.6827\n",
      "Epoch 3/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6318 - accuracy: 0.7197\n",
      "Epoch 4/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5855 - accuracy: 0.7354\n",
      "Epoch 5/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.5470 - accuracy: 0.7549\n",
      "Epoch 6/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5262 - accuracy: 0.7581\n",
      "Epoch 7/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4959 - accuracy: 0.7817\n",
      "Epoch 8/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4737 - accuracy: 0.7808\n",
      "Epoch 9/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4707 - accuracy: 0.7877\n",
      "Epoch 10/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.8020\n",
      "Epoch 11/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.7965\n",
      "Epoch 12/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.8006\n",
      "Epoch 13/1500\n",
      "68/68 [==============================] - 0s 6ms/step - loss: 0.4042 - accuracy: 0.8164\n",
      "Epoch 14/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4173 - accuracy: 0.8154\n",
      "Epoch 15/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.4199 - accuracy: 0.8154\n",
      "Epoch 16/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4002 - accuracy: 0.8247\n",
      "Epoch 17/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3901 - accuracy: 0.8233\n",
      "Epoch 18/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4003 - accuracy: 0.8247\n",
      "Epoch 19/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3573 - accuracy: 0.8353\n",
      "Epoch 20/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3411 - accuracy: 0.8525\n",
      "Epoch 21/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3664 - accuracy: 0.8316\n",
      "Epoch 22/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3573 - accuracy: 0.8446\n",
      "Epoch 23/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3547 - accuracy: 0.8432\n",
      "Epoch 24/1500\n",
      "68/68 [==============================] - 0s 986us/step - loss: 0.3573 - accuracy: 0.8488\n",
      "Epoch 25/1500\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.3453 - accuracy: 0.8414\n",
      "Epoch 26/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.3205 - accuracy: 0.8599\n",
      "Epoch 27/1500\n",
      "68/68 [==============================] - 0s 957us/step - loss: 0.3315 - accuracy: 0.8451\n",
      "Epoch 28/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3168 - accuracy: 0.8599\n",
      "Epoch 29/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3151 - accuracy: 0.8603\n",
      "Epoch 30/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3075 - accuracy: 0.8686\n",
      "Epoch 31/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2924 - accuracy: 0.8719\n",
      "Epoch 32/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2956 - accuracy: 0.8700\n",
      "Epoch 33/1500\n",
      "68/68 [==============================] - 0s 981us/step - loss: 0.2986 - accuracy: 0.8631\n",
      "Epoch 34/1500\n",
      "68/68 [==============================] - 0s 962us/step - loss: 0.3086 - accuracy: 0.8622\n",
      "Epoch 35/1500\n",
      "68/68 [==============================] - 0s 991us/step - loss: 0.2978 - accuracy: 0.8636\n",
      "Epoch 36/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.2897 - accuracy: 0.8686\n",
      "Epoch 37/1500\n",
      "68/68 [==============================] - 0s 981us/step - loss: 0.2771 - accuracy: 0.8784\n",
      "Epoch 38/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2930 - accuracy: 0.8654\n",
      "Epoch 39/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2831 - accuracy: 0.8742\n",
      "Epoch 40/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2703 - accuracy: 0.8848\n",
      "Epoch 41/1500\n",
      "68/68 [==============================] - 0s 998us/step - loss: 0.2677 - accuracy: 0.8876\n",
      "Epoch 42/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2594 - accuracy: 0.8848\n",
      "Epoch 43/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8760\n",
      "Epoch 44/1500\n",
      "68/68 [==============================] - 0s 962us/step - loss: 0.2499 - accuracy: 0.8945\n",
      "Epoch 45/1500\n",
      "68/68 [==============================] - 0s 984us/step - loss: 0.2602 - accuracy: 0.8825\n",
      "Epoch 46/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2728 - accuracy: 0.8797\n",
      "Epoch 47/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2451 - accuracy: 0.8918\n",
      "Epoch 48/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2404 - accuracy: 0.9001\n",
      "Epoch 49/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.8908\n",
      "Epoch 50/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2502 - accuracy: 0.8885\n",
      "Epoch 51/1500\n",
      "68/68 [==============================] - 0s 994us/step - loss: 0.2293 - accuracy: 0.9024\n",
      "Epoch 52/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9033\n",
      "Epoch 53/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2254 - accuracy: 0.8987\n",
      "Epoch 54/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2312 - accuracy: 0.8964\n",
      "Epoch 55/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2316 - accuracy: 0.9010\n",
      "Epoch 56/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2391 - accuracy: 0.8969\n",
      "Epoch 57/1500\n",
      "68/68 [==============================] - 0s 981us/step - loss: 0.2382 - accuracy: 0.8982\n",
      "Epoch 58/1500\n",
      "68/68 [==============================] - 0s 987us/step - loss: 0.2264 - accuracy: 0.9019\n",
      "Epoch 59/1500\n",
      "68/68 [==============================] - 0s 986us/step - loss: 0.2237 - accuracy: 0.9024\n",
      "Epoch 60/1500\n",
      "68/68 [==============================] - 0s 984us/step - loss: 0.2283 - accuracy: 0.9029\n",
      "Epoch 61/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2184 - accuracy: 0.9043\n",
      "Epoch 62/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2147 - accuracy: 0.9033\n",
      "Epoch 63/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2251 - accuracy: 0.9001\n",
      "Epoch 64/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2059 - accuracy: 0.9093\n",
      "Epoch 65/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2232 - accuracy: 0.8987\n",
      "Epoch 66/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9066\n",
      "Epoch 67/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2147 - accuracy: 0.9084\n",
      "Epoch 68/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2097 - accuracy: 0.9135\n",
      "Epoch 69/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9177\n",
      "Epoch 70/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9140\n",
      "Epoch 71/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2124 - accuracy: 0.9103\n",
      "Epoch 72/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9093\n",
      "Epoch 73/1500\n",
      "68/68 [==============================] - 0s 995us/step - loss: 0.1946 - accuracy: 0.9167\n",
      "Epoch 74/1500\n",
      "68/68 [==============================] - 0s 982us/step - loss: 0.2251 - accuracy: 0.9010\n",
      "Epoch 75/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.2150 - accuracy: 0.9052\n",
      "Epoch 76/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.1990 - accuracy: 0.9066\n",
      "Epoch 77/1500\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.1975 - accuracy: 0.9149\n",
      "Epoch 78/1500\n",
      "68/68 [==============================] - 0s 959us/step - loss: 0.1959 - accuracy: 0.9149\n",
      "Epoch 79/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9112\n",
      "Epoch 80/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9163\n",
      "Epoch 81/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2034 - accuracy: 0.9177\n",
      "Epoch 82/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1891 - accuracy: 0.9232\n",
      "Epoch 83/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9214\n",
      "Epoch 84/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9246\n",
      "Epoch 85/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.9306\n",
      "Epoch 86/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.9237\n",
      "Epoch 87/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9209\n",
      "Epoch 88/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1909 - accuracy: 0.9200\n",
      "Epoch 89/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1991 - accuracy: 0.9130\n",
      "Epoch 90/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.9186\n",
      "Epoch 91/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.1865 - accuracy: 0.9195\n",
      "Epoch 92/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9177\n",
      "Epoch 93/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.9246\n",
      "Epoch 94/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9288\n",
      "Epoch 95/1500\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.1682 - accuracy: 0.9339\n",
      "Epoch 96/1500\n",
      "68/68 [==============================] - 0s 973us/step - loss: 0.1731 - accuracy: 0.9302\n",
      "Epoch 97/1500\n",
      "68/68 [==============================] - 0s 962us/step - loss: 0.1787 - accuracy: 0.9246\n",
      "Epoch 98/1500\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.1855 - accuracy: 0.9232\n",
      "Epoch 99/1500\n",
      "68/68 [==============================] - 0s 993us/step - loss: 0.1632 - accuracy: 0.9283\n",
      "Epoch 100/1500\n",
      "68/68 [==============================] - 0s 950us/step - loss: 0.1524 - accuracy: 0.9348\n",
      "Epoch 101/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.1657 - accuracy: 0.9329\n",
      "Epoch 102/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.9329\n",
      "Epoch 103/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1523 - accuracy: 0.9417\n",
      "Epoch 104/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1719 - accuracy: 0.9269\n",
      "Epoch 105/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9329\n",
      "Epoch 106/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.9154\n",
      "Epoch 107/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.9352\n",
      "Epoch 108/1500\n",
      "68/68 [==============================] - 0s 962us/step - loss: 0.1671 - accuracy: 0.9315\n",
      "Epoch 109/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9376\n",
      "Epoch 110/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9315\n",
      "Epoch 111/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9362\n",
      "Epoch 112/1500\n",
      "68/68 [==============================] - 0s 979us/step - loss: 0.1580 - accuracy: 0.9352\n",
      "Epoch 113/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9343\n",
      "Epoch 114/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1587 - accuracy: 0.9334\n",
      "Epoch 115/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9274\n",
      "Epoch 116/1500\n",
      "68/68 [==============================] - 0s 996us/step - loss: 0.1565 - accuracy: 0.9339\n",
      "Epoch 117/1500\n",
      "68/68 [==============================] - 0s 961us/step - loss: 0.1612 - accuracy: 0.9357\n",
      "Epoch 118/1500\n",
      "68/68 [==============================] - 0s 964us/step - loss: 0.1478 - accuracy: 0.9399\n",
      "Epoch 119/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9445\n",
      "Epoch 120/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9357\n",
      "Epoch 121/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.9413\n",
      "Epoch 122/1500\n",
      "68/68 [==============================] - 0s 978us/step - loss: 0.1419 - accuracy: 0.9408\n",
      "Epoch 123/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9380\n",
      "Epoch 124/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9403\n",
      "Epoch 125/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.9496\n",
      "Epoch 126/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9417\n",
      "Epoch 127/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9385\n",
      "Epoch 128/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.9440\n",
      "Epoch 129/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9376\n",
      "Epoch 130/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9413\n",
      "Epoch 131/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1518 - accuracy: 0.9320\n",
      "Epoch 132/1500\n",
      "68/68 [==============================] - 0s 964us/step - loss: 0.1595 - accuracy: 0.9399\n",
      "Epoch 133/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9403\n",
      "Epoch 134/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9440\n",
      "Epoch 135/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1388 - accuracy: 0.9403\n",
      "Epoch 136/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9417\n",
      "Epoch 137/1500\n",
      "68/68 [==============================] - 0s 995us/step - loss: 0.1334 - accuracy: 0.9487\n",
      "Epoch 138/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9440\n",
      "Epoch 139/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9482\n",
      "Epoch 140/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9505\n",
      "Epoch 141/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9371\n",
      "Epoch 142/1500\n",
      "68/68 [==============================] - 0s 977us/step - loss: 0.1350 - accuracy: 0.9385\n",
      "Epoch 143/1500\n",
      "68/68 [==============================] - 0s 988us/step - loss: 0.1464 - accuracy: 0.9413\n",
      "Epoch 144/1500\n",
      "68/68 [==============================] - 0s 984us/step - loss: 0.1354 - accuracy: 0.9491\n",
      "Epoch 145/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.1358 - accuracy: 0.9459\n",
      "Epoch 146/1500\n",
      "68/68 [==============================] - 0s 959us/step - loss: 0.1389 - accuracy: 0.9454\n",
      "Epoch 147/1500\n",
      "68/68 [==============================] - 0s 974us/step - loss: 0.1282 - accuracy: 0.9514\n",
      "Epoch 148/1500\n",
      "68/68 [==============================] - 0s 981us/step - loss: 0.1423 - accuracy: 0.9403\n",
      "Epoch 149/1500\n",
      "68/68 [==============================] - 0s 984us/step - loss: 0.1205 - accuracy: 0.9510\n",
      "Epoch 150/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9436\n",
      "Epoch 151/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9450\n",
      "Epoch 152/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9482\n",
      "Epoch 153/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9413\n",
      "Epoch 154/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9487\n",
      "Epoch 155/1500\n",
      "68/68 [==============================] - 0s 984us/step - loss: 0.1266 - accuracy: 0.9510\n",
      "Epoch 156/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.1173 - accuracy: 0.9537\n",
      "Epoch 157/1500\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.1243 - accuracy: 0.9477\n",
      "Epoch 158/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9477\n",
      "Epoch 159/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9487\n",
      "Epoch 160/1500\n",
      "68/68 [==============================] - 0s 961us/step - loss: 0.1434 - accuracy: 0.9389\n",
      "Epoch 161/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9389\n",
      "Epoch 162/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9445\n",
      "Epoch 163/1500\n",
      "68/68 [==============================] - 0s 986us/step - loss: 0.1358 - accuracy: 0.9431\n",
      "Epoch 164/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.1161 - accuracy: 0.9542\n",
      "Epoch 165/1500\n",
      "68/68 [==============================] - 0s 983us/step - loss: 0.1359 - accuracy: 0.9491\n",
      "Epoch 166/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9454\n",
      "Epoch 167/1500\n",
      "68/68 [==============================] - 0s 986us/step - loss: 0.1249 - accuracy: 0.9496\n",
      "Epoch 168/1500\n",
      "68/68 [==============================] - 0s 974us/step - loss: 0.1192 - accuracy: 0.9514\n",
      "Epoch 169/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9473\n",
      "Epoch 170/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9598\n",
      "Epoch 171/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9482\n",
      "Epoch 172/1500\n",
      "68/68 [==============================] - 0s 1000us/step - loss: 0.1106 - accuracy: 0.9598\n",
      "Epoch 173/1500\n",
      "68/68 [==============================] - 0s 974us/step - loss: 0.1277 - accuracy: 0.9473\n",
      "Epoch 174/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9561\n",
      "Epoch 175/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9607\n",
      "Epoch 176/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9565\n",
      "Epoch 177/1500\n",
      "68/68 [==============================] - 0s 916us/step - loss: 0.1224 - accuracy: 0.9561\n",
      "Epoch 178/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9584\n",
      "Epoch 179/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9491\n",
      "Epoch 180/1500\n",
      "68/68 [==============================] - 0s 965us/step - loss: 0.1156 - accuracy: 0.9528\n",
      "Epoch 181/1500\n",
      "68/68 [==============================] - 0s 960us/step - loss: 0.1305 - accuracy: 0.9459\n",
      "Epoch 182/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9579\n",
      "Epoch 183/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9607\n",
      "Epoch 184/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9565\n",
      "Epoch 185/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9524\n",
      "Epoch 186/1500\n",
      "68/68 [==============================] - 0s 943us/step - loss: 0.1050 - accuracy: 0.9602\n",
      "Epoch 187/1500\n",
      "68/68 [==============================] - 0s 972us/step - loss: 0.1134 - accuracy: 0.9579\n",
      "Epoch 188/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9621\n",
      "Epoch 189/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1204 - accuracy: 0.9533\n",
      "Epoch 190/1500\n",
      "68/68 [==============================] - 0s 962us/step - loss: 0.1340 - accuracy: 0.9505\n",
      "Epoch 191/1500\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.1032 - accuracy: 0.9658\n",
      "Epoch 192/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.1091 - accuracy: 0.9584\n",
      "Epoch 193/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9551\n",
      "Epoch 194/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9500\n",
      "Epoch 195/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.1132 - accuracy: 0.9570\n",
      "Epoch 196/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9551\n",
      "Epoch 197/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9667\n",
      "Epoch 198/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9561\n",
      "Epoch 199/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9528\n",
      "Epoch 200/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9658\n",
      "Epoch 201/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9556\n",
      "Epoch 202/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9648\n",
      "Epoch 203/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9537\n",
      "Epoch 204/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9487\n",
      "Epoch 205/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9556\n",
      "Epoch 206/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9630\n",
      "Epoch 207/1500\n",
      "68/68 [==============================] - 0s 950us/step - loss: 0.1090 - accuracy: 0.9565\n",
      "Epoch 208/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9598\n",
      "Epoch 209/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9616\n",
      "Epoch 210/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9561\n",
      "Epoch 211/1500\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.1075 - accuracy: 0.9579\n",
      "Epoch 212/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9579\n",
      "Epoch 213/1500\n",
      "68/68 [==============================] - 0s 984us/step - loss: 0.1005 - accuracy: 0.9593\n",
      "Epoch 214/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9533\n",
      "Epoch 215/1500\n",
      "68/68 [==============================] - 0s 940us/step - loss: 0.0976 - accuracy: 0.9570\n",
      "Epoch 216/1500\n",
      "68/68 [==============================] - 0s 964us/step - loss: 0.0978 - accuracy: 0.9607\n",
      "Epoch 217/1500\n",
      "68/68 [==============================] - 0s 973us/step - loss: 0.0972 - accuracy: 0.9621\n",
      "Epoch 218/1500\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.0953 - accuracy: 0.9639\n",
      "Epoch 219/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9542\n",
      "Epoch 220/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9524\n",
      "Epoch 221/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9561\n",
      "Epoch 222/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.9542\n",
      "Epoch 223/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9602\n",
      "Epoch 224/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9607\n",
      "Epoch 225/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9611\n",
      "Epoch 226/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9584\n",
      "Epoch 227/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9570\n",
      "Epoch 228/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9561\n",
      "Epoch 229/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.9639\n",
      "Epoch 230/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9644\n",
      "Epoch 231/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9709\n",
      "Epoch 232/1500\n",
      "68/68 [==============================] - 0s 990us/step - loss: 0.1001 - accuracy: 0.9607\n",
      "Epoch 233/1500\n",
      "68/68 [==============================] - 0s 995us/step - loss: 0.0968 - accuracy: 0.9588\n",
      "Epoch 234/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9611\n",
      "Epoch 235/1500\n",
      "68/68 [==============================] - 0s 987us/step - loss: 0.0981 - accuracy: 0.9556\n",
      "Epoch 236/1500\n",
      "68/68 [==============================] - 0s 977us/step - loss: 0.0883 - accuracy: 0.9690\n",
      "Epoch 237/1500\n",
      "68/68 [==============================] - 0s 977us/step - loss: 0.1038 - accuracy: 0.9616\n",
      "Epoch 238/1500\n",
      "68/68 [==============================] - 0s 982us/step - loss: 0.0922 - accuracy: 0.9681\n",
      "Epoch 239/1500\n",
      "68/68 [==============================] - 0s 977us/step - loss: 0.0977 - accuracy: 0.9598\n",
      "Epoch 240/1500\n",
      "68/68 [==============================] - 0s 968us/step - loss: 0.1011 - accuracy: 0.9602\n",
      "Epoch 241/1500\n",
      "68/68 [==============================] - 0s 952us/step - loss: 0.0868 - accuracy: 0.9676\n",
      "Epoch 242/1500\n",
      "68/68 [==============================] - 0s 950us/step - loss: 0.0883 - accuracy: 0.9616\n",
      "Epoch 243/1500\n",
      "68/68 [==============================] - 0s 964us/step - loss: 0.1154 - accuracy: 0.9496\n",
      "Epoch 244/1500\n",
      "68/68 [==============================] - 0s 966us/step - loss: 0.0844 - accuracy: 0.9685\n",
      "Epoch 245/1500\n",
      "68/68 [==============================] - 0s 956us/step - loss: 0.0887 - accuracy: 0.9695\n",
      "Epoch 246/1500\n",
      "68/68 [==============================] - 0s 962us/step - loss: 0.0945 - accuracy: 0.9611\n",
      "Epoch 247/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.0893 - accuracy: 0.9658\n",
      "Epoch 248/1500\n",
      "68/68 [==============================] - 0s 944us/step - loss: 0.0918 - accuracy: 0.9630\n",
      "Epoch 249/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.0936 - accuracy: 0.9621\n",
      "Epoch 250/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9690\n",
      "Epoch 251/1500\n",
      "68/68 [==============================] - 0s 966us/step - loss: 0.0948 - accuracy: 0.9667\n",
      "Epoch 252/1500\n",
      "68/68 [==============================] - 0s 993us/step - loss: 0.0798 - accuracy: 0.9732\n",
      "Epoch 253/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9528\n",
      "Epoch 254/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9588\n",
      "Epoch 255/1500\n",
      "68/68 [==============================] - 0s 962us/step - loss: 0.0845 - accuracy: 0.9635\n",
      "Epoch 256/1500\n",
      "68/68 [==============================] - 0s 976us/step - loss: 0.0933 - accuracy: 0.9662\n",
      "Epoch 257/1500\n",
      "68/68 [==============================] - 0s 966us/step - loss: 0.0849 - accuracy: 0.9704\n",
      "Epoch 258/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9695\n",
      "Epoch 259/1500\n",
      "68/68 [==============================] - 0s 991us/step - loss: 0.0822 - accuracy: 0.9718\n",
      "Epoch 260/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9653\n",
      "Epoch 261/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.0840 - accuracy: 0.9695\n",
      "Epoch 262/1500\n",
      "68/68 [==============================] - 0s 981us/step - loss: 0.0844 - accuracy: 0.9658\n",
      "Epoch 263/1500\n",
      "68/68 [==============================] - 0s 972us/step - loss: 0.1066 - accuracy: 0.9570\n",
      "Epoch 264/1500\n",
      "68/68 [==============================] - 0s 957us/step - loss: 0.1001 - accuracy: 0.9621\n",
      "Epoch 265/1500\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0860 - accuracy: 0.9621\n",
      "Epoch 266/1500\n",
      "68/68 [==============================] - 0s 959us/step - loss: 0.0743 - accuracy: 0.9741\n",
      "Epoch 267/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.9736\n",
      "Epoch 268/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.9621\n",
      "Epoch 269/1500\n",
      "68/68 [==============================] - 0s 962us/step - loss: 0.0811 - accuracy: 0.9699\n",
      "Epoch 270/1500\n",
      "68/68 [==============================] - 0s 959us/step - loss: 0.0726 - accuracy: 0.9741\n",
      "Epoch 271/1500\n",
      "68/68 [==============================] - 0s 995us/step - loss: 0.0846 - accuracy: 0.9722\n",
      "Epoch 272/1500\n",
      "68/68 [==============================] - 0s 982us/step - loss: 0.0854 - accuracy: 0.9648\n",
      "Epoch 273/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9690\n",
      "Epoch 274/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9644\n",
      "Epoch 275/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9653\n",
      "Epoch 276/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9644\n",
      "Epoch 277/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9658\n",
      "Epoch 278/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.9685\n",
      "Epoch 279/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9658\n",
      "Epoch 280/1500\n",
      "68/68 [==============================] - 0s 987us/step - loss: 0.0783 - accuracy: 0.9690\n",
      "Epoch 281/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9722\n",
      "Epoch 282/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9676\n",
      "Epoch 283/1500\n",
      "68/68 [==============================] - 0s 974us/step - loss: 0.0956 - accuracy: 0.9621\n",
      "Epoch 284/1500\n",
      "68/68 [==============================] - 0s 977us/step - loss: 0.0914 - accuracy: 0.9598\n",
      "Epoch 285/1500\n",
      "68/68 [==============================] - 0s 942us/step - loss: 0.0910 - accuracy: 0.9635\n",
      "Epoch 286/1500\n",
      "68/68 [==============================] - 0s 968us/step - loss: 0.0842 - accuracy: 0.9644\n",
      "Epoch 287/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9759\n",
      "Epoch 288/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9681\n",
      "Epoch 289/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.9639\n",
      "Epoch 290/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9625\n",
      "Epoch 291/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9579\n",
      "Epoch 292/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9718\n",
      "Epoch 293/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0705 - accuracy: 0.9709\n",
      "Epoch 294/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9625\n",
      "Epoch 295/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9616\n",
      "Epoch 296/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9602\n",
      "Epoch 297/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9699\n",
      "Epoch 298/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.9732\n",
      "Epoch 299/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9685\n",
      "Epoch 300/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9616\n",
      "Epoch 301/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.9718\n",
      "Epoch 302/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.9732\n",
      "Epoch 303/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.9718\n",
      "Epoch 304/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9616\n",
      "Epoch 305/1500\n",
      "68/68 [==============================] - 0s 995us/step - loss: 0.0850 - accuracy: 0.9658\n",
      "Epoch 306/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9672\n",
      "Epoch 307/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9653\n",
      "Epoch 308/1500\n",
      "68/68 [==============================] - 0s 948us/step - loss: 0.0925 - accuracy: 0.9607\n",
      "Epoch 309/1500\n",
      "68/68 [==============================] - 0s 989us/step - loss: 0.0849 - accuracy: 0.9644\n",
      "Epoch 310/1500\n",
      "68/68 [==============================] - 0s 991us/step - loss: 0.0752 - accuracy: 0.9704\n",
      "Epoch 311/1500\n",
      "68/68 [==============================] - 0s 977us/step - loss: 0.0737 - accuracy: 0.9699\n",
      "Epoch 312/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9713\n",
      "Epoch 313/1500\n",
      "68/68 [==============================] - 0s 972us/step - loss: 0.0754 - accuracy: 0.9695\n",
      "Epoch 314/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9611\n",
      "Epoch 315/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9635\n",
      "Epoch 316/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9672\n",
      "Epoch 317/1500\n",
      "45/68 [==================>...........] - ETA: 0s - loss: 0.0793 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 287.\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9685\n",
      "Epoch 317: early stopping\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1919, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 242\u001b[0m\n\u001b[1;32m    238\u001b[0m history_full \u001b[38;5;241m=\u001b[39m model_full\u001b[38;5;241m.\u001b[39mfit(X_train_full_scaled, y_train_full_encoded, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m    239\u001b[0m                               verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping], class_weight\u001b[38;5;241m=\u001b[39mweight_dict)\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the held-out test set\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_full\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m y_test_pred_prob \u001b[38;5;241m=\u001b[39m model_full\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n\u001b[1;32m    245\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test_pred_prob, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \n",
      "File \u001b[0;32m~/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/rh/n5q9_hkd2hx420s0lpvtx7z00000gn/T/__autograph_generated_filewoxtgvmi.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1919, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/Users/astrid/miniforge3/envs/april2-env/lib/python3.10/site-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 2) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=int(random_seeds[3]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train_val),\n",
    "        y=y_train_val\n",
    "    )\n",
    "    weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping], class_weight=weight_dict)\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "# Append averages to total lists\n",
    "all_f1.append(unseen_set_avg_f1)\n",
    "all_losses.append(unseen_set_avg_loss)\n",
    "all_accuracies.append(unseen_set_avg_acc)\n",
    "all_precisions.append(unseen_set_avg_precision)\n",
    "all_recalls.append(unseen_set_avg_recall)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7ccddc-d685-4c7a-8327-cb08e99b9fdc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "seed_4_folds_values = (unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1)\n",
    "\n",
    "# Plotting all fold metrics\n",
    "seed_4_folds_plot = plot_all_metrics(unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1, \"Folds\", \"Fold Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad1a0a0-252b-4426-ac52-0134ab6452f6",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84023b43-4cc3-4887-b07c-a161532a80a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f755178-0845-4e4f-aa4f-a4c4d3e2ed00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c568052-7601-4af0-bad2-8ab7c3ceca38",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "# store for final evaluation \n",
    "all_majority_vote_accuracies.append(majority_vote_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a602f5-2010-4e09-878b-feec22a66304",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e07797-471a-48de-b37d-bd43be264e4e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5fc354-901d-442d-9faa-667b182f514a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbae1b10-6d42-4e2b-82a1-f12950f4173f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# store for final evaluation \n",
    "all_majority_vote_details.append(class_stats)\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d430f-dcdd-404f-b0ca-2719f7fb5cdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fed1d4-66e3-4561-9846-304d82fafe5e",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7554bc-9473-40a5-b74d-c8d8dfe799bb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# store for final evaluation \n",
    "all_class_stats.append(class_stats)\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e143cf26-9c9d-4399-a17e-dd9e23e1d1de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ba99a-a284-42d7-9424-2687143c89c2",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15157e9c-bfce-48f9-b038-3ddcaf1078be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)\n",
    "\n",
    "# store for final evaluation \n",
    "all_gender_stats.append(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104097eb-5624-484a-9b54-8be830d05758",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2399f41-66a5-4a4b-8a12-ed0fcf26b8a8",
   "metadata": {},
   "source": [
    "### Log current total CV Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61244188-59e9-42f2-ac32-bae254b54b32",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"Total Majority Vote Accuracy so far:\", all_majority_vote_accuracies)\n",
    "print(\"Total Class Statistics so far:\\n\", all_class_stats)\n",
    "print(\"Total Gender Accuracy so far:\\n\", all_gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40270db-853d-46a1-acb4-8dab8a4f9c81",
   "metadata": {},
   "source": [
    "# RANDOM SEED 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e6cfb-6ee5-4b51-a9ee-79dcbfbb208c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[4]))\n",
    "np.random.seed(int(random_seeds[4]))\n",
    "tf.random.set_seed(int(random_seeds[4]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_19.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a9515-e7f1-482c-bdaf-73cb07a9132e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8231e441-4472-4d9b-8ddf-dc285c07923b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9ed6b-6d2f-491e-95d8-73d33edeef6a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd63258-8a3b-4bc0-9ac6-ae8aa4b515b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=int(random_seeds[4]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train_val),\n",
    "        y=y_train_val\n",
    "    )\n",
    "    weight_dict = {i: class_weights[i] for i in range(len(class_weights))}\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping], class_weight=weight_dict)\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "# Append averages to total lists\n",
    "all_f1.append(unseen_set_avg_f1)\n",
    "all_losses.append(unseen_set_avg_loss)\n",
    "all_accuracies.append(unseen_set_avg_acc)\n",
    "all_precisions.append(unseen_set_avg_precision)\n",
    "all_recalls.append(unseen_set_avg_recall)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5cbb09-cd41-444e-beb3-17d931e9ef56",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "seed_5_folds_values = (unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1)\n",
    "\n",
    "# Plotting all fold metrics|\n",
    "seed_5_folds_plot = plot_all_metrics(unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1, \"Folds\", \"Fold Number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d454db3-1c02-4873-8b41-6a0512ba7381",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c4ef2c-2688-4fb1-8697-2004b19dd895",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd2ecc0-5e47-406f-bdcf-cfa174e7dbce",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6ec4b7-d64f-496a-a0e2-02cb1fb7c20f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "# store for final evaluation \n",
    "all_majority_vote_accuracies.append(majority_vote_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be3e59b-f415-440d-844d-0422773b5b23",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c583b292-e333-42ee-9bdc-b95997f3efb5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b406472-7dd9-4ee4-a943-27127dbddc76",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f6604-6731-4540-ac55-46030ffcad31",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# store for final evaluation \n",
    "all_majority_vote_details.append(class_stats)\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fcb644-b376-416f-82d8-1874f9cbea67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21741d2-c21b-444d-a881-72a1297a0e4a",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b316b4a-9bd9-466a-9fba-10f3db4497ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# store for final evaluation \n",
    "all_class_stats.append(class_stats)\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d052d53-e1b4-4c4e-84bb-5aa187d48ff6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a95e5d-0c77-43c5-ae0e-487fa0f3af96",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee7bdf2-e4fc-4268-b352-d2a6fd1d2f35",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)\n",
    "\n",
    "# store for final evaluation \n",
    "all_gender_stats.append(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b512c67d-6814-4d6b-88ba-99c3be01fc57",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d81a31f-b5be-441f-a797-cb3f81072d23",
   "metadata": {},
   "source": [
    "### Log current total CV Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84341206-bab6-4a67-b85c-0ec640b865ca",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Total Majority Vote Accuracy so far:\", all_majority_vote_accuracies)\n",
    "print(\"Total Class Statistics so far:\\n\", all_class_stats) \n",
    "print(\"Total Majority Vote Statistics so far:\\n\", all_majority_vote_details) \n",
    "print(\"Total Gender Accuracy so far:\\n\", all_gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe16e0f-5fac-4b3f-9ba7-de47e81b96eb",
   "metadata": {},
   "source": [
    "# --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f444b950-4d04-4d0a-aee8-04c0efd202c9",
   "metadata": {},
   "source": [
    "# Total Final Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe1313-e9ef-4e2b-afc2-ed85d46f12c1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Count unique cat_id per age group \n",
    "unique_cats_per_age_group = dataframe.groupby('age_group')['cat_id'].nunique()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Unique cat_id counts in original dataframe:\", unique_cats_per_age_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38de6b5e-19c0-46c6-96f1-b506a92bd9da",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842647cb-0c49-4703-9919-83649f0f6475",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate overall metrics\n",
    "overall_avg_loss = np.mean(all_losses)\n",
    "overall_avg_acc = np.mean(all_accuracies)\n",
    "overall_avg_precision = np.mean(all_precisions)\n",
    "overall_avg_recall = np.mean(all_recalls)\n",
    "overall_avg_f1 = np.mean(all_f1)\n",
    "\n",
    "print(\"Overall Average Metrics:\")\n",
    "print(\"Loss:\", overall_avg_loss)\n",
    "print(\"Accuracy:\", overall_avg_acc)\n",
    "print(\"Precision:\", overall_avg_precision)\n",
    "print(\"Recall:\", overall_avg_recall)\n",
    "print(\"F1 Score:\", overall_avg_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23ba0e0-d5fa-461c-9ea4-663e9ecdbbcb",
   "metadata": {},
   "source": [
    "## Majority Votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2625591-b765-47ac-a26e-51c29a8dd192",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Average of majority vote accuracies\n",
    "average_majority_vote_accuracy = np.mean(all_majority_vote_accuracies)\n",
    "print(\"Average Majority Vote Accuracy:\", average_majority_vote_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3dbec-afe0-4550-8d27-8c159cd7bcff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all class stats DataFrames and calculate the mean of accuracies\n",
    "total_majority_vote_details = pd.concat(all_majority_vote_details)\n",
    "average_majority_vote_details = total_majority_vote_details.groupby('actual_age_group').agg({\n",
    "    'total_count': 'sum', \n",
    "    'correct_count': 'sum' \n",
    "}).reset_index()\n",
    "average_majority_vote_details['accuracy'] = (average_majority_vote_details['correct_count'] / average_majority_vote_details['total_count']) * 100\n",
    "\n",
    "print(\"Average Class Statistics:\")\n",
    "print(average_majority_vote_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790d5ea4-da4b-4df8-b123-13c575d2839c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(average_majority_vote_details, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf540e5-94e9-4afe-aa71-18e2ea0fe409",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a356a0-56e1-43b1-83a6-23dd0a1ac405",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all class stats DataFrames and calculate the mean of accuracies\n",
    "total_class_stats = pd.concat(all_class_stats)\n",
    "average_class_stats = total_class_stats.groupby('actual_age_group').agg({\n",
    "    'total_count': 'sum',  \n",
    "    'correct_count': 'sum' \n",
    "}).reset_index()\n",
    "average_class_stats['accuracy'] = (average_class_stats['correct_count'] / average_class_stats['total_count']) * 100\n",
    "\n",
    "print(\"Average Class Statistics:\")\n",
    "print(average_class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9daf01f-8150-4865-a266-8e9888c30b54",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(average_class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94deaa-9457-40e2-bf9b-689a2b9aec30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate all gender stats DataFrames and calculate the mean of accuracies\n",
    "total_gender_stats = pd.concat(all_gender_stats)\n",
    "average_gender_stats = total_gender_stats.groupby('all_gender').agg({\n",
    "    'count': 'sum',  \n",
    "    'correct': 'sum'  \n",
    "}).reset_index()\n",
    "average_gender_stats['accuracy'] = (average_gender_stats['correct'] / average_gender_stats['count']) * 100\n",
    "\n",
    "print(\"Average Gender Accuracy:\")\n",
    "print(average_gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d1e35c-3de3-4ac4-8217-9ba0031bdddf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(total_gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0568439-9b05-4183-9c84-fb72ee579e57",
   "metadata": {},
   "source": [
    "# Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bfe0b9-6d0a-4220-b2a6-81fe12fe667c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plotting all metrics\n",
    "metrics_across_seeds = plot_all_metrics(all_losses, all_accuracies, all_precisions, all_recalls, all_f1, \"Seeds\", \"Seed Number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9763b9-92a8-43ba-8f93-27daf625cc9f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the standard deviation for each metric\n",
    "std_loss = np.std(all_losses)\n",
    "std_accuracy = np.std(all_accuracies)\n",
    "std_precision = np.std(all_precisions)\n",
    "std_recall = np.std(all_recalls)\n",
    "std_f1 = np.std(all_f1)\n",
    "\n",
    "print(\"Standard Deviations:\")\n",
    "print(f\"Loss: {std_loss}\")\n",
    "print(f\"Accuracy: {std_accuracy}\")\n",
    "print(f\"Precision: {std_precision}\")\n",
    "print(f\"Recall: {std_recall}\")\n",
    "print(f\"F1 Score: {std_f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb66eb-c6ed-44d3-a0c0-0075cb1f732d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the interquartile range for each metric\n",
    "iqr_loss = np.percentile(all_losses, 75) - np.percentile(all_losses, 25)\n",
    "iqr_accuracy = np.percentile(all_accuracies, 75) - np.percentile(all_accuracies, 25)\n",
    "iqr_precision = np.percentile(all_precisions, 75) - np.percentile(all_precisions, 25)\n",
    "iqr_recall = np.percentile(all_recalls, 75) - np.percentile(all_recalls, 25)\n",
    "iqr_f1 = np.percentile(all_f1, 75) - np.percentile(all_f1, 25)\n",
    "\n",
    "print(\"Interquartile Ranges:\")\n",
    "print(f\"Loss: {iqr_loss}\")\n",
    "print(f\"Accuracy: {iqr_accuracy}\")\n",
    "print(f\"Precision: {iqr_precision}\")\n",
    "print(f\"Recall: {iqr_recall}\")\n",
    "print(f\"F1 Score: {iqr_f1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892740e6-bb87-4fa0-aa41-8e126013895b",
   "metadata": {},
   "source": [
    "## Display the seed folds results together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92ae98c-dd2b-46e5-9bb0-3fbb20afdec1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 12))  \n",
    "\n",
    "# Flatten the axes array for easier indexing\n",
    "axes = axes.flatten()\n",
    "\n",
    "def plot_metrics_on_axes(ax, losses, accuracies, precisions, recalls, f1, title):\n",
    "    ax.plot(range(1, len(losses)+1), losses, marker='o', color='blue', label='Loss')\n",
    "    ax.plot(range(1, len(accuracies)+1), accuracies, marker='o', color='green', label='Accuracy')\n",
    "    ax.plot(range(1, len(precisions)+1), precisions, marker='o', color='red', label='Precision')\n",
    "    ax.plot(range(1, len(recalls)+1), recalls, marker='o', color='purple', label='Recall')\n",
    "    ax.plot(range(1, len(f1)+1), f1, marker='o', color='orange', label='F1 Score')\n",
    "    ax.set_xticks([1, 2, 3, 4])  # Set x-axis ticks to full integers\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Fold Number')\n",
    "    ax.set_ylabel('Metric Value')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "# Plot onto each axes object\n",
    "plot_metrics_on_axes(axes[0], *seed_1_folds_values, \"Seed 1 Metrics\")\n",
    "plot_metrics_on_axes(axes[1], *seed_2_folds_values, \"Seed 2 Metrics\")\n",
    "plot_metrics_on_axes(axes[2], *seed_3_folds_values, \"Seed 3 Metrics\")\n",
    "plot_metrics_on_axes(axes[3], *seed_4_folds_values, \"Seed 4 Metrics\")\n",
    "plot_metrics_on_axes(axes[4], *seed_5_folds_values, \"Seed 5 Metrics\")\n",
    "\n",
    "# Hide the unused subplot \n",
    "for ax in axes[5:]:\n",
    "    ax.set_visible(False)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68f7a02-823d-4b68-bb3a-8ff330ee383f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75834079-ef26-4bc0-8a73-547bc0df246f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
