{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17da2043-9a5b-40fe-b3cb-f755b9a98deb",
   "metadata": {},
   "source": [
    "# VGGIsh Multi Seed Validation\n",
    "#### 5 Random (but reproducible) seeds are selected for 5 different runs of train/test\n",
    "#### Models have been optimised and verified on validation sets thoroughly, running a final train/test evaluation here\n",
    "#### The setup:\n",
    "\n",
    "- 4 fold StratifiedGroupKFold for stratification and ensuring each cat_id group only appears in one set at a time\n",
    "- Final scores averaged over the 4 folds\n",
    "- For each seed run we will explore the cat_id predictions through majority voting\n",
    "- For each run we will explore the potential impact of gender\n",
    "\n",
    "The dataset is highly unbalanced, resources for an unbiased estimate have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac1e09ae-387c-4347-b6f5-8d09dd1bf3f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, SGD, RMSprop, AdamW\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import shap\n",
    "from keras.regularizers import l1, l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d005cfd2-3eda-44c4-bbb5-af343e979bc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [7270  860 5390 5191 5734]\n"
     ]
    }
   ],
   "source": [
    "# Set an initial seed for reproducibility\n",
    "np.random.seed(42)  \n",
    "\n",
    "# Generate a list of 5 random seeds\n",
    "random_seeds = np.random.randint(0, 10000, size=5)\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae122a-9f9c-47a3-8835-2d46d2f8e2f9",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5097e68-5153-440c-9294-dfa9e6061652",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_initial_group_split(groups_train, groups_test):\n",
    "    \"\"\"\n",
    "    Check if any group is present in both the train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - groups_train: Array of group identifiers for the train set\n",
    "    - groups_test: Array of group identifiers for the test set\n",
    "\n",
    "    Returns:\n",
    "    - Prints out any groups found in both sets and the count of such groups\n",
    "    \"\"\"\n",
    "    train_groups = set(groups_train)\n",
    "    test_groups = set(groups_test)\n",
    "    common_groups = train_groups.intersection(test_groups)\n",
    "\n",
    "    if common_groups:\n",
    "        print(f\"Warning: Found {len(common_groups)} common groups in both train/validation and test sets: {common_groups}\")\n",
    "    else:\n",
    "        print(\"No common groups found between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88dfe2de-bb1e-4d49-9260-e056c39627bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to perform the swaps based on cat_id, ensuring swaps within the same age_group\n",
    "def swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids):\n",
    "    for cat_id in specific_cat_ids:\n",
    "        # Check if the specific cat_id is not in the training set\n",
    "        if cat_id not in dataframe.iloc[train_val_idx]['cat_id'].values:\n",
    "            # Get the age_group of this cat_id\n",
    "            age_group = dataframe[dataframe['cat_id'] == cat_id]['age_group'].iloc[0]\n",
    "                \n",
    "            # Find a different cat_id within the same age_group in the train set that is not in the test set\n",
    "            other_cat_ids_in_age_group = dataframe[(dataframe['age_group'] == age_group) & \n",
    "                                                   (dataframe['cat_id'] != cat_id) &\n",
    "                                                   (~dataframe['cat_id'].isin(dataframe.iloc[test_idx]['cat_id']))]['cat_id'].unique()\n",
    "            \n",
    "            # Choose one other cat_id for swapping\n",
    "            if len(other_cat_ids_in_age_group) > 0:\n",
    "                other_cat_id = np.random.choice(other_cat_ids_in_age_group)\n",
    "\n",
    "                # Find all instances of the other_cat_id in the train set\n",
    "                other_cat_id_train_val_indices = train_val_idx[dataframe.iloc[train_val_idx]['cat_id'] == other_cat_id]\n",
    "                \n",
    "                # Find all instances of the specific cat_id in the test set\n",
    "                cat_id_test_indices = test_idx[dataframe.iloc[test_idx]['cat_id'] == cat_id]\n",
    "                \n",
    "                # Swap the indices\n",
    "                train_val_idx = np.setdiff1d(train_val_idx, other_cat_id_train_val_indices, assume_unique=True)\n",
    "                test_idx = np.setdiff1d(test_idx, cat_id_test_indices, assume_unique=True)\n",
    "\n",
    "                train_val_idx = np.concatenate((train_val_idx, cat_id_test_indices))\n",
    "                test_idx = np.concatenate((test_idx, other_cat_id_train_val_indices))\n",
    "            else:\n",
    "                print(f\"No alternative cat_id found in the same age_group as {cat_id} for swapping.\")\n",
    "                \n",
    "    return train_val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e6c2d87-cfab-493e-bca8-b3b8976a2bda",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify differences in groups\n",
    "def find_group_differences(original, new):\n",
    "    # Convert numpy arrays to sets for easy difference computation\n",
    "    original_set = set(original)\n",
    "    new_set = set(new)\n",
    "    # Find differences\n",
    "    moved_to_new = new_set - original_set\n",
    "    moved_to_original = original_set - new_set\n",
    "    return moved_to_new, moved_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6164b1c3-75dd-4549-aafd-cb6106297941",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create custom logger function for local logs & stored in a .txt\n",
    "def logger(message, file=None):\n",
    "    print(message)\n",
    "    if file is not None:\n",
    "        with open(file, \"a\") as log_file:\n",
    "            log_file.write(message + \"\\n\")\n",
    "\n",
    "log_file_path = \"multi-seed-val-D13.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "899fb535-0e25-4c7b-b6a5-13231e26c502",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "# Define a custom color palette\n",
    "colors = [\"#6aabd1\", \"#b6e2d3\", \"#dac292\"] \n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "# Function to create bar plots with enhanced style\n",
    "def styled_barplot(data, x, y, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bar_plot = sns.barplot(x=x, y=y, data=data, errorbar=None, width=0.5)  \n",
    "    plt.title(title, fontsize=16, fontweight='bold', color=\"#333333\")\n",
    "    plt.xlabel(xlabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.ylabel(ylabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.xticks(fontsize=12, color=\"#333333\")\n",
    "    plt.yticks(fontsize=12, color=\"#333333\")\n",
    "    plt.ylim(0, 100) \n",
    "\n",
    "    # Adding value labels on top of each bar\n",
    "    for p in bar_plot.patches:\n",
    "        height = p.get_height()\n",
    "        # Annotate the height value on the bar\n",
    "        bar_plot.annotate(f'{height:.1f}', \n",
    "                          (p.get_x() + p.get_width() / 2., height), \n",
    "                          ha='center', va='center', \n",
    "                          xytext=(0, 9), \n",
    "                          textcoords='offset points', fontsize=12, color=\"#333333\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936a24a-13bd-4bc3-be13-0b210a09b5da",
   "metadata": {},
   "source": [
    "# RANDOM SEED 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70136a37-0507-4c2e-8307-c2dd905432e8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14bb802b-f4bd-4464-a5fd-1977438428b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[0])) \n",
    "np.random.seed(int(random_seeds[0]))\n",
    "tf.random.set_seed(int(random_seeds[0]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce97c28-4210-4150-a60b-acdbf0e7716c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c26f17c3-4d21-4537-a090-9ca00f2be51a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91a9c3-3474-4e83-8a5b-c5eb4a9a9223",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4605c0b-f99d-48c3-8c2a-d87dd22c8946",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "        ..\n",
      "066A     1\n",
      "096A     1\n",
      "026C     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 87, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "055A    20\n",
      "001A    14\n",
      "025A    11\n",
      "016A    10\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "117A     7\n",
      "008A     6\n",
      "053A     6\n",
      "104A     4\n",
      "009A     4\n",
      "060A     3\n",
      "056A     3\n",
      "058A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "073A     1\n",
      "076A     1\n",
      "092A     1\n",
      "091A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    325\n",
      "M    253\n",
      "F    197\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    84\n",
      "F    55\n",
      "X    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [097A, 057A, 106A, 059A, 113A, 116A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 001A, 103A, 091A, 009A, 025A, 069A, 032...\n",
      "kitten                                         [045A, 110A]\n",
      "senior    [093A, 104A, 055A, 117A, 056A, 058A, 016A, 094...\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 14, 'senior': 13}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 2, 'senior': 9}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "Length of X_train_val:\n",
      "775\n",
      "Length of y_train_val:\n",
      "775\n",
      "Length of groups_train_val:\n",
      "775\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 988, 1: 805, 2: 600})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.1998 - accuracy: 0.4877\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.9312 - accuracy: 0.5942\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 929us/step - loss: 0.8536 - accuracy: 0.6389\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.7871 - accuracy: 0.6569\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.7656 - accuracy: 0.6649\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.7436 - accuracy: 0.6862\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.7214 - accuracy: 0.6995\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.7031 - accuracy: 0.7041\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.6787 - accuracy: 0.7104\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.6767 - accuracy: 0.7075\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.6369 - accuracy: 0.7158\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.6528 - accuracy: 0.7183\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.6352 - accuracy: 0.7280\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.6043 - accuracy: 0.7417\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.6035 - accuracy: 0.7317\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.5890 - accuracy: 0.7547\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.5860 - accuracy: 0.7568\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 947us/step - loss: 0.5846 - accuracy: 0.7493\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 967us/step - loss: 0.5672 - accuracy: 0.7576\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.5606 - accuracy: 0.7564\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.5416 - accuracy: 0.7748\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.5415 - accuracy: 0.7739\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.5345 - accuracy: 0.7756\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.5117 - accuracy: 0.7810\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 910us/step - loss: 0.5282 - accuracy: 0.7768\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.5020 - accuracy: 0.7877\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 913us/step - loss: 0.5125 - accuracy: 0.7802\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.5198 - accuracy: 0.7789\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 950us/step - loss: 0.5080 - accuracy: 0.7848\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.5053 - accuracy: 0.7873\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.4855 - accuracy: 0.7860\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.4920 - accuracy: 0.7810\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.4933 - accuracy: 0.7890\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.4964 - accuracy: 0.7894\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.4778 - accuracy: 0.8003\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 995us/step - loss: 0.4800 - accuracy: 0.7965\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.4694 - accuracy: 0.7973\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.4746 - accuracy: 0.7865\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.4669 - accuracy: 0.8078\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.4720 - accuracy: 0.8011\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.4593 - accuracy: 0.8048\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.4559 - accuracy: 0.7994\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 959us/step - loss: 0.4521 - accuracy: 0.8061\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.4549 - accuracy: 0.8044\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 910us/step - loss: 0.4561 - accuracy: 0.8053\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.4331 - accuracy: 0.8161\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.4472 - accuracy: 0.7977\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.4213 - accuracy: 0.8211\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.4268 - accuracy: 0.8203\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4287 - accuracy: 0.8203\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.4168 - accuracy: 0.8333\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.4275 - accuracy: 0.8157\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.4157 - accuracy: 0.8257\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.4265 - accuracy: 0.8207\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.4299 - accuracy: 0.8111\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.4338 - accuracy: 0.8178\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.4183 - accuracy: 0.8245\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.4252 - accuracy: 0.8165\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.4054 - accuracy: 0.8249\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.4048 - accuracy: 0.8324\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.4016 - accuracy: 0.8312\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.3872 - accuracy: 0.8349\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.3997 - accuracy: 0.8249\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 913us/step - loss: 0.4026 - accuracy: 0.8299\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3975 - accuracy: 0.8266\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.4018 - accuracy: 0.8316\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.3933 - accuracy: 0.8253\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.3991 - accuracy: 0.8312\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.3811 - accuracy: 0.8341\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3737 - accuracy: 0.8445\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.3916 - accuracy: 0.8416\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.3699 - accuracy: 0.8454\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3816 - accuracy: 0.8370\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.3754 - accuracy: 0.8437\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.3671 - accuracy: 0.8437\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.3720 - accuracy: 0.8404\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.3621 - accuracy: 0.8491\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.3780 - accuracy: 0.8391\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.3701 - accuracy: 0.8433\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.3576 - accuracy: 0.8517\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.3677 - accuracy: 0.8454\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.3698 - accuracy: 0.8454\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.3540 - accuracy: 0.8475\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3861 - accuracy: 0.8333\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 929us/step - loss: 0.3522 - accuracy: 0.8450\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.3523 - accuracy: 0.8433\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.3622 - accuracy: 0.8487\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.3446 - accuracy: 0.8479\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3504 - accuracy: 0.8471\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.3498 - accuracy: 0.8487\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.3574 - accuracy: 0.8471\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3630 - accuracy: 0.8458\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.3602 - accuracy: 0.8454\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3423 - accuracy: 0.8525\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 925us/step - loss: 0.3403 - accuracy: 0.8588\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3417 - accuracy: 0.8521\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.3489 - accuracy: 0.8546\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.3300 - accuracy: 0.8517\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 905us/step - loss: 0.3383 - accuracy: 0.8600\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.3285 - accuracy: 0.8600\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.3354 - accuracy: 0.8496\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3340 - accuracy: 0.8617\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.3375 - accuracy: 0.8533\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3264 - accuracy: 0.8696\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 951us/step - loss: 0.3189 - accuracy: 0.8675\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 940us/step - loss: 0.3312 - accuracy: 0.8654\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.3182 - accuracy: 0.8671\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.3154 - accuracy: 0.8663\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.3160 - accuracy: 0.8717\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.3312 - accuracy: 0.8621\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.3172 - accuracy: 0.8654\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.3246 - accuracy: 0.8600\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.3149 - accuracy: 0.8709\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3007 - accuracy: 0.8730\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.3150 - accuracy: 0.8684\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.3191 - accuracy: 0.8667\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.3140 - accuracy: 0.8579\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3025 - accuracy: 0.8709\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3011 - accuracy: 0.8776\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2992 - accuracy: 0.8763\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.3152 - accuracy: 0.8659\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.3146 - accuracy: 0.8721\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.3102 - accuracy: 0.8617\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.3142 - accuracy: 0.8742\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.3014 - accuracy: 0.8721\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2969 - accuracy: 0.8788\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3001 - accuracy: 0.8746\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2875 - accuracy: 0.8868\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.3030 - accuracy: 0.8763\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.3087 - accuracy: 0.8646\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2985 - accuracy: 0.8780\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.3009 - accuracy: 0.8730\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2998 - accuracy: 0.8755\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2840 - accuracy: 0.8872\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2898 - accuracy: 0.8868\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2778 - accuracy: 0.8855\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.2823 - accuracy: 0.8868\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2893 - accuracy: 0.8788\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2882 - accuracy: 0.8796\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.2778 - accuracy: 0.8901\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.2810 - accuracy: 0.8805\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2767 - accuracy: 0.8847\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 909us/step - loss: 0.2801 - accuracy: 0.8868\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.2721 - accuracy: 0.8876\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2772 - accuracy: 0.8880\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2956 - accuracy: 0.8746\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.2832 - accuracy: 0.8863\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.2714 - accuracy: 0.8847\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.2668 - accuracy: 0.8876\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2669 - accuracy: 0.8939\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.2892 - accuracy: 0.8713\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2786 - accuracy: 0.8855\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2740 - accuracy: 0.8876\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.2806 - accuracy: 0.8792\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.2617 - accuracy: 0.8926\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2738 - accuracy: 0.8922\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.2876 - accuracy: 0.8805\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2655 - accuracy: 0.8930\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2620 - accuracy: 0.8947\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2604 - accuracy: 0.8959\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2655 - accuracy: 0.8947\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2743 - accuracy: 0.8784\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2656 - accuracy: 0.8947\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.2705 - accuracy: 0.8876\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2776 - accuracy: 0.8863\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2689 - accuracy: 0.8934\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.2564 - accuracy: 0.8959\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2725 - accuracy: 0.8905\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2671 - accuracy: 0.8926\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.2502 - accuracy: 0.8943\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2436 - accuracy: 0.9031\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2567 - accuracy: 0.8893\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2538 - accuracy: 0.8893\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2614 - accuracy: 0.8972\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2486 - accuracy: 0.8985\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.2540 - accuracy: 0.9039\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.2545 - accuracy: 0.8993\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2439 - accuracy: 0.8997\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.2436 - accuracy: 0.9056\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2390 - accuracy: 0.9014\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2387 - accuracy: 0.9039\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2465 - accuracy: 0.9022\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2394 - accuracy: 0.9076\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2396 - accuracy: 0.9005\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.2378 - accuracy: 0.9031\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2466 - accuracy: 0.9047\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2480 - accuracy: 0.9031\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2405 - accuracy: 0.9089\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2431 - accuracy: 0.8997\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2323 - accuracy: 0.9043\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2283 - accuracy: 0.9135\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2351 - accuracy: 0.9064\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2249 - accuracy: 0.9085\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2292 - accuracy: 0.9131\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 896us/step - loss: 0.2366 - accuracy: 0.9081\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2218 - accuracy: 0.9122\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2343 - accuracy: 0.9106\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2378 - accuracy: 0.9039\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.2293 - accuracy: 0.9102\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2418 - accuracy: 0.9026\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2351 - accuracy: 0.9072\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2390 - accuracy: 0.9039\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2255 - accuracy: 0.9056\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2369 - accuracy: 0.9026\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2318 - accuracy: 0.9014\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2175 - accuracy: 0.9106\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2163 - accuracy: 0.9122\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.2361 - accuracy: 0.9022\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 896us/step - loss: 0.2342 - accuracy: 0.9047\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2302 - accuracy: 0.9072\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2373 - accuracy: 0.9022\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.2303 - accuracy: 0.9043\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.2214 - accuracy: 0.9089\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.2101 - accuracy: 0.9164\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2210 - accuracy: 0.9102\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2314 - accuracy: 0.9039\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2206 - accuracy: 0.9106\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2312 - accuracy: 0.9035\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2119 - accuracy: 0.9118\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2171 - accuracy: 0.9056\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2227 - accuracy: 0.9102\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.2320 - accuracy: 0.9110\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2270 - accuracy: 0.9035\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2206 - accuracy: 0.9097\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2225 - accuracy: 0.9106\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2089 - accuracy: 0.9168\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2215 - accuracy: 0.9102\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2161 - accuracy: 0.9122\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2223 - accuracy: 0.9110\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2278 - accuracy: 0.9068\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2240 - accuracy: 0.9143\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2009 - accuracy: 0.9185\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2022 - accuracy: 0.9198\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2002 - accuracy: 0.9269\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2157 - accuracy: 0.9131\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2121 - accuracy: 0.9164\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.2227 - accuracy: 0.9131\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2139 - accuracy: 0.9148\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2116 - accuracy: 0.9143\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2059 - accuracy: 0.9206\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2026 - accuracy: 0.9193\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.2129 - accuracy: 0.9089\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.2054 - accuracy: 0.9185\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2095 - accuracy: 0.9202\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2125 - accuracy: 0.9118\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.2120 - accuracy: 0.9156\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2104 - accuracy: 0.9156\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.2100 - accuracy: 0.9164\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2019 - accuracy: 0.9177\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.1978 - accuracy: 0.9206\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2108 - accuracy: 0.9143\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.1939 - accuracy: 0.9227\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1961 - accuracy: 0.9198\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2056 - accuracy: 0.9219\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2011 - accuracy: 0.9185\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2120 - accuracy: 0.9181\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2002 - accuracy: 0.9227\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1947 - accuracy: 0.9265\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.2166 - accuracy: 0.9185\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 905us/step - loss: 0.2039 - accuracy: 0.9231\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.1831 - accuracy: 0.9269\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2053 - accuracy: 0.9164\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2056 - accuracy: 0.9164\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.2028 - accuracy: 0.9193\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.2094 - accuracy: 0.9152\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.1896 - accuracy: 0.9310\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2012 - accuracy: 0.9206\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.1858 - accuracy: 0.9260\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.1930 - accuracy: 0.9181\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.1919 - accuracy: 0.9248\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.1888 - accuracy: 0.9239\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1991 - accuracy: 0.9281\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.2004 - accuracy: 0.9156\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.1948 - accuracy: 0.9260\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1952 - accuracy: 0.9214\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1952 - accuracy: 0.9252\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1924 - accuracy: 0.9244\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2000 - accuracy: 0.9173\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.1874 - accuracy: 0.9252\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2046 - accuracy: 0.9122\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.1871 - accuracy: 0.9260\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1777 - accuracy: 0.9310\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.1867 - accuracy: 0.9223\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1888 - accuracy: 0.9248\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1987 - accuracy: 0.9223\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1958 - accuracy: 0.9231\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1992 - accuracy: 0.9202\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1864 - accuracy: 0.9306\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 966us/step - loss: 0.1835 - accuracy: 0.9265\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1772 - accuracy: 0.9285\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1952 - accuracy: 0.9193\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.1876 - accuracy: 0.9260\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.1944 - accuracy: 0.9298\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1870 - accuracy: 0.9302\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.1963 - accuracy: 0.9214\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.1948 - accuracy: 0.9223\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1882 - accuracy: 0.9315\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1858 - accuracy: 0.9256\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.1727 - accuracy: 0.9277\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1825 - accuracy: 0.9323\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1761 - accuracy: 0.9340\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.1804 - accuracy: 0.9269\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1939 - accuracy: 0.9235\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1688 - accuracy: 0.9398\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1908 - accuracy: 0.9248\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1894 - accuracy: 0.9290\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1782 - accuracy: 0.9298\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1793 - accuracy: 0.9298\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1735 - accuracy: 0.9373\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.1844 - accuracy: 0.9290\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.1655 - accuracy: 0.9340\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1689 - accuracy: 0.9352\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1873 - accuracy: 0.9269\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1776 - accuracy: 0.9248\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.1829 - accuracy: 0.9290\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1826 - accuracy: 0.9285\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1813 - accuracy: 0.9294\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1872 - accuracy: 0.9310\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1674 - accuracy: 0.9340\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1708 - accuracy: 0.9398\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1648 - accuracy: 0.9331\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1826 - accuracy: 0.9323\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1887 - accuracy: 0.9252\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1806 - accuracy: 0.9260\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1738 - accuracy: 0.9331\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1682 - accuracy: 0.9365\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1634 - accuracy: 0.9365\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1773 - accuracy: 0.9336\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1698 - accuracy: 0.9386\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.1736 - accuracy: 0.9310\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1753 - accuracy: 0.9306\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1525 - accuracy: 0.9390\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1719 - accuracy: 0.9319\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.1788 - accuracy: 0.9310\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.1801 - accuracy: 0.9306\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1732 - accuracy: 0.9306\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1872 - accuracy: 0.9306\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.1757 - accuracy: 0.9348\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1609 - accuracy: 0.9336\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1783 - accuracy: 0.9265\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1629 - accuracy: 0.9402\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1707 - accuracy: 0.9294\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1782 - accuracy: 0.9256\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1761 - accuracy: 0.9340\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1711 - accuracy: 0.9356\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.1645 - accuracy: 0.9419\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1742 - accuracy: 0.9348\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.1768 - accuracy: 0.9319\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1574 - accuracy: 0.9394\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.1690 - accuracy: 0.9352\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.1659 - accuracy: 0.9302\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1546 - accuracy: 0.9394\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.1589 - accuracy: 0.9427\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1654 - accuracy: 0.9386\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.1600 - accuracy: 0.9361\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1588 - accuracy: 0.9373\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1622 - accuracy: 0.9331\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1551 - accuracy: 0.9444\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.1652 - accuracy: 0.9327\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1571 - accuracy: 0.9382\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.1870 - accuracy: 0.9285\n",
      "Epoch 362/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.1630 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 332.\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.1660 - accuracy: 0.9302\n",
      "Epoch 362: early stopping\n",
      "6/6 [==============================] - 0s 881us/step - loss: 0.8781 - accuracy: 0.6852\n",
      "6/6 [==============================] - 0s 651us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (20/25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "Final Test Results - Loss: 0.8780808448791504, Accuracy: 0.6851851940155029, Precision: 0.6524374176548089, Recall: 0.7365370506236243, F1 Score: 0.6843175690677832\n",
      "Confusion Matrix:\n",
      " [[68  5 21]\n",
      " [ 1  9  0]\n",
      " [23  1 34]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "        ..\n",
      "041A     1\n",
      "092A     1\n",
      "076A     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, Length: 83, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "057A    27\n",
      "000B    19\n",
      "029A    17\n",
      "106A    14\n",
      "028A    13\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "071A    10\n",
      "005A    10\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "037A     6\n",
      "044A     5\n",
      "070A     5\n",
      "105A     4\n",
      "014A     3\n",
      "054A     2\n",
      "025B     2\n",
      "096A     1\n",
      "049A     1\n",
      "048A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    264\n",
      "X    198\n",
      "F    193\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    150\n",
      "M     73\n",
      "F     59\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 019...\n",
      "kitten    [014B, 111A, 040A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 116A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 028A, 022A, 029A, 095A, 005A, 039A, 013...\n",
      "kitten                             [044A, 046A, 049A, 048A]\n",
      "senior                             [057A, 106A, 051B, 054A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '047A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '046A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'047A'}\n",
      "Moved to Test Set:\n",
      "{'047A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '046A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '047A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "Length of X_train_val:\n",
      "690\n",
      "Length of y_train_val:\n",
      "690\n",
      "Length of groups_train_val:\n",
      "690\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    126\n",
      "kitten    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     70\n",
      "senior     52\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "kitten    136\n",
      "senior    126\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "senior     52\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 856, 1: 680, 2: 630})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.1728 - accuracy: 0.4621\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.9759 - accuracy: 0.5706\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 970us/step - loss: 0.8714 - accuracy: 0.6191\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.8424 - accuracy: 0.6348\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.7455 - accuracy: 0.6847\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.7243 - accuracy: 0.6934\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.6903 - accuracy: 0.7091\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.6911 - accuracy: 0.7073\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.6626 - accuracy: 0.7276\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.6510 - accuracy: 0.7341\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.6297 - accuracy: 0.7470\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.6015 - accuracy: 0.7530\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.5931 - accuracy: 0.7535\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.6026 - accuracy: 0.7535\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.5821 - accuracy: 0.7678\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.5609 - accuracy: 0.7724\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.5639 - accuracy: 0.7599\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.5702 - accuracy: 0.7636\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.5369 - accuracy: 0.7696\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.5159 - accuracy: 0.7890\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.5403 - accuracy: 0.7775\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.5298 - accuracy: 0.7872\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.5317 - accuracy: 0.7798\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.5007 - accuracy: 0.7909\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.5202 - accuracy: 0.7821\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.4999 - accuracy: 0.7964\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.5047 - accuracy: 0.7853\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.4855 - accuracy: 0.7987\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.4742 - accuracy: 0.8061\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.4891 - accuracy: 0.7992\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.4713 - accuracy: 0.7955\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.4704 - accuracy: 0.8038\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.4649 - accuracy: 0.8121\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.4577 - accuracy: 0.8139\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.4708 - accuracy: 0.8024\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.4550 - accuracy: 0.8149\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.4559 - accuracy: 0.8098\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.4509 - accuracy: 0.8158\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.4276 - accuracy: 0.8269\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.4473 - accuracy: 0.8241\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.4515 - accuracy: 0.8126\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.4212 - accuracy: 0.8310\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.4303 - accuracy: 0.8338\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.4222 - accuracy: 0.8264\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4103 - accuracy: 0.8218\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.4139 - accuracy: 0.8241\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.4123 - accuracy: 0.8269\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.3906 - accuracy: 0.8380\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.4000 - accuracy: 0.8324\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.4047 - accuracy: 0.8315\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.3994 - accuracy: 0.8356\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.4031 - accuracy: 0.8343\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3935 - accuracy: 0.8426\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.3823 - accuracy: 0.8384\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.3860 - accuracy: 0.8361\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.3900 - accuracy: 0.8407\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.3645 - accuracy: 0.8476\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.3694 - accuracy: 0.8500\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.3609 - accuracy: 0.8601\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.3771 - accuracy: 0.8472\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.3702 - accuracy: 0.8463\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.3720 - accuracy: 0.8393\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.3542 - accuracy: 0.8546\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.3644 - accuracy: 0.8532\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.3717 - accuracy: 0.8444\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.3668 - accuracy: 0.8472\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.3371 - accuracy: 0.8652\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.3802 - accuracy: 0.8416\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.3410 - accuracy: 0.8638\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.3475 - accuracy: 0.8629\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.3551 - accuracy: 0.8560\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.3422 - accuracy: 0.8596\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.3462 - accuracy: 0.8569\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.3486 - accuracy: 0.8592\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 987us/step - loss: 0.3484 - accuracy: 0.8518\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.3432 - accuracy: 0.8606\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 964us/step - loss: 0.3262 - accuracy: 0.8680\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3244 - accuracy: 0.8712\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3244 - accuracy: 0.8758\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 986us/step - loss: 0.3269 - accuracy: 0.8763\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3171 - accuracy: 0.8781\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 952us/step - loss: 0.3484 - accuracy: 0.8555\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 949us/step - loss: 0.3353 - accuracy: 0.8532\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.3159 - accuracy: 0.8712\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.3247 - accuracy: 0.8753\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.3322 - accuracy: 0.8675\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.3179 - accuracy: 0.8735\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 978us/step - loss: 0.3312 - accuracy: 0.8629\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.3220 - accuracy: 0.8647\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.3043 - accuracy: 0.8813\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.3161 - accuracy: 0.8730\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.2965 - accuracy: 0.8777\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.3130 - accuracy: 0.8735\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2921 - accuracy: 0.8901\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.3043 - accuracy: 0.8813\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 902us/step - loss: 0.2952 - accuracy: 0.8781\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.3095 - accuracy: 0.8781\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.2908 - accuracy: 0.8809\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.3096 - accuracy: 0.8721\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.3006 - accuracy: 0.8800\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 885us/step - loss: 0.3020 - accuracy: 0.8777\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.3052 - accuracy: 0.8841\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2937 - accuracy: 0.8832\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.3019 - accuracy: 0.8790\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.2823 - accuracy: 0.8887\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2934 - accuracy: 0.8786\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.2880 - accuracy: 0.8818\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 968us/step - loss: 0.2812 - accuracy: 0.8869\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.2781 - accuracy: 0.8892\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.2811 - accuracy: 0.8901\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 947us/step - loss: 0.2740 - accuracy: 0.8938\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.2746 - accuracy: 0.8952\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.2865 - accuracy: 0.8827\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2867 - accuracy: 0.8841\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.2863 - accuracy: 0.8883\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.2806 - accuracy: 0.8860\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 902us/step - loss: 0.2710 - accuracy: 0.8901\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 960us/step - loss: 0.2664 - accuracy: 0.8860\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.2687 - accuracy: 0.8901\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 953us/step - loss: 0.2655 - accuracy: 0.8980\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.2851 - accuracy: 0.8827\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 921us/step - loss: 0.2874 - accuracy: 0.8818\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.2628 - accuracy: 0.8952\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.2662 - accuracy: 0.8915\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.2705 - accuracy: 0.8901\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.2557 - accuracy: 0.9007\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2559 - accuracy: 0.8952\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.2681 - accuracy: 0.8887\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.2461 - accuracy: 0.9077\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.2503 - accuracy: 0.9049\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.2717 - accuracy: 0.8920\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.2504 - accuracy: 0.9030\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.2645 - accuracy: 0.8957\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2640 - accuracy: 0.8947\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.2675 - accuracy: 0.8883\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 944us/step - loss: 0.2503 - accuracy: 0.9012\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 944us/step - loss: 0.2569 - accuracy: 0.9021\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.2543 - accuracy: 0.8998\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.2467 - accuracy: 0.8975\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.2503 - accuracy: 0.9017\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.2514 - accuracy: 0.9026\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.2397 - accuracy: 0.8994\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.2484 - accuracy: 0.9017\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.2537 - accuracy: 0.8970\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.2408 - accuracy: 0.9040\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.2471 - accuracy: 0.9007\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.2492 - accuracy: 0.9035\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 928us/step - loss: 0.2381 - accuracy: 0.9095\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 938us/step - loss: 0.2403 - accuracy: 0.9026\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 926us/step - loss: 0.2465 - accuracy: 0.9003\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 966us/step - loss: 0.2266 - accuracy: 0.9123\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.2273 - accuracy: 0.9049\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 918us/step - loss: 0.2384 - accuracy: 0.9090\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 934us/step - loss: 0.2506 - accuracy: 0.9007\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.2261 - accuracy: 0.9077\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 964us/step - loss: 0.2450 - accuracy: 0.8994\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.2359 - accuracy: 0.9072\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.9040\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.2299 - accuracy: 0.9132\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 872us/step - loss: 0.2355 - accuracy: 0.9044\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2503 - accuracy: 0.8961\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.2373 - accuracy: 0.9021\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.2224 - accuracy: 0.9063\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2334 - accuracy: 0.9077\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.2290 - accuracy: 0.9109\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.2439 - accuracy: 0.9077\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 922us/step - loss: 0.2143 - accuracy: 0.9178\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.2142 - accuracy: 0.9178\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.2254 - accuracy: 0.9164\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.2212 - accuracy: 0.9141\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 963us/step - loss: 0.2265 - accuracy: 0.9183\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.2125 - accuracy: 0.9174\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 937us/step - loss: 0.2145 - accuracy: 0.9197\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.2303 - accuracy: 0.9077\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2226 - accuracy: 0.9164\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2125 - accuracy: 0.9183\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2168 - accuracy: 0.9197\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.2192 - accuracy: 0.9164\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.2139 - accuracy: 0.9201\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.2368 - accuracy: 0.9077\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.2138 - accuracy: 0.9164\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2151 - accuracy: 0.9141\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.2143 - accuracy: 0.9132\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.2232 - accuracy: 0.9067\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.2126 - accuracy: 0.9174\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2333 - accuracy: 0.8989\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.2180 - accuracy: 0.9160\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2117 - accuracy: 0.9178\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.2231 - accuracy: 0.9127\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.2115 - accuracy: 0.9160\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.2135 - accuracy: 0.9224\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.2174 - accuracy: 0.9132\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.2197 - accuracy: 0.9086\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2026 - accuracy: 0.9178\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2288 - accuracy: 0.9090\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.2149 - accuracy: 0.9211\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.1978 - accuracy: 0.9307\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.1982 - accuracy: 0.9192\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 934us/step - loss: 0.1962 - accuracy: 0.9252\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.2035 - accuracy: 0.9169\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 977us/step - loss: 0.2147 - accuracy: 0.9206\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2037 - accuracy: 0.9252\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9137\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9229\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 968us/step - loss: 0.1980 - accuracy: 0.9243\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9155\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9252\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9215\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.1962 - accuracy: 0.9252\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.1982 - accuracy: 0.9247\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.2110 - accuracy: 0.9160\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1969 - accuracy: 0.9243\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.1886 - accuracy: 0.9238\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.1938 - accuracy: 0.9266\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.1950 - accuracy: 0.9243\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.2095 - accuracy: 0.9247\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1921 - accuracy: 0.9238\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9280\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9238\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9294\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 910us/step - loss: 0.2253 - accuracy: 0.9104\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9247\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1931 - accuracy: 0.9266\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9294\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 980us/step - loss: 0.1931 - accuracy: 0.9284\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1926 - accuracy: 0.9280\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.2002 - accuracy: 0.9201\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.1976 - accuracy: 0.9183\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.1988 - accuracy: 0.9224\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.1979 - accuracy: 0.9275\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1926 - accuracy: 0.9187\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.1739 - accuracy: 0.9326\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.2064 - accuracy: 0.9238\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1881 - accuracy: 0.9312\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.1793 - accuracy: 0.9284\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.1821 - accuracy: 0.9340\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 886us/step - loss: 0.2056 - accuracy: 0.9187\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 986us/step - loss: 0.1940 - accuracy: 0.9257\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.1837 - accuracy: 0.9229\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.1732 - accuracy: 0.9317\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.1993 - accuracy: 0.9183\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.1830 - accuracy: 0.9289\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.1658 - accuracy: 0.9372\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.1817 - accuracy: 0.9317\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.1919 - accuracy: 0.9280\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1799 - accuracy: 0.9331\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.1667 - accuracy: 0.9344\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.1758 - accuracy: 0.9294\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.1936 - accuracy: 0.9261\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.1928 - accuracy: 0.9229\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1876 - accuracy: 0.9266\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.1975 - accuracy: 0.9271\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1868 - accuracy: 0.9271\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1725 - accuracy: 0.9344\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.1737 - accuracy: 0.9307\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1720 - accuracy: 0.9331\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.1814 - accuracy: 0.9266\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1675 - accuracy: 0.9335\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.1714 - accuracy: 0.9321\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.1827 - accuracy: 0.9298\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1817 - accuracy: 0.9280\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1778 - accuracy: 0.9335\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1785 - accuracy: 0.9326\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9312\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.1875 - accuracy: 0.9317\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.1905 - accuracy: 0.9284\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1707 - accuracy: 0.9358\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.1819 - accuracy: 0.9266\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1744 - accuracy: 0.9326\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 910us/step - loss: 0.1735 - accuracy: 0.9321\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.1638 - accuracy: 0.9349\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9317\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1595 - accuracy: 0.9391\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 965us/step - loss: 0.1544 - accuracy: 0.9400\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.1896 - accuracy: 0.9284\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 924us/step - loss: 0.1769 - accuracy: 0.9349\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.1542 - accuracy: 0.9409\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.1740 - accuracy: 0.9280\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.1670 - accuracy: 0.9377\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 917us/step - loss: 0.1753 - accuracy: 0.9261\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.1673 - accuracy: 0.9381\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 950us/step - loss: 0.1669 - accuracy: 0.9317\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 921us/step - loss: 0.1732 - accuracy: 0.9354\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.1668 - accuracy: 0.9377\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9321\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9391\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 969us/step - loss: 0.1586 - accuracy: 0.9386\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.1497 - accuracy: 0.9441\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1535 - accuracy: 0.9441\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.1671 - accuracy: 0.9349\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.1481 - accuracy: 0.9428\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 996us/step - loss: 0.1669 - accuracy: 0.9372\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.1665 - accuracy: 0.9414\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1603 - accuracy: 0.9428\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.1622 - accuracy: 0.9354\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1564 - accuracy: 0.9428\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.1661 - accuracy: 0.9395\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.1634 - accuracy: 0.9335\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 908us/step - loss: 0.1445 - accuracy: 0.9432\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1508 - accuracy: 0.9437\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.1589 - accuracy: 0.9469\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1594 - accuracy: 0.9367\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.1613 - accuracy: 0.9377\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.1513 - accuracy: 0.9404\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1467 - accuracy: 0.9501\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.1669 - accuracy: 0.9344\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1438 - accuracy: 0.9492\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.1552 - accuracy: 0.9414\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1602 - accuracy: 0.9437\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.1530 - accuracy: 0.9423\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1627 - accuracy: 0.9381\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.1524 - accuracy: 0.9460\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.1606 - accuracy: 0.9344\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.1563 - accuracy: 0.9344\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.1613 - accuracy: 0.9344\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1511 - accuracy: 0.9395\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.1511 - accuracy: 0.9428\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.1650 - accuracy: 0.9340\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.1502 - accuracy: 0.9414\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.1636 - accuracy: 0.9344\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.1435 - accuracy: 0.9469\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.1462 - accuracy: 0.9446\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.1498 - accuracy: 0.9377\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.1514 - accuracy: 0.9478\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.1676 - accuracy: 0.9298\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.1490 - accuracy: 0.9441\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.1526 - accuracy: 0.9437\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 937us/step - loss: 0.1470 - accuracy: 0.9441\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.1383 - accuracy: 0.9488\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1482 - accuracy: 0.9437\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1514 - accuracy: 0.9437\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1493 - accuracy: 0.9464\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.1517 - accuracy: 0.9409\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.1546 - accuracy: 0.9414\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1469 - accuracy: 0.9372\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1491 - accuracy: 0.9418\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 977us/step - loss: 0.1387 - accuracy: 0.9524\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 977us/step - loss: 0.1436 - accuracy: 0.9400\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 939us/step - loss: 0.1428 - accuracy: 0.9497\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1609 - accuracy: 0.9404\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1502 - accuracy: 0.9395\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 992us/step - loss: 0.1440 - accuracy: 0.9428\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 952us/step - loss: 0.1489 - accuracy: 0.9437\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1468 - accuracy: 0.9391\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1305 - accuracy: 0.9529\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.1404 - accuracy: 0.9446\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.1386 - accuracy: 0.9501\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.1439 - accuracy: 0.9460\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1496 - accuracy: 0.9414\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1624 - accuracy: 0.9381\n",
      "Epoch 351/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.1447 - accuracy: 0.9372\n",
      "Epoch 352/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1318 - accuracy: 0.9543\n",
      "Epoch 353/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.1453 - accuracy: 0.9441\n",
      "Epoch 354/1500\n",
      "34/34 [==============================] - 0s 967us/step - loss: 0.1435 - accuracy: 0.9460\n",
      "Epoch 355/1500\n",
      "34/34 [==============================] - 0s 996us/step - loss: 0.1424 - accuracy: 0.9414\n",
      "Epoch 356/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.1575 - accuracy: 0.9418\n",
      "Epoch 357/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.1474 - accuracy: 0.9423\n",
      "Epoch 358/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.1471 - accuracy: 0.9400\n",
      "Epoch 359/1500\n",
      "34/34 [==============================] - 0s 962us/step - loss: 0.1320 - accuracy: 0.9506\n",
      "Epoch 360/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.1445 - accuracy: 0.9455\n",
      "Epoch 361/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.1400 - accuracy: 0.9474\n",
      "Epoch 362/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9446\n",
      "Epoch 363/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9423\n",
      "Epoch 364/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.1345 - accuracy: 0.9441\n",
      "Epoch 365/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.1311 - accuracy: 0.9543\n",
      "Epoch 366/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.1416 - accuracy: 0.9446\n",
      "Epoch 367/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.1400 - accuracy: 0.9506\n",
      "Epoch 368/1500\n",
      "34/34 [==============================] - 0s 928us/step - loss: 0.1524 - accuracy: 0.9446\n",
      "Epoch 369/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9478\n",
      "Epoch 370/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9594\n",
      "Epoch 371/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.1403 - accuracy: 0.9446\n",
      "Epoch 372/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9395\n",
      "Epoch 373/1500\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.1384 - accuracy: 0.9497\n",
      "Epoch 374/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9446\n",
      "Epoch 375/1500\n",
      "34/34 [==============================] - 0s 912us/step - loss: 0.1335 - accuracy: 0.9552\n",
      "Epoch 376/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.1537 - accuracy: 0.9441\n",
      "Epoch 377/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9409\n",
      "Epoch 378/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9460\n",
      "Epoch 379/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9377\n",
      "Epoch 380/1500\n",
      "34/34 [==============================] - 0s 915us/step - loss: 0.1370 - accuracy: 0.9497\n",
      "Epoch 381/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.1337 - accuracy: 0.9460\n",
      "Epoch 382/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.1401 - accuracy: 0.9455\n",
      "Epoch 383/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1412 - accuracy: 0.9455\n",
      "Epoch 384/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1251 - accuracy: 0.9501\n",
      "Epoch 385/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.1297 - accuracy: 0.9515\n",
      "Epoch 386/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.1399 - accuracy: 0.9501\n",
      "Epoch 387/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.1369 - accuracy: 0.9474\n",
      "Epoch 388/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.1261 - accuracy: 0.9520\n",
      "Epoch 389/1500\n",
      "34/34 [==============================] - 0s 996us/step - loss: 0.1334 - accuracy: 0.9488\n",
      "Epoch 390/1500\n",
      "34/34 [==============================] - 0s 920us/step - loss: 0.1248 - accuracy: 0.9538\n",
      "Epoch 391/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.1344 - accuracy: 0.9511\n",
      "Epoch 392/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1364 - accuracy: 0.9497\n",
      "Epoch 393/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.1256 - accuracy: 0.9520\n",
      "Epoch 394/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.1334 - accuracy: 0.9488\n",
      "Epoch 395/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.1407 - accuracy: 0.9446\n",
      "Epoch 396/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1464 - accuracy: 0.9400\n",
      "Epoch 397/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.1448 - accuracy: 0.9441\n",
      "Epoch 398/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.1296 - accuracy: 0.9506\n",
      "Epoch 399/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.1289 - accuracy: 0.9492\n",
      "Epoch 400/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.1095 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 370.\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1350 - accuracy: 0.9460\n",
      "Epoch 400: early stopping\n",
      "8/8 [==============================] - 0s 775us/step - loss: 1.0192 - accuracy: 0.6599\n",
      "8/8 [==============================] - 0s 643us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (21/29)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "After appending - Cat IDs: 409, Predictions: 409, Actuals: 409, Gender: 409\n",
      "Final Test Results - Loss: 1.0191649198532104, Accuracy: 0.659919023513794, Precision: 0.6494135317542663, Recall: 0.6090201465201465, F1 Score: 0.6252415142046132\n",
      "Confusion Matrix:\n",
      " [[118   3  39]\n",
      " [ 10  24   1]\n",
      " [ 30   1  21]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "055A    20\n",
      "        ..\n",
      "096A     1\n",
      "043A     1\n",
      "073A     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 82, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "059A    14\n",
      "097B    14\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "072A     9\n",
      "033A     9\n",
      "027A     7\n",
      "099A     7\n",
      "050A     7\n",
      "023A     6\n",
      "034A     5\n",
      "025C     5\n",
      "075A     5\n",
      "052A     4\n",
      "003A     4\n",
      "012A     3\n",
      "006A     3\n",
      "018A     2\n",
      "026C     1\n",
      "019B     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    250\n",
      "F    202\n",
      "M    180\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    157\n",
      "X     98\n",
      "F     50\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 103A, 071A, 028A, 062A, 101...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 043...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 097B, 019A, 074A, 067A, 020A, 002...\n",
      "kitten                                   [047A, 050A, 115A]\n",
      "senior                       [097A, 059A, 116A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 52, 'kitten': 13, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 22, 'kitten': 3, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "632\n",
      "Length of y_train_val:\n",
      "632\n",
      "Length of groups_train_val:\n",
      "632\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 748, 1: 675, 2: 615})\n",
      "Epoch 1/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1313 - accuracy: 0.4794\n",
      "Epoch 2/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9388 - accuracy: 0.5765\n",
      "Epoch 3/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8624 - accuracy: 0.6197\n",
      "Epoch 4/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.8289 - accuracy: 0.6251\n",
      "Epoch 5/1500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.7570 - accuracy: 0.6629\n",
      "Epoch 6/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.7547 - accuracy: 0.6811\n",
      "Epoch 7/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.7283 - accuracy: 0.6811\n",
      "Epoch 8/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.7360 - accuracy: 0.6649\n",
      "Epoch 9/1500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.6971 - accuracy: 0.6860\n",
      "Epoch 10/1500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.6895 - accuracy: 0.6791\n",
      "Epoch 11/1500\n",
      "32/32 [==============================] - 0s 880us/step - loss: 0.6705 - accuracy: 0.6997\n",
      "Epoch 12/1500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.6538 - accuracy: 0.7179\n",
      "Epoch 13/1500\n",
      "32/32 [==============================] - 0s 870us/step - loss: 0.6483 - accuracy: 0.7115\n",
      "Epoch 14/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.6490 - accuracy: 0.7120\n",
      "Epoch 15/1500\n",
      "32/32 [==============================] - 0s 821us/step - loss: 0.6271 - accuracy: 0.7164\n",
      "Epoch 16/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.6045 - accuracy: 0.7321\n",
      "Epoch 17/1500\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.6298 - accuracy: 0.7125\n",
      "Epoch 18/1500\n",
      "32/32 [==============================] - 0s 986us/step - loss: 0.6162 - accuracy: 0.7252\n",
      "Epoch 19/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.5944 - accuracy: 0.7326\n",
      "Epoch 20/1500\n",
      "32/32 [==============================] - 0s 865us/step - loss: 0.5843 - accuracy: 0.7399\n",
      "Epoch 21/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.5922 - accuracy: 0.7360\n",
      "Epoch 22/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.5918 - accuracy: 0.7409\n",
      "Epoch 23/1500\n",
      "32/32 [==============================] - 0s 879us/step - loss: 0.5755 - accuracy: 0.7429\n",
      "Epoch 24/1500\n",
      "32/32 [==============================] - 0s 896us/step - loss: 0.5734 - accuracy: 0.7434\n",
      "Epoch 25/1500\n",
      "32/32 [==============================] - 0s 912us/step - loss: 0.5630 - accuracy: 0.7566\n",
      "Epoch 26/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5783 - accuracy: 0.7463\n",
      "Epoch 27/1500\n",
      "32/32 [==============================] - 0s 990us/step - loss: 0.5323 - accuracy: 0.7679\n",
      "Epoch 28/1500\n",
      "32/32 [==============================] - 0s 972us/step - loss: 0.5348 - accuracy: 0.7718\n",
      "Epoch 29/1500\n",
      "32/32 [==============================] - 0s 917us/step - loss: 0.5456 - accuracy: 0.7635\n",
      "Epoch 30/1500\n",
      "32/32 [==============================] - 0s 990us/step - loss: 0.5434 - accuracy: 0.7552\n",
      "Epoch 31/1500\n",
      "32/32 [==============================] - 0s 939us/step - loss: 0.5319 - accuracy: 0.7576\n",
      "Epoch 32/1500\n",
      "32/32 [==============================] - 0s 979us/step - loss: 0.5128 - accuracy: 0.7723\n",
      "Epoch 33/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5261 - accuracy: 0.7738\n",
      "Epoch 34/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5200 - accuracy: 0.7679\n",
      "Epoch 35/1500\n",
      "32/32 [==============================] - 0s 925us/step - loss: 0.5145 - accuracy: 0.7772\n",
      "Epoch 36/1500\n",
      "32/32 [==============================] - 0s 990us/step - loss: 0.5171 - accuracy: 0.7723\n",
      "Epoch 37/1500\n",
      "32/32 [==============================] - 0s 943us/step - loss: 0.4958 - accuracy: 0.7826\n",
      "Epoch 38/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5168 - accuracy: 0.7753\n",
      "Epoch 39/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4888 - accuracy: 0.7816\n",
      "Epoch 40/1500\n",
      "32/32 [==============================] - 0s 953us/step - loss: 0.5111 - accuracy: 0.7738\n",
      "Epoch 41/1500\n",
      "32/32 [==============================] - 0s 913us/step - loss: 0.5011 - accuracy: 0.7758\n",
      "Epoch 42/1500\n",
      "32/32 [==============================] - 0s 925us/step - loss: 0.4862 - accuracy: 0.7890\n",
      "Epoch 43/1500\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.4756 - accuracy: 0.7939\n",
      "Epoch 44/1500\n",
      "32/32 [==============================] - 0s 900us/step - loss: 0.4755 - accuracy: 0.7969\n",
      "Epoch 45/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.4823 - accuracy: 0.7826\n",
      "Epoch 46/1500\n",
      "32/32 [==============================] - 0s 875us/step - loss: 0.4663 - accuracy: 0.7978\n",
      "Epoch 47/1500\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.4676 - accuracy: 0.8057\n",
      "Epoch 48/1500\n",
      "32/32 [==============================] - 0s 952us/step - loss: 0.4624 - accuracy: 0.8052\n",
      "Epoch 49/1500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.4581 - accuracy: 0.8013\n",
      "Epoch 50/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.4691 - accuracy: 0.7964\n",
      "Epoch 51/1500\n",
      "32/32 [==============================] - 0s 959us/step - loss: 0.4784 - accuracy: 0.7915\n",
      "Epoch 52/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.7978\n",
      "Epoch 53/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4540 - accuracy: 0.8081\n",
      "Epoch 54/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4429 - accuracy: 0.8067\n",
      "Epoch 55/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4387 - accuracy: 0.8111\n",
      "Epoch 56/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.7954\n",
      "Epoch 57/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4520 - accuracy: 0.7978\n",
      "Epoch 58/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4301 - accuracy: 0.8140\n",
      "Epoch 59/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4185 - accuracy: 0.8145\n",
      "Epoch 60/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4406 - accuracy: 0.8057\n",
      "Epoch 61/1500\n",
      "32/32 [==============================] - 0s 988us/step - loss: 0.4460 - accuracy: 0.7974\n",
      "Epoch 62/1500\n",
      "32/32 [==============================] - 0s 977us/step - loss: 0.4506 - accuracy: 0.8077\n",
      "Epoch 63/1500\n",
      "32/32 [==============================] - 0s 972us/step - loss: 0.4471 - accuracy: 0.8067\n",
      "Epoch 64/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4404 - accuracy: 0.8086\n",
      "Epoch 65/1500\n",
      "32/32 [==============================] - 0s 971us/step - loss: 0.4281 - accuracy: 0.8184\n",
      "Epoch 66/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8243\n",
      "Epoch 67/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4302 - accuracy: 0.8121\n",
      "Epoch 68/1500\n",
      "32/32 [==============================] - 0s 969us/step - loss: 0.4128 - accuracy: 0.8219\n",
      "Epoch 69/1500\n",
      "32/32 [==============================] - 0s 911us/step - loss: 0.4331 - accuracy: 0.8131\n",
      "Epoch 70/1500\n",
      "32/32 [==============================] - 0s 951us/step - loss: 0.4205 - accuracy: 0.8140\n",
      "Epoch 71/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4050 - accuracy: 0.8253\n",
      "Epoch 72/1500\n",
      "32/32 [==============================] - 0s 994us/step - loss: 0.4112 - accuracy: 0.8175\n",
      "Epoch 73/1500\n",
      "32/32 [==============================] - 0s 906us/step - loss: 0.4074 - accuracy: 0.8194\n",
      "Epoch 74/1500\n",
      "32/32 [==============================] - 0s 894us/step - loss: 0.4211 - accuracy: 0.8204\n",
      "Epoch 75/1500\n",
      "32/32 [==============================] - 0s 963us/step - loss: 0.4164 - accuracy: 0.8224\n",
      "Epoch 76/1500\n",
      "32/32 [==============================] - 0s 926us/step - loss: 0.4216 - accuracy: 0.8106\n",
      "Epoch 77/1500\n",
      "32/32 [==============================] - 0s 914us/step - loss: 0.4040 - accuracy: 0.8175\n",
      "Epoch 78/1500\n",
      "32/32 [==============================] - 0s 876us/step - loss: 0.4076 - accuracy: 0.8209\n",
      "Epoch 79/1500\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.4036 - accuracy: 0.8288\n",
      "Epoch 80/1500\n",
      "32/32 [==============================] - 0s 910us/step - loss: 0.3847 - accuracy: 0.8425\n",
      "Epoch 81/1500\n",
      "32/32 [==============================] - 0s 961us/step - loss: 0.4052 - accuracy: 0.8234\n",
      "Epoch 82/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3964 - accuracy: 0.8356\n",
      "Epoch 83/1500\n",
      "32/32 [==============================] - 0s 899us/step - loss: 0.4112 - accuracy: 0.8145\n",
      "Epoch 84/1500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.3848 - accuracy: 0.8400\n",
      "Epoch 85/1500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.4146 - accuracy: 0.8126\n",
      "Epoch 86/1500\n",
      "32/32 [==============================] - 0s 902us/step - loss: 0.4228 - accuracy: 0.8194\n",
      "Epoch 87/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.3917 - accuracy: 0.8327\n",
      "Epoch 88/1500\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.3946 - accuracy: 0.8263\n",
      "Epoch 89/1500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.3985 - accuracy: 0.8253\n",
      "Epoch 90/1500\n",
      "32/32 [==============================] - 0s 894us/step - loss: 0.3741 - accuracy: 0.8405\n",
      "Epoch 91/1500\n",
      "32/32 [==============================] - 0s 978us/step - loss: 0.3842 - accuracy: 0.8346\n",
      "Epoch 92/1500\n",
      "32/32 [==============================] - 0s 916us/step - loss: 0.3905 - accuracy: 0.8342\n",
      "Epoch 93/1500\n",
      "32/32 [==============================] - 0s 918us/step - loss: 0.3886 - accuracy: 0.8322\n",
      "Epoch 94/1500\n",
      "32/32 [==============================] - 0s 894us/step - loss: 0.3725 - accuracy: 0.8449\n",
      "Epoch 95/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3721 - accuracy: 0.8454\n",
      "Epoch 96/1500\n",
      "32/32 [==============================] - 0s 961us/step - loss: 0.3842 - accuracy: 0.8317\n",
      "Epoch 97/1500\n",
      "32/32 [==============================] - 0s 884us/step - loss: 0.3740 - accuracy: 0.8327\n",
      "Epoch 98/1500\n",
      "32/32 [==============================] - 0s 893us/step - loss: 0.3653 - accuracy: 0.8508\n",
      "Epoch 99/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3667 - accuracy: 0.8479\n",
      "Epoch 100/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3614 - accuracy: 0.8479\n",
      "Epoch 101/1500\n",
      "32/32 [==============================] - 0s 934us/step - loss: 0.3678 - accuracy: 0.8464\n",
      "Epoch 102/1500\n",
      "32/32 [==============================] - 0s 995us/step - loss: 0.3516 - accuracy: 0.8606\n",
      "Epoch 103/1500\n",
      "32/32 [==============================] - 0s 967us/step - loss: 0.3703 - accuracy: 0.8435\n",
      "Epoch 104/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3602 - accuracy: 0.8464\n",
      "Epoch 105/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3715 - accuracy: 0.8474\n",
      "Epoch 106/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8440\n",
      "Epoch 107/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3756 - accuracy: 0.8366\n",
      "Epoch 108/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3472 - accuracy: 0.8479\n",
      "Epoch 109/1500\n",
      "32/32 [==============================] - 0s 927us/step - loss: 0.3721 - accuracy: 0.8445\n",
      "Epoch 110/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.3520 - accuracy: 0.8440\n",
      "Epoch 111/1500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.3635 - accuracy: 0.8503\n",
      "Epoch 112/1500\n",
      "32/32 [==============================] - 0s 815us/step - loss: 0.3751 - accuracy: 0.8395\n",
      "Epoch 113/1500\n",
      "32/32 [==============================] - 0s 888us/step - loss: 0.3474 - accuracy: 0.8513\n",
      "Epoch 114/1500\n",
      "32/32 [==============================] - 0s 949us/step - loss: 0.3405 - accuracy: 0.8533\n",
      "Epoch 115/1500\n",
      "32/32 [==============================] - 0s 913us/step - loss: 0.3560 - accuracy: 0.8587\n",
      "Epoch 116/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3442 - accuracy: 0.8582\n",
      "Epoch 117/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3425 - accuracy: 0.8577\n",
      "Epoch 118/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3385 - accuracy: 0.8651\n",
      "Epoch 119/1500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3555 - accuracy: 0.8528\n",
      "Epoch 120/1500\n",
      "32/32 [==============================] - 0s 992us/step - loss: 0.3525 - accuracy: 0.8459\n",
      "Epoch 121/1500\n",
      "32/32 [==============================] - 0s 974us/step - loss: 0.3507 - accuracy: 0.8548\n",
      "Epoch 122/1500\n",
      "32/32 [==============================] - 0s 922us/step - loss: 0.3371 - accuracy: 0.8557\n",
      "Epoch 123/1500\n",
      "32/32 [==============================] - 0s 910us/step - loss: 0.3378 - accuracy: 0.8611\n",
      "Epoch 124/1500\n",
      "32/32 [==============================] - 0s 876us/step - loss: 0.3552 - accuracy: 0.8420\n",
      "Epoch 125/1500\n",
      "32/32 [==============================] - 0s 880us/step - loss: 0.3332 - accuracy: 0.8631\n",
      "Epoch 126/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.3277 - accuracy: 0.8675\n",
      "Epoch 127/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.3354 - accuracy: 0.8562\n",
      "Epoch 128/1500\n",
      "32/32 [==============================] - 0s 958us/step - loss: 0.3403 - accuracy: 0.8557\n",
      "Epoch 129/1500\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3347 - accuracy: 0.8503\n",
      "Epoch 130/1500\n",
      "32/32 [==============================] - 0s 897us/step - loss: 0.3224 - accuracy: 0.8636\n",
      "Epoch 131/1500\n",
      "32/32 [==============================] - 0s 900us/step - loss: 0.3352 - accuracy: 0.8523\n",
      "Epoch 132/1500\n",
      "32/32 [==============================] - 0s 956us/step - loss: 0.3445 - accuracy: 0.8553\n",
      "Epoch 133/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3339 - accuracy: 0.8577\n",
      "Epoch 134/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3185 - accuracy: 0.8587\n",
      "Epoch 135/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3372 - accuracy: 0.8484\n",
      "Epoch 136/1500\n",
      "32/32 [==============================] - 0s 902us/step - loss: 0.3299 - accuracy: 0.8685\n",
      "Epoch 137/1500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.3324 - accuracy: 0.8616\n",
      "Epoch 138/1500\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.3150 - accuracy: 0.8646\n",
      "Epoch 139/1500\n",
      "32/32 [==============================] - 0s 933us/step - loss: 0.3250 - accuracy: 0.8705\n",
      "Epoch 140/1500\n",
      "32/32 [==============================] - 0s 864us/step - loss: 0.3370 - accuracy: 0.8592\n",
      "Epoch 141/1500\n",
      "32/32 [==============================] - 0s 945us/step - loss: 0.3214 - accuracy: 0.8656\n",
      "Epoch 142/1500\n",
      "32/32 [==============================] - 0s 915us/step - loss: 0.3180 - accuracy: 0.8606\n",
      "Epoch 143/1500\n",
      "32/32 [==============================] - 0s 876us/step - loss: 0.3255 - accuracy: 0.8626\n",
      "Epoch 144/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.3269 - accuracy: 0.8685\n",
      "Epoch 145/1500\n",
      "32/32 [==============================] - 0s 879us/step - loss: 0.3261 - accuracy: 0.8680\n",
      "Epoch 146/1500\n",
      "32/32 [==============================] - 0s 907us/step - loss: 0.3128 - accuracy: 0.8651\n",
      "Epoch 147/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3133 - accuracy: 0.8700\n",
      "Epoch 148/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3258 - accuracy: 0.8621\n",
      "Epoch 149/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3100 - accuracy: 0.8759\n",
      "Epoch 150/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3167 - accuracy: 0.8705\n",
      "Epoch 151/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3200 - accuracy: 0.8646\n",
      "Epoch 152/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3070 - accuracy: 0.8739\n",
      "Epoch 153/1500\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.3214 - accuracy: 0.8670\n",
      "Epoch 154/1500\n",
      "32/32 [==============================] - 0s 921us/step - loss: 0.3183 - accuracy: 0.8700\n",
      "Epoch 155/1500\n",
      "32/32 [==============================] - 0s 947us/step - loss: 0.3134 - accuracy: 0.8734\n",
      "Epoch 156/1500\n",
      "32/32 [==============================] - 0s 952us/step - loss: 0.3132 - accuracy: 0.8602\n",
      "Epoch 157/1500\n",
      "32/32 [==============================] - 0s 939us/step - loss: 0.3039 - accuracy: 0.8749\n",
      "Epoch 158/1500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.3103 - accuracy: 0.8754\n",
      "Epoch 159/1500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.3341 - accuracy: 0.8543\n",
      "Epoch 160/1500\n",
      "32/32 [==============================] - 0s 969us/step - loss: 0.3103 - accuracy: 0.8739\n",
      "Epoch 161/1500\n",
      "32/32 [==============================] - 0s 948us/step - loss: 0.3008 - accuracy: 0.8729\n",
      "Epoch 162/1500\n",
      "32/32 [==============================] - 0s 908us/step - loss: 0.3101 - accuracy: 0.8714\n",
      "Epoch 163/1500\n",
      "32/32 [==============================] - 0s 906us/step - loss: 0.3075 - accuracy: 0.8724\n",
      "Epoch 164/1500\n",
      "32/32 [==============================] - 0s 957us/step - loss: 0.3094 - accuracy: 0.8631\n",
      "Epoch 165/1500\n",
      "32/32 [==============================] - 0s 991us/step - loss: 0.3225 - accuracy: 0.8641\n",
      "Epoch 166/1500\n",
      "32/32 [==============================] - 0s 998us/step - loss: 0.3189 - accuracy: 0.8665\n",
      "Epoch 167/1500\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.2979 - accuracy: 0.8749\n",
      "Epoch 168/1500\n",
      "32/32 [==============================] - 0s 930us/step - loss: 0.2990 - accuracy: 0.8763\n",
      "Epoch 169/1500\n",
      "32/32 [==============================] - 0s 872us/step - loss: 0.3116 - accuracy: 0.8651\n",
      "Epoch 170/1500\n",
      "32/32 [==============================] - 0s 913us/step - loss: 0.2848 - accuracy: 0.8817\n",
      "Epoch 171/1500\n",
      "32/32 [==============================] - 0s 974us/step - loss: 0.2913 - accuracy: 0.8808\n",
      "Epoch 172/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2943 - accuracy: 0.8744\n",
      "Epoch 173/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2814 - accuracy: 0.8906\n",
      "Epoch 174/1500\n",
      "32/32 [==============================] - 0s 931us/step - loss: 0.3030 - accuracy: 0.8754\n",
      "Epoch 175/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2895 - accuracy: 0.8734\n",
      "Epoch 176/1500\n",
      "32/32 [==============================] - 0s 967us/step - loss: 0.2935 - accuracy: 0.8768\n",
      "Epoch 177/1500\n",
      "32/32 [==============================] - 0s 993us/step - loss: 0.2831 - accuracy: 0.8832\n",
      "Epoch 178/1500\n",
      "32/32 [==============================] - 0s 978us/step - loss: 0.3010 - accuracy: 0.8710\n",
      "Epoch 179/1500\n",
      "32/32 [==============================] - 0s 979us/step - loss: 0.2945 - accuracy: 0.8768\n",
      "Epoch 180/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8768\n",
      "Epoch 181/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2982 - accuracy: 0.8778\n",
      "Epoch 182/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2962 - accuracy: 0.8778\n",
      "Epoch 183/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2760 - accuracy: 0.8896\n",
      "Epoch 184/1500\n",
      "32/32 [==============================] - 0s 935us/step - loss: 0.2836 - accuracy: 0.8803\n",
      "Epoch 185/1500\n",
      "32/32 [==============================] - 0s 911us/step - loss: 0.2862 - accuracy: 0.8724\n",
      "Epoch 186/1500\n",
      "32/32 [==============================] - 0s 879us/step - loss: 0.2753 - accuracy: 0.8862\n",
      "Epoch 187/1500\n",
      "32/32 [==============================] - 0s 929us/step - loss: 0.2756 - accuracy: 0.8896\n",
      "Epoch 188/1500\n",
      "32/32 [==============================] - 0s 962us/step - loss: 0.2976 - accuracy: 0.8734\n",
      "Epoch 189/1500\n",
      "32/32 [==============================] - 0s 973us/step - loss: 0.2848 - accuracy: 0.8724\n",
      "Epoch 190/1500\n",
      "32/32 [==============================] - 0s 965us/step - loss: 0.2951 - accuracy: 0.8808\n",
      "Epoch 191/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2747 - accuracy: 0.8876\n",
      "Epoch 192/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2662 - accuracy: 0.8955\n",
      "Epoch 193/1500\n",
      "32/32 [==============================] - 0s 996us/step - loss: 0.2775 - accuracy: 0.8886\n",
      "Epoch 194/1500\n",
      "32/32 [==============================] - 0s 898us/step - loss: 0.2835 - accuracy: 0.8749\n",
      "Epoch 195/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2711 - accuracy: 0.8832\n",
      "Epoch 196/1500\n",
      "32/32 [==============================] - 0s 927us/step - loss: 0.2763 - accuracy: 0.8871\n",
      "Epoch 197/1500\n",
      "32/32 [==============================] - 0s 909us/step - loss: 0.2895 - accuracy: 0.8778\n",
      "Epoch 198/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.2746 - accuracy: 0.8847\n",
      "Epoch 199/1500\n",
      "32/32 [==============================] - 0s 853us/step - loss: 0.2776 - accuracy: 0.8822\n",
      "Epoch 200/1500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.2684 - accuracy: 0.8930\n",
      "Epoch 201/1500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.2784 - accuracy: 0.8813\n",
      "Epoch 202/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.2843 - accuracy: 0.8881\n",
      "Epoch 203/1500\n",
      "32/32 [==============================] - 0s 896us/step - loss: 0.2752 - accuracy: 0.8808\n",
      "Epoch 204/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.2811 - accuracy: 0.8925\n",
      "Epoch 205/1500\n",
      "32/32 [==============================] - 0s 889us/step - loss: 0.2703 - accuracy: 0.8906\n",
      "Epoch 206/1500\n",
      "32/32 [==============================] - 0s 907us/step - loss: 0.2631 - accuracy: 0.8906\n",
      "Epoch 207/1500\n",
      "32/32 [==============================] - 0s 873us/step - loss: 0.2737 - accuracy: 0.8827\n",
      "Epoch 208/1500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.2572 - accuracy: 0.8955\n",
      "Epoch 209/1500\n",
      "32/32 [==============================] - 0s 870us/step - loss: 0.2614 - accuracy: 0.8970\n",
      "Epoch 210/1500\n",
      "32/32 [==============================] - 0s 811us/step - loss: 0.2672 - accuracy: 0.8896\n",
      "Epoch 211/1500\n",
      "32/32 [==============================] - 0s 912us/step - loss: 0.2779 - accuracy: 0.8847\n",
      "Epoch 212/1500\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.2643 - accuracy: 0.8974\n",
      "Epoch 213/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.2703 - accuracy: 0.8930\n",
      "Epoch 214/1500\n",
      "32/32 [==============================] - 0s 893us/step - loss: 0.2758 - accuracy: 0.8793\n",
      "Epoch 215/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2690 - accuracy: 0.8867\n",
      "Epoch 216/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.2589 - accuracy: 0.8901\n",
      "Epoch 217/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.2656 - accuracy: 0.8955\n",
      "Epoch 218/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.2656 - accuracy: 0.8970\n",
      "Epoch 219/1500\n",
      "32/32 [==============================] - 0s 932us/step - loss: 0.2539 - accuracy: 0.8965\n",
      "Epoch 220/1500\n",
      "32/32 [==============================] - 0s 942us/step - loss: 0.2779 - accuracy: 0.8857\n",
      "Epoch 221/1500\n",
      "32/32 [==============================] - 0s 888us/step - loss: 0.2610 - accuracy: 0.8930\n",
      "Epoch 222/1500\n",
      "32/32 [==============================] - 0s 883us/step - loss: 0.2624 - accuracy: 0.8911\n",
      "Epoch 223/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2437 - accuracy: 0.9033\n",
      "Epoch 224/1500\n",
      "32/32 [==============================] - 0s 897us/step - loss: 0.2528 - accuracy: 0.9009\n",
      "Epoch 225/1500\n",
      "32/32 [==============================] - 0s 924us/step - loss: 0.2589 - accuracy: 0.8955\n",
      "Epoch 226/1500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.2497 - accuracy: 0.8979\n",
      "Epoch 227/1500\n",
      "32/32 [==============================] - 0s 865us/step - loss: 0.2637 - accuracy: 0.8847\n",
      "Epoch 228/1500\n",
      "32/32 [==============================] - 0s 875us/step - loss: 0.2582 - accuracy: 0.8935\n",
      "Epoch 229/1500\n",
      "32/32 [==============================] - 0s 799us/step - loss: 0.2477 - accuracy: 0.9019\n",
      "Epoch 230/1500\n",
      "32/32 [==============================] - 0s 905us/step - loss: 0.2593 - accuracy: 0.8901\n",
      "Epoch 231/1500\n",
      "32/32 [==============================] - 0s 893us/step - loss: 0.2710 - accuracy: 0.8886\n",
      "Epoch 232/1500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.2622 - accuracy: 0.8940\n",
      "Epoch 233/1500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2678 - accuracy: 0.8930\n",
      "Epoch 234/1500\n",
      "32/32 [==============================] - 0s 906us/step - loss: 0.2622 - accuracy: 0.8876\n",
      "Epoch 235/1500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.2400 - accuracy: 0.9038\n",
      "Epoch 236/1500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.2626 - accuracy: 0.8970\n",
      "Epoch 237/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.2575 - accuracy: 0.8940\n",
      "Epoch 238/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.2533 - accuracy: 0.8950\n",
      "Epoch 239/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.2528 - accuracy: 0.9004\n",
      "Epoch 240/1500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.2492 - accuracy: 0.9038\n",
      "Epoch 241/1500\n",
      "32/32 [==============================] - 0s 822us/step - loss: 0.2529 - accuracy: 0.8984\n",
      "Epoch 242/1500\n",
      "32/32 [==============================] - 0s 898us/step - loss: 0.2443 - accuracy: 0.8984\n",
      "Epoch 243/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2488 - accuracy: 0.8950\n",
      "Epoch 244/1500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2538 - accuracy: 0.8945\n",
      "Epoch 245/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2601 - accuracy: 0.8955\n",
      "Epoch 246/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2368 - accuracy: 0.9014\n",
      "Epoch 247/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.9028\n",
      "Epoch 248/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.2549 - accuracy: 0.8965\n",
      "Epoch 249/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.2415 - accuracy: 0.8974\n",
      "Epoch 250/1500\n",
      "32/32 [==============================] - 0s 917us/step - loss: 0.2504 - accuracy: 0.8994\n",
      "Epoch 251/1500\n",
      "32/32 [==============================] - 0s 887us/step - loss: 0.2346 - accuracy: 0.9038\n",
      "Epoch 252/1500\n",
      "32/32 [==============================] - 0s 875us/step - loss: 0.2381 - accuracy: 0.9082\n",
      "Epoch 253/1500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.2320 - accuracy: 0.9038\n",
      "Epoch 254/1500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.2299 - accuracy: 0.9048\n",
      "Epoch 255/1500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.2454 - accuracy: 0.9004\n",
      "Epoch 256/1500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.2433 - accuracy: 0.9092\n",
      "Epoch 257/1500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.2276 - accuracy: 0.9078\n",
      "Epoch 258/1500\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2358 - accuracy: 0.9117\n",
      "Epoch 259/1500\n",
      "32/32 [==============================] - 0s 821us/step - loss: 0.2446 - accuracy: 0.9038\n",
      "Epoch 260/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2395 - accuracy: 0.9014\n",
      "Epoch 261/1500\n",
      "32/32 [==============================] - 0s 826us/step - loss: 0.2485 - accuracy: 0.8955\n",
      "Epoch 262/1500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.2515 - accuracy: 0.9014\n",
      "Epoch 263/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.2646 - accuracy: 0.8950\n",
      "Epoch 264/1500\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.2375 - accuracy: 0.9014\n",
      "Epoch 265/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.2406 - accuracy: 0.9024\n",
      "Epoch 266/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2405 - accuracy: 0.9078\n",
      "Epoch 267/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.2467 - accuracy: 0.9082\n",
      "Epoch 268/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.2436 - accuracy: 0.8984\n",
      "Epoch 269/1500\n",
      "32/32 [==============================] - 0s 811us/step - loss: 0.2292 - accuracy: 0.9014\n",
      "Epoch 270/1500\n",
      "32/32 [==============================] - 0s 786us/step - loss: 0.2335 - accuracy: 0.9048\n",
      "Epoch 271/1500\n",
      "32/32 [==============================] - 0s 811us/step - loss: 0.2529 - accuracy: 0.8965\n",
      "Epoch 272/1500\n",
      "32/32 [==============================] - 0s 814us/step - loss: 0.2377 - accuracy: 0.9048\n",
      "Epoch 273/1500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.2348 - accuracy: 0.9043\n",
      "Epoch 274/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.2499 - accuracy: 0.9004\n",
      "Epoch 275/1500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.2269 - accuracy: 0.9092\n",
      "Epoch 276/1500\n",
      "32/32 [==============================] - 0s 913us/step - loss: 0.2184 - accuracy: 0.9210\n",
      "Epoch 277/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.2421 - accuracy: 0.8950\n",
      "Epoch 278/1500\n",
      "32/32 [==============================] - 0s 899us/step - loss: 0.2343 - accuracy: 0.9078\n",
      "Epoch 279/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2295 - accuracy: 0.9102\n",
      "Epoch 280/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.2260 - accuracy: 0.9068\n",
      "Epoch 281/1500\n",
      "32/32 [==============================] - 0s 869us/step - loss: 0.2558 - accuracy: 0.8930\n",
      "Epoch 282/1500\n",
      "32/32 [==============================] - 0s 878us/step - loss: 0.2248 - accuracy: 0.9112\n",
      "Epoch 283/1500\n",
      "32/32 [==============================] - 0s 877us/step - loss: 0.2448 - accuracy: 0.8940\n",
      "Epoch 284/1500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.2301 - accuracy: 0.9151\n",
      "Epoch 285/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.2252 - accuracy: 0.9112\n",
      "Epoch 286/1500\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.2313 - accuracy: 0.9092\n",
      "Epoch 287/1500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2349 - accuracy: 0.9097\n",
      "Epoch 288/1500\n",
      "32/32 [==============================] - 0s 823us/step - loss: 0.2202 - accuracy: 0.9112\n",
      "Epoch 289/1500\n",
      "32/32 [==============================] - 0s 874us/step - loss: 0.2116 - accuracy: 0.9102\n",
      "Epoch 290/1500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.2305 - accuracy: 0.9078\n",
      "Epoch 291/1500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.2264 - accuracy: 0.9107\n",
      "Epoch 292/1500\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.2289 - accuracy: 0.9068\n",
      "Epoch 293/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.2086 - accuracy: 0.9269\n",
      "Epoch 294/1500\n",
      "32/32 [==============================] - 0s 872us/step - loss: 0.2438 - accuracy: 0.9019\n",
      "Epoch 295/1500\n",
      "32/32 [==============================] - 0s 876us/step - loss: 0.2297 - accuracy: 0.9097\n",
      "Epoch 296/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.2230 - accuracy: 0.9156\n",
      "Epoch 297/1500\n",
      "32/32 [==============================] - 0s 893us/step - loss: 0.2170 - accuracy: 0.9161\n",
      "Epoch 298/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.2180 - accuracy: 0.9166\n",
      "Epoch 299/1500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.2405 - accuracy: 0.9033\n",
      "Epoch 300/1500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.2218 - accuracy: 0.9058\n",
      "Epoch 301/1500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.2062 - accuracy: 0.9092\n",
      "Epoch 302/1500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.2297 - accuracy: 0.9112\n",
      "Epoch 303/1500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.2156 - accuracy: 0.9230\n",
      "Epoch 304/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.2334 - accuracy: 0.9078\n",
      "Epoch 305/1500\n",
      "32/32 [==============================] - 0s 827us/step - loss: 0.2345 - accuracy: 0.8965\n",
      "Epoch 306/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2173 - accuracy: 0.9181\n",
      "Epoch 307/1500\n",
      "32/32 [==============================] - 0s 822us/step - loss: 0.2247 - accuracy: 0.9127\n",
      "Epoch 308/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2146 - accuracy: 0.9122\n",
      "Epoch 309/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.2249 - accuracy: 0.9063\n",
      "Epoch 310/1500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.2118 - accuracy: 0.9190\n",
      "Epoch 311/1500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.2286 - accuracy: 0.9019\n",
      "Epoch 312/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.2334 - accuracy: 0.9033\n",
      "Epoch 313/1500\n",
      "32/32 [==============================] - 0s 827us/step - loss: 0.2119 - accuracy: 0.9151\n",
      "Epoch 314/1500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.2058 - accuracy: 0.9156\n",
      "Epoch 315/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.2221 - accuracy: 0.9082\n",
      "Epoch 316/1500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.2192 - accuracy: 0.9082\n",
      "Epoch 317/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2078 - accuracy: 0.9171\n",
      "Epoch 318/1500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.2179 - accuracy: 0.9146\n",
      "Epoch 319/1500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.2174 - accuracy: 0.9136\n",
      "Epoch 320/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.2141 - accuracy: 0.9141\n",
      "Epoch 321/1500\n",
      "32/32 [==============================] - 0s 884us/step - loss: 0.2182 - accuracy: 0.9185\n",
      "Epoch 322/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.2299 - accuracy: 0.9107\n",
      "Epoch 323/1500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.2165 - accuracy: 0.9156\n",
      "Epoch 324/1500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.2166 - accuracy: 0.9132\n",
      "Epoch 325/1500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.2345 - accuracy: 0.9058\n",
      "Epoch 326/1500\n",
      "32/32 [==============================] - 0s 818us/step - loss: 0.2252 - accuracy: 0.9078\n",
      "Epoch 327/1500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.2058 - accuracy: 0.9176\n",
      "Epoch 328/1500\n",
      "32/32 [==============================] - 0s 872us/step - loss: 0.2111 - accuracy: 0.9161\n",
      "Epoch 329/1500\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.2171 - accuracy: 0.9087\n",
      "Epoch 330/1500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.2199 - accuracy: 0.9102\n",
      "Epoch 331/1500\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.2012 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 301.\n",
      "32/32 [==============================] - 0s 897us/step - loss: 0.2178 - accuracy: 0.9136\n",
      "Epoch 331: early stopping\n",
      "10/10 [==============================] - 0s 756us/step - loss: 0.4890 - accuracy: 0.8033\n",
      "10/10 [==============================] - 0s 566us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.77 (23/30)\n",
      "Before appending - Cat IDs: 409, Predictions: 409, Actuals: 409, Gender: 409\n",
      "After appending - Cat IDs: 714, Predictions: 714, Actuals: 714, Gender: 714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Results - Loss: 0.4889756441116333, Accuracy: 0.8032786846160889, Precision: 0.7668690180786956, Recall: 0.772167154410145, F1 Score: 0.7687456343086808\n",
      "Confusion Matrix:\n",
      " [[181   3  30]\n",
      " [  4  32   0]\n",
      " [ 23   0  32]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "        ..\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 84, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "065A     9\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "021A     5\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "113A     3\n",
      "064A     3\n",
      "087A     2\n",
      "038A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "043A     1\n",
      "041A     1\n",
      "066A     1\n",
      "004A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    314\n",
      "X    271\n",
      "F    164\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    88\n",
      "X    77\n",
      "M    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 097B, 028...\n",
      "kitten    [044A, 046A, 047A, 050A, 049A, 045A, 048A, 115...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 062A, 101A, 065A, 063A, 038A, 007A, 087...\n",
      "kitten           [014B, 111A, 040A, 042A, 109A, 043A, 041A]\n",
      "senior                             [113A, 108A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 9, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 7, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'037A'}\n",
      "Moved to Test Set:\n",
      "{'037A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A'\n",
      " '029A' '032A' '033A' '034A' '036A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A' '037A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "782\n",
      "Length of y_train_val:\n",
      "782\n",
      "Length of groups_train_val:\n",
      "782\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "kitten     55\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     501\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     87\n",
      "kitten    55\n",
      "senior    13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1002, 2: 825, 1: 580})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.2620 - accuracy: 0.4724\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.0146 - accuracy: 0.5550\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.9256 - accuracy: 0.6132\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.8901 - accuracy: 0.6327\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.8495 - accuracy: 0.6510\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 0.8082 - accuracy: 0.6602\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.7919 - accuracy: 0.6622\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.7738 - accuracy: 0.6668\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.7429 - accuracy: 0.7025\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.7444 - accuracy: 0.6847\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.7093 - accuracy: 0.7029\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.7100 - accuracy: 0.7017\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.6777 - accuracy: 0.7092\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.6824 - accuracy: 0.7113\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.6517 - accuracy: 0.7250\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.6648 - accuracy: 0.7158\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.6411 - accuracy: 0.7345\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.6330 - accuracy: 0.7387\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.6161 - accuracy: 0.7374\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.6113 - accuracy: 0.7420\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.6199 - accuracy: 0.7462\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.6174 - accuracy: 0.7354\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.5740 - accuracy: 0.7524\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.5880 - accuracy: 0.7491\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.5888 - accuracy: 0.7432\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.5823 - accuracy: 0.7570\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.5607 - accuracy: 0.7640\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.5605 - accuracy: 0.7549\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.5617 - accuracy: 0.7673\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.5579 - accuracy: 0.7673\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.5356 - accuracy: 0.7640\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.5573 - accuracy: 0.7628\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.5206 - accuracy: 0.7786\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.5343 - accuracy: 0.7665\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.5542 - accuracy: 0.7673\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.5104 - accuracy: 0.7889\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.5102 - accuracy: 0.7910\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.5207 - accuracy: 0.7682\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.5030 - accuracy: 0.7835\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.5019 - accuracy: 0.7877\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.5049 - accuracy: 0.7877\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.5045 - accuracy: 0.7819\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4907 - accuracy: 0.7811\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.4976 - accuracy: 0.7873\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.4870 - accuracy: 0.7956\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4878 - accuracy: 0.7877\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4681 - accuracy: 0.7989\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4695 - accuracy: 0.8022\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4803 - accuracy: 0.7977\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 980us/step - loss: 0.4844 - accuracy: 0.7881\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 965us/step - loss: 0.4584 - accuracy: 0.8064\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.4638 - accuracy: 0.8039\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 997us/step - loss: 0.4554 - accuracy: 0.7973\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 976us/step - loss: 0.4647 - accuracy: 0.8056\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.4608 - accuracy: 0.7977\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.4500 - accuracy: 0.8010\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.4601 - accuracy: 0.8072\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.4506 - accuracy: 0.8031\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4512 - accuracy: 0.8031\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.4419 - accuracy: 0.8022\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.4371 - accuracy: 0.8222\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4381 - accuracy: 0.8064\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4331 - accuracy: 0.8164\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.4406 - accuracy: 0.8180\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.4230 - accuracy: 0.8238\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 940us/step - loss: 0.4477 - accuracy: 0.8130\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.4306 - accuracy: 0.8126\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.4239 - accuracy: 0.8168\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 958us/step - loss: 0.4166 - accuracy: 0.8226\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 977us/step - loss: 0.4266 - accuracy: 0.8106\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.4165 - accuracy: 0.8122\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.4070 - accuracy: 0.8297\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4089 - accuracy: 0.8226\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.4158 - accuracy: 0.8234\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.4019 - accuracy: 0.8317\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.4103 - accuracy: 0.8326\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.4063 - accuracy: 0.8309\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4053 - accuracy: 0.8309\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4091 - accuracy: 0.8276\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4068 - accuracy: 0.8272\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3958 - accuracy: 0.8334\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.3886 - accuracy: 0.8413\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.3932 - accuracy: 0.8367\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.4052 - accuracy: 0.8326\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.4033 - accuracy: 0.8288\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.3778 - accuracy: 0.8421\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3822 - accuracy: 0.8334\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.3883 - accuracy: 0.8380\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.3818 - accuracy: 0.8392\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.3815 - accuracy: 0.8388\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3833 - accuracy: 0.8430\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3784 - accuracy: 0.8450\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3967 - accuracy: 0.8338\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.3727 - accuracy: 0.8442\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.3852 - accuracy: 0.8342\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.3725 - accuracy: 0.8459\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.3766 - accuracy: 0.8425\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.3769 - accuracy: 0.8409\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8471\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 928us/step - loss: 0.3530 - accuracy: 0.8538\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.3559 - accuracy: 0.8521\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.3631 - accuracy: 0.8538\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.3740 - accuracy: 0.8371\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.3618 - accuracy: 0.8525\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.3649 - accuracy: 0.8484\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.3490 - accuracy: 0.8496\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3568 - accuracy: 0.8604\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.3644 - accuracy: 0.8430\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.3540 - accuracy: 0.8513\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3510 - accuracy: 0.8467\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 949us/step - loss: 0.3503 - accuracy: 0.8575\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.3476 - accuracy: 0.8550\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.3308 - accuracy: 0.8716\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.3365 - accuracy: 0.8650\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3406 - accuracy: 0.8637\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3387 - accuracy: 0.8604\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.3540 - accuracy: 0.8587\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 978us/step - loss: 0.3322 - accuracy: 0.8650\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3487 - accuracy: 0.8521\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8571\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8617\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3409 - accuracy: 0.8637\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.3316 - accuracy: 0.8650\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.3365 - accuracy: 0.8550\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3462 - accuracy: 0.8542\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.8654\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.3276 - accuracy: 0.8691\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.3268 - accuracy: 0.8625\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.3358 - accuracy: 0.8650\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3326 - accuracy: 0.8600\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 929us/step - loss: 0.3189 - accuracy: 0.8725\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.3249 - accuracy: 0.8662\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 987us/step - loss: 0.3155 - accuracy: 0.8683\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 0.3355 - accuracy: 0.8612\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3036 - accuracy: 0.8799\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.3211 - accuracy: 0.8683\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.3239 - accuracy: 0.8695\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3189 - accuracy: 0.8695\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.3179 - accuracy: 0.8671\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.3144 - accuracy: 0.8633\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.3028 - accuracy: 0.8833\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.3231 - accuracy: 0.8720\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.3239 - accuracy: 0.8608\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3129 - accuracy: 0.8695\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2919 - accuracy: 0.8833\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3200 - accuracy: 0.8695\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.3161 - accuracy: 0.8671\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.3053 - accuracy: 0.8733\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.3007 - accuracy: 0.8795\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.3103 - accuracy: 0.8762\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3058 - accuracy: 0.8787\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2945 - accuracy: 0.8816\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.3077 - accuracy: 0.8803\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3086 - accuracy: 0.8779\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.2943 - accuracy: 0.8754\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2928 - accuracy: 0.8816\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3161 - accuracy: 0.8737\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.3070 - accuracy: 0.8766\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.2932 - accuracy: 0.8766\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2992 - accuracy: 0.8779\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2903 - accuracy: 0.8783\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2808 - accuracy: 0.8874\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.3061 - accuracy: 0.8787\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2961 - accuracy: 0.8733\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2931 - accuracy: 0.8779\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2966 - accuracy: 0.8866\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.2883 - accuracy: 0.8837\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2807 - accuracy: 0.8882\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2920 - accuracy: 0.8754\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2981 - accuracy: 0.8745\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.8808\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2820 - accuracy: 0.8895\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2865 - accuracy: 0.8853\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2849 - accuracy: 0.8824\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.2842 - accuracy: 0.8828\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 968us/step - loss: 0.2734 - accuracy: 0.8837\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 992us/step - loss: 0.2711 - accuracy: 0.8874\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 974us/step - loss: 0.2957 - accuracy: 0.8774\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 966us/step - loss: 0.2683 - accuracy: 0.8912\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2732 - accuracy: 0.8916\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 944us/step - loss: 0.2826 - accuracy: 0.8862\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 992us/step - loss: 0.2864 - accuracy: 0.8837\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 967us/step - loss: 0.2778 - accuracy: 0.8820\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 962us/step - loss: 0.2689 - accuracy: 0.8945\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 925us/step - loss: 0.2644 - accuracy: 0.8966\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 928us/step - loss: 0.2801 - accuracy: 0.8824\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2669 - accuracy: 0.8945\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8891\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.2770 - accuracy: 0.8841\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 945us/step - loss: 0.2918 - accuracy: 0.8766\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.2560 - accuracy: 0.8986\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2614 - accuracy: 0.8970\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.2805 - accuracy: 0.8912\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.2801 - accuracy: 0.8841\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.2683 - accuracy: 0.8903\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.2625 - accuracy: 0.8953\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.2519 - accuracy: 0.8982\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2563 - accuracy: 0.9044\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.2534 - accuracy: 0.8949\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.2520 - accuracy: 0.8928\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 991us/step - loss: 0.2589 - accuracy: 0.8995\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2592 - accuracy: 0.8903\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2676 - accuracy: 0.8882\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2752 - accuracy: 0.8891\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2547 - accuracy: 0.8949\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.9003\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 994us/step - loss: 0.2589 - accuracy: 0.8995\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2488 - accuracy: 0.8999\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2529 - accuracy: 0.8999\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2518 - accuracy: 0.9036\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 925us/step - loss: 0.2520 - accuracy: 0.8995\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2561 - accuracy: 0.8986\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.2578 - accuracy: 0.8945\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.2513 - accuracy: 0.8949\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2493 - accuracy: 0.8957\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2482 - accuracy: 0.9053\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.2527 - accuracy: 0.9024\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.2629 - accuracy: 0.8982\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2558 - accuracy: 0.8912\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2487 - accuracy: 0.9061\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2497 - accuracy: 0.8957\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2409 - accuracy: 0.9069\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2379 - accuracy: 0.9032\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2541 - accuracy: 0.8936\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2433 - accuracy: 0.9020\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.9028\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2525 - accuracy: 0.9036\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2440 - accuracy: 0.8995\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2319 - accuracy: 0.9107\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.9065\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.2454 - accuracy: 0.9044\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2471 - accuracy: 0.9074\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2389 - accuracy: 0.9036\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2443 - accuracy: 0.9011\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2390 - accuracy: 0.9036\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.2444 - accuracy: 0.9036\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2429 - accuracy: 0.9074\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2152 - accuracy: 0.9182\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2411 - accuracy: 0.9115\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2400 - accuracy: 0.9049\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2441 - accuracy: 0.9053\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2209 - accuracy: 0.9157\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2555 - accuracy: 0.8936\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.2274 - accuracy: 0.9057\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2297 - accuracy: 0.9082\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2402 - accuracy: 0.8949\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2267 - accuracy: 0.9074\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.2288 - accuracy: 0.9111\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2385 - accuracy: 0.9053\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2380 - accuracy: 0.9119\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2370 - accuracy: 0.9032\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.2298 - accuracy: 0.9086\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.2381 - accuracy: 0.9015\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2345 - accuracy: 0.9082\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2141 - accuracy: 0.9173\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2307 - accuracy: 0.9003\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2304 - accuracy: 0.9082\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2077 - accuracy: 0.9173\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2274 - accuracy: 0.9057\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2248 - accuracy: 0.9103\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2226 - accuracy: 0.9144\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2332 - accuracy: 0.9036\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 955us/step - loss: 0.2331 - accuracy: 0.9078\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2190 - accuracy: 0.9090\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2104 - accuracy: 0.9161\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2417 - accuracy: 0.9090\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2218 - accuracy: 0.9103\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2233 - accuracy: 0.9098\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2178 - accuracy: 0.9177\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2226 - accuracy: 0.9044\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2172 - accuracy: 0.9123\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2127 - accuracy: 0.9169\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2160 - accuracy: 0.9128\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2107 - accuracy: 0.9136\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2162 - accuracy: 0.9169\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2126 - accuracy: 0.9206\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2286 - accuracy: 0.9053\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2195 - accuracy: 0.9107\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2195 - accuracy: 0.9094\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2205 - accuracy: 0.9152\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2138 - accuracy: 0.9107\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2282 - accuracy: 0.9082\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2048 - accuracy: 0.9186\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2134 - accuracy: 0.9169\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2092 - accuracy: 0.9132\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2059 - accuracy: 0.9190\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2101 - accuracy: 0.9190\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2138 - accuracy: 0.9165\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2249 - accuracy: 0.9111\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2276 - accuracy: 0.9132\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2324 - accuracy: 0.9057\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2060 - accuracy: 0.9194\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2029 - accuracy: 0.9198\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.2112 - accuracy: 0.9132\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2143 - accuracy: 0.9173\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1967 - accuracy: 0.9219\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1985 - accuracy: 0.9190\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2131 - accuracy: 0.9115\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2091 - accuracy: 0.9161\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2150 - accuracy: 0.9144\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2028 - accuracy: 0.9186\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2088 - accuracy: 0.9194\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.2057 - accuracy: 0.9211\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.2078 - accuracy: 0.9169\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2137 - accuracy: 0.9169\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.2046 - accuracy: 0.9161\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2093 - accuracy: 0.9165\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9194\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.9190\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2076 - accuracy: 0.9198\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 993us/step - loss: 0.2126 - accuracy: 0.9165\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.2033 - accuracy: 0.9256\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.1929 - accuracy: 0.9227\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9219\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9236\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 980us/step - loss: 0.2104 - accuracy: 0.9157\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 995us/step - loss: 0.1968 - accuracy: 0.9194\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2099 - accuracy: 0.9136\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.1844 - accuracy: 0.9277\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 942us/step - loss: 0.2050 - accuracy: 0.9190\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9169\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 917us/step - loss: 0.1994 - accuracy: 0.9136\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1938 - accuracy: 0.9244\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1990 - accuracy: 0.9215\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.1871 - accuracy: 0.9215\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.1995 - accuracy: 0.9198\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9190\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9219\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9219\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2052 - accuracy: 0.9194\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2049 - accuracy: 0.9182\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1957 - accuracy: 0.9198\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9244\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.1902 - accuracy: 0.9236\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 905us/step - loss: 0.1967 - accuracy: 0.9211\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.1828 - accuracy: 0.9344\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.1876 - accuracy: 0.9244\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.1984 - accuracy: 0.9281\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2073 - accuracy: 0.9144\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1970 - accuracy: 0.9202\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.1767 - accuracy: 0.9260\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.1938 - accuracy: 0.9265\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1942 - accuracy: 0.9248\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2161 - accuracy: 0.9148\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.2229 - accuracy: 0.9148\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1876 - accuracy: 0.9290\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1765 - accuracy: 0.9344\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1762 - accuracy: 0.9285\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.1976 - accuracy: 0.9206\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1861 - accuracy: 0.9269\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1973 - accuracy: 0.9211\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1881 - accuracy: 0.9227\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.1931 - accuracy: 0.9231\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1986 - accuracy: 0.9169\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.1907 - accuracy: 0.9290\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1830 - accuracy: 0.9314\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1805 - accuracy: 0.9256\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2035 - accuracy: 0.9152\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1918 - accuracy: 0.9260\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.1819 - accuracy: 0.9339\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.1797 - accuracy: 0.9277\n",
      "Epoch 362/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1841 - accuracy: 0.9290\n",
      "Epoch 363/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1970 - accuracy: 0.9202\n",
      "Epoch 364/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1854 - accuracy: 0.9290\n",
      "Epoch 365/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1928 - accuracy: 0.9182\n",
      "Epoch 366/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1898 - accuracy: 0.9186\n",
      "Epoch 367/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.1719 - accuracy: 0.9319\n",
      "Epoch 368/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1940 - accuracy: 0.9194\n",
      "Epoch 369/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.1669 - accuracy: 0.9377\n",
      "Epoch 370/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1857 - accuracy: 0.9294\n",
      "Epoch 371/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1930 - accuracy: 0.9294\n",
      "Epoch 372/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.1947 - accuracy: 0.9211\n",
      "Epoch 373/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1746 - accuracy: 0.9344\n",
      "Epoch 374/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1868 - accuracy: 0.9252\n",
      "Epoch 375/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1843 - accuracy: 0.9273\n",
      "Epoch 376/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.1818 - accuracy: 0.9269\n",
      "Epoch 377/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.1853 - accuracy: 0.9290\n",
      "Epoch 378/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.1693 - accuracy: 0.9331\n",
      "Epoch 379/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9294\n",
      "Epoch 380/1500\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.1809 - accuracy: 0.9236\n",
      "Epoch 381/1500\n",
      "38/38 [==============================] - 0s 997us/step - loss: 0.1922 - accuracy: 0.9223\n",
      "Epoch 382/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1693 - accuracy: 0.9314\n",
      "Epoch 383/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1808 - accuracy: 0.9298\n",
      "Epoch 384/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1835 - accuracy: 0.9277\n",
      "Epoch 385/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.1830 - accuracy: 0.9277\n",
      "Epoch 386/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1741 - accuracy: 0.9398\n",
      "Epoch 387/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1707 - accuracy: 0.9302\n",
      "Epoch 388/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.1743 - accuracy: 0.9310\n",
      "Epoch 389/1500\n",
      "38/38 [==============================] - 0s 965us/step - loss: 0.1773 - accuracy: 0.9331\n",
      "Epoch 390/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.1776 - accuracy: 0.9260\n",
      "Epoch 391/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.1807 - accuracy: 0.9290\n",
      "Epoch 392/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.1830 - accuracy: 0.9240\n",
      "Epoch 393/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.1741 - accuracy: 0.9335\n",
      "Epoch 394/1500\n",
      "38/38 [==============================] - 0s 950us/step - loss: 0.1729 - accuracy: 0.9335\n",
      "Epoch 395/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.1815 - accuracy: 0.9339\n",
      "Epoch 396/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9298\n",
      "Epoch 397/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9319\n",
      "Epoch 398/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.1853 - accuracy: 0.9265\n",
      "Epoch 399/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1588 - accuracy: 0.9335\n",
      "Epoch 400/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.1790 - accuracy: 0.9269\n",
      "Epoch 401/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1741 - accuracy: 0.9281\n",
      "Epoch 402/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1746 - accuracy: 0.9319\n",
      "Epoch 403/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1642 - accuracy: 0.9369\n",
      "Epoch 404/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1803 - accuracy: 0.9323\n",
      "Epoch 405/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.1829 - accuracy: 0.9277\n",
      "Epoch 406/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1657 - accuracy: 0.9348\n",
      "Epoch 407/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.1670 - accuracy: 0.9348\n",
      "Epoch 408/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1598 - accuracy: 0.9423\n",
      "Epoch 409/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1700 - accuracy: 0.9327\n",
      "Epoch 410/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1642 - accuracy: 0.9348\n",
      "Epoch 411/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1890 - accuracy: 0.9273\n",
      "Epoch 412/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.1774 - accuracy: 0.9319\n",
      "Epoch 413/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1832 - accuracy: 0.9294\n",
      "Epoch 414/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1749 - accuracy: 0.9331\n",
      "Epoch 415/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1684 - accuracy: 0.9269\n",
      "Epoch 416/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1731 - accuracy: 0.9331\n",
      "Epoch 417/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1926 - accuracy: 0.9173\n",
      "Epoch 418/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.1816 - accuracy: 0.9244\n",
      "Epoch 419/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1699 - accuracy: 0.9306\n",
      "Epoch 420/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1578 - accuracy: 0.9406\n",
      "Epoch 421/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.1742 - accuracy: 0.9298\n",
      "Epoch 422/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1690 - accuracy: 0.9302\n",
      "Epoch 423/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1652 - accuracy: 0.9356\n",
      "Epoch 424/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1621 - accuracy: 0.9356\n",
      "Epoch 425/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.1581 - accuracy: 0.9406\n",
      "Epoch 426/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1593 - accuracy: 0.9360\n",
      "Epoch 427/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.1673 - accuracy: 0.9360\n",
      "Epoch 428/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1761 - accuracy: 0.9314\n",
      "Epoch 429/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.1233 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 399.\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.1728 - accuracy: 0.9294\n",
      "Epoch 429: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5299 - accuracy: 0.7935\n",
      "5/5 [==============================] - 0s 714us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.82 (23/28)\n",
      "Before appending - Cat IDs: 714, Predictions: 714, Actuals: 714, Gender: 714\n",
      "After appending - Cat IDs: 869, Predictions: 869, Actuals: 869, Gender: 869\n",
      "Final Test Results - Loss: 0.5299147963523865, Accuracy: 0.7935484051704407, Precision: 0.7109225874867445, Recall: 0.7678643195884577, F1 Score: 0.7245703014933785\n",
      "Confusion Matrix:\n",
      " [[69  5 13]\n",
      " [ 9 45  1]\n",
      " [ 4  0  9]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.7007187547686139\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.7290340512990952\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7354828268289566\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6949106387436289\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7213971677855934\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[0]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79e793-c694-4bc7-8cc6-05e124ddf71f",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c3c239d-6215-4589-a583-b37a18ca22cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 869, Predictions: 869, Actuals: 869, Gender: 869\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99978976-e4a9-4c04-a6b2-6b14a3111277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "189b812b-d37f-462f-a99f-6de8e4a7899f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.77 (85/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "907d957f-7340-4a76-b3fa-7cf166f43049",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7016fa20-094f-409a-9a2f-b6d6793aed03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, senior, kitten]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, kitten, adult, kitten, kitten, adult, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, adult...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, senior, adult, adult, kitten, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, senior, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, adult, kitten, adult, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, adult]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, adult, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, senior, kitten, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, s...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, adult, senior, adult, senior, senior,...        senior           senior                   True\n",
       "64    058A                           [senior, senior, kitten]        senior           senior                   True\n",
       "63    057A  [adult, adult, senior, senior, senior, senior,...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [adult, adult, senior, senior, adult, senior, ...        senior           senior                   True\n",
       "59    053A       [adult, senior, adult, senior, adult, adult]         adult            adult                   True\n",
       "58    052A                      [adult, senior, adult, adult]         adult            adult                   True\n",
       "1     001A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, kitten, adult, kitten, kitten, adult, ...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, adult...        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "97    102A                                    [senior, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, adult,...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A    [senior, adult, senior, senior, senior, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, senior, adult, adult, kitten, ...         adult            adult                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "50    044A           [kitten, senior, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "48    042A  [adult, kitten, kitten, adult, kitten, adult, ...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A        [adult, adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, se...         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "43    037A  [senior, senior, adult, adult, adult, adult, a...         adult            adult                   True\n",
       "42    036A  [adult, senior, senior, senior, adult, adult, ...         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "40    034A               [adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "109   117A  [senior, senior, adult, adult, senior, adult, ...        senior           senior                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "66    060A                            [kitten, kitten, adult]        kitten            adult                  False\n",
       "57    051B  [senior, adult, kitten, adult, adult, adult, a...         adult           senior                  False\n",
       "30    025C             [senior, senior, adult, senior, adult]        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "7     006A                            [senior, senior, adult]        senior            adult                  False\n",
       "101   106A  [adult, adult, senior, senior, adult, adult, a...         adult           senior                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "56    051A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False\n",
       "34    027A  [adult, senior, senior, senior, senior, senior...        senior            adult                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "12    011A                                    [adult, senior]         adult           senior                  False\n",
       "69    063A  [adult, senior, senior, kitten, senior, senior...        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "74    068A  [adult, senior, adult, senior, senior, adult, ...        senior            adult                  False\n",
       "18    016A  [senior, adult, adult, adult, adult, adult, ad...         adult           senior                  False\n",
       "89    094A  [senior, senior, adult, adult, senior, adult, ...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "19    018A                                   [senior, senior]        senior            adult                  False\n",
       "82    076A                                           [kitten]        kitten            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "994531fd-e6fb-4491-9eab-86e41ac1ed3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     59\n",
      "kitten    13\n",
      "senior    13\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "229af3e2-6a64-4f3d-a594-e04108fce87f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             59  80.821918\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22             13  59.090909\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3960450a-db52-4464-a965-00f1e600bda5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbf768-64c2-48ec-80e3-3a961b0b12a6",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe16003e-4015-4d28-ae37-ca0c94952758",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          555            436  78.558559\n",
      "1           kitten          136            110  80.882353\n",
      "2           senior          178             96  53.932584\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b5b8766-b4bd-4dd2-92e2-6513741b03ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABhNklEQVR4nO3dd3gU1f/28fcmBFKBEAgQeseA9BKa9CpNqX7FAtKkCIqI0hWxUaQXQRADUlR6ExRUCER6k9AJBEINIZBCSNnnjzyZX9YECCkkYe/XdXm5OzM785nNDnvvmTNnTGaz2YyIiIiIiJWwyegCRERERESeJQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFiVbBldgIg1CgsLY+3atfj4+HDx4kXu3r1Ljhw5yJ8/P9WrV+fVV1+ldOnSGV1mmgkMDKR9+/bG8wMHDhiP27Vrx7Vr1wCYN28eNWrUSPZ6IyIiaNWqFWFhYQCUK1eOZcuWpVHVklKP+3tnhI0bNzJ+/Hjj+bBhw3jttdcyrqCnEB0dzfbt29m+fTvnz58nKCgIs9lM7ty5KVu2LE2bNqVVq1Zky6avc5GnoSNG5Bk7dOgQn3zyCUFBQRbTo6KiCA0N5fz58/z888906dKFDz74QF9sj7F9+3Yj/AKcPn2af//9lwoVKmRgVZLZrF+/3uL5mjVrskQA9vf3Z+zYsZw8eTLRvBs3bnDjxg127drFsmXL+PbbbylQoEAGVCmSNembVeQZOnbsGIMHDyYyMhIAW1tbatWqRfHixYmIiGD//v1cvXoVs9nMqlWruHPnDl999VUGV515rVu3LtG0NWvWKACL4fLlyxw6dMhi2oULFzhy5AhVqlTJmKKS4cqVK/Ts2ZP79+8DYGNjQ/Xq1SlVqhSRkZEcO3aM8+fPA3D27Fnee+89li1bhp2dXUaWLZJlKACLPCORkZGMHj3aCL+FChViypQpFl0dYmJiWLhwIQsWLADg999/Z82aNbzyyisZUnNm5u/vz9GjRwHImTMn9+7dA2Dbtm28//77ODk5ZWR5kkkkbP1N+DlZs2ZNpg3A0dHRfPTRR0b4LVCgAFOmTKFcuXIWy/388898/fXXQFyo37RpEx07dnzW5YpkSQrAIs/Ib7/9RmBgIBDXmjNp0qRE/XxtbW3p168fFy9e5Pfffwdg8eLFdOzYkb///pthw4YB4OHhwbp16zCZTBav79KlCxcvXgRg2rRp1K9fH4gL3ytWrGDLli0EBASQPXt2ypQpw6uvvkrLli0t1nPgwAH69+8PQPPmzWnTpg1Tp07l+vXr5M+fn9mzZ1OoUCFu377N999/z969e7l58yYxMTHkzp0bT09PevbsSaVKldLhXfw/CVt/u3Tpgq+vL//++y/h4eFs3bqVTp06PfK1p06dwtvbm0OHDnH37l3y5MlDqVKl6N69O3Xr1k20fGhoKMuWLWPnzp1cuXIFOzs7PDw8aNGiBV26dMHR0dFYdvz48WzcuBGAPn360K9fP2Newve2YMGCbNiwwZgX3/fZzc2NBQsWMH78ePz8/MiZMycfffQRTZs25eHDhyxbtozt27cTEBBAZGQkTk5OlChRgk6dOvHyyy+nuPZevXpx7NgxAIYOHUqPHj0s1rN8+XKmTJkCQP369Zk2bdoj39//evjwIYsXL2bDhg3cuXOHwoUL0759e7p372508Rk1ahS//fYbAF27duWjjz6yWMeff/7Jhx9+CECpUqVYuXLlE7cbHR1t/C0g7m/zwQcfAHE/Lj/88ENcXFySfG1YWBiLFi1i+/bt3L59Gw8PDzp37ky3bt3w8vIiJiYm0d8Q4j5bixYt4tChQ4SFheHu7k6dOnXo2bMn+fPnT9b79fvvv3PmzBkg7t+KqVOnUrZs2UTLdenShfPnzxMSEkLJkiUpVaqUMS+5xzHAtWvXWLVqFbt27eL69etky5aN0qVL06ZNG9q3b5+oG1bCfvrr16/Hw8PD4j1O6vO/YcMGPv30UwB69OjBa6+9xuzZs9mzZw+RkZG88MIL9OnTh5o1aybrPRJJLQVgkWfk77//Nh7XrFkzyS+0eK+//roRgAMDAzl37hz16tXDzc2NoKAgAgMDOXr0qEULlp+fnxF+8+XLR506dYC4L/JBgwZx/PhxY9nIyEgOHTrEoUOH8PX1Zdy4cYnCNMSdWv3oo4+IiooC4vope3h4EBwcTN++fbl8+bLF8kFBQezatYs9e/YwY8YMateu/ZTvUvJER0ezadMm43m7du0oUKAA//77LxDXuveoALxx40YmTJhATEyMMS2+P+WePXsYNGgQb7/9tjHv+vXrvPvuuwQEBBjTHjx4wOnTpzl9+jR//PEH8+bNswjBqfHgwQMGDRpk/FgKCgqibNmyxMbGMmrUKHbu3Gmx/P379zl27BjHjh3jypUrFoH7aWpv3769EYC3bduWKABv377deNy2bdun2qehQ4eyb98+4/mFCxeYNm0aR48e5ZtvvsFkMtGhQwcjAP/xxx98+OGH2Nj830BFKdm+j48Pt2/fBqBq1aq89NJLVKpUiWPHjhEZGcmmTZvo3r17oteFhobSp08fzp49a0zz9/dn8uTJnDt37pHb27p1K+PGjbP4bF29epVffvmF7du3M3PmTDw9PZ9Yd8J99fLyeuy/FR9//PET1/eo4xhgz549jBw5ktDQUIvXHDlyhCNHjrB161amTp2Ks7PzE7eTXIGBgfTo0YPg4GBj2qFDhxg4cCBjxoyhXbt2abYtkUfRMGgiz0jCL9MnnXp94YUXLPry+fn5kS1bNosv/q1bt1q8ZvPmzcbjl19+GVtbWwCmTJlihF8HBwfatWvHyy+/TI4cOYC4QLhmzZok6/D398dkMtGuXTuaNWtG69atMZlM/PDDD0b4LVSoEN27d+fVV18lb968QFxXjhUrVjx2H1Nj165d3LlzB4gLNoULF6ZFixY4ODgAca1wfn5+iV534cIFJk6caASUMmXK0KVLF7y8vIxlZs2axenTp43no0aNMgKks7Mzbdu2pUOHDkYXi5MnTzJ37tw027ewsDACAwNp0KABr7zyCrVr16ZIkSLs3r3bCL9OTk506NCB7t27W4Sjn376CbPZnKLaW7RoYYT4kydPcuXKFWM9169fNz5DOXPm5KWXXnqqfdq3bx8vvPACXbp0oXz58sb0nTt3Gi35NWvWNFokg4KCOHjwoLFcZGQku3btAuLOkrRu3TpZ2014liD+2OnQoYMxbe3atUm+bsaMGRbHa926dXn11Vfx8PBg7dq1FgE33qVLlyx+WFWoUMFif0NCQvjkk0+MLlCPc+rUKeNx5cqVn7j8kzzqOA4MDOSTTz4xwm/+/Pl55ZVXaNKkidHqe+jQIcaMGZPqGhLasWMHwcHB1K1bl1deeQV3d3cAYmNj+eqrr4xRYUTSk1qARZ6RhK0dbm5uj102W7Zs5MyZ0xgp4u7duwC0b9+eJUuWAHGtRB9++CHZsmUjJiaGbdu2Ga+PH4Lq9u3bRkupnZ0dixYtokyZMgB07tyZd955h9jYWJYuXcqrr76aZC3vvfdeolayIkWK0LJlSy5fvsz06dPJkycPAK1bt6ZPnz5AXMtXekkYbOJbi5ycnGjWrJlxSnr16tWMGjXK4nXLly83WsEaNWrEV199ZXzRf/7556xduxYnJyf27dtHuXLlOHr0qNHP2MnJiaVLl1K4cGFju71798bW1pZ///2X2NhYixbL1GjcuDGTJk2ymJY9e3Y6duzI2bNn6d+/v9HC/+DBA5o3b05ERARhYWHcvXsXV1fXp67d0dGRZs2aGX1mt23bRq9evYC4U/LxwbpFixZkz579qfanefPmTJw4ERsbG2JjYxkzZozR2rt69Wo6duxoBLR58+YZ248/He7j40N4eDgAtWvXNn5oPc7t27fx8fEB4n74NW/e3KhlypQphIeHc+7cOY4dO2bRXSciIsLi7ELC7iBhYWH06dPH6J6Q0IoVK4xw26pVKyZMmIDJZCI2NpZhw4axa9curl69yo4dO54Y4BOOEBN/bMWLjo62+MGWUFJdMuIldRwvXrzYGEXF09OTOXPmGC29hw8fpn///sTExLBr1y4OHDjwVEMUPsmHH35o1BMcHEyPHj24ceMGkZGRrFmzhgEDBqTZtkSSohZgkWckOjraeJywle5REi4T/7hYsWJUrVoViGtR2rt3LxDXwhb/pVmlShWKFi0KwMGDB40WqSpVqhjhF+DFF1+kePHiQNyV8vGn3P+rZcuWiaZ17tyZiRMn4u3tTZ48eQgJCWH37t0WwSE5LV0pcfPmTWO/HRwcaNasmTEvYevetm3bjNAUL+F4tF27drXo2zhw4EDWrl3Ln3/+yRtvvJFo+ZdeeskIkBD3fi5dupS///6bRYsWpVn4haTfcy8vL0aPHs2SJUuoU6cOkZGRHDlyBG9vb4vPSvz7npLa//v+xYvvjgNP3/0BoGfPnsY2bGxsePPNN415p0+fNn6UtG3b1lhux44dxjGTsEtAck+Pb9y40fjsN2nSxGjddnR0NMIwkOjsh5+fn/Eeuri4WIRGJycni9oTStjFo1OnTkaXIhsbG4u+2f/8888Ta48/OwMk2dqcEkl9phK+r4MGDbLo5lC1alVatGhhPP/zzz/TpA6IawDo2rWr8dzV1ZUuXboYz+N/uImkJ7UAizwjuXLl4tatWwBGv8RHefjwISEhIcbz3LlzG487dOjA4cOHgbhuEA0aNLDo/pDwBgTXr183Hu/fv/+xLTgXL160uJgFwN7eHldX1ySXP3HiBOvWrePgwYOJ+gJD3OnM9LBhwwYjFNja2hoXRsUzmUyYzWbCwsL47bffLEbQuHnzpvG4YMGCFq9zdXVNtK+PWx6wOJ2fHMn54fOobUHc33P16tX4+vpy+vTpJMNR/PuektorV65M8eLF8ff359y5c1y8eBEHBwdOnDgBQPHixalYsWKy9iGh+B9k8eJ/eEFcwAsJCSFv3rwUKFAALy8v9uzZQ0hICP/88w/Vq1dn9+7dQFwgTW73i4SjP5w8edKiRTHh8bd9+3aGDRtmhL/4YxTiuvf89wKwEiVKJLm9hMda/FmQpMT303+c/Pnzc+HCBSCuf3pCNjY2vPXWW8bzc+fOGS3dj5LUcXz37l2Lfr9JfR7Kly/Pli1bACz6kT9Oco77IkWKJPrBmPB9/e8Y6SLpQQFY5BkpW7as8eWasH9jUo4dO2YRbhJ+OTVr1oxJkyYRFhbG33//zf379/nrr7+AxK1bCb+McuTI8dgLWeJb4RJ61FBiy5cvZ+rUqZjNZuzt7WnYsCFVqlShQIECfPLJJ4/dt9Qwm80WwSY0NNSi5e2/HjeE3NO2rKWkJe6/gTep9zgpSb3vR48eZfDgwYSHh2MymahSpQrVqlWjUqVKfP755xbB7b+epvYOHTowffp0IK4VOOHFfSlp/YW4/ba3t39kPfH91SHuB9yePXuM7UdERBAREQHEdV9I2Dr6KIcOHbL4UXbx4sVHBs8HDx6wefNmo0Uy4d/saX7EJVw2d+7cFvuUUHJubFOhQgUjAP/3Lno2NjYMHjzYeL5hw4YnBuCkPk/JqSPhe5HURbKQ+D1Kzmf84cOHiaYlvObhUdsSSUsKwCLPSIMGDYwvqsOHD3P8+HFefPHFJJf19vY2HhcoUMCi64K9vT0tWrRgzZo1REREMGfOHONUf7NmzYwLwSBuNIh4VatWZdasWRbbiYmJeeQXNZDkoPr37t1j5syZmM1m7OzsWLVqldFyHP+lnV4OHjz4VH2LT548yenTp43xU93d3Y2WLH9/f4uWyMuXL/Prr79SsmRJypUrR/ny5Y2LcyDuIqf/mjt3Li4uLpQqVYqqVatib29v0bL14MEDi+Xj+3I/SVLv+9SpU42/84QJE2jVqpUxL2H3mngpqR3iLqCcPXs20dHRbNu2zQhPNjY2tGnTJln1/9fZs2epVq2a8TxhOM2RIwc5c+Y0njds2JDcuXNz9+5d/vzzT2PcXkh+94ekbpDyOGvXrjUCcMJjJjAwkOjoaIuw+KhRINzd3Y3P5tSpUy36FT/pOPuv1q1bG315jx8/zsGDB6levXqSyyYnpCf1eXJ2dsbZ2dloBT59+nSiIcgSXgxapEgR43F8X25I/BlPeObqUeKH8Ev4YybhZyLh30AkvagPsMgz0rZtW+PiHbPZzEcffZToFqdRUVFMnTrVokXn7bffTnS6MGFfzV9//dV4nLD7A0D16tWN1pSDBw9afKGdOXOGBg0a0K1bN0aNGpXoiwySbom5dOmS0YJja2trMY5qwq4Y6dEFIuFV+927d+fAgQNJ/lerVi1judWrVxuPE4aIVatWWbRWrVq1imXLljFhwgS+//77RMvv3bvXuPMWxF2p//333zNt2jSGDh1qvCcJw9x/fxD88ccfydrPRw1JFy9hl5i9e/daXGAZ/76npHaIu+iqQYMGQNzfOv4zWqtWLYtQ/TQWLVpkhHSz2WxcyAlQsWJFi3BoZ2dnBO2wsDBj9IeiRYs+8gdjQqGhoRbv89KlS5P8jGzcuNF4n8+cOWN083jhhReMYBYaGmoxmsm9e/f44YcfktxuwoC/fPlyi8//xx9/TIsWLejfv79Fv9tHqVmzpsX6Ro4caQxRl9COHTuYPXv2E9f3qBbVhN1JZs+ebXFb8SNHjlj0A2/SpInxOOExn/AzfuPGDYvhFh/l/v37Fp+B0NBQi+M0/joHkfSkFmCRZ8Te3p6JEycycOBAoqOjuXXrFm+//TY1atSgVKlShIeH4+vra9Hn76WXXkpyPNuKFStSqlQpzp8/b3zRFitWLNHwagULFqRx48bs2LGDqKgoevXqRZMmTXBycuL333/n4cOHnD9/npIlS1qcon6chFfgP3jwgJ49e1K7dm38/PwsvqTT+iK4+/fvW4yBm/Dit/9q2bKl0TVi69atDB06FAcHB7p3787GjRuJjo5m3759vPbaa9SsWZOrV68ap90BunXrBsRdLJZw3NiePXvSsGFD7O3tLYJMmzZtjOCbsLV+z549fPnll5QrV46//vrriaeqHydv3rzGhYojR46kRYsWBAUFWYwvDf/3vqek9ngdOnRINN5wSrs/APj6+tKjRw9q1KjBiRMnjLAJWFwMlXD7P/30U4q2v3XrVuPHXOHChR/ZT7tAgQJUqVLF6E+/evVqKlasiKOjI+3ateOXX34B4m4oc+DAAfLly8eePXsS9cmN99prr7F582ZiYmLYvn07ly5domrVqly8eNH4LN69e5fhw4c/cR9MJhOffvopPXr0ICQkhKCgIN555x2qVq1K2bJliYyMTLLv/dPe/fDNN9/kjz/+IDIykhMnTtCtWzfq1KnDvXv3+Ouvv4yuKo0aNbIIpWXLlmX//v0ATJ48mZs3b2I2m1mxYoXRXeVJvvvuOw4fPkzRokXZu3ev8dl2cHCw+IEvkl7UAizyDFWvXp1Zs2YZw6DFxsayb98+li9fzrp16yy+XDt27MjXX3/9yNab/35JPOr08MiRIylZsiQQF462bNnCL7/8YpyOL126NCNGjEj2PhQsWNAifPr7+7Ny5UqOHTtGtmzZjCAdEhJicfo6tbZs2WKEu3z58j12fNQmTZoYp33jL4aDuH395JNPjBZHf39/fv75Z4vw27NnT4uLBT///HNjfNrw8HC2bNnCmjVrjFPHJUuWZOjQoRbbjl8e4lrov/jiC3x8fCyudH9a8SNTQFxL5C+//MLOnTuJiYmx6Nud8GKlp609Xp06dSxOQzs5OdGoUaMU1V22bFmqVavGuXPnWLFihUX4bd++PU2bNk30mlKlSllcbPc03S8S9hF/3I8ksBwZYfv27cb7MmjQIOOYAdi9ezdr1qzhxo0bFkE84ZmZsmXLMnz4cItW5ZUrVxrh12Qy8dFHH1ncre1xChYsyNKlS40bZ5jNZg4dOsSKFStYs2aNRfi1tbWlTZs2Tz0edenSpfnss8+M4Hz9+nXWrFnDH3/8YbTYV69enfHjx1u87vXXXzf2886dO0ybNo3p06dz7969ZP1QKV68OIUKFWL//v38+uuvFnfIHDVqVIrPNIg8DQVgkWesRo0arFu3juHDh+Pl5YWbmxvZsmUzbmnbuXNnli5dyujRo5PsuxevTZs2xnxbW9tHfvHkzp2bH3/8kQEDBlCuXDkcHR1xdHSkdOnSvPvuuyxcuNDilHpyfPbZZwwYMIDixYuTPXt2cuXKRf369Vm4cCGNGzcG4r6wd+zY8VTrfZyE/TqbNGny2AtlXFxcLG5pnHCoqw4dOrB48WKaN2+Om5sbtra25MyZk9q1azN58mQGDhxosS4PDw+8vb3p1asXJUqUIEeOHOTIkYNSpUrRt29flixZQq5cuYzlHRwcWLhwIa1btyZ37tzY29tTsWJFPv/88yTDZnJ16dKFr776Ck9PTxwdHXFwcKBixYpMmDDBYr0JT/8/be3xbG1tqVChgvG8WbNmyT5D8F/Zs2dn1qxZ9OnTBw8PD7Jnz07JkiX5+OOPH3uDhYTdHWrUqEGBAgWeuK2zZ89adCt6UgBu1qyZ8WMoIiLCuLmMs7MzixYtonv37ri7u5M9e3bKli3LF198weuvv268/r/vSefOnfn+++9p1qwZefPmxc7Ojvz58/PSSy+xYMECOnfu/MR9SKhgwYIsXryYL7/8kqZNm1KwYEGyZ89Ojhw5KFCgAPXq1WPo0KFs2LCBzz777JEjtjxO06ZNWb58OW+88QYlSpTA3t4eJycnKleuzKhRo5g9e3aii2fr16/Pt99+S6VKlYwRJlq0aMHSpUuTNUpInjx5WLx4MS+//DI5c+bE3t6e6tWrM3fuXIu+7SLpyWRO7rg8IiJiFS5fvkz37t2NvsHz589/5EVY6eHu3bt06dLF6Ns8fvz4VHXBeFrff/89OXPmJFeuXJQtW9biYsmNGzcaLaINGjTg22+/fWZ1ZWUbNmzg008/BeL6S3/33XcZXJFYO/UBFhERrl27xqpVq4iJiWHr1q1G+C1VqtQzCb8RERHMnTsXW1tb41a5EDc+85NactPa+vXrjREdXFxcaNq0KU5OTly/ft24KA/iWkJFJGvKtAH4xo0bdOvWjcmTJ1v0xwsICGDq1KkcPnwYW1tbmjVrxuDBgy1O0YSHhzNz5kx27NhBeHg4VatW5YMPPrD4FS8iIv/HZDJZDL8HcSMyJOeirbSQI0cOVq1aZTGkm8lk4oMPPkhx94uU6t+/P2PHjsVsNnP//n2L0UfiVapUKdnDsolI5pMpA/D169cZPHiwxV1qIO4q8P79++Pm5sb48eMJDg5mxowZBAYGMnPmTGO5UaNGceLECd577z2cnJxYsGAB/fv3Z9WqVYmudhYRkbgLC4sUKcLNmzext7enXLly9OrV67F3D0xLNjY2vPjii/j5+WFnZ0eJEiXo0aOHxfBbz0rr1q0pWLAgq1at4t9//+X27dtER0fj6OhIiRIlaNKkCV27diV79uzPvDYRSRuZqg9wbGwsmzZtYtq0aUDcVeTz5s0z/gFevHgx33//PRs3bjQu2vHx8WHIkCEsXLiQKlWqcOzYMXr16sX06dOpV68eAMHBwbRv3563336bd955JyN2TUREREQyiUw1CsTZs2f58ssvefnll43O8gnt3buXqlWrWlyx7uXlhZOTkzG+5t69e3FwcMDLy8tYxtXVlWrVqqVqDE4REREReT5kqgBcoEAB1qxZ88g+X/7+/hQtWtRimq2tLR4eHsatPv39/SlUqFCi204WKVIkyduBioiIiIh1yVR9gHPlypXkmJTxQkNDk7zTjaOjo3ELx+Qs87ROnz5tvPZx47KKiIiISMaJiorCZDI98ZbamSoAP0nCe6v/V/wdeZKzTErEd5WOHxpIRERERLKmLBWAnZ2dCQ8PTzQ9LCzMuHWis7Mzd+7cSXKZ/97NJrnKlSvH8ePHMZvNlC5dOkXrEBEREZH0de7cucfeKTRelgrAxYoVs7jPPUBMTAyBgYHG7VeLFSuGr68vsbGxFi2+AQEBqR4H2GQy4ejomKp1iIiIiEj6SE74hUx2EdyTeHl5cejQIeMOQQC+vr6Eh4cboz54eXkRFhbG3r17jWWCg4M5fPiwxcgQIiIiImKdslQA7ty5Mzly5GDgwIHs3LmTtWvXMmbMGOrWrUvlypWBuHuMV69enTFjxrB27Vp27tzJgAEDcHFxoXPnzhm8ByIiIiKS0bJUFwhXV1fmzZvH1KlTGT16NE5OTjRt2pShQ4daLDdp0iS+/fZbpk+fTmxsLJUrV+bLL7/UXeBEREREJHPdCS4zO378OAAvvvhiBlciIiIiIklJbl7LUl0gRERERERSSwFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrki2jCxARkdRbs2YNy5cvJzAwkAIFCtC1a1e6dOmCyWQCICAggKlTp3L48GFsbW1p1qwZgwcPxtnZ+bHr3bBhA97e3ly5coV8+fLRtm1bevbsSbZs+voQkaxL/4KJiGRxa9euZeLEiXTr1o2GDRty+PBhJk2axMOHD+nRowf379+nf//+uLm5MX78eIKDg5kxYwaBgYHMnDnzketdvnw5U6ZMoWnTpgwZMoTg4GDmz5/PmTNnmDRp0jPcQxGRtKUALCKSxa1fv54qVaowfPhwAGrVqsWlS5dYtWoVPXr04JdffiEkJIRly5aRO3duANzd3RkyZAhHjhyhSpUqidYZExPDwoULqV27Nl9//bUxvXz58nTv3h1fX1+8vLyexe6JiKQ59QEWEcniIiMjcXJyspiWK1cuQkJCANi7dy9Vq1Y1wi+Al5cXTk5O+Pj4JLnOO3fuEBISQoMGDSymly5dmty5cz/ydSIiWYECsIhIFvfaa6/h6+vL5s2bCQ0NZe/evWzatIk2bdoA4O/vT9GiRS1eY2tri4eHB5cuXUpynS4uLtja2nLt2jWL6ffu3eP+/ftcuXIlfXZGROQZUBcIEZEsrmXLlhw8eJCxY8ca0+rUqcOwYcMACA0NTdRCDODo6EhYWFiS67S3t6dFixasWrWKkiVL0rhxY+7cucOUKVOwtbXlwYMH6bMzIiLPgAKwiEgWN2zYMI4cOcJ7771HhQoVOHfuHN999x0jRoxg8uTJxMbGPvK1NjaPPhH4ySefYGdnx+eff86ECRPIkSMHb7/9NmFhYdjb26fHroiIPBMKwJIpHDhwgP79+z9yft++fenbty+HDx9m9uzZnD17FmdnZxo3bsy7776bZOtWQv7+/kyfPp1Dhw5ha2tLtWrVGDp0KIULF07rXRF5po4ePcqePXsYPXo0HTt2BKB69eoUKlSIoUOHsnv3bpydnQkPD0/02rCwMNzd3R+5bkdHR8aOHcuHH37ItWvXKFiwII6Ojqxdu5YiRYqk1y6JiKQ7BWDJFMqXL8/ixYsTTZ87dy7//vsvLVu25Pz58wwcOJAqVarw5ZdfcvPmTWbOnMnVq1f59ttvH7nu69ev884771CsWDEmTpzIgwcPmDNnDoMGDWLFihVqyZIsLb6PbuXKlS2mV6tWDYDz589TrFgxAgICLObHxMQQGBhI48aNH7nuXbt24eLiQpUqVShVqhQQd3HczZs3KV++fFruhojIM6UALJmCs7MzL774osW0v/76i3379vHVV19RrFgxZs+ejclkYvLkyTg6OgJxX+Jffvml0TqVlO+++w5nZ2fmzJljhF0PDw8++OAD/Pz8qFq1avrunEg6Kl68OACHDx+mRIkSxvSjR48CULhwYby8vPjxxx8JDg7G1dUVAF9fX8LDwx87lNmvv/5KSEiIxY/T5cuXY2Njk2h0CBGRrEQBWDKlBw8eMGnSJOrXr0+zZs2AuKGesmXLZtFimytXLgBCQkKSDMBms5kdO3bQo0cPi9d5enqydevWdN4LkfRXvnx5mjRpwrfffsu9e/eoWLEiFy5c4LvvvuOFF16gUaNGVK9enZUrVzJw4ED69OlDSEgIM2bMoG7duhYtx8ePH8fV1dXoGtS9e3cGDRrElClTaNiwIfv27WPx4sW89dZb6j4kIlmayWw2mzO6iKzg+PHjAIlaKSV9/PDDD8ydO5dffvnF6Gt47tw53nnnHdq3b88777xDUFAQI0eOxGQysWzZMmxtbROt5+rVq3To0IFPP/2UEydO8Ntvv/HgwQO8vLwYMWIE+fPnf9a7JpLmoqKi+P7779m8eTO3bt2iQIECNGrUiD59+hhnS86dO8fUqVM5evQoTk5ONGzYkKFDh1r0n69RowZt27Zl/PjxxrStW7eyaNEirl69SsGCBencuTPdu3d/1rsoIpIsyc1rCsDJpAD87ERFRdG2bVtq1arFhAkTLOb98ssvfPPNN8ZV7QULFmTBggUUKFAgyXWdOHGCt99+m7x581KhQgW6dOnCnTt3mD17NtmzZ+enn37CwcEh3fdJRERE0l9y85q6QEim88cffxAUFMQbb7xhMf2HH35g1qxZdOnShSZNmnD37l0WLlzIgAEDWLBgAW5ubonWFR0dDUCePHmYNGmSMeRTkSJF6NmzJ1u2bOHVV19N/50SERGRTEMBWDKdP/74g5IlS1K2bFljWnR0NAsXLqR169aMGDHCmF69enU6duyIt7c3Q4cOTbSu+NO/9erVsxjv9MUXX8TZ2ZnTp0+n346IiIhIpqRbIUumEh0dzd69e2nevLnF9Lt37/LgwYNEQz3lyZOHYsWKceHChSTXV7hwYUwmEw8fPkw0LyYmhhw5cqRd8SIiIpIlKABLpnLu3Lkkg66rqyu5cuXi8OHDFtPv3r3L5cuXKVSoUJLrc3R0pGrVquzcudMiBO/bt4+IiAgNgSYiImKFsmQXiDVr1rB8+XICAwMpUKAAXbt2pUuXLphMJgACAgKYOnUqhw8fxtbWlmbNmjF48GCcnZ0zuHJ5knPnzgFQsmRJi+m2trb07duXSZMm4eTkRLNmzbh79y4//PADNjY2vP7668ay/x3KadCgQfTr148hQ4bQo0cP7ty5w8yZM6lYsSIvvfTSs9s5ERERyRSyXABeu3YtEydOpFu3bjRs2JDDhw8zadIkHj58SI8ePbh//z79+/fHzc2N8ePHExwczIwZMwgMDGTmzJkZXb48QVBQEAAuLi6J5nXr1g0XFxeWLl3Khg0byJ07N1WqVGHSpEkWLcA9e/a0GMqpUqVKzJs3jzlz5vDRRx9hb29Po0aNGDp0aJJDp4mIiMjzLcsNg9arVy9sbGxYuHChMW3kyJGcOHGC9evXs3jxYr7//ns2btxI7ty5AfDx8WHIkCEsXLiQKlWqpGi7GgZNREREJHNLbl7Lcn2AIyMjLQZuh7i7gYWEhACwd+9eqlataoRfAC8vL5ycnPDx8XmWpYrIcyg2a7UZWBX9bUQkubJcF4jXXnuNCRMmsHnzZl566SWOHz/Opk2bePnllwHw9/dPNIKAra0tHh4eXLp0KSNKFpHniI3JxArfM9y8F57RpUgC7jkd6e5V9skLioiQBQNwy5YtOXjwIGPHjjWm1alTh2HDhgEQGhqaqIUY4kYDCAsLS9W2zWYz4eH60hOxViaTCQcHB27eCycwOHX/nkj6iIiIIIv17BORNGQ2m41BER4nywXgYcOGceTIEd577z0qVKjAuXPn+O677xgxYgSTJ082bpGblIQ3QkiJqKgo/Pz8UrUOEcm6HBwc8PT0zOgy5DEuXrxIRERERpchIhkoe/bsT1wmSwXgo0ePsmfPHkaPHk3Hjh2BuDuBFSpUiKFDh7J7926cnZ2TbKUNCwvD3d09Vdu3s7OjdOnSqVqHiGRdyWlVkIxVokQJtQCLWLH44VSfJEsF4GvXrgEkuklCtWrVADh//jzFihUjICDAYn5MTAyBgYE0btw4Vds3mUzGrXVFRCTzcXBwyOgSRCQDJbehIkuNAlG8eHGARHcDO3r0KBB321svLy8OHTpEcHCwMd/X15fw8HC8vLyeWa0iIiIikjllqRbg8uXL06RJE7799lvu3btHxYoVuXDhAt999x0vvPACjRo1onr16qxcuZKBAwfSp08fQkJCmDFjBnXr1k3UcmzNYs1mbHQ6N9PS30dERCT9ZLkbYURFRfH999+zefNmbt26RYECBWjUqBF9+vQxuiecO3eOqVOncvToUZycnGjYsCFDhw5NcnSI5Hoeb4ShoZwyJw3nlPnN2HZEo0BkMh6uTrzXokpGlyEiGSy5eS1LtQBD3IVo/fv3p3///o9cpnTp0syZM+cZVpU1aSgnERERsUZZqg+wiIiIiEhqKQCLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqZEvNi69cucKNGzcIDg4mW7Zs5M6dm5IlS5IzZ860qk9EREREJE09dQA+ceIEa9aswdfXl1u3biW5TNGiRWnQoAHt2rWjZMmSqS5SRERERCStJDsAHzlyhBkzZnDixAkAzGbzI5e9dOkSly9fZtmyZVSpUoWhQ4fi6emZ+mpFRERERFIpWQF44sSJrF+/ntjYWACKFy/Oiy++SJkyZciXLx9OTk4A3Lt3j1u3bnH27FlOnTrFhQsXOHz4MD179qRNmzaMGzcu/fZERERERCQZkhWA165di7u7O6+++irNmjWjWLFiyVp5UFAQv//+O6tXr2bTpk0KwCIiIiKS4ZIVgL/55hsaNmyIjc3TDRrh5uZGt27d6NatG76+vikqUEREREQkLSUrADdu3DjVG/Ly8kr1OkREREREUitVw6ABhIaGMnfuXHbv3k1QUBDu7u60atWKnj17YmdnlxY1ioiIiIikmVQH4M8++4ydO3cazwMCAli4cCEREREMGTIktasXEREREUlTqQrAUVFR/PXXXzRp0oQ33niD3LlzExoayrp16/jtt98UgEVEREQk00nWVW0TJ07k9u3biaZHRkYSGxtLyZIlqVChAoULF6Z8+fJUqFCByMjINC9WRERERCS1kj0M2pYtW+jatStvv/22catjZ2dnypQpw/fff8+yZctwcXEhPDycsLAwGjZsmK6Fi4iIiIikRLJagD/99FPc3Nzw9vamQ4cOLF68mAcPHhjzihcvTkREBDdv3iQ0NJRKlSoxfPjwdC1cRERERCQlktUC3KZNG1q0aMHq1atZtGgRc+bMYeXKlfTu3ZtXXnmFlStXcu3aNe7cuYO7uzvu7u7pXbeIiIiISIok+84W2bJlo2vXrqxdu5Z3332Xhw8f8s0339C5c2d+++03PDw8qFixosKviIiIiGRqT3drN8De3p5evXqxbt063njjDW7dusXYsWP53//+h4+PT3rUKCIiIiKSZpIdgIOCgti0aRPe3t789ttvmEwmBg8ezNq1a3nllVe4ePEi77//Pn379uXYsWPpWbOIiIiISIolqw/wgQMHGDZsGBEREcY0V1dX5s+fT/Hixfnkk0944403mDt3Ltu3b6d3797Ur1+fqVOnplvhIiIiIiIpkawW4BkzZpAtWzbq1atHy5YtadiwIdmyZWPOnDnGMoULF2bixIksXbqUOnXqsHv37nQrWkREREQkpZLVAuzv78+MGTOoUqWKMe3+/fv07t070bJly5Zl+vTpHDlyJK1qFBERERFJM8kKwAUKFGDChAnUrVsXZ2dnIiIiOHLkCAULFnzkaxKGZRERERGRzCJZAbhXr16MGzeOFStWYDKZMJvN2NnZWXSBEBERERHJCpIVgFu1akWJEiX466+/jJtdtGjRgsKFC6d3fSIiIiIiaSpZARigXLlylCtXLj1rERERERFJd8kaBWLYsGHs27cvxRs5efIko0ePTvHr/+v48eP069eP+vXr06JFC8aNG8edO3eM+QEBAbz//vs0atSIpk2b8uWXXxIaGppm2xcRERGRrCtZLcC7du1i165dFC5cmKZNm9KoUSNeeOEFbGySzs/R0dEcPXqUffv2sWvXLs6dOwfA559/nuqC/fz86N+/P7Vq1WLy5MncunWLWbNmERAQwKJFi7h//z79+/fHzc2N8ePHExwczIwZMwgMDGTmzJmp3r6IiIiIZG3JCsALFizg66+/5uzZsyxZsoQlS5ZgZ2dHiRIlyJcvH05OTphMJsLDw7l+/TqXL18mMjISALPZTPny5Rk2bFiaFDxjxgzKlSvHlClTjADu5OTElClTuHr1Ktu2bSMkJIRly5aRO3duANzd3RkyZAhHjhzR6BQiIiIiVi5ZAbhy5cosXbqUP/74A29vb/z8/Hj48CGnT5/mzJkzFsuazWYATCYTtWrVolOnTjRq1AiTyZTqYu/evcvBgwcZP368RetzkyZNaNKkCQB79+6latWqRvgF8PLywsnJCR8fHwVgERERESuX7IvgbGxsaN68Oc2bNycwMJA9e/Zw9OhRbt26ZfS/zZMnD4ULF6ZKlSrUrFmT/Pnzp2mx586dIzY2FldXV0aPHs3ff/+N2WymcePGDB8+HBcXF/z9/WnevLnF62xtbfHw8ODSpUup2r7ZbCY8PDxV68gMTCYTDg4OGV2GPEFERITxg1IyBx07mZ+OG3kakZGRtGrVipiYGIvpDg4O/PbbbwBs2bKFFStWcPXqVfLnz88rr7xCp06dHtuw9/DhQxYvXmyclS5WrBj/+9//aNq0abruj8RlteQ0uiY7ACfk4eFB586d6dy5c0penmLBwcEAfPbZZ9StW5fJkydz+fJlZs+ezdWrV1m4cCGhoaE4OTkleq2joyNhYWGp2n5UVBR+fn6pWkdm4ODggKenZ0aXIU9w8eJFIiIiMroMSUDHTuan40aehr+/PzExMfTq1Yt8+fIZ021sbPDz82P37t14e3vTokULOnTowMWLF5k1axaXLl2iTZs2j1zv3LlzOXbsGC1atKB8+fJcunSJL7/8klOnThlnrCX9ZM+e/YnLpCgAZ5SoqCgAypcvz5gxYwCoVasWLi4ujBo1in/++YfY2NhHvv5RF+0ll52dHaVLl07VOjKDtOiOIumvRIkSasnKZHTsZH46buRpnD9/HltbW/73v/8lGZrGjx9Po0aNLEayioyMZNeuXY+8tunMmTMcOXKE3r178+abbxrTixYtynfffcebb76Ji4tL2u+MABgDLzxJlgrAjo6OADRo0MBiet26dQE4deoUzs7OSXZTCAsLw93dPVXbN5lMRg0i6U2n2kWeno4beRoXL16kePHiFtcNJTRjxgxy5Mhh8d3v4OBAVFTUI/PA9evXAWjatKnFMnXr1mX69On4+fnRqFGjNNsHsZTchorUNYk+Y0WLFgXi+tYkFB0dDYC9vT3FihUjICDAYn5MTAyBgYEUL178mdQpIiIimd+ZM2ewtbVl4MCB1K9fnyZNmjBx4kSjy2SJEiXw8PDAbDYTEhLC2rVr2bRp02O7gMaH6WvXrllMv3LlisX/JWNlqRbg+A/itm3b6Natm5Hy//rrLwCqVKnC/fv3+fHHHwkODsbV1RUAX19fwsPD8fLyyrDaRUREJPMwm82cO3cOs9lMx44deeeddzh58iQLFizg4sWLfPfdd0bXyePHj9OrVy8APD096dGjxyPXW716dQoVKsSkSZOwt7fH09OTs2fPMnPmTEwmEw8ePHgm+yePl6UCsMlk4r333uOTTz5h5MiRdOzYkYsXLzJnzhyaNGlC+fLlyZ8/PytXrmTgwIH06dOHkJAQZsyYQd26dalcuXJG74KIiIhkAmazmSlTpuDq6kqpUqUAqFatGm5ubowZM4a9e/dSr149AAoWLMj8+fMJDAxk7ty59OrVi2XLlmFvb59ovXZ2dsyaNYvPPvuMAQMGAJA3b14+/PBDPvnkkyRfI89eigLwiRMnqFixYlrXkizNmjUjR44cLFiwgPfff5+cOXPSqVMn3n33XQBcXV2ZN28eU6dOZfTo0Tg5OdG0aVOGDh2aIfWKiIhI5mNjY0ONGjUSTa9fvz4AZ8+eNQJwvnz5yJcvn9G627dvX37//Xfatm2b5LqLFCnCggULuHPnDiEhIRQpUoTr169jNpvJmTNn+u2UJFuKAnDPnj0pUaIEL7/8Mm3atLEYOuRZaNCgQaIL4RIqXbo0c+bMeYYViYiISFZy69Ytdu/eTZ06dShQoIAxPf5Otjly5GDr1q1UqFCBIkWKGPPLly8PwO3bt5Nc74MHD9ixYweVK1emUKFC5MmTB4i7UD/h6yVjpfgiOH9/f2bPnk3btm0ZNGgQv/32m/GhEREREcnMYmJimDhxIr/++qvF9G3btmFra0uNGjWYMGECP/74o8V8X19fgEcOi2pnZ8c333zDmjVrjGnR0dGsWrWKwoULPxfDqT4PUtQC/NZbb/HHH39w5coVzGYz+/btY9++fTg6OtK8eXNefvll3XJYREREMq0CBQrQrl07vL29yZEjB5UqVeLIkSMsXryYrl27UqZMGXr27Mn8+fPJkycPNWrU4MyZMyxYsIBatWoZ3SNCQ0O5ePEihQsXxtXVFVtbW7p06cJPP/2Eu7s7xYoV4+eff+bo0aNMnjw51fckkLRhMqdixPDTp0/z+++/88cffxhDj8WPzODh4UHbtm1p27atxamFrOr48eMAvPjiixlcSdqZse0IgcGpuzuepD0PVyfea1Elo8uQx9Cxk/nouJGUePjwIT/++CObN2/m+vXruLu707FjR958801sbGwwm838+uuvrFq1iqtXr5I7d25atWpF3759yZEjBwAHDhygf//+jBs3jnbt2gFxLb7fffcdmzZt4t69e5QtW5Y+ffpoNKpnILl5LVUBOKEzZ86watUq1q1bF7fi/x+EbWxs6NSpE8OGDcvSv3oUgOVZ0Rd55qdjJ/PRcSMikPy8luph0O7fv88ff/zB9u3bOXjwICaTCbPZbNyKMiYmhp9//pmcOXPSr1+/1G5ORERERCRVUhSAw8PD+fPPP9m2bRv79u0z7sRmNpuxsbGhdu3atG/fHpPJxMyZMwkMDGTr1q0KwCIiIiKS4VIUgJs3b05UVBSA0dLr4eFBu3btEvX5dXd355133uHmzZtpUK6IiIiISOqkKAA/fPgQgOzZs9OkSRM6dOiQ5GDSEBeMAVxcXFJYooiIiIhI2klRAH7hhRdo3749rVq1wtnZ+bHLOjg4MHv2bAoVKpSiAkVERERE0lKKAnD8oNDh4eFERUVhZ2cHwKVLl8ibNy9OTk7Gsk5OTtSqVSsNShURERERSb0Uj0u2bt062rZtaww3AbB06VJat27N+vXr06Q4EREREZG0lqIA7OPjw+eff05oaCjnzp0zpvv7+xMREcHnn3/Ovn370qxIERERydpi0+a2A5IOrPFvk6IuEMuWLQOgYMGClCpVypj++uuvExQUREBAAN7e3ur6ICIiIgDYmEys8D3DzXvhGV2KJOCe05HuXmUzuoxnLkUB+Pz585hMJsaOHUv16tWN6Y0aNSJXrlz07duXs2fPplmRIiIikvXdvBeuuyhKppCiLhChoaEAuLq6JpoXP9zZ/fv3U1GWiIiIiEj6SFEAzp8/PwCrV6+2mG42m1mxYoXFMiIiIiIimUmKukA0atQIb29vVq1aha+vL2XKlCE6OpozZ85w7do1TCYTDRs2TOtaRURERERSLUUBuFevXvz5558EBARw+fJlLl++bMwzm80UKVKEd955J82KFBERERFJKynqAuHs7MzixYvp2LEjzs7OmM1mzGYzTk5OdOzYkUWLFj3xDnEiIiIiIhkhRS3AALly5WLUqFGMHDmSu3fvYjabcXV1xWQypWV9IiIiIiJpKsV3gotnMplwdXUlT548RviNjY1lz549qS5ORERERCStpagF2Gw2s2jRIv7++2/u3btHbGysMS86Opq7d+8SHR3NP//8k2aFioiIiIikhRQF4JUrVzJv3jxMJhPm/9w+L36aukKIiIiISGaUoi4QmzZtAsDBwYEiRYpgMpmoUKECJUqUMMLviBEj0rRQEREREZG0kKIAfOXKFUwmE19//TVffvklZrOZfv36sWrVKv73v/9hNpvx9/dP41JFRERERFIvRQE4MjISgKJFi1K2bFkcHR05ceIEAK+88goAPj4+aVSiiIiIiEjaSVEAzpMnDwCnT5/GZDJRpkwZI/BeuXIFgJs3b6ZRiSIiIiIiaSdFAbhy5cqYzWbGjBlDQEAAVatW5eTJk3Tt2pWRI0cC/xeSRUREREQykxQF4N69e5MzZ06ioqLIly8fLVu2xGQy4e/vT0REBCaTiWbNmqV1rSIiIiIiqZaiAFyiRAm8vb3p06cP9vb2lC5dmnHjxpE/f35y5sxJhw4d6NevX1rXKiIiIiKSaikaB9jHx4dKlSrRu3dvY1qbNm1o06ZNmhUmIiIiIpIeUtQCPHbsWFq1asXff/+d1vWIiIiIiKSrFAXgBw8eEBUVRfHixdO4HBERERGR9JWiANy0aVMAdu7cmabFiIiIiIiktxT1AS5btiy7d+9m9uzZrF69mpIlS+Ls7Ey2bP+3OpPJxNixY9OsUBERERGRtJCiADx9+nRMJhMA165d49q1a0kupwAsIiIiIplNigIwgNlsfuz8+IAsIiIiIpKZpCgAr1+/Pq3rEBERERF5JlIUgAsWLJjWdYiIiIiIPBMpCsCHDh1K1nLVqlVLyepFRERERNJNigJwv379ntjH12Qy8c8//6SoKBERERGR9JJuF8GJiIiIiGRGKQrAffr0sXhuNpt5+PAh169fZ+fOnZQvX55evXqlSYEiIiIiImkpRQG4b9++j5z3+++/M3LkSO7fv5/iokRERERE0kuKboX8OE2aNAFg+fLlab1qEREREZFUS/MAvH//fsxmM+fPn0/rVYuIiIiIpFqKukD0798/0bTY2FhCQ0O5cOECAHny5EldZSIiIiIi6SBFAfjgwYOPHAYtfnSItm3bprwqEREREZF0kqbDoNnZ2ZEvXz5atmxJ7969U1VYcg0fPpxTp06xYcMGY1pAQABTp07l8OHD2Nra0qxZMwYPHoyzs/MzqUlEREREMq8UBeD9+/endR0psnnzZnbu3Glxa+b79+/Tv39/3NzcGD9+PMHBwcyYMYPAwEBmzpyZgdWKiIiISGaQ4hbgpERFRWFnZ5eWq3ykW7duMXnyZPLnz28x/ZdffiEkJIRly5aRO3duANzd3RkyZAhHjhyhSpUqz6Q+EREREcmcUjwKxOnTpxkwYACnTp0yps2YMYPevXtz9uzZNCnucSZMmEDt2rWpWbOmxfS9e/dStWpVI/wCeHl54eTkhI+PT7rXJSIiIiKZW4oC8IULF+jXrx8HDhywCLv+/v4cPXqUvn374u/vn1Y1JrJ27VpOnTrFiBEjEs3z9/enaNGiFtNsbW3x8PDg0qVL6VaTiIiIiGQNKeoCsWjRIsLCwsiePbvFaBAvvPAChw4dIiwsjB9++IHx48enVZ2Ga9eu8e233zJ27FiLVt54oaGhODk5JZru6OhIWFhYqrZtNpsJDw9P1ToyA5PJhIODQ0aXIU8QERGR5MWmknF07GR+Om4yJx07md/zcuyYzeZHjlSWUIoC8JEjRzCZTIwePZrWrVsb0wcMGEDp0qUZNWoUhw8fTsmqH8tsNvPZZ59Rt25dmjZtmuQysbGxj3y9jU3q7vsRFRWFn59fqtaRGTg4OODp6ZnRZcgTXLx4kYiIiIwuQxLQsZP56bjJnHTsZH7P07GTPXv2Jy6TogB8584dACpWrJhoXrly5QC4fft2Slb9WKtWreLs2bOsWLGC6Oho4P+GY4uOjsbGxgZnZ+ckW2nDwsJwd3dP1fbt7OwoXbp0qtaRGSTnl5FkvBIlSjwXv8afJzp2Mj8dN5mTjp3M73k5ds6dO5es5VIUgHPlykVQUBD79++nSJEiFvP27NkDgIuLS0pW/Vh//PEHd+/epVWrVonmeXl50adPH4oVK0ZAQIDFvJiYGAIDA2ncuHGqtm8ymXB0dEzVOkSSS6cLRZ6ejhuRlHlejp3k/thKUQCuUaMGW7duZcqUKfj5+VGuXDmio6M5efIk27dvx2QyJRqdIS2MHDkyUevuggUL8PPzY+rUqeTLlw8bGxt+/PFHgoODcXV1BcDX15fw8HC8vLzSvCYRERERyVpSFIB79+7N33//TUREBOvWrbOYZzabcXBw4J133kmTAhMqXrx4omm5cuXCzs7O6FvUuXNnVq5cycCBA+nTpw8hISHMmDGDunXrUrly5TSvSURERESylhRdFVasWDFmzpxJ0aJFMZvNFv8VLVqUmTNnJhlWnwVXV1fmzZtH7ty5GT16NHPmzKFp06Z8+eWXGVKPiIiIiGQuKb4TXKVKlfjll184ffo0AQEBmM1mihQpQrly5Z5pZ/ekhlorXbo0c+bMeWY1iIiIiEjWkapbIYeHh1OyZElj5IdLly4RHh6e5Di8IiIiIiKZQYoHxl23bh1t27bl+PHjxrSlS5fSunVr1q9fnybFiYiIiIiktRQFYB8fHz7//HNCQ0Mtxlvz9/cnIiKCzz//nH379qVZkSIiIiIiaSVFAXjZsmUAFCxYkFKlShnTX3/9dYoUKYLZbMbb2zttKhQRERERSUMp6gN8/vx5TCYTY8eOpXr16sb0Ro0akStXLvr27cvZs2fTrEgRERERkbSSohbg0NBQAONGEwnF3wHu/v37qShLRERERCR9pCgA58+fH4DVq1dbTDebzaxYscJiGRERERGRzCRFXSAaNWqEt7c3q1atwtfXlzJlyhAdHc2ZM2e4du0aJpOJhg0bpnWtIiIiIiKplqIA3KtXL/78808CAgK4fPkyly9fNubF3xAjPW6FLCIiIiKSWinqAuHs7MzixYvp2LEjzs7Oxm2QnZyc6NixI4sWLcLZ2TmtaxURERERSbUU3wkuV65cjBo1ipEjR3L37l3MZjOurq7P9DbIIiIiIiJPK8V3gotnMplwdXUlT548mEwmIiIiWLNmDW+++WZa1CciIiIikqZS3AL8X35+fqxevZpt27YRERGRVqsVEREREUlTqQrA4eHhbNmyhbVr13L69GljutlsVlcIEREREcmUUhSA//33X9asWcP27duN1l6z2QyAra0tDRs2pFOnTmlXpYiIiIhIGkl2AA4LC2PLli2sWbPGuM1xfOiNZzKZ2LhxI3nz5k3bKkVERERE0kiyAvBnn33G77//zoMHDyxCr6OjI02aNKFAgQIsXLgQQOFXRERERDK1ZAXgDRs2YDKZMJvNZMuWDS8vL1q3bk3Dhg3JkSMHe/fuTe86RURERETSxFMNg2YymXB3d6dixYp4enqSI0eO9KpLRERERCRdJKsFuEqVKhw5cgSAa9euMX/+fObPn4+npyetWrXSXd9EREREJMtIVgBesGABly9fZu3atWzevJmgoCAATp48ycmTJy2WjYmJwdbWNu0rFRERERFJA8nuAlG0aFHee+89Nm3axKRJk6hfv77RLzjhuL+tWrVi2rRpnD9/Pt2KFhERERFJqaceB9jW1pZGjRrRqFEjbt++zfr169mwYQNXrlwBICQkhJ9++only5fzzz//pHnBIiIiIiKp8VQXwf1X3rx56dWrF2vWrGHu3Lm0atUKOzs7o1VYRERERCSzSdWtkBOqUaMGNWrUYMSIEWzevJn169en1apFRERERNJMmgXgeM7OznTt2pWuXbum9apFRERERFItVV0gRERERESyGgVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlYlW0YX8LRiY2NZvXo1v/zyC1evXiVPnjy89NJL9OvXD2dnZwACAgKYOnUqhw8fxtbWlmbNmjF48GBjvoiIiIhYrywXgH/88Ufmzp3LG2+8Qc2aNbl8+TLz5s3j/PnzzJ49m9DQUPr374+bmxvjx48nODiYGTNmEBgYyMyZMzO6fBERERHJYFkqAMfGxrJkyRJeffVVBg0aBEDt2rXJlSsXI0eOxM/Pj3/++YeQkBCWLVtG7ty5AXB3d2fIkCEcOXKEKlWqZNwOiIiIiEiGy1J9gMPCwmjTpg0tW7a0mF68eHEArly5wt69e6lataoRfgG8vLxwcnLCx8fnGVYrIiIiIplRlmoBdnFxYfjw4Ymm//nnnwCULFkSf39/mjdvbjHf1tYWDw8PLl269CzKFBEREZFMLEsF4KScOHGCJUuW0KBBA0qXLk1oaChOTk6JlnN0dCQsLCxV2zKbzYSHh6dqHZmByWTCwcEho8uQJ4iIiMBsNmd0GZKAjp3MT8dN5qRjJ/N7Xo4ds9mMyWR64nJZOgAfOXKE999/Hw8PD8aNGwfE9RN+FBub1PX4iIqKws/PL1XryAwcHBzw9PTM6DLkCS5evEhERERGlyEJ6NjJ/HTcZE46djK/5+nYyZ49+xOXybIBeNu2bXz66acULVqUmTNnGn1+nZ2dk2ylDQsLw93dPVXbtLOzo3Tp0qlaR2aQnF9GkvFKlCjxXPwaf57o2Mn8dNxkTjp2Mr/n5dg5d+5cspbLkgHY29ubGTNmUL16dSZPnmwxvm+xYsUICAiwWD4mJobAwEAaN26cqu2aTCYcHR1TtQ6R5NLpQpGnp+NGJGWel2MnuT+2stQoEAC//vor06dPp1mzZsycOTPRzS28vLw4dOgQwcHBxjRfX1/Cw8Px8vJ61uWKiIiISCaTpVqAb9++zdSpU/Hw8KBbt26cOnXKYn7hwoXp3LkzK1euZODAgfTp04eQkBBmzJhB3bp1qVy5cgZVLiIiIiKZRZYKwD4+PkRGRhIYGEjv3r0TzR83bhzt2rVj3rx5TJ06ldGjR+Pk5ETTpk0ZOnTosy9YRERERDKdLBWAO3ToQIcOHZ64XOnSpZkzZ84zqEhEREREspos1wdYRERERCQ1FIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKs91APb19eXNN9+kXr16tG/fHm9vb8xmc0aXJSIiIiIZ6LkNwMePH2fo0KEUK1aMSZMm0apVK2bMmMGSJUsyujQRERERyUDZMrqA9DJ//nzKlSvHhAkTAKhbty7R0dEsXryY7t27Y29vn8EVioiIiEhGeC5bgB8+fMjBgwdp3LixxfSmTZsSFhbGkSNHMqYwEREREclwz2UAvnr1KlFRURQtWtRiepEiRQC4dOlSRpQlIiIiIpnAc9kFIjQ0FAAnJyeL6Y6OjgCEhYU91fpOnz7Nw4cPATh27FgaVJjxTCYTtfLEEpNbXUEyG1ubWI4fP64LNjMpHTuZk46bzE/HTub0vB07UVFRmEymJy73XAbg2NjYx863sXn6hu/4NzM5b2pW4ZTDLqNLkMd4nj5rzxsdO5mXjpvMTcdO5vW8HDsmk8l6A7CzszMA4eHhFtPjW37j5ydXuXLl0qYwEREREclwz2Uf4MKFC2Nra0tAQIDF9PjnxYsXz4CqRERERCQzeC4DcI4cOahatSo7d+606NOyY8cOnJ2dqVixYgZWJyIiIiIZ6bkMwADvvPMOJ06c4OOPP8bHx4e5c+fi7e1Nz549NQawiIiIiBUzmZ+Xy/6SsHPnTubPn8+lS5dwd3enS5cu9OjRI6PLEhEREZEM9FwHYBERERGR/3puu0CIiIiIiCRFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi9XTSIDyvEvqM67PvYhYMwVgyZICAwOpUaMGGzZsSPFr7t+/z9ixYzl8+HB6lSmSLtq1a8f48eOTnDd//nxq1KhhPD9y5AhDhgyxWGbhwoV4e3unZ4kiViUl30mSsRSAxWqdPn2azZs3Exsbm9GliKSZjh07snjxYuP52rVruXjxosUy8+bNIyIi4lmXJvLcyps3L4sXL6Z+/foZXYokU7aMLkBERNJO/vz5yZ8/f0aXIWJVsmfPzosvvpjRZchTUAuwZLgHDx4wa9YsXnnlFerUqUPDhg0ZMGAAp0+fNpbZsWMHr732GvXq1eP111/nzJkzFuvYsGEDNWrUIDAw0GL6o04VHzhwgP79+wPQv39/+vbtm/Y7JvKMrFu3jpo1a7Jw4UKLLhDjx49n48aNXLt2zTg9Gz9vwYIFFl0lzp07x9ChQ2nYsCENGzbkww8/5MqVK8b8AwcOUKNGDfbt28fAgQOpV68eLVu2ZMaMGcTExDzbHRZ5Cn5+frz77rs0bNiQl156iQEDBnD8+HFj/uHDh+nbty/16tWjSZMmjBs3juDgYGP+hg0bqF27NidOnKBnz57UrVuXtm3bWnQjSqoLxOXLl/noo49o2bIl9evXp1+/fhw5ciTRa5YuXUqnTp2oV68e69evT983QwwKwJLhxo0bx/r163n77beZNWsW77//PhcuXGD06NGYzWb+/vtvRowYQenSpZk8eTLNmzdnzJgxqdpm+fLlGTFiBAAjRozg448/TotdEXnmtm3bxsSJE+nduze9e/e2mNe7d2/q1auHm5ubcXo2vntEhw4djMeXLl3inXfe4c6dO4wfP54xY8Zw9epVY1pCY8aMoWrVqkybNo2WLVvy448/snbt2meyryJPKzQ0lMGDB5M7d26++eYbvvjiCyIiIhg0aBChoaEcOnSId999F3t7e7766is++OADDh48SL9+/Xjw4IGxntjYWD7++GNatGjB9OnTqVKlCtOnT2fv3r1JbvfChQu88cYbXLt2jeHDh/P5559jMpno378/Bw8etFh2wYIFvPXWW3z22WfUrl07Xd8P+T/qAiEZKioqivDwcIYPH07z5s0BqF69OqGhoUybNo2goCAWLlxIhQoVmDBhAgB16tQBYNasWSnerrOzMyVKlACgRIkSlCxZMpV7IvLs7dq1i7Fjx/L222/Tr1+/RPMLFy6Mq6urxelZV1dXANzd3Y1pCxYswN7enjlz5uDs7AxAzZo16dChA97e3hYX0XXs2NEI2jVr1uSvv/5i9+7ddOrUKV33VSQlLl68yN27d+nevTuVK1cGoHjx4qxevZqwsDBmzZpFsWLF+Pbbb7G1tQXgxRdfpGvXrqxfv56uXbsCcaOm9O7dm44dOwJQuXJldu7cya5du4zvpIQWLFiAnZ0d8+bNw8nJCYD69evTrVs3pk+fzo8//mgs26xZM9q3b5+eb4MkQS3AkqHs7OyYOXMmzZs35+bNmxw4cIBff/2V3bt3A3EB2c/PjwYNGli8Lj4si1grPz8/Pv74Y9zd3Y3uPCm1f/9+qlWrhr29PdHR0URHR+Pk5ETVqlX5559/LJb9bz9Hd3d3XVAnmVapUqVwdXXl/fff54svvmDnzp24ubnx3nvvkStXLk6cOEH9+vUxm83GZ79QoUIUL1480We/UqVKxuPs2bOTO3fuR372Dx48SIMGDYzwC5AtWzZatGiBn58f4eHhxvSyZcum8V5LcqgFWDLc3r17mTJlCv7+/jg5OVGmTBkcHR0BuHnzJmazmdy5c1u8Jm/evBlQqUjmcf78eerXr8/u3btZtWoV3bt3T/G67t69y/bt29m+fXuiefEtxvHs7e0tnptMJo2kIpmWo6MjCxYs4Pvvv2f79u2sXr2aHDly8PLLL9OzZ09iY2NZsmQJS5YsSfTaHDlyWDz/72ffxsbmkeNph4SE4Obmlmi6m5sbZrOZsLAwixrl2VMAlgx15coVPvzwQxo2bMi0adMoVKgQJpOJn3/+mT179pArVy5sbGwS9UMMCQmxeG4ymQASfREn/JUt8jypW7cu06ZN45NPPmHOnDk0atSIAgUKpGhdLi4u1KpVix49eiSaF39aWCSrKl68OBMmTCAmJoZ///2XzZs388svv+Du7o7JZOJ///sfLVu2TPS6/wbep5ErVy6CgoISTY+flitXLm7fvp3i9UvqqQuEZCg/Pz8iIyN5++23KVy4sBFk9+zZA8SdMqpUqRI7duyw+KX9999/W6wn/jTTjRs3jGn+/v6JgnJC+mKXrCxPnjwADBs2DBsbG7766qskl7OxSfzP/H+nVatWjYsXL1K2bFk8PT3x9PTkhRdeYNmyZfz5559pXrvIs/L777/TrFkzbt++ja2tLZUqVeLjjz/GxcWFoKAgypcvj7+/v/G59/T0pGTJksyfPz/RxWpPo1q1auzatcuipTcmJobffvsNT09PsmfPnha7J6mgACwZqnz58tja2jJz5kx8fX3ZtWsXw4cPN/oAP3jwgIEDB3LhwgWGDx/Onj17WL58OfPnz7dYT40aNciRIwfTpk3Dx8eHbdu2MWzYMHLlyvXIbbu4uADg4+OTaFg1kawib968DBw4kN27d7N169ZE811cXLhz5w4+Pj5Gi5OLiwtHjx7l0KFDmM1m+vTpQ0BAAO+//z5//vkne/fu5aOPPmLbtm2UKVPmWe+SSJqpUqUKsbGxfPjhh/z555/s37+fiRMnEhoaStOmTRk4cCC+vr6MHj2a3bt38/fff/Pee++xf/9+ypcvn+Lt9unTh8jISPr378/vv//OX3/9xeDBg7l69SoDBw5Mwz2UlFIAlgxVpEgRJk6cyI0bNxg2bBhffPEFEHc7V5PJxOHDh6latSozZszg5s2bDB8+nNWrVzN27FiL9bi4uDBp0iRiYmL48MMPmTdvHn369MHT0/OR2y5ZsiQtW7Zk1apVjB49Ol33UyQ9derUiQoVKjBlypREZz3atWtHwYIFGTZsGBs3bgSgZ8+e+Pn58d5773Hjxg3KlCnDwoULMZlMjBs3jhEjRnD79m0mT55MkyZNMmKXRNJE3rx5mTlzJs7OzkyYMIGhQ4dy+vRpvvnmG2rUqIGXlxczZ87kxo0bjBgxgrFjx2Jra8ucOXNSdWOLUqVKsXDhQlxdXfnss8+M76z58+drqLNMwmR+VA9uEREREZHnkFqARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKtkyugARkedBnz59OHz4MBB384lx48ZlcEWJnTt3jl9//ZV9+/Zx+/ZtHj58iKurKy+88ALt27enYcOGGV2iiMgzoRthiIik0qVLl+jUqZPx3N7enq1bt+Ls7JyBVVn64YcfmDdvHtHR0Y9cpnXr1nz66afY2OjkoIg83/SvnIhIKq1bt87i+YMHD9i8eXMGVZPYqlWrmDVrFtHR0eTPn5+RI0fy888/s2LFCoYOHYqTkxMAW7Zs4aeffsrgakVE0p9agEVEUiE6OpqXX36ZoKAgPDw8uHHjBjExMZQtWzZThMnbt2/Trl07oqKiyJ8/Pz/++CNubm4Wy/j4+DBkyBAA8uXLx+bNmzGZTBlRrojIM6E+wCIiqbB7926CgoIAaN++PSdOnGD37t2cOXOGEydOULFixUSvCQwMZNasWfj6+hIVFUXVqlX54IMP+OKLLzh06BDVqlXju+++M5b39/dn/vz57N+/n/DwcAoWLEjr1q154403yJEjx2Pr27hxI1FRUQD07t07UfgFqFevHkOHDsXDwwNPT08j/G7YsIFPP/0UgKlTp7JkyRJOnjyJq6sr3t7euLm5ERUVxYoVK9i6dSsBAQEAlCpVio4dO9K+fXuLIN23b18OHToEwIEDB4zpBw4coH///kBcX+p+/fpZLF+2bFm+/vprpk+fzv79+zGZTNSpU4fBgwfj4eHx2P0XEUmKArCISCok7P7QsmVLihQpwu7duwFYvXp1ogB87do13nrrLYKDg41pe/bs4eTJk0n2Gf73338ZMGAAYWFhxrRLly4xb9489u3bx5w5c8iW7dH/lMcHTgAvL69HLtejR4/H7CWMGzeO+/fvA+Dm5oabmxvh4eH07duXU6dOWSx7/Phxjh8/jo+PD19++SW2traPXfeTBAcH07NnT+7evWtM2759O4cOHWLJkiUUKFAgVesXEeujPsAiIil069Yt9uzZA4CnpydFihShYcOGRp/a7du3ExoaavGaWbNmGeG3devWLF++nLlz55InTx6uXLlisazZbOazzz4jLCyM3LlzM2nSJH799VeGDx+OjY0Nhw4dYuXKlY+t8caNG8bjfPnyWcy7ffs2N27cSPTfw4cPE60nKiqKqVOn8tNPP/HBBx8AMG3aNCP8tmjRgqVLl7Jo0SJq164NwI4dO/D29n78m5gMt27dImfOnMyaNYvly5fTunVrAIKCgpg5c2aq1y8i1kcBWEQkhTZs2EBMTAwArVq1AuJGgGjcuDEAERERbN261Vg+NjbWaB3Onz8/48aNo0yZMtSsWZOJEycmWv/Zs2c5f/48AG3btsXT0xN7e3saNWpEtWrVANi0adNja0w4osN/R4B48803efnllxP9d+zYsUTradasGS+99BJly5alatWqhIWFGdsuVaoUEyZMoHz58lSqVInJkycbXS2eFNCTa8yYMXh5eVGmTBnGjRtHwYIFAdi1a5fxNxARSS4FYBGRFDCbzaxfv9547uzszJ49e9izZ4/FKfk1a9YYj4ODg42uDJ6enhZdF8qUKWO0HMe7fPmy8Xjp0qUWITW+D+358+eTbLGNlz9/fuNxYGDg0+6moVSpUolqi4yMBKBGjRoW3RwcHByoVKkSENd6m7DrQkqYTCaLriTZsmXD09MTgPDw8FSvX0Ssj/oAi4ikwMGDBy26LHz22WdJLnf69Gn+/fdfKlSogJ2dnTE9OQPwJKfvbExMDPfu3SNv3rxJzq9Vq5bR6rx7925KlixpzEs4VNv48ePZuHHjI7fz3/7JT6rtSfsXExNjrCM+SD9uXdHR0Y98/zRihYg8LbUAi4ikwH/H/n2c+FbgnDlz4uLiAoCfn59Fl4RTp05ZXOgGUKRIEePxgAEDOHDggPHf0qVL2bp1KwcOHHhk+IW4vrn29vYALFmy5JGtwP/d9n/990K7QoUKkT17diBuFIfY2FhjXkREBMePHwfiWqBz584NYCz/3+1dv379sduGuB8c8WJiYjh9+jQQF8zj1y8iklwKwCIiT+n+/fvs2LEDgFy5crF3716LcHrgwAG2bt1qtHBu27bNCHwtW7YE4i5O+/TTTzl37hy+vr6MGjUq0XZKlSpF2bJlgbguEL/99htXrlxh8+bNvPXWW7Rq1Yrhw4c/tta8efPy/vvvAxASEkLPnj35+eef8ff3x9/fn61bt9KvXz927tz5VO+Bk5MTTZs2BeK6YYwdO5ZTp05x/PhxPvroI2NouK5duxqvSXgR3vLly4mNjeX06dMsWbLkidv76quv2LVrF+fOneOrr77i6tWrADRq1Eh3rhORp6YuECIiT2nLli3Gafs2bdpYnJqPlzdvXho2bMiOHTsIDw9n69atdOrUiV69erFz506CgoLYsmULW7ZsAaBAgQI4ODgQERFhnNI3mUwMGzaM9957j3v37iUKybly5TLGzH2cTp06ERUVxfTp0wkKCuLrr79OcjlbW1s6dOhg9K99kuHDh3PmzBnOnz/P1q1bLS74A2jSpInF8GotW7Zkw4YNACxYsICFCxdiNpt58cUXn9g/2Ww2G0E+Xr58+Rg0aFCyahURSUg/m0VEnlLC7g8dOnR45HKdOnUyHsd3g3B3d+f777+ncePGODk54eTkRJMmTVi4cKHRRSBhV4Hq1avzww8/0Lx5c9zc3LCzsyN//vy0a9eOH374gdKlSyer5u7du/Pzzz/Ts2dPypUrR65cubCzsyNv3rzUqlWLQYMGsWHDBkaOHImjo2Oy1pkzZ068vb0ZMmQIL7zwAo6Ojtjb21OxYkVGjx7N119/bdFX2MvLiwkTJlCqVCmyZ89OwYIF6dOnD99+++0TtxX/njk4OODs7EyLFi1YvHjxY7t/iIg8im6FLCLyDPn6+pI9e3bc3d0pUKCA0bc2NjaWBg0aEBkZSYsWLfjiiy8yuNKM96g7x4mIpJa6QIiIPEMrV65k165dAHTs2JG33nqLhw8fsnHjRqNbRXK7IIiISMooAIuIPEPdunXDx8eH2NhY1q5dy9q1ay3m58+fn/bt22dMcSIiVkJ9gEVEniEvLy/mzJlDgwYNcHNzw9bWluzZs1O4cGE6derEDz/8QM6cOTO6TBGR55r6AIuIiIiIVVELsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFiV/wcC+t8PQdC2aAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efa447-1e32-42df-8664-a3f1f0e0f812",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eecd6a6-4094-4747-8029-f795a87f8ff0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      168     78.87\n",
      "1          M    337      230     68.25\n",
      "2          X    319      244     76.49\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "115592e9-47eb-42fe-8cb5-26beeb7328ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae066af0-659b-43f0-924c-2c50be0e6c40",
   "metadata": {},
   "source": [
    "# RANDOM SEED 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc76296a-4029-447e-b12c-41520160251a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[1]))\n",
    "np.random.seed(int(random_seeds[1]))\n",
    "tf.random.set_seed(int(random_seeds[1]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "093977a2-1813-4a02-9573-f757b3d2a567",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c89dbfa-4c9b-4e46-a0d9-f659db30532f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a62fdf-86b5-45e7-9249-73a55985936a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee1b36c7-4a91-4071-a713-c904da913c3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "106A    14\n",
      "097B    14\n",
      "001A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "005A    10\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "022A     9\n",
      "051B     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "033A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "027A     7\n",
      "117A     7\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "053A     6\n",
      "008A     6\n",
      "023A     6\n",
      "037A     6\n",
      "007A     6\n",
      "034A     5\n",
      "025C     5\n",
      "070A     5\n",
      "023B     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "062A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "052A     4\n",
      "009A     4\n",
      "060A     3\n",
      "012A     3\n",
      "006A     3\n",
      "064A     3\n",
      "058A     3\n",
      "054A     2\n",
      "061A     2\n",
      "087A     2\n",
      "069A     2\n",
      "032A     2\n",
      "011A     2\n",
      "018A     2\n",
      "025B     2\n",
      "093A     2\n",
      "088A     1\n",
      "091A     1\n",
      "100A     1\n",
      "090A     1\n",
      "019B     1\n",
      "115A     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "073A     1\n",
      "026C     1\n",
      "076A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "014B    10\n",
      "015A     9\n",
      "099A     7\n",
      "050A     7\n",
      "021A     5\n",
      "003A     4\n",
      "056A     3\n",
      "014A     3\n",
      "113A     3\n",
      "038A     2\n",
      "102A     2\n",
      "096A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    233\n",
      "M    226\n",
      "F    210\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    115\n",
      "M    111\n",
      "F     42\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 040A, 046A, 109A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 059A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 103A, 074A, 002B, 101A, 038A, 099A, 014...\n",
      "kitten                 [014B, 111A, 047A, 042A, 050A, 110A]\n",
      "senior                       [097A, 113A, 056A, 051A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "669\n",
      "Length of y_train_val:\n",
      "669\n",
      "Length of groups_train_val:\n",
      "669\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 856, 2: 715, 1: 490})\n",
      "Epoch 1/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.1703 - accuracy: 0.4852\n",
      "Epoch 2/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.0019 - accuracy: 0.5682\n",
      "Epoch 3/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.9426 - accuracy: 0.5958\n",
      "Epoch 4/1500\n",
      "33/33 [==============================] - 0s 900us/step - loss: 0.9068 - accuracy: 0.5978\n",
      "Epoch 5/1500\n",
      "33/33 [==============================] - 0s 821us/step - loss: 0.8613 - accuracy: 0.6342\n",
      "Epoch 6/1500\n",
      "33/33 [==============================] - 0s 933us/step - loss: 0.8306 - accuracy: 0.6434\n",
      "Epoch 7/1500\n",
      "33/33 [==============================] - 0s 890us/step - loss: 0.8124 - accuracy: 0.6482\n",
      "Epoch 8/1500\n",
      "33/33 [==============================] - 0s 869us/step - loss: 0.7593 - accuracy: 0.6846\n",
      "Epoch 9/1500\n",
      "33/33 [==============================] - 0s 906us/step - loss: 0.7557 - accuracy: 0.6691\n",
      "Epoch 10/1500\n",
      "33/33 [==============================] - 0s 912us/step - loss: 0.7347 - accuracy: 0.6778\n",
      "Epoch 11/1500\n",
      "33/33 [==============================] - 0s 899us/step - loss: 0.7268 - accuracy: 0.6938\n",
      "Epoch 12/1500\n",
      "33/33 [==============================] - 0s 944us/step - loss: 0.7286 - accuracy: 0.6987\n",
      "Epoch 13/1500\n",
      "33/33 [==============================] - 0s 914us/step - loss: 0.7155 - accuracy: 0.6929\n",
      "Epoch 14/1500\n",
      "33/33 [==============================] - 0s 989us/step - loss: 0.6996 - accuracy: 0.6972\n",
      "Epoch 15/1500\n",
      "33/33 [==============================] - 0s 891us/step - loss: 0.6661 - accuracy: 0.7215\n",
      "Epoch 16/1500\n",
      "33/33 [==============================] - 0s 956us/step - loss: 0.6650 - accuracy: 0.7060\n",
      "Epoch 17/1500\n",
      "33/33 [==============================] - 0s 986us/step - loss: 0.6836 - accuracy: 0.7089\n",
      "Epoch 18/1500\n",
      "33/33 [==============================] - 0s 857us/step - loss: 0.6496 - accuracy: 0.7171\n",
      "Epoch 19/1500\n",
      "33/33 [==============================] - 0s 891us/step - loss: 0.6627 - accuracy: 0.7249\n",
      "Epoch 20/1500\n",
      "33/33 [==============================] - 0s 868us/step - loss: 0.6402 - accuracy: 0.7230\n",
      "Epoch 21/1500\n",
      "33/33 [==============================] - 0s 856us/step - loss: 0.6353 - accuracy: 0.7317\n",
      "Epoch 22/1500\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.6359 - accuracy: 0.7273\n",
      "Epoch 23/1500\n",
      "33/33 [==============================] - 0s 853us/step - loss: 0.6064 - accuracy: 0.7356\n",
      "Epoch 24/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.6078 - accuracy: 0.7370\n",
      "Epoch 25/1500\n",
      "33/33 [==============================] - 0s 835us/step - loss: 0.5959 - accuracy: 0.7487\n",
      "Epoch 26/1500\n",
      "33/33 [==============================] - 0s 860us/step - loss: 0.5834 - accuracy: 0.7501\n",
      "Epoch 27/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.5780 - accuracy: 0.7525\n",
      "Epoch 28/1500\n",
      "33/33 [==============================] - 0s 827us/step - loss: 0.5843 - accuracy: 0.7462\n",
      "Epoch 29/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.5838 - accuracy: 0.7589\n",
      "Epoch 30/1500\n",
      "33/33 [==============================] - 0s 815us/step - loss: 0.5752 - accuracy: 0.7540\n",
      "Epoch 31/1500\n",
      "33/33 [==============================] - 0s 806us/step - loss: 0.5407 - accuracy: 0.7715\n",
      "Epoch 32/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.5777 - accuracy: 0.7579\n",
      "Epoch 33/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.5498 - accuracy: 0.7686\n",
      "Epoch 34/1500\n",
      "33/33 [==============================] - 0s 891us/step - loss: 0.5424 - accuracy: 0.7676\n",
      "Epoch 35/1500\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.5483 - accuracy: 0.7715\n",
      "Epoch 36/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.5446 - accuracy: 0.7569\n",
      "Epoch 37/1500\n",
      "33/33 [==============================] - 0s 923us/step - loss: 0.5436 - accuracy: 0.7642\n",
      "Epoch 38/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.5262 - accuracy: 0.7754\n",
      "Epoch 39/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.5379 - accuracy: 0.7724\n",
      "Epoch 40/1500\n",
      "33/33 [==============================] - 0s 800us/step - loss: 0.5121 - accuracy: 0.7763\n",
      "Epoch 41/1500\n",
      "33/33 [==============================] - 0s 874us/step - loss: 0.5228 - accuracy: 0.7686\n",
      "Epoch 42/1500\n",
      "33/33 [==============================] - 0s 875us/step - loss: 0.5240 - accuracy: 0.7734\n",
      "Epoch 43/1500\n",
      "33/33 [==============================] - 0s 874us/step - loss: 0.5183 - accuracy: 0.7821\n",
      "Epoch 44/1500\n",
      "33/33 [==============================] - 0s 850us/step - loss: 0.5156 - accuracy: 0.7758\n",
      "Epoch 45/1500\n",
      "33/33 [==============================] - 0s 846us/step - loss: 0.5111 - accuracy: 0.7855\n",
      "Epoch 46/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.5294 - accuracy: 0.7758\n",
      "Epoch 47/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.4948 - accuracy: 0.7807\n",
      "Epoch 48/1500\n",
      "33/33 [==============================] - 0s 833us/step - loss: 0.4998 - accuracy: 0.7865\n",
      "Epoch 49/1500\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.5126 - accuracy: 0.7773\n",
      "Epoch 50/1500\n",
      "33/33 [==============================] - 0s 840us/step - loss: 0.4983 - accuracy: 0.7865\n",
      "Epoch 51/1500\n",
      "33/33 [==============================] - 0s 837us/step - loss: 0.4823 - accuracy: 0.7923\n",
      "Epoch 52/1500\n",
      "33/33 [==============================] - 0s 819us/step - loss: 0.4907 - accuracy: 0.7899\n",
      "Epoch 53/1500\n",
      "33/33 [==============================] - 0s 843us/step - loss: 0.4955 - accuracy: 0.7841\n",
      "Epoch 54/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.4743 - accuracy: 0.7957\n",
      "Epoch 55/1500\n",
      "33/33 [==============================] - 0s 914us/step - loss: 0.4788 - accuracy: 0.7957\n",
      "Epoch 56/1500\n",
      "33/33 [==============================] - 0s 862us/step - loss: 0.4884 - accuracy: 0.7773\n",
      "Epoch 57/1500\n",
      "33/33 [==============================] - 0s 853us/step - loss: 0.4769 - accuracy: 0.8025\n",
      "Epoch 58/1500\n",
      "33/33 [==============================] - 0s 860us/step - loss: 0.4687 - accuracy: 0.8045\n",
      "Epoch 59/1500\n",
      "33/33 [==============================] - 0s 843us/step - loss: 0.4617 - accuracy: 0.7977\n",
      "Epoch 60/1500\n",
      "33/33 [==============================] - 0s 834us/step - loss: 0.4777 - accuracy: 0.8011\n",
      "Epoch 61/1500\n",
      "33/33 [==============================] - 0s 850us/step - loss: 0.4570 - accuracy: 0.8006\n",
      "Epoch 62/1500\n",
      "33/33 [==============================] - 0s 871us/step - loss: 0.4774 - accuracy: 0.7957\n",
      "Epoch 63/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.4714 - accuracy: 0.7904\n",
      "Epoch 64/1500\n",
      "33/33 [==============================] - 0s 839us/step - loss: 0.4528 - accuracy: 0.8093\n",
      "Epoch 65/1500\n",
      "33/33 [==============================] - 0s 840us/step - loss: 0.4588 - accuracy: 0.7967\n",
      "Epoch 66/1500\n",
      "33/33 [==============================] - 0s 824us/step - loss: 0.4658 - accuracy: 0.8035\n",
      "Epoch 67/1500\n",
      "33/33 [==============================] - 0s 932us/step - loss: 0.4487 - accuracy: 0.8069\n",
      "Epoch 68/1500\n",
      "33/33 [==============================] - 0s 996us/step - loss: 0.4392 - accuracy: 0.8074\n",
      "Epoch 69/1500\n",
      "33/33 [==============================] - 0s 978us/step - loss: 0.4529 - accuracy: 0.8035\n",
      "Epoch 70/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4438 - accuracy: 0.8040\n",
      "Epoch 71/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.8142\n",
      "Epoch 72/1500\n",
      "33/33 [==============================] - 0s 895us/step - loss: 0.4533 - accuracy: 0.8088\n",
      "Epoch 73/1500\n",
      "33/33 [==============================] - 0s 901us/step - loss: 0.4174 - accuracy: 0.8292\n",
      "Epoch 74/1500\n",
      "33/33 [==============================] - 0s 929us/step - loss: 0.4305 - accuracy: 0.8219\n",
      "Epoch 75/1500\n",
      "33/33 [==============================] - 0s 866us/step - loss: 0.4251 - accuracy: 0.8205\n",
      "Epoch 76/1500\n",
      "33/33 [==============================] - 0s 916us/step - loss: 0.4380 - accuracy: 0.8117\n",
      "Epoch 77/1500\n",
      "33/33 [==============================] - 0s 906us/step - loss: 0.4103 - accuracy: 0.8224\n",
      "Epoch 78/1500\n",
      "33/33 [==============================] - 0s 920us/step - loss: 0.4221 - accuracy: 0.8161\n",
      "Epoch 79/1500\n",
      "33/33 [==============================] - 0s 986us/step - loss: 0.4394 - accuracy: 0.8142\n",
      "Epoch 80/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4197 - accuracy: 0.8195\n",
      "Epoch 81/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4373 - accuracy: 0.8195\n",
      "Epoch 82/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.4205 - accuracy: 0.8219\n",
      "Epoch 83/1500\n",
      "33/33 [==============================] - 0s 959us/step - loss: 0.4197 - accuracy: 0.8253\n",
      "Epoch 84/1500\n",
      "33/33 [==============================] - 0s 909us/step - loss: 0.4151 - accuracy: 0.8180\n",
      "Epoch 85/1500\n",
      "33/33 [==============================] - 0s 886us/step - loss: 0.4265 - accuracy: 0.8244\n",
      "Epoch 86/1500\n",
      "33/33 [==============================] - 0s 917us/step - loss: 0.4109 - accuracy: 0.8248\n",
      "Epoch 87/1500\n",
      "33/33 [==============================] - 0s 903us/step - loss: 0.3972 - accuracy: 0.8341\n",
      "Epoch 88/1500\n",
      "33/33 [==============================] - 0s 914us/step - loss: 0.4025 - accuracy: 0.8292\n",
      "Epoch 89/1500\n",
      "33/33 [==============================] - 0s 954us/step - loss: 0.4199 - accuracy: 0.8151\n",
      "Epoch 90/1500\n",
      "33/33 [==============================] - 0s 905us/step - loss: 0.4016 - accuracy: 0.8326\n",
      "Epoch 91/1500\n",
      "33/33 [==============================] - 0s 878us/step - loss: 0.3924 - accuracy: 0.8331\n",
      "Epoch 92/1500\n",
      "33/33 [==============================] - 0s 876us/step - loss: 0.3883 - accuracy: 0.8345\n",
      "Epoch 93/1500\n",
      "33/33 [==============================] - 0s 855us/step - loss: 0.3982 - accuracy: 0.8389\n",
      "Epoch 94/1500\n",
      "33/33 [==============================] - 0s 884us/step - loss: 0.4132 - accuracy: 0.8248\n",
      "Epoch 95/1500\n",
      "33/33 [==============================] - 0s 915us/step - loss: 0.3961 - accuracy: 0.8273\n",
      "Epoch 96/1500\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.3860 - accuracy: 0.8379\n",
      "Epoch 97/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3897 - accuracy: 0.8360\n",
      "Epoch 98/1500\n",
      "33/33 [==============================] - 0s 950us/step - loss: 0.3952 - accuracy: 0.8336\n",
      "Epoch 99/1500\n",
      "33/33 [==============================] - 0s 874us/step - loss: 0.3975 - accuracy: 0.8316\n",
      "Epoch 100/1500\n",
      "33/33 [==============================] - 0s 912us/step - loss: 0.3937 - accuracy: 0.8287\n",
      "Epoch 101/1500\n",
      "33/33 [==============================] - 0s 855us/step - loss: 0.3807 - accuracy: 0.8472\n",
      "Epoch 102/1500\n",
      "33/33 [==============================] - 0s 883us/step - loss: 0.3976 - accuracy: 0.8282\n",
      "Epoch 103/1500\n",
      "33/33 [==============================] - 0s 893us/step - loss: 0.3795 - accuracy: 0.8491\n",
      "Epoch 104/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3971 - accuracy: 0.8316\n",
      "Epoch 105/1500\n",
      "33/33 [==============================] - 0s 929us/step - loss: 0.3794 - accuracy: 0.8438\n",
      "Epoch 106/1500\n",
      "33/33 [==============================] - 0s 857us/step - loss: 0.3665 - accuracy: 0.8394\n",
      "Epoch 107/1500\n",
      "33/33 [==============================] - 0s 864us/step - loss: 0.3797 - accuracy: 0.8365\n",
      "Epoch 108/1500\n",
      "33/33 [==============================] - 0s 846us/step - loss: 0.3676 - accuracy: 0.8428\n",
      "Epoch 109/1500\n",
      "33/33 [==============================] - 0s 859us/step - loss: 0.3750 - accuracy: 0.8409\n",
      "Epoch 110/1500\n",
      "33/33 [==============================] - 0s 901us/step - loss: 0.3504 - accuracy: 0.8569\n",
      "Epoch 111/1500\n",
      "33/33 [==============================] - 0s 855us/step - loss: 0.3722 - accuracy: 0.8486\n",
      "Epoch 112/1500\n",
      "33/33 [==============================] - 0s 901us/step - loss: 0.3730 - accuracy: 0.8462\n",
      "Epoch 113/1500\n",
      "33/33 [==============================] - 0s 855us/step - loss: 0.3754 - accuracy: 0.8384\n",
      "Epoch 114/1500\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.3589 - accuracy: 0.8462\n",
      "Epoch 115/1500\n",
      "33/33 [==============================] - 0s 858us/step - loss: 0.3571 - accuracy: 0.8525\n",
      "Epoch 116/1500\n",
      "33/33 [==============================] - 0s 869us/step - loss: 0.3645 - accuracy: 0.8428\n",
      "Epoch 117/1500\n",
      "33/33 [==============================] - 0s 875us/step - loss: 0.3903 - accuracy: 0.8336\n",
      "Epoch 118/1500\n",
      "33/33 [==============================] - 0s 854us/step - loss: 0.3582 - accuracy: 0.8467\n",
      "Epoch 119/1500\n",
      "33/33 [==============================] - 0s 872us/step - loss: 0.3586 - accuracy: 0.8612\n",
      "Epoch 120/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3629 - accuracy: 0.8423\n",
      "Epoch 121/1500\n",
      "33/33 [==============================] - 0s 864us/step - loss: 0.3560 - accuracy: 0.8496\n",
      "Epoch 122/1500\n",
      "33/33 [==============================] - 0s 859us/step - loss: 0.3628 - accuracy: 0.8525\n",
      "Epoch 123/1500\n",
      "33/33 [==============================] - 0s 820us/step - loss: 0.3619 - accuracy: 0.8428\n",
      "Epoch 124/1500\n",
      "33/33 [==============================] - 0s 930us/step - loss: 0.3596 - accuracy: 0.8525\n",
      "Epoch 125/1500\n",
      "33/33 [==============================] - 0s 874us/step - loss: 0.3437 - accuracy: 0.8496\n",
      "Epoch 126/1500\n",
      "33/33 [==============================] - 0s 984us/step - loss: 0.3541 - accuracy: 0.8515\n",
      "Epoch 127/1500\n",
      "33/33 [==============================] - 0s 899us/step - loss: 0.3677 - accuracy: 0.8491\n",
      "Epoch 128/1500\n",
      "33/33 [==============================] - 0s 936us/step - loss: 0.3495 - accuracy: 0.8462\n",
      "Epoch 129/1500\n",
      "33/33 [==============================] - 0s 910us/step - loss: 0.3509 - accuracy: 0.8632\n",
      "Epoch 130/1500\n",
      "33/33 [==============================] - 0s 937us/step - loss: 0.3403 - accuracy: 0.8622\n",
      "Epoch 131/1500\n",
      "33/33 [==============================] - 0s 905us/step - loss: 0.3415 - accuracy: 0.8588\n",
      "Epoch 132/1500\n",
      "33/33 [==============================] - 0s 916us/step - loss: 0.3236 - accuracy: 0.8685\n",
      "Epoch 133/1500\n",
      "33/33 [==============================] - 0s 955us/step - loss: 0.3362 - accuracy: 0.8569\n",
      "Epoch 134/1500\n",
      "33/33 [==============================] - 0s 902us/step - loss: 0.3439 - accuracy: 0.8520\n",
      "Epoch 135/1500\n",
      "33/33 [==============================] - 0s 837us/step - loss: 0.3460 - accuracy: 0.8578\n",
      "Epoch 136/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8680\n",
      "Epoch 137/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.8656\n",
      "Epoch 138/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8593\n",
      "Epoch 139/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3402 - accuracy: 0.8656\n",
      "Epoch 140/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3380 - accuracy: 0.8627\n",
      "Epoch 141/1500\n",
      "33/33 [==============================] - 0s 960us/step - loss: 0.3243 - accuracy: 0.8675\n",
      "Epoch 142/1500\n",
      "33/33 [==============================] - 0s 968us/step - loss: 0.3425 - accuracy: 0.8554\n",
      "Epoch 143/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3123 - accuracy: 0.8700\n",
      "Epoch 144/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3370 - accuracy: 0.8641\n",
      "Epoch 145/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3286 - accuracy: 0.8729\n",
      "Epoch 146/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3241 - accuracy: 0.8637\n",
      "Epoch 147/1500\n",
      "33/33 [==============================] - 0s 938us/step - loss: 0.3150 - accuracy: 0.8695\n",
      "Epoch 148/1500\n",
      "33/33 [==============================] - 0s 963us/step - loss: 0.3263 - accuracy: 0.8724\n",
      "Epoch 149/1500\n",
      "33/33 [==============================] - 0s 860us/step - loss: 0.3296 - accuracy: 0.8549\n",
      "Epoch 150/1500\n",
      "33/33 [==============================] - 0s 885us/step - loss: 0.3106 - accuracy: 0.8772\n",
      "Epoch 151/1500\n",
      "33/33 [==============================] - 0s 941us/step - loss: 0.3457 - accuracy: 0.8481\n",
      "Epoch 152/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3302 - accuracy: 0.8593\n",
      "Epoch 153/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8651\n",
      "Epoch 154/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8690\n",
      "Epoch 155/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.8782\n",
      "Epoch 156/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3038 - accuracy: 0.8738\n",
      "Epoch 157/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3056 - accuracy: 0.8753\n",
      "Epoch 158/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3102 - accuracy: 0.8714\n",
      "Epoch 159/1500\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.3199 - accuracy: 0.8690\n",
      "Epoch 160/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3178 - accuracy: 0.8748\n",
      "Epoch 161/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3142 - accuracy: 0.8719\n",
      "Epoch 162/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3063 - accuracy: 0.8743\n",
      "Epoch 163/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3129 - accuracy: 0.8705\n",
      "Epoch 164/1500\n",
      "33/33 [==============================] - 0s 919us/step - loss: 0.3128 - accuracy: 0.8777\n",
      "Epoch 165/1500\n",
      "33/33 [==============================] - 0s 925us/step - loss: 0.3215 - accuracy: 0.8714\n",
      "Epoch 166/1500\n",
      "33/33 [==============================] - 0s 991us/step - loss: 0.2969 - accuracy: 0.8772\n",
      "Epoch 167/1500\n",
      "33/33 [==============================] - 0s 919us/step - loss: 0.3143 - accuracy: 0.8680\n",
      "Epoch 168/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8772\n",
      "Epoch 169/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2983 - accuracy: 0.8869\n",
      "Epoch 170/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3245 - accuracy: 0.8705\n",
      "Epoch 171/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.3118 - accuracy: 0.8753\n",
      "Epoch 172/1500\n",
      "33/33 [==============================] - 0s 985us/step - loss: 0.3035 - accuracy: 0.8729\n",
      "Epoch 173/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2982 - accuracy: 0.8836\n",
      "Epoch 174/1500\n",
      "33/33 [==============================] - 0s 879us/step - loss: 0.2972 - accuracy: 0.8836\n",
      "Epoch 175/1500\n",
      "33/33 [==============================] - 0s 873us/step - loss: 0.2814 - accuracy: 0.8928\n",
      "Epoch 176/1500\n",
      "33/33 [==============================] - 0s 805us/step - loss: 0.3003 - accuracy: 0.8802\n",
      "Epoch 177/1500\n",
      "33/33 [==============================] - 0s 837us/step - loss: 0.2982 - accuracy: 0.8797\n",
      "Epoch 178/1500\n",
      "33/33 [==============================] - 0s 890us/step - loss: 0.3087 - accuracy: 0.8758\n",
      "Epoch 179/1500\n",
      "33/33 [==============================] - 0s 857us/step - loss: 0.3002 - accuracy: 0.8806\n",
      "Epoch 180/1500\n",
      "33/33 [==============================] - 0s 824us/step - loss: 0.3001 - accuracy: 0.8802\n",
      "Epoch 181/1500\n",
      "33/33 [==============================] - 0s 843us/step - loss: 0.3072 - accuracy: 0.8705\n",
      "Epoch 182/1500\n",
      "33/33 [==============================] - 0s 875us/step - loss: 0.2910 - accuracy: 0.8884\n",
      "Epoch 183/1500\n",
      "33/33 [==============================] - 0s 900us/step - loss: 0.2868 - accuracy: 0.8826\n",
      "Epoch 184/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2902 - accuracy: 0.8869\n",
      "Epoch 185/1500\n",
      "33/33 [==============================] - 0s 918us/step - loss: 0.2794 - accuracy: 0.8889\n",
      "Epoch 186/1500\n",
      "33/33 [==============================] - 0s 907us/step - loss: 0.2740 - accuracy: 0.8860\n",
      "Epoch 187/1500\n",
      "33/33 [==============================] - 0s 871us/step - loss: 0.3067 - accuracy: 0.8865\n",
      "Epoch 188/1500\n",
      "33/33 [==============================] - 0s 945us/step - loss: 0.2795 - accuracy: 0.8933\n",
      "Epoch 189/1500\n",
      "33/33 [==============================] - 0s 995us/step - loss: 0.2762 - accuracy: 0.8869\n",
      "Epoch 190/1500\n",
      "33/33 [==============================] - 0s 930us/step - loss: 0.3046 - accuracy: 0.8821\n",
      "Epoch 191/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2960 - accuracy: 0.8869\n",
      "Epoch 192/1500\n",
      "33/33 [==============================] - 0s 926us/step - loss: 0.2715 - accuracy: 0.8879\n",
      "Epoch 193/1500\n",
      "33/33 [==============================] - 0s 884us/step - loss: 0.2783 - accuracy: 0.8884\n",
      "Epoch 194/1500\n",
      "33/33 [==============================] - 0s 824us/step - loss: 0.2938 - accuracy: 0.8826\n",
      "Epoch 195/1500\n",
      "33/33 [==============================] - 0s 815us/step - loss: 0.2873 - accuracy: 0.8850\n",
      "Epoch 196/1500\n",
      "33/33 [==============================] - 0s 901us/step - loss: 0.2851 - accuracy: 0.8845\n",
      "Epoch 197/1500\n",
      "33/33 [==============================] - 0s 898us/step - loss: 0.2763 - accuracy: 0.8845\n",
      "Epoch 198/1500\n",
      "33/33 [==============================] - 0s 946us/step - loss: 0.2896 - accuracy: 0.8763\n",
      "Epoch 199/1500\n",
      "33/33 [==============================] - 0s 985us/step - loss: 0.2838 - accuracy: 0.8874\n",
      "Epoch 200/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2779 - accuracy: 0.8952\n",
      "Epoch 201/1500\n",
      "33/33 [==============================] - 0s 970us/step - loss: 0.2794 - accuracy: 0.8884\n",
      "Epoch 202/1500\n",
      "33/33 [==============================] - 0s 961us/step - loss: 0.2591 - accuracy: 0.8937\n",
      "Epoch 203/1500\n",
      "33/33 [==============================] - 0s 932us/step - loss: 0.2741 - accuracy: 0.8971\n",
      "Epoch 204/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2665 - accuracy: 0.8933\n",
      "Epoch 205/1500\n",
      "33/33 [==============================] - 0s 900us/step - loss: 0.2928 - accuracy: 0.8787\n",
      "Epoch 206/1500\n",
      "33/33 [==============================] - 0s 938us/step - loss: 0.2800 - accuracy: 0.8836\n",
      "Epoch 207/1500\n",
      "33/33 [==============================] - 0s 986us/step - loss: 0.2607 - accuracy: 0.8962\n",
      "Epoch 208/1500\n",
      "33/33 [==============================] - 0s 939us/step - loss: 0.2803 - accuracy: 0.8952\n",
      "Epoch 209/1500\n",
      "33/33 [==============================] - 0s 952us/step - loss: 0.2567 - accuracy: 0.8981\n",
      "Epoch 210/1500\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2671 - accuracy: 0.8942\n",
      "Epoch 211/1500\n",
      "33/33 [==============================] - 0s 882us/step - loss: 0.2608 - accuracy: 0.9049\n",
      "Epoch 212/1500\n",
      "33/33 [==============================] - 0s 847us/step - loss: 0.2696 - accuracy: 0.8884\n",
      "Epoch 213/1500\n",
      "33/33 [==============================] - 0s 829us/step - loss: 0.2789 - accuracy: 0.8967\n",
      "Epoch 214/1500\n",
      "33/33 [==============================] - 0s 863us/step - loss: 0.2706 - accuracy: 0.8874\n",
      "Epoch 215/1500\n",
      "33/33 [==============================] - 0s 848us/step - loss: 0.2750 - accuracy: 0.8840\n",
      "Epoch 216/1500\n",
      "33/33 [==============================] - 0s 815us/step - loss: 0.2618 - accuracy: 0.8962\n",
      "Epoch 217/1500\n",
      "33/33 [==============================] - 0s 866us/step - loss: 0.2581 - accuracy: 0.9020\n",
      "Epoch 218/1500\n",
      "33/33 [==============================] - 0s 880us/step - loss: 0.2605 - accuracy: 0.8981\n",
      "Epoch 219/1500\n",
      "33/33 [==============================] - 0s 821us/step - loss: 0.2701 - accuracy: 0.8952\n",
      "Epoch 220/1500\n",
      "33/33 [==============================] - 0s 888us/step - loss: 0.2627 - accuracy: 0.9020\n",
      "Epoch 221/1500\n",
      "33/33 [==============================] - 0s 963us/step - loss: 0.2547 - accuracy: 0.9044\n",
      "Epoch 222/1500\n",
      "33/33 [==============================] - 0s 842us/step - loss: 0.2680 - accuracy: 0.8928\n",
      "Epoch 223/1500\n",
      "33/33 [==============================] - 0s 859us/step - loss: 0.2701 - accuracy: 0.8879\n",
      "Epoch 224/1500\n",
      "33/33 [==============================] - 0s 844us/step - loss: 0.2726 - accuracy: 0.8908\n",
      "Epoch 225/1500\n",
      "33/33 [==============================] - 0s 808us/step - loss: 0.2569 - accuracy: 0.8957\n",
      "Epoch 226/1500\n",
      "33/33 [==============================] - 0s 867us/step - loss: 0.2636 - accuracy: 0.8865\n",
      "Epoch 227/1500\n",
      "33/33 [==============================] - 0s 869us/step - loss: 0.2490 - accuracy: 0.8986\n",
      "Epoch 228/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2581 - accuracy: 0.8991\n",
      "Epoch 229/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.2516 - accuracy: 0.8976\n",
      "Epoch 230/1500\n",
      "33/33 [==============================] - 0s 2ms/step - loss: 0.2655 - accuracy: 0.8923\n",
      "Epoch 231/1500\n",
      "33/33 [==============================] - 0s 918us/step - loss: 0.2663 - accuracy: 0.9000\n",
      "Epoch 232/1500\n",
      "33/33 [==============================] - 0s 922us/step - loss: 0.2523 - accuracy: 0.8971\n",
      "Epoch 233/1500\n",
      "33/33 [==============================] - 0s 883us/step - loss: 0.2684 - accuracy: 0.8908\n",
      "Epoch 234/1500\n",
      "33/33 [==============================] - 0s 865us/step - loss: 0.2628 - accuracy: 0.8942\n",
      "Epoch 235/1500\n",
      "33/33 [==============================] - 0s 883us/step - loss: 0.2609 - accuracy: 0.8962\n",
      "Epoch 236/1500\n",
      "33/33 [==============================] - 0s 817us/step - loss: 0.2508 - accuracy: 0.9015\n",
      "Epoch 237/1500\n",
      "33/33 [==============================] - 0s 920us/step - loss: 0.2544 - accuracy: 0.8991\n",
      "Epoch 238/1500\n",
      "33/33 [==============================] - 0s 824us/step - loss: 0.2602 - accuracy: 0.9030\n",
      "Epoch 239/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.9015\n",
      "Epoch 240/1500\n",
      "33/33 [==============================] - 0s 928us/step - loss: 0.2737 - accuracy: 0.8908\n",
      "Epoch 241/1500\n",
      "33/33 [==============================] - 0s 895us/step - loss: 0.2339 - accuracy: 0.9059\n",
      "Epoch 242/1500\n",
      "33/33 [==============================] - 0s 901us/step - loss: 0.2747 - accuracy: 0.8884\n",
      "Epoch 243/1500\n",
      "33/33 [==============================] - 0s 907us/step - loss: 0.2586 - accuracy: 0.8947\n",
      "Epoch 244/1500\n",
      "33/33 [==============================] - 0s 857us/step - loss: 0.2665 - accuracy: 0.8918\n",
      "Epoch 245/1500\n",
      "33/33 [==============================] - 0s 884us/step - loss: 0.2260 - accuracy: 0.9199\n",
      "Epoch 246/1500\n",
      "33/33 [==============================] - 0s 871us/step - loss: 0.2323 - accuracy: 0.9068\n",
      "Epoch 247/1500\n",
      "33/33 [==============================] - 0s 861us/step - loss: 0.2414 - accuracy: 0.9034\n",
      "Epoch 248/1500\n",
      "33/33 [==============================] - 0s 854us/step - loss: 0.2536 - accuracy: 0.9049\n",
      "Epoch 249/1500\n",
      "33/33 [==============================] - 0s 827us/step - loss: 0.2395 - accuracy: 0.9039\n",
      "Epoch 250/1500\n",
      "33/33 [==============================] - 0s 850us/step - loss: 0.2417 - accuracy: 0.9039\n",
      "Epoch 251/1500\n",
      "33/33 [==============================] - 0s 850us/step - loss: 0.2613 - accuracy: 0.8947\n",
      "Epoch 252/1500\n",
      "33/33 [==============================] - 0s 861us/step - loss: 0.2464 - accuracy: 0.9078\n",
      "Epoch 253/1500\n",
      "33/33 [==============================] - 0s 935us/step - loss: 0.2359 - accuracy: 0.9064\n",
      "Epoch 254/1500\n",
      "33/33 [==============================] - 0s 889us/step - loss: 0.2361 - accuracy: 0.9034\n",
      "Epoch 255/1500\n",
      "33/33 [==============================] - 0s 848us/step - loss: 0.2475 - accuracy: 0.9020\n",
      "Epoch 256/1500\n",
      "33/33 [==============================] - 0s 853us/step - loss: 0.2431 - accuracy: 0.8996\n",
      "Epoch 257/1500\n",
      "33/33 [==============================] - 0s 797us/step - loss: 0.2351 - accuracy: 0.9165\n",
      "Epoch 258/1500\n",
      "33/33 [==============================] - 0s 835us/step - loss: 0.2604 - accuracy: 0.9025\n",
      "Epoch 259/1500\n",
      "33/33 [==============================] - 0s 866us/step - loss: 0.2344 - accuracy: 0.9098\n",
      "Epoch 260/1500\n",
      "33/33 [==============================] - 0s 803us/step - loss: 0.2338 - accuracy: 0.9127\n",
      "Epoch 261/1500\n",
      "33/33 [==============================] - 0s 861us/step - loss: 0.2374 - accuracy: 0.9098\n",
      "Epoch 262/1500\n",
      "33/33 [==============================] - 0s 832us/step - loss: 0.2394 - accuracy: 0.9073\n",
      "Epoch 263/1500\n",
      "33/33 [==============================] - 0s 935us/step - loss: 0.2242 - accuracy: 0.9107\n",
      "Epoch 264/1500\n",
      "33/33 [==============================] - 0s 857us/step - loss: 0.2305 - accuracy: 0.9098\n",
      "Epoch 265/1500\n",
      "33/33 [==============================] - 0s 987us/step - loss: 0.2442 - accuracy: 0.9054\n",
      "Epoch 266/1500\n",
      "33/33 [==============================] - 0s 877us/step - loss: 0.2371 - accuracy: 0.9102\n",
      "Epoch 267/1500\n",
      "33/33 [==============================] - 0s 877us/step - loss: 0.2176 - accuracy: 0.9141\n",
      "Epoch 268/1500\n",
      "33/33 [==============================] - 0s 886us/step - loss: 0.2237 - accuracy: 0.9093\n",
      "Epoch 269/1500\n",
      "33/33 [==============================] - 0s 851us/step - loss: 0.2267 - accuracy: 0.9122\n",
      "Epoch 270/1500\n",
      "33/33 [==============================] - 0s 839us/step - loss: 0.2272 - accuracy: 0.9131\n",
      "Epoch 271/1500\n",
      "33/33 [==============================] - 0s 818us/step - loss: 0.2242 - accuracy: 0.9102\n",
      "Epoch 272/1500\n",
      "33/33 [==============================] - 0s 844us/step - loss: 0.2366 - accuracy: 0.9068\n",
      "Epoch 273/1500\n",
      "33/33 [==============================] - 0s 823us/step - loss: 0.2278 - accuracy: 0.9098\n",
      "Epoch 274/1500\n",
      "33/33 [==============================] - 0s 845us/step - loss: 0.2470 - accuracy: 0.8967\n",
      "Epoch 275/1500\n",
      "33/33 [==============================] - 0s 852us/step - loss: 0.2262 - accuracy: 0.9044\n",
      "Epoch 276/1500\n",
      "33/33 [==============================] - 0s 944us/step - loss: 0.2383 - accuracy: 0.9030\n",
      "Epoch 277/1500\n",
      "33/33 [==============================] - 0s 972us/step - loss: 0.2225 - accuracy: 0.9088\n",
      "Epoch 278/1500\n",
      "33/33 [==============================] - 0s 869us/step - loss: 0.2360 - accuracy: 0.9088\n",
      "Epoch 279/1500\n",
      "33/33 [==============================] - 0s 875us/step - loss: 0.2318 - accuracy: 0.9098\n",
      "Epoch 280/1500\n",
      "33/33 [==============================] - 0s 937us/step - loss: 0.2242 - accuracy: 0.9190\n",
      "Epoch 281/1500\n",
      "33/33 [==============================] - 0s 925us/step - loss: 0.2130 - accuracy: 0.9204\n",
      "Epoch 282/1500\n",
      "33/33 [==============================] - 0s 880us/step - loss: 0.2206 - accuracy: 0.9127\n",
      "Epoch 283/1500\n",
      "33/33 [==============================] - 0s 853us/step - loss: 0.2273 - accuracy: 0.9098\n",
      "Epoch 284/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.2380 - accuracy: 0.9034\n",
      "Epoch 285/1500\n",
      "33/33 [==============================] - 0s 832us/step - loss: 0.2266 - accuracy: 0.9098\n",
      "Epoch 286/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.2185 - accuracy: 0.9131\n",
      "Epoch 287/1500\n",
      "33/33 [==============================] - 0s 848us/step - loss: 0.2049 - accuracy: 0.9204\n",
      "Epoch 288/1500\n",
      "33/33 [==============================] - 0s 811us/step - loss: 0.2382 - accuracy: 0.8981\n",
      "Epoch 289/1500\n",
      "33/33 [==============================] - 0s 798us/step - loss: 0.2235 - accuracy: 0.9195\n",
      "Epoch 290/1500\n",
      "33/33 [==============================] - 0s 850us/step - loss: 0.2293 - accuracy: 0.9146\n",
      "Epoch 291/1500\n",
      "33/33 [==============================] - 0s 848us/step - loss: 0.2269 - accuracy: 0.9102\n",
      "Epoch 292/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.2119 - accuracy: 0.9127\n",
      "Epoch 293/1500\n",
      "33/33 [==============================] - 0s 917us/step - loss: 0.2198 - accuracy: 0.9117\n",
      "Epoch 294/1500\n",
      "33/33 [==============================] - 0s 875us/step - loss: 0.2374 - accuracy: 0.9025\n",
      "Epoch 295/1500\n",
      "33/33 [==============================] - 0s 852us/step - loss: 0.2081 - accuracy: 0.9195\n",
      "Epoch 296/1500\n",
      "33/33 [==============================] - 0s 844us/step - loss: 0.2324 - accuracy: 0.9117\n",
      "Epoch 297/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.2122 - accuracy: 0.9146\n",
      "Epoch 298/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.2153 - accuracy: 0.9161\n",
      "Epoch 299/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.2312 - accuracy: 0.9146\n",
      "Epoch 300/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.2208 - accuracy: 0.9122\n",
      "Epoch 301/1500\n",
      "33/33 [==============================] - 0s 835us/step - loss: 0.2377 - accuracy: 0.8962\n",
      "Epoch 302/1500\n",
      "33/33 [==============================] - 0s 828us/step - loss: 0.2216 - accuracy: 0.9093\n",
      "Epoch 303/1500\n",
      "33/33 [==============================] - 0s 819us/step - loss: 0.2147 - accuracy: 0.9170\n",
      "Epoch 304/1500\n",
      "33/33 [==============================] - 0s 879us/step - loss: 0.2257 - accuracy: 0.9117\n",
      "Epoch 305/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.2047 - accuracy: 0.9175\n",
      "Epoch 306/1500\n",
      "33/33 [==============================] - 0s 864us/step - loss: 0.2133 - accuracy: 0.9199\n",
      "Epoch 307/1500\n",
      "33/33 [==============================] - 0s 870us/step - loss: 0.2197 - accuracy: 0.9073\n",
      "Epoch 308/1500\n",
      "33/33 [==============================] - 0s 897us/step - loss: 0.2098 - accuracy: 0.9136\n",
      "Epoch 309/1500\n",
      "33/33 [==============================] - 0s 909us/step - loss: 0.1928 - accuracy: 0.9292\n",
      "Epoch 310/1500\n",
      "33/33 [==============================] - 0s 839us/step - loss: 0.2421 - accuracy: 0.9044\n",
      "Epoch 311/1500\n",
      "33/33 [==============================] - 0s 844us/step - loss: 0.1976 - accuracy: 0.9253\n",
      "Epoch 312/1500\n",
      "33/33 [==============================] - 0s 860us/step - loss: 0.2043 - accuracy: 0.9204\n",
      "Epoch 313/1500\n",
      "33/33 [==============================] - 0s 835us/step - loss: 0.2203 - accuracy: 0.9122\n",
      "Epoch 314/1500\n",
      "33/33 [==============================] - 0s 832us/step - loss: 0.2242 - accuracy: 0.9122\n",
      "Epoch 315/1500\n",
      "33/33 [==============================] - 0s 837us/step - loss: 0.2041 - accuracy: 0.9224\n",
      "Epoch 316/1500\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.2136 - accuracy: 0.9127\n",
      "Epoch 317/1500\n",
      "33/33 [==============================] - 0s 845us/step - loss: 0.2088 - accuracy: 0.9190\n",
      "Epoch 318/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.2162 - accuracy: 0.9175\n",
      "Epoch 319/1500\n",
      "33/33 [==============================] - 0s 803us/step - loss: 0.2183 - accuracy: 0.9136\n",
      "Epoch 320/1500\n",
      "33/33 [==============================] - 0s 835us/step - loss: 0.1979 - accuracy: 0.9238\n",
      "Epoch 321/1500\n",
      "33/33 [==============================] - 0s 858us/step - loss: 0.2110 - accuracy: 0.9229\n",
      "Epoch 322/1500\n",
      "33/33 [==============================] - 0s 798us/step - loss: 0.1958 - accuracy: 0.9262\n",
      "Epoch 323/1500\n",
      "33/33 [==============================] - 0s 870us/step - loss: 0.2132 - accuracy: 0.9175\n",
      "Epoch 324/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.2193 - accuracy: 0.9161\n",
      "Epoch 325/1500\n",
      "33/33 [==============================] - 0s 884us/step - loss: 0.2058 - accuracy: 0.9209\n",
      "Epoch 326/1500\n",
      "33/33 [==============================] - 0s 851us/step - loss: 0.2104 - accuracy: 0.9161\n",
      "Epoch 327/1500\n",
      "33/33 [==============================] - 0s 856us/step - loss: 0.2087 - accuracy: 0.9165\n",
      "Epoch 328/1500\n",
      "33/33 [==============================] - 0s 828us/step - loss: 0.1989 - accuracy: 0.9219\n",
      "Epoch 329/1500\n",
      "33/33 [==============================] - 0s 840us/step - loss: 0.1943 - accuracy: 0.9253\n",
      "Epoch 330/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.1945 - accuracy: 0.9262\n",
      "Epoch 331/1500\n",
      "33/33 [==============================] - 0s 884us/step - loss: 0.2001 - accuracy: 0.9267\n",
      "Epoch 332/1500\n",
      "33/33 [==============================] - 0s 825us/step - loss: 0.1953 - accuracy: 0.9204\n",
      "Epoch 333/1500\n",
      "33/33 [==============================] - 0s 889us/step - loss: 0.2091 - accuracy: 0.9156\n",
      "Epoch 334/1500\n",
      "33/33 [==============================] - 0s 959us/step - loss: 0.2084 - accuracy: 0.9165\n",
      "Epoch 335/1500\n",
      "33/33 [==============================] - 0s 994us/step - loss: 0.2272 - accuracy: 0.9127\n",
      "Epoch 336/1500\n",
      "33/33 [==============================] - 0s 890us/step - loss: 0.2004 - accuracy: 0.9229\n",
      "Epoch 337/1500\n",
      "33/33 [==============================] - 0s 847us/step - loss: 0.2038 - accuracy: 0.9170\n",
      "Epoch 338/1500\n",
      "33/33 [==============================] - 0s 877us/step - loss: 0.1939 - accuracy: 0.9287\n",
      "Epoch 339/1500\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.2211 - accuracy: 0.8594Restoring model weights from the end of the best epoch: 309.\n",
      "33/33 [==============================] - 0s 954us/step - loss: 0.1980 - accuracy: 0.9190\n",
      "Epoch 339: early stopping\n",
      "9/9 [==============================] - 0s 748us/step - loss: 0.7026 - accuracy: 0.7015\n",
      "9/9 [==============================] - 0s 665us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (18/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "Final Test Results - Loss: 0.702578067779541, Accuracy: 0.7014925479888916, Precision: 0.7014230979748222, Recall: 0.669553163731246, F1 Score: 0.6559595890489059\n",
      "Confusion Matrix:\n",
      " [[124   2  34]\n",
      " [ 33  40   0]\n",
      " [ 11   0  24]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "053A     6\n",
      "108A     6\n",
      "023A     6\n",
      "007A     6\n",
      "025C     5\n",
      "044A     5\n",
      "023B     5\n",
      "021A     5\n",
      "070A     5\n",
      "034A     5\n",
      "003A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "104A     4\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "113A     3\n",
      "056A     3\n",
      "018A     2\n",
      "038A     2\n",
      "025B     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "102A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "073A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "063A    11\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "095A     8\n",
      "037A     6\n",
      "008A     6\n",
      "109A     6\n",
      "075A     5\n",
      "105A     4\n",
      "064A     3\n",
      "060A     3\n",
      "093A     2\n",
      "087A     2\n",
      "011A     2\n",
      "061A     2\n",
      "076A     1\n",
      "043A     1\n",
      "091A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    238\n",
      "F    229\n",
      "X    221\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    127\n",
      "M     99\n",
      "F     23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 015A, 103A, 071A, 097B, 028A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 050A, 049...\n",
      "senior    [097A, 057A, 106A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 001A, 019A, 067A, 022A, 029A, 095A, 072...\n",
      "kitten                             [046A, 109A, 043A, 045A]\n",
      "senior                             [093A, 051B, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '044A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '043A'\n",
      " '045A' '046A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'040A'}\n",
      "Moved to Test Set:\n",
      "{'040A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '041A' '042A'\n",
      " '044A' '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '040A'\n",
      " '043A' '045A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "Length of X_train_val:\n",
      "741\n",
      "Length of y_train_val:\n",
      "741\n",
      "Length of groups_train_val:\n",
      "741\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     155\n",
      "kitten     79\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten    145\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     155\n",
      "kitten     26\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 866, 2: 815, 1: 725})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.1701 - accuracy: 0.4896\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.8790 - accuracy: 0.6197\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.8037 - accuracy: 0.6604\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.7724 - accuracy: 0.6775\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.7278 - accuracy: 0.7070\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.6879 - accuracy: 0.7178\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.6838 - accuracy: 0.7207\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.6496 - accuracy: 0.7344\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 896us/step - loss: 0.6250 - accuracy: 0.7481\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.6131 - accuracy: 0.7531\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.6243 - accuracy: 0.7481\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.5852 - accuracy: 0.7581\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.5738 - accuracy: 0.7614\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.5839 - accuracy: 0.7556\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5638 - accuracy: 0.7681\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.5491 - accuracy: 0.7664\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.5602 - accuracy: 0.7747\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5466 - accuracy: 0.7760\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.5183 - accuracy: 0.7889\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.4921 - accuracy: 0.8071\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.5171 - accuracy: 0.7930\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.5073 - accuracy: 0.7868\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.4953 - accuracy: 0.7909\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 984us/step - loss: 0.4826 - accuracy: 0.7993\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 936us/step - loss: 0.5066 - accuracy: 0.7880\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.4847 - accuracy: 0.8009\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.4880 - accuracy: 0.8009\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.4815 - accuracy: 0.8059\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.4686 - accuracy: 0.8059\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.4583 - accuracy: 0.8059\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.4597 - accuracy: 0.8080\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.4720 - accuracy: 0.8134\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.4660 - accuracy: 0.8096\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.4535 - accuracy: 0.8159\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.4542 - accuracy: 0.8096\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.4530 - accuracy: 0.8084\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.4319 - accuracy: 0.8300\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.4451 - accuracy: 0.8117\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.4438 - accuracy: 0.8184\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.4397 - accuracy: 0.8192\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.4249 - accuracy: 0.8221\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.4162 - accuracy: 0.8275\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.4286 - accuracy: 0.8238\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.4308 - accuracy: 0.8246\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.4135 - accuracy: 0.8329\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.4099 - accuracy: 0.8292\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 997us/step - loss: 0.4080 - accuracy: 0.8292\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.4166 - accuracy: 0.8325\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.4158 - accuracy: 0.8283\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 961us/step - loss: 0.3936 - accuracy: 0.8392\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4075 - accuracy: 0.8288\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4021 - accuracy: 0.8350\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4110 - accuracy: 0.8358\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 947us/step - loss: 0.3969 - accuracy: 0.8441\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.3969 - accuracy: 0.8325\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 944us/step - loss: 0.3932 - accuracy: 0.8313\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.4083 - accuracy: 0.8250\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.3779 - accuracy: 0.8458\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.3827 - accuracy: 0.8412\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.3834 - accuracy: 0.8446\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.3734 - accuracy: 0.8466\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.3991 - accuracy: 0.8396\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3798 - accuracy: 0.8416\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3780 - accuracy: 0.8504\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3869 - accuracy: 0.8450\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3771 - accuracy: 0.8483\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3595 - accuracy: 0.8579\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.3662 - accuracy: 0.8458\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.3717 - accuracy: 0.8466\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.3527 - accuracy: 0.8599\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3611 - accuracy: 0.8487\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.3655 - accuracy: 0.8433\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3383 - accuracy: 0.8637\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.3576 - accuracy: 0.8487\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 978us/step - loss: 0.3594 - accuracy: 0.8554\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.3615 - accuracy: 0.8533\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.3389 - accuracy: 0.8633\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.3515 - accuracy: 0.8533\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 979us/step - loss: 0.3403 - accuracy: 0.8628\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 925us/step - loss: 0.3409 - accuracy: 0.8608\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.3301 - accuracy: 0.8658\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.3300 - accuracy: 0.8624\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8628\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3386 - accuracy: 0.8587\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.3477 - accuracy: 0.8529\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.3343 - accuracy: 0.8583\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.3347 - accuracy: 0.8703\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.3320 - accuracy: 0.8637\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3249 - accuracy: 0.8658\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3198 - accuracy: 0.8678\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.3268 - accuracy: 0.8599\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 955us/step - loss: 0.3271 - accuracy: 0.8753\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.3309 - accuracy: 0.8583\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.3209 - accuracy: 0.8695\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3240 - accuracy: 0.8720\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.3284 - accuracy: 0.8624\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.3221 - accuracy: 0.8670\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.3186 - accuracy: 0.8724\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.3052 - accuracy: 0.8815\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.3020 - accuracy: 0.8757\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.3137 - accuracy: 0.8712\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3019 - accuracy: 0.8728\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.3278 - accuracy: 0.8691\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.3025 - accuracy: 0.8782\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.3157 - accuracy: 0.8728\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.3188 - accuracy: 0.8749\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.3179 - accuracy: 0.8707\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.3092 - accuracy: 0.8662\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.3100 - accuracy: 0.8803\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3041 - accuracy: 0.8820\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.3046 - accuracy: 0.8795\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.3018 - accuracy: 0.8828\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3088 - accuracy: 0.8728\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.3056 - accuracy: 0.8699\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3102 - accuracy: 0.8736\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.3035 - accuracy: 0.8791\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.3015 - accuracy: 0.8774\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.3039 - accuracy: 0.8757\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2937 - accuracy: 0.8836\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2948 - accuracy: 0.8857\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.2879 - accuracy: 0.8849\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3021 - accuracy: 0.8761\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2775 - accuracy: 0.8857\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.2749 - accuracy: 0.8878\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3034 - accuracy: 0.8716\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2771 - accuracy: 0.8832\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2703 - accuracy: 0.8948\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 896us/step - loss: 0.2751 - accuracy: 0.8882\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.2897 - accuracy: 0.8753\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.2790 - accuracy: 0.8845\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2793 - accuracy: 0.8878\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 896us/step - loss: 0.2682 - accuracy: 0.8924\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.2911 - accuracy: 0.8840\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2817 - accuracy: 0.8886\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2860 - accuracy: 0.8849\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.2814 - accuracy: 0.8915\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2721 - accuracy: 0.8878\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2768 - accuracy: 0.8903\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.2847 - accuracy: 0.8828\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2601 - accuracy: 0.8965\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2849 - accuracy: 0.8886\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 990us/step - loss: 0.2713 - accuracy: 0.8928\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2659 - accuracy: 0.8919\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2739 - accuracy: 0.8936\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2665 - accuracy: 0.8911\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2555 - accuracy: 0.9007\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.8878\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2675 - accuracy: 0.8953\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2595 - accuracy: 0.8973\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.2666 - accuracy: 0.8978\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 929us/step - loss: 0.2604 - accuracy: 0.8915\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 909us/step - loss: 0.2650 - accuracy: 0.8965\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 917us/step - loss: 0.2624 - accuracy: 0.8944\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2624 - accuracy: 0.8940\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2468 - accuracy: 0.8961\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.2698 - accuracy: 0.8845\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.2682 - accuracy: 0.8882\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.2515 - accuracy: 0.8990\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2633 - accuracy: 0.8932\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.2578 - accuracy: 0.8982\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.2529 - accuracy: 0.8953\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.2461 - accuracy: 0.9081\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.2602 - accuracy: 0.8957\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2410 - accuracy: 0.9027\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2441 - accuracy: 0.8957\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.2457 - accuracy: 0.8932\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2505 - accuracy: 0.9015\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2363 - accuracy: 0.9073\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2580 - accuracy: 0.9040\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2518 - accuracy: 0.8986\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2625 - accuracy: 0.8944\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2701 - accuracy: 0.8907\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2371 - accuracy: 0.9081\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2498 - accuracy: 0.9002\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2459 - accuracy: 0.8924\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2510 - accuracy: 0.8973\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2226 - accuracy: 0.9065\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2427 - accuracy: 0.9044\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.2424 - accuracy: 0.8957\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 997us/step - loss: 0.2600 - accuracy: 0.8932\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.2522 - accuracy: 0.9002\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2465 - accuracy: 0.8924\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.2281 - accuracy: 0.9061\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2319 - accuracy: 0.9077\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2475 - accuracy: 0.9044\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2366 - accuracy: 0.9048\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2262 - accuracy: 0.9094\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2374 - accuracy: 0.9036\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2358 - accuracy: 0.9131\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2416 - accuracy: 0.9002\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.2154 - accuracy: 0.9185\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2380 - accuracy: 0.9086\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2120 - accuracy: 0.9165\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.2452 - accuracy: 0.9007\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2328 - accuracy: 0.9061\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.2277 - accuracy: 0.9065\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 949us/step - loss: 0.2527 - accuracy: 0.8969\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.2332 - accuracy: 0.9148\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.2338 - accuracy: 0.9015\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.2182 - accuracy: 0.9144\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.2231 - accuracy: 0.9119\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2245 - accuracy: 0.9094\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2164 - accuracy: 0.9152\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2234 - accuracy: 0.9065\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2171 - accuracy: 0.9152\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.2175 - accuracy: 0.9156\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.2208 - accuracy: 0.9098\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.2289 - accuracy: 0.9106\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.2217 - accuracy: 0.9144\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2095 - accuracy: 0.9140\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2217 - accuracy: 0.9106\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2185 - accuracy: 0.9123\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2066 - accuracy: 0.9169\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2293 - accuracy: 0.9073\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2230 - accuracy: 0.9135\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2181 - accuracy: 0.9152\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2218 - accuracy: 0.9152\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2315 - accuracy: 0.9044\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2250 - accuracy: 0.9073\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.2127 - accuracy: 0.9198\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2298 - accuracy: 0.9094\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.2232 - accuracy: 0.9123\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2091 - accuracy: 0.9165\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2335 - accuracy: 0.9019\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2094 - accuracy: 0.9214\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2061 - accuracy: 0.9210\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2038 - accuracy: 0.9231\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2070 - accuracy: 0.9181\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2254 - accuracy: 0.9131\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2027 - accuracy: 0.9202\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9231\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.2076 - accuracy: 0.9194\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2129 - accuracy: 0.9090\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.2079 - accuracy: 0.9148\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2090 - accuracy: 0.9115\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2036 - accuracy: 0.9219\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2068 - accuracy: 0.9190\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2185 - accuracy: 0.9144\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2074 - accuracy: 0.9235\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2136 - accuracy: 0.9177\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.1972 - accuracy: 0.9244\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1901 - accuracy: 0.9239\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1927 - accuracy: 0.9227\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2065 - accuracy: 0.9190\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2035 - accuracy: 0.9181\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2102 - accuracy: 0.9202\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1947 - accuracy: 0.9219\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.1834 - accuracy: 0.9244\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2035 - accuracy: 0.9210\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 990us/step - loss: 0.2007 - accuracy: 0.9231\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1976 - accuracy: 0.9206\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2073 - accuracy: 0.9173\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1928 - accuracy: 0.9273\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1893 - accuracy: 0.9252\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2002 - accuracy: 0.9206\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 988us/step - loss: 0.1882 - accuracy: 0.9293\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.1922 - accuracy: 0.9256\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.1934 - accuracy: 0.9235\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.1884 - accuracy: 0.9293\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.1966 - accuracy: 0.9148\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.1913 - accuracy: 0.9244\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.1834 - accuracy: 0.9206\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1970 - accuracy: 0.9231\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1831 - accuracy: 0.9339\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.2015 - accuracy: 0.9210\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.1940 - accuracy: 0.9264\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.1899 - accuracy: 0.9285\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.1844 - accuracy: 0.9335\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.1918 - accuracy: 0.9181\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.1986 - accuracy: 0.9219\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1946 - accuracy: 0.9206\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.1901 - accuracy: 0.9252\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.1849 - accuracy: 0.9281\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2194 - accuracy: 0.9073\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.1874 - accuracy: 0.9314\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1920 - accuracy: 0.9227\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.1914 - accuracy: 0.9239\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.1743 - accuracy: 0.9343\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.1852 - accuracy: 0.9227\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.1698 - accuracy: 0.9298\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.1858 - accuracy: 0.9268\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1822 - accuracy: 0.9260\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.1727 - accuracy: 0.9318\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.1928 - accuracy: 0.9281\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.1969 - accuracy: 0.9264\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 963us/step - loss: 0.1862 - accuracy: 0.9252\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 940us/step - loss: 0.1805 - accuracy: 0.9306\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.1886 - accuracy: 0.9285\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.1647 - accuracy: 0.9360\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1853 - accuracy: 0.9293\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1836 - accuracy: 0.9273\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1934 - accuracy: 0.9214\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.1875 - accuracy: 0.9231\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.1768 - accuracy: 0.9314\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.1704 - accuracy: 0.9364\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.1816 - accuracy: 0.9306\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.1876 - accuracy: 0.9235\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1742 - accuracy: 0.9331\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1780 - accuracy: 0.9277\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.1787 - accuracy: 0.9343\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.1685 - accuracy: 0.9364\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.1633 - accuracy: 0.9356\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1793 - accuracy: 0.9277\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1787 - accuracy: 0.9306\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1674 - accuracy: 0.9314\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1843 - accuracy: 0.9298\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.1676 - accuracy: 0.9352\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.1724 - accuracy: 0.9331\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.1731 - accuracy: 0.9364\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1759 - accuracy: 0.9323\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1761 - accuracy: 0.9323\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1696 - accuracy: 0.9327\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.1654 - accuracy: 0.9356\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1687 - accuracy: 0.9318\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.1904 - accuracy: 0.9306\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1730 - accuracy: 0.9289\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9293\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.9335\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.9331\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9285\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9314\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.9260\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.9335\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.1704 - accuracy: 0.9372\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.9306\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 989us/step - loss: 0.1666 - accuracy: 0.9410\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.1609 - accuracy: 0.9360\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.1659 - accuracy: 0.9364\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.1633 - accuracy: 0.9368\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 972us/step - loss: 0.1748 - accuracy: 0.9323\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9323\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 989us/step - loss: 0.1724 - accuracy: 0.9327\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9406\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 925us/step - loss: 0.1687 - accuracy: 0.9331\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.1658 - accuracy: 0.9360\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.1509 - accuracy: 0.9422\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 917us/step - loss: 0.1737 - accuracy: 0.9285\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.1689 - accuracy: 0.9331\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.1705 - accuracy: 0.9327\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 972us/step - loss: 0.1601 - accuracy: 0.9414\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 970us/step - loss: 0.1675 - accuracy: 0.9339\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.1687 - accuracy: 0.9381\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.1651 - accuracy: 0.9343\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.1438 - accuracy: 0.9497\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.1579 - accuracy: 0.9439\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.1640 - accuracy: 0.9410\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.1721 - accuracy: 0.9360\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.1586 - accuracy: 0.9393\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1678 - accuracy: 0.9372\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1586 - accuracy: 0.9393\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1564 - accuracy: 0.9414\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.1475 - accuracy: 0.9451\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.1736 - accuracy: 0.9323\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.1596 - accuracy: 0.9372\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1516 - accuracy: 0.9401\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1620 - accuracy: 0.9397\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.1535 - accuracy: 0.9377\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.1674 - accuracy: 0.9335\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.1449 - accuracy: 0.9468\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1467 - accuracy: 0.9497\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.1623 - accuracy: 0.9385\n",
      "Epoch 362/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.1513 - accuracy: 0.9431\n",
      "Epoch 363/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.1482 - accuracy: 0.9406\n",
      "Epoch 364/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1543 - accuracy: 0.9426\n",
      "Epoch 365/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.1684 - accuracy: 0.9331\n",
      "Epoch 366/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.1456 - accuracy: 0.9422\n",
      "Epoch 367/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.1530 - accuracy: 0.9389\n",
      "Epoch 368/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1548 - accuracy: 0.9414\n",
      "Epoch 369/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.1624 - accuracy: 0.9360\n",
      "Epoch 370/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1624 - accuracy: 0.9401\n",
      "Epoch 371/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1423 - accuracy: 0.9460\n",
      "Epoch 372/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.1693 - accuracy: 0.9310\n",
      "Epoch 373/1500\n",
      "38/38 [==============================] - 0s 969us/step - loss: 0.1508 - accuracy: 0.9414\n",
      "Epoch 374/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1462 - accuracy: 0.9468\n",
      "Epoch 375/1500\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.1586 - accuracy: 0.9418\n",
      "Epoch 376/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1689 - accuracy: 0.9364\n",
      "Epoch 377/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1619 - accuracy: 0.9431\n",
      "Epoch 378/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.1531 - accuracy: 0.9418\n",
      "Epoch 379/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.1469 - accuracy: 0.9422\n",
      "Epoch 380/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.1593 - accuracy: 0.9410\n",
      "Epoch 381/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.1623 - accuracy: 0.9389\n",
      "Epoch 382/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.1649 - accuracy: 0.9422\n",
      "Epoch 383/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.1541 - accuracy: 0.9410\n",
      "Epoch 384/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.1584 - accuracy: 0.9368\n",
      "Epoch 385/1500\n",
      "38/38 [==============================] - 0s 961us/step - loss: 0.1363 - accuracy: 0.9505\n",
      "Epoch 386/1500\n",
      "38/38 [==============================] - 0s 999us/step - loss: 0.1477 - accuracy: 0.9368\n",
      "Epoch 387/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9422\n",
      "Epoch 388/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1474 - accuracy: 0.9431\n",
      "Epoch 389/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1487 - accuracy: 0.9385\n",
      "Epoch 390/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1441 - accuracy: 0.9439\n",
      "Epoch 391/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.1492 - accuracy: 0.9389\n",
      "Epoch 392/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.1497 - accuracy: 0.9426\n",
      "Epoch 393/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.1490 - accuracy: 0.9435\n",
      "Epoch 394/1500\n",
      "38/38 [==============================] - 0s 956us/step - loss: 0.1531 - accuracy: 0.9385\n",
      "Epoch 395/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.1655 - accuracy: 0.9372\n",
      "Epoch 396/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 0.1526 - accuracy: 0.9385\n",
      "Epoch 397/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.1424 - accuracy: 0.9518\n",
      "Epoch 398/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1407 - accuracy: 0.9468\n",
      "Epoch 399/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.1425 - accuracy: 0.9443\n",
      "Epoch 400/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.1710 - accuracy: 0.9347\n",
      "Epoch 401/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1448 - accuracy: 0.9456\n",
      "Epoch 402/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.1408 - accuracy: 0.9468\n",
      "Epoch 403/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1500 - accuracy: 0.9422\n",
      "Epoch 404/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.1738 - accuracy: 0.9281\n",
      "Epoch 405/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.1335 - accuracy: 0.9493\n",
      "Epoch 406/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9381\n",
      "Epoch 407/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.1551 - accuracy: 0.9414\n",
      "Epoch 408/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.1302 - accuracy: 0.9514\n",
      "Epoch 409/1500\n",
      "38/38 [==============================] - 0s 955us/step - loss: 0.1599 - accuracy: 0.9377\n",
      "Epoch 410/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.1459 - accuracy: 0.9489\n",
      "Epoch 411/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.1419 - accuracy: 0.9472\n",
      "Epoch 412/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.1437 - accuracy: 0.9472\n",
      "Epoch 413/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.1470 - accuracy: 0.9489\n",
      "Epoch 414/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.1477 - accuracy: 0.9476\n",
      "Epoch 415/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.1587 - accuracy: 0.9389\n",
      "Epoch 416/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1544 - accuracy: 0.9456\n",
      "Epoch 417/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1392 - accuracy: 0.9480\n",
      "Epoch 418/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.1490 - accuracy: 0.9464\n",
      "Epoch 419/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.1456 - accuracy: 0.9460\n",
      "Epoch 420/1500\n",
      "38/38 [==============================] - 0s 962us/step - loss: 0.1438 - accuracy: 0.9480\n",
      "Epoch 421/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.1451 - accuracy: 0.9447\n",
      "Epoch 422/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1425 - accuracy: 0.9426\n",
      "Epoch 423/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1395 - accuracy: 0.9472\n",
      "Epoch 424/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.1451 - accuracy: 0.9426\n",
      "Epoch 425/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1310 - accuracy: 0.9518\n",
      "Epoch 426/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.1483 - accuracy: 0.9401\n",
      "Epoch 427/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.1636 - accuracy: 0.9360\n",
      "Epoch 428/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.1476 - accuracy: 0.9439\n",
      "Epoch 429/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1387 - accuracy: 0.9472\n",
      "Epoch 430/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1686 - accuracy: 0.9389\n",
      "Epoch 431/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.1337 - accuracy: 0.9522\n",
      "Epoch 432/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1439 - accuracy: 0.9476\n",
      "Epoch 433/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.1409 - accuracy: 0.9443\n",
      "Epoch 434/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.1558 - accuracy: 0.9381\n",
      "Epoch 435/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.1406 - accuracy: 0.9472\n",
      "Epoch 436/1500\n",
      "38/38 [==============================] - 0s 908us/step - loss: 0.1322 - accuracy: 0.9472\n",
      "Epoch 437/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1437 - accuracy: 0.9476\n",
      "Epoch 438/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.2965 - accuracy: 0.8594Restoring model weights from the end of the best epoch: 408.\n",
      "38/38 [==============================] - 0s 985us/step - loss: 0.1411 - accuracy: 0.9489\n",
      "Epoch 438: early stopping\n",
      "7/7 [==============================] - 0s 806us/step - loss: 0.6602 - accuracy: 0.7347\n",
      "7/7 [==============================] - 0s 728us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.69 (20/29)\n",
      "Before appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "After appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "Final Test Results - Loss: 0.6602287888526917, Accuracy: 0.7346938848495483, Precision: 0.534623889101501, Recall: 0.5952302178108629, F1 Score: 0.5543419570818099\n",
      "Confusion Matrix:\n",
      " [[119  12  24]\n",
      " [  3  23   0]\n",
      " [ 12   1   2]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "042A    14\n",
      "097B    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "095A     8\n",
      "117A     7\n",
      "050A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "008A     6\n",
      "109A     6\n",
      "053A     6\n",
      "037A     6\n",
      "108A     6\n",
      "023A     6\n",
      "023B     5\n",
      "075A     5\n",
      "021A     5\n",
      "105A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "003A     4\n",
      "056A     3\n",
      "113A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "012A     3\n",
      "058A     3\n",
      "061A     2\n",
      "011A     2\n",
      "102A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "069A     2\n",
      "041A     1\n",
      "019B     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "096A     1\n",
      "076A     1\n",
      "088A     1\n",
      "092A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "073A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "000B    19\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "025A    11\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "010A     8\n",
      "013B     8\n",
      "007A     6\n",
      "070A     5\n",
      "044A     5\n",
      "034A     5\n",
      "025C     5\n",
      "104A     4\n",
      "009A     4\n",
      "006A     3\n",
      "018A     2\n",
      "054A     2\n",
      "025B     2\n",
      "032A     2\n",
      "049A     1\n",
      "026C     1\n",
      "066A     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    322\n",
      "M    241\n",
      "F    159\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "F    93\n",
      "X    26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 001A, 103A, 097B, 019A, 074...\n",
      "kitten    [014B, 111A, 040A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 057A, 055A, 113A, 051B, 117A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 071A, 028A, 020A, 034A, 005A, 002A, 009...\n",
      "kitten                                   [044A, 049A, 115A]\n",
      "senior           [106A, 104A, 059A, 116A, 054A, 016A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 13, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 3, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "722\n",
      "Length of y_train_val:\n",
      "722\n",
      "Length of groups_train_val:\n",
      "722\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 874, 1: 820, 2: 605})\n",
      "Epoch 1/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 1.1603 - accuracy: 0.4976\n",
      "Epoch 2/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.9071 - accuracy: 0.6133\n",
      "Epoch 3/1500\n",
      "36/36 [==============================] - 0s 956us/step - loss: 0.8324 - accuracy: 0.6425\n",
      "Epoch 4/1500\n",
      "36/36 [==============================] - 0s 890us/step - loss: 0.7554 - accuracy: 0.6807\n",
      "Epoch 5/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.7401 - accuracy: 0.6903\n",
      "Epoch 6/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.6972 - accuracy: 0.6999\n",
      "Epoch 7/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.7147 - accuracy: 0.6925\n",
      "Epoch 8/1500\n",
      "36/36 [==============================] - 0s 850us/step - loss: 0.6788 - accuracy: 0.7173\n",
      "Epoch 9/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.6610 - accuracy: 0.7381\n",
      "Epoch 10/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.6543 - accuracy: 0.7264\n",
      "Epoch 11/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.6356 - accuracy: 0.7316\n",
      "Epoch 12/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.6247 - accuracy: 0.7364\n",
      "Epoch 13/1500\n",
      "36/36 [==============================] - 0s 835us/step - loss: 0.6132 - accuracy: 0.7316\n",
      "Epoch 14/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.6069 - accuracy: 0.7360\n",
      "Epoch 15/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.5967 - accuracy: 0.7551\n",
      "Epoch 16/1500\n",
      "36/36 [==============================] - 0s 869us/step - loss: 0.5693 - accuracy: 0.7551\n",
      "Epoch 17/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.5480 - accuracy: 0.7616\n",
      "Epoch 18/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.5514 - accuracy: 0.7621\n",
      "Epoch 19/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.5467 - accuracy: 0.7638\n",
      "Epoch 20/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.5369 - accuracy: 0.7799\n",
      "Epoch 21/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.5253 - accuracy: 0.7734\n",
      "Epoch 22/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.5096 - accuracy: 0.7834\n",
      "Epoch 23/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.5164 - accuracy: 0.7834\n",
      "Epoch 24/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.4888 - accuracy: 0.7960\n",
      "Epoch 25/1500\n",
      "36/36 [==============================] - 0s 850us/step - loss: 0.4968 - accuracy: 0.7856\n",
      "Epoch 26/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.5050 - accuracy: 0.7921\n",
      "Epoch 27/1500\n",
      "36/36 [==============================] - 0s 839us/step - loss: 0.4954 - accuracy: 0.7856\n",
      "Epoch 28/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.4924 - accuracy: 0.7999\n",
      "Epoch 29/1500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.4822 - accuracy: 0.8034\n",
      "Epoch 30/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.4551 - accuracy: 0.8034\n",
      "Epoch 31/1500\n",
      "36/36 [==============================] - 0s 926us/step - loss: 0.4778 - accuracy: 0.7947\n",
      "Epoch 32/1500\n",
      "36/36 [==============================] - 0s 921us/step - loss: 0.4728 - accuracy: 0.7990\n",
      "Epoch 33/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.4785 - accuracy: 0.7969\n",
      "Epoch 34/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.4580 - accuracy: 0.8073\n",
      "Epoch 35/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.4554 - accuracy: 0.8017\n",
      "Epoch 36/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.4503 - accuracy: 0.8108\n",
      "Epoch 37/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.4635 - accuracy: 0.8077\n",
      "Epoch 38/1500\n",
      "36/36 [==============================] - 0s 891us/step - loss: 0.4452 - accuracy: 0.8086\n",
      "Epoch 39/1500\n",
      "36/36 [==============================] - 0s 830us/step - loss: 0.4388 - accuracy: 0.8086\n",
      "Epoch 40/1500\n",
      "36/36 [==============================] - 0s 837us/step - loss: 0.4332 - accuracy: 0.8090\n",
      "Epoch 41/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.4206 - accuracy: 0.8177\n",
      "Epoch 42/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.4295 - accuracy: 0.8212\n",
      "Epoch 43/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.4444 - accuracy: 0.8043\n",
      "Epoch 44/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.4377 - accuracy: 0.8177\n",
      "Epoch 45/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.4161 - accuracy: 0.8264\n",
      "Epoch 46/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.4174 - accuracy: 0.8221\n",
      "Epoch 47/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.4209 - accuracy: 0.8138\n",
      "Epoch 48/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.4109 - accuracy: 0.8299\n",
      "Epoch 49/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.4049 - accuracy: 0.8351\n",
      "Epoch 50/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.4104 - accuracy: 0.8282\n",
      "Epoch 51/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.4068 - accuracy: 0.8243\n",
      "Epoch 52/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.4130 - accuracy: 0.8273\n",
      "Epoch 53/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.4008 - accuracy: 0.8273\n",
      "Epoch 54/1500\n",
      "36/36 [==============================] - 0s 909us/step - loss: 0.3985 - accuracy: 0.8282\n",
      "Epoch 55/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.3928 - accuracy: 0.8269\n",
      "Epoch 56/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.3933 - accuracy: 0.8378\n",
      "Epoch 57/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.4051 - accuracy: 0.8256\n",
      "Epoch 58/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.3869 - accuracy: 0.8373\n",
      "Epoch 59/1500\n",
      "36/36 [==============================] - 0s 830us/step - loss: 0.3884 - accuracy: 0.8391\n",
      "Epoch 60/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.3803 - accuracy: 0.8369\n",
      "Epoch 61/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.3764 - accuracy: 0.8443\n",
      "Epoch 62/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.3924 - accuracy: 0.8351\n",
      "Epoch 63/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.3835 - accuracy: 0.8408\n",
      "Epoch 64/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.3928 - accuracy: 0.8343\n",
      "Epoch 65/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.3798 - accuracy: 0.8443\n",
      "Epoch 66/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.3660 - accuracy: 0.8447\n",
      "Epoch 67/1500\n",
      "36/36 [==============================] - 0s 758us/step - loss: 0.3664 - accuracy: 0.8460\n",
      "Epoch 68/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.3764 - accuracy: 0.8369\n",
      "Epoch 69/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.3775 - accuracy: 0.8330\n",
      "Epoch 70/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.3613 - accuracy: 0.8438\n",
      "Epoch 71/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.3743 - accuracy: 0.8399\n",
      "Epoch 72/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.3678 - accuracy: 0.8378\n",
      "Epoch 73/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.3586 - accuracy: 0.8521\n",
      "Epoch 74/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3647 - accuracy: 0.8456\n",
      "Epoch 75/1500\n",
      "36/36 [==============================] - 0s 874us/step - loss: 0.3524 - accuracy: 0.8517\n",
      "Epoch 76/1500\n",
      "36/36 [==============================] - 0s 930us/step - loss: 0.3564 - accuracy: 0.8556\n",
      "Epoch 77/1500\n",
      "36/36 [==============================] - 0s 968us/step - loss: 0.3576 - accuracy: 0.8504\n",
      "Epoch 78/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.3473 - accuracy: 0.8504\n",
      "Epoch 79/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.3648 - accuracy: 0.8447\n",
      "Epoch 80/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3623 - accuracy: 0.8456\n",
      "Epoch 81/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.3428 - accuracy: 0.8582\n",
      "Epoch 82/1500\n",
      "36/36 [==============================] - 0s 984us/step - loss: 0.3484 - accuracy: 0.8547\n",
      "Epoch 83/1500\n",
      "36/36 [==============================] - 0s 993us/step - loss: 0.3566 - accuracy: 0.8495\n",
      "Epoch 84/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3306 - accuracy: 0.8652\n",
      "Epoch 85/1500\n",
      "36/36 [==============================] - 0s 931us/step - loss: 0.3413 - accuracy: 0.8495\n",
      "Epoch 86/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.3420 - accuracy: 0.8569\n",
      "Epoch 87/1500\n",
      "36/36 [==============================] - 0s 895us/step - loss: 0.3341 - accuracy: 0.8599\n",
      "Epoch 88/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.3392 - accuracy: 0.8599\n",
      "Epoch 89/1500\n",
      "36/36 [==============================] - 0s 747us/step - loss: 0.3352 - accuracy: 0.8612\n",
      "Epoch 90/1500\n",
      "36/36 [==============================] - 0s 769us/step - loss: 0.3482 - accuracy: 0.8621\n",
      "Epoch 91/1500\n",
      "36/36 [==============================] - 0s 801us/step - loss: 0.3354 - accuracy: 0.8625\n",
      "Epoch 92/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.3321 - accuracy: 0.8630\n",
      "Epoch 93/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 0.3315 - accuracy: 0.8652\n",
      "Epoch 94/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 0.3222 - accuracy: 0.8634\n",
      "Epoch 95/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.3258 - accuracy: 0.8591\n",
      "Epoch 96/1500\n",
      "36/36 [==============================] - 0s 763us/step - loss: 0.3215 - accuracy: 0.8678\n",
      "Epoch 97/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.3158 - accuracy: 0.8708\n",
      "Epoch 98/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.3302 - accuracy: 0.8660\n",
      "Epoch 99/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.3190 - accuracy: 0.8669\n",
      "Epoch 100/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.3446 - accuracy: 0.8560\n",
      "Epoch 101/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.3345 - accuracy: 0.8504\n",
      "Epoch 102/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.3100 - accuracy: 0.8704\n",
      "Epoch 103/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.3106 - accuracy: 0.8817\n",
      "Epoch 104/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.3130 - accuracy: 0.8621\n",
      "Epoch 105/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.3268 - accuracy: 0.8660\n",
      "Epoch 106/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.3033 - accuracy: 0.8730\n",
      "Epoch 107/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.3017 - accuracy: 0.8786\n",
      "Epoch 108/1500\n",
      "36/36 [==============================] - 0s 817us/step - loss: 0.3094 - accuracy: 0.8708\n",
      "Epoch 109/1500\n",
      "36/36 [==============================] - 0s 837us/step - loss: 0.3155 - accuracy: 0.8673\n",
      "Epoch 110/1500\n",
      "36/36 [==============================] - 0s 873us/step - loss: 0.3048 - accuracy: 0.8739\n",
      "Epoch 111/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.3071 - accuracy: 0.8704\n",
      "Epoch 112/1500\n",
      "36/36 [==============================] - 0s 880us/step - loss: 0.3024 - accuracy: 0.8821\n",
      "Epoch 113/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.2906 - accuracy: 0.8847\n",
      "Epoch 114/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.3102 - accuracy: 0.8682\n",
      "Epoch 115/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.3069 - accuracy: 0.8717\n",
      "Epoch 116/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 0.3053 - accuracy: 0.8769\n",
      "Epoch 117/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.3189 - accuracy: 0.8625\n",
      "Epoch 118/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.3043 - accuracy: 0.8739\n",
      "Epoch 119/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.2916 - accuracy: 0.8773\n",
      "Epoch 120/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.2989 - accuracy: 0.8799\n",
      "Epoch 121/1500\n",
      "36/36 [==============================] - 0s 847us/step - loss: 0.2942 - accuracy: 0.8752\n",
      "Epoch 122/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.2900 - accuracy: 0.8786\n",
      "Epoch 123/1500\n",
      "36/36 [==============================] - 0s 947us/step - loss: 0.2861 - accuracy: 0.8808\n",
      "Epoch 124/1500\n",
      "36/36 [==============================] - 0s 903us/step - loss: 0.3120 - accuracy: 0.8708\n",
      "Epoch 125/1500\n",
      "36/36 [==============================] - 0s 942us/step - loss: 0.2931 - accuracy: 0.8782\n",
      "Epoch 126/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.2974 - accuracy: 0.8734\n",
      "Epoch 127/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.2841 - accuracy: 0.8882\n",
      "Epoch 128/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.2868 - accuracy: 0.8856\n",
      "Epoch 129/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.2965 - accuracy: 0.8786\n",
      "Epoch 130/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.2780 - accuracy: 0.8847\n",
      "Epoch 131/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.2873 - accuracy: 0.8791\n",
      "Epoch 132/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.2785 - accuracy: 0.8834\n",
      "Epoch 133/1500\n",
      "36/36 [==============================] - 0s 810us/step - loss: 0.2940 - accuracy: 0.8873\n",
      "Epoch 134/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2899 - accuracy: 0.8795\n",
      "Epoch 135/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.2797 - accuracy: 0.8839\n",
      "Epoch 136/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2736 - accuracy: 0.8839\n",
      "Epoch 137/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.2807 - accuracy: 0.8895\n",
      "Epoch 138/1500\n",
      "36/36 [==============================] - 0s 949us/step - loss: 0.2903 - accuracy: 0.8747\n",
      "Epoch 139/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.2788 - accuracy: 0.8830\n",
      "Epoch 140/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.2883 - accuracy: 0.8847\n",
      "Epoch 141/1500\n",
      "36/36 [==============================] - 0s 880us/step - loss: 0.2681 - accuracy: 0.8930\n",
      "Epoch 142/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.2726 - accuracy: 0.8895\n",
      "Epoch 143/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.2873 - accuracy: 0.8795\n",
      "Epoch 144/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.2841 - accuracy: 0.8804\n",
      "Epoch 145/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.2885 - accuracy: 0.8756\n",
      "Epoch 146/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.2857 - accuracy: 0.8878\n",
      "Epoch 147/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.2657 - accuracy: 0.8926\n",
      "Epoch 148/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.2731 - accuracy: 0.8891\n",
      "Epoch 149/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.2648 - accuracy: 0.8917\n",
      "Epoch 150/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.2781 - accuracy: 0.8921\n",
      "Epoch 151/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.2572 - accuracy: 0.8952\n",
      "Epoch 152/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.2777 - accuracy: 0.8873\n",
      "Epoch 153/1500\n",
      "36/36 [==============================] - 0s 807us/step - loss: 0.2601 - accuracy: 0.8965\n",
      "Epoch 154/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.2608 - accuracy: 0.8973\n",
      "Epoch 155/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.2717 - accuracy: 0.8834\n",
      "Epoch 156/1500\n",
      "36/36 [==============================] - 0s 968us/step - loss: 0.2551 - accuracy: 0.8965\n",
      "Epoch 157/1500\n",
      "36/36 [==============================] - 0s 977us/step - loss: 0.2547 - accuracy: 0.9030\n",
      "Epoch 158/1500\n",
      "36/36 [==============================] - 0s 975us/step - loss: 0.2689 - accuracy: 0.8930\n",
      "Epoch 159/1500\n",
      "36/36 [==============================] - 0s 908us/step - loss: 0.2700 - accuracy: 0.8839\n",
      "Epoch 160/1500\n",
      "36/36 [==============================] - 0s 900us/step - loss: 0.2597 - accuracy: 0.8991\n",
      "Epoch 161/1500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.2611 - accuracy: 0.9004\n",
      "Epoch 162/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2562 - accuracy: 0.8991\n",
      "Epoch 163/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2357 - accuracy: 0.9100\n",
      "Epoch 164/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.8900\n",
      "Epoch 165/1500\n",
      "36/36 [==============================] - 0s 932us/step - loss: 0.2620 - accuracy: 0.8921\n",
      "Epoch 166/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.2604 - accuracy: 0.8987\n",
      "Epoch 167/1500\n",
      "36/36 [==============================] - 0s 887us/step - loss: 0.2512 - accuracy: 0.8947\n",
      "Epoch 168/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.2490 - accuracy: 0.8978\n",
      "Epoch 169/1500\n",
      "36/36 [==============================] - 0s 837us/step - loss: 0.2589 - accuracy: 0.8917\n",
      "Epoch 170/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.2492 - accuracy: 0.8965\n",
      "Epoch 171/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 0.2566 - accuracy: 0.8995\n",
      "Epoch 172/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.2596 - accuracy: 0.8965\n",
      "Epoch 173/1500\n",
      "36/36 [==============================] - 0s 897us/step - loss: 0.2500 - accuracy: 0.8969\n",
      "Epoch 174/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.2483 - accuracy: 0.8978\n",
      "Epoch 175/1500\n",
      "36/36 [==============================] - 0s 911us/step - loss: 0.2635 - accuracy: 0.8960\n",
      "Epoch 176/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.2431 - accuracy: 0.9026\n",
      "Epoch 177/1500\n",
      "36/36 [==============================] - 0s 895us/step - loss: 0.2439 - accuracy: 0.9078\n",
      "Epoch 178/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.2340 - accuracy: 0.9021\n",
      "Epoch 179/1500\n",
      "36/36 [==============================] - 0s 817us/step - loss: 0.2517 - accuracy: 0.8943\n",
      "Epoch 180/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.2364 - accuracy: 0.9065\n",
      "Epoch 181/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.2500 - accuracy: 0.8991\n",
      "Epoch 182/1500\n",
      "36/36 [==============================] - 0s 863us/step - loss: 0.2512 - accuracy: 0.8973\n",
      "Epoch 183/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.2495 - accuracy: 0.8982\n",
      "Epoch 184/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 0.2345 - accuracy: 0.9017\n",
      "Epoch 185/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.2426 - accuracy: 0.9047\n",
      "Epoch 186/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.2469 - accuracy: 0.9030\n",
      "Epoch 187/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.2417 - accuracy: 0.9060\n",
      "Epoch 188/1500\n",
      "36/36 [==============================] - 0s 864us/step - loss: 0.2308 - accuracy: 0.9039\n",
      "Epoch 189/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.2689 - accuracy: 0.8930\n",
      "Epoch 190/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.2319 - accuracy: 0.9121\n",
      "Epoch 191/1500\n",
      "36/36 [==============================] - 0s 923us/step - loss: 0.2216 - accuracy: 0.9104\n",
      "Epoch 192/1500\n",
      "36/36 [==============================] - 0s 807us/step - loss: 0.2364 - accuracy: 0.9060\n",
      "Epoch 193/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.2173 - accuracy: 0.9187\n",
      "Epoch 194/1500\n",
      "36/36 [==============================] - 0s 871us/step - loss: 0.2443 - accuracy: 0.9078\n",
      "Epoch 195/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.2387 - accuracy: 0.9021\n",
      "Epoch 196/1500\n",
      "36/36 [==============================] - 0s 825us/step - loss: 0.2384 - accuracy: 0.9039\n",
      "Epoch 197/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.2373 - accuracy: 0.9082\n",
      "Epoch 198/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.2266 - accuracy: 0.9208\n",
      "Epoch 199/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.2457 - accuracy: 0.9043\n",
      "Epoch 200/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.2323 - accuracy: 0.9069\n",
      "Epoch 201/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.2205 - accuracy: 0.9187\n",
      "Epoch 202/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.2268 - accuracy: 0.9104\n",
      "Epoch 203/1500\n",
      "36/36 [==============================] - 0s 948us/step - loss: 0.2283 - accuracy: 0.9091\n",
      "Epoch 204/1500\n",
      "36/36 [==============================] - 0s 891us/step - loss: 0.2275 - accuracy: 0.9130\n",
      "Epoch 205/1500\n",
      "36/36 [==============================] - 0s 910us/step - loss: 0.2265 - accuracy: 0.9082\n",
      "Epoch 206/1500\n",
      "36/36 [==============================] - 0s 931us/step - loss: 0.2211 - accuracy: 0.9126\n",
      "Epoch 207/1500\n",
      "36/36 [==============================] - 0s 937us/step - loss: 0.2088 - accuracy: 0.9247\n",
      "Epoch 208/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.9074\n",
      "Epoch 209/1500\n",
      "36/36 [==============================] - 0s 955us/step - loss: 0.2235 - accuracy: 0.9104\n",
      "Epoch 210/1500\n",
      "36/36 [==============================] - 0s 979us/step - loss: 0.2219 - accuracy: 0.9143\n",
      "Epoch 211/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.2196 - accuracy: 0.9143\n",
      "Epoch 212/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.2264 - accuracy: 0.9082\n",
      "Epoch 213/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.2235 - accuracy: 0.9143\n",
      "Epoch 214/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.2289 - accuracy: 0.9100\n",
      "Epoch 215/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.2242 - accuracy: 0.9069\n",
      "Epoch 216/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.2196 - accuracy: 0.9143\n",
      "Epoch 217/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.2072 - accuracy: 0.9152\n",
      "Epoch 218/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.2082 - accuracy: 0.9226\n",
      "Epoch 219/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.2299 - accuracy: 0.9056\n",
      "Epoch 220/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 0.2062 - accuracy: 0.9165\n",
      "Epoch 221/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.2260 - accuracy: 0.9130\n",
      "Epoch 222/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.2204 - accuracy: 0.9134\n",
      "Epoch 223/1500\n",
      "36/36 [==============================] - 0s 847us/step - loss: 0.2083 - accuracy: 0.9113\n",
      "Epoch 224/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.2025 - accuracy: 0.9256\n",
      "Epoch 225/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.2188 - accuracy: 0.9191\n",
      "Epoch 226/1500\n",
      "36/36 [==============================] - 0s 847us/step - loss: 0.2219 - accuracy: 0.9130\n",
      "Epoch 227/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 0.2095 - accuracy: 0.9208\n",
      "Epoch 228/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.2224 - accuracy: 0.9147\n",
      "Epoch 229/1500\n",
      "36/36 [==============================] - 0s 875us/step - loss: 0.2126 - accuracy: 0.9230\n",
      "Epoch 230/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.2121 - accuracy: 0.9165\n",
      "Epoch 231/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.2198 - accuracy: 0.9087\n",
      "Epoch 232/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 0.1965 - accuracy: 0.9256\n",
      "Epoch 233/1500\n",
      "36/36 [==============================] - 0s 896us/step - loss: 0.2158 - accuracy: 0.9147\n",
      "Epoch 234/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2265 - accuracy: 0.9034\n",
      "Epoch 235/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.2071 - accuracy: 0.9195\n",
      "Epoch 236/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.2027 - accuracy: 0.9213\n",
      "Epoch 237/1500\n",
      "36/36 [==============================] - 0s 815us/step - loss: 0.2158 - accuracy: 0.9134\n",
      "Epoch 238/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.2183 - accuracy: 0.9134\n",
      "Epoch 239/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.2049 - accuracy: 0.9178\n",
      "Epoch 240/1500\n",
      "36/36 [==============================] - 0s 871us/step - loss: 0.2078 - accuracy: 0.9104\n",
      "Epoch 241/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.2216 - accuracy: 0.9161\n",
      "Epoch 242/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.2121 - accuracy: 0.9200\n",
      "Epoch 243/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.2025 - accuracy: 0.9208\n",
      "Epoch 244/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.2032 - accuracy: 0.9204\n",
      "Epoch 245/1500\n",
      "36/36 [==============================] - 0s 897us/step - loss: 0.2060 - accuracy: 0.9169\n",
      "Epoch 246/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.2113 - accuracy: 0.9126\n",
      "Epoch 247/1500\n",
      "36/36 [==============================] - 0s 871us/step - loss: 0.1932 - accuracy: 0.9217\n",
      "Epoch 248/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2023 - accuracy: 0.9208\n",
      "Epoch 249/1500\n",
      "36/36 [==============================] - 0s 954us/step - loss: 0.1969 - accuracy: 0.9247\n",
      "Epoch 250/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.2013 - accuracy: 0.9191\n",
      "Epoch 251/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.2037 - accuracy: 0.9252\n",
      "Epoch 252/1500\n",
      "36/36 [==============================] - 0s 825us/step - loss: 0.2004 - accuracy: 0.9256\n",
      "Epoch 253/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.1981 - accuracy: 0.9243\n",
      "Epoch 254/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.2023 - accuracy: 0.9182\n",
      "Epoch 255/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.1853 - accuracy: 0.9252\n",
      "Epoch 256/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.1981 - accuracy: 0.9252\n",
      "Epoch 257/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.2129 - accuracy: 0.9182\n",
      "Epoch 258/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.2141 - accuracy: 0.9156\n",
      "Epoch 259/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.2038 - accuracy: 0.9221\n",
      "Epoch 260/1500\n",
      "36/36 [==============================] - 0s 990us/step - loss: 0.2010 - accuracy: 0.9230\n",
      "Epoch 261/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.2008 - accuracy: 0.9217\n",
      "Epoch 262/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.1960 - accuracy: 0.9243\n",
      "Epoch 263/1500\n",
      "36/36 [==============================] - 0s 901us/step - loss: 0.1995 - accuracy: 0.9287\n",
      "Epoch 264/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.1991 - accuracy: 0.9252\n",
      "Epoch 265/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.1996 - accuracy: 0.9269\n",
      "Epoch 266/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.2003 - accuracy: 0.9234\n",
      "Epoch 267/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.1920 - accuracy: 0.9278\n",
      "Epoch 268/1500\n",
      "36/36 [==============================] - 0s 799us/step - loss: 0.1890 - accuracy: 0.9300\n",
      "Epoch 269/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.1891 - accuracy: 0.9269\n",
      "Epoch 270/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.1965 - accuracy: 0.9213\n",
      "Epoch 271/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.1851 - accuracy: 0.9265\n",
      "Epoch 272/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.2042 - accuracy: 0.9191\n",
      "Epoch 273/1500\n",
      "36/36 [==============================] - 0s 913us/step - loss: 0.1797 - accuracy: 0.9321\n",
      "Epoch 274/1500\n",
      "36/36 [==============================] - 0s 948us/step - loss: 0.1902 - accuracy: 0.9295\n",
      "Epoch 275/1500\n",
      "36/36 [==============================] - 0s 903us/step - loss: 0.2103 - accuracy: 0.9121\n",
      "Epoch 276/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.2023 - accuracy: 0.9226\n",
      "Epoch 277/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.1877 - accuracy: 0.9239\n",
      "Epoch 278/1500\n",
      "36/36 [==============================] - 0s 920us/step - loss: 0.1805 - accuracy: 0.9295\n",
      "Epoch 279/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.1905 - accuracy: 0.9230\n",
      "Epoch 280/1500\n",
      "36/36 [==============================] - 0s 900us/step - loss: 0.1881 - accuracy: 0.9226\n",
      "Epoch 281/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.1869 - accuracy: 0.9287\n",
      "Epoch 282/1500\n",
      "36/36 [==============================] - 0s 907us/step - loss: 0.1904 - accuracy: 0.9256\n",
      "Epoch 283/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.2062 - accuracy: 0.9274\n",
      "Epoch 284/1500\n",
      "36/36 [==============================] - 0s 883us/step - loss: 0.1751 - accuracy: 0.9313\n",
      "Epoch 285/1500\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.1819 - accuracy: 0.9282\n",
      "Epoch 286/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.1858 - accuracy: 0.9230\n",
      "Epoch 287/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.1906 - accuracy: 0.9217\n",
      "Epoch 288/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.1830 - accuracy: 0.9239\n",
      "Epoch 289/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1993 - accuracy: 0.9234\n",
      "Epoch 290/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.1944 - accuracy: 0.9213\n",
      "Epoch 291/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.1832 - accuracy: 0.9308\n",
      "Epoch 292/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.1893 - accuracy: 0.9282\n",
      "Epoch 293/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.1902 - accuracy: 0.9261\n",
      "Epoch 294/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.1744 - accuracy: 0.9308\n",
      "Epoch 295/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.1776 - accuracy: 0.9387\n",
      "Epoch 296/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.1897 - accuracy: 0.9239\n",
      "Epoch 297/1500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.1801 - accuracy: 0.9313\n",
      "Epoch 298/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.9330\n",
      "Epoch 299/1500\n",
      "36/36 [==============================] - 0s 919us/step - loss: 0.1900 - accuracy: 0.9261\n",
      "Epoch 300/1500\n",
      "36/36 [==============================] - 0s 873us/step - loss: 0.1837 - accuracy: 0.9326\n",
      "Epoch 301/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1939 - accuracy: 0.9187\n",
      "Epoch 302/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.1704 - accuracy: 0.9343\n",
      "Epoch 303/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.1796 - accuracy: 0.9326\n",
      "Epoch 304/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.1887 - accuracy: 0.9304\n",
      "Epoch 305/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.1838 - accuracy: 0.9278\n",
      "Epoch 306/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.1704 - accuracy: 0.9326\n",
      "Epoch 307/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.1953 - accuracy: 0.9234\n",
      "Epoch 308/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1828 - accuracy: 0.9282\n",
      "Epoch 309/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1740 - accuracy: 0.9300\n",
      "Epoch 310/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.1598 - accuracy: 0.9439\n",
      "Epoch 311/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.2104 - accuracy: 0.9208\n",
      "Epoch 312/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.1677 - accuracy: 0.9400\n",
      "Epoch 313/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.1789 - accuracy: 0.9326\n",
      "Epoch 314/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.1759 - accuracy: 0.9295\n",
      "Epoch 315/1500\n",
      "36/36 [==============================] - 0s 852us/step - loss: 0.1685 - accuracy: 0.9348\n",
      "Epoch 316/1500\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.1844 - accuracy: 0.9247\n",
      "Epoch 317/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.1728 - accuracy: 0.9317\n",
      "Epoch 318/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1677 - accuracy: 0.9291\n",
      "Epoch 319/1500\n",
      "36/36 [==============================] - 0s 825us/step - loss: 0.1723 - accuracy: 0.9313\n",
      "Epoch 320/1500\n",
      "36/36 [==============================] - 0s 828us/step - loss: 0.1629 - accuracy: 0.9395\n",
      "Epoch 321/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.1787 - accuracy: 0.9321\n",
      "Epoch 322/1500\n",
      "36/36 [==============================] - 0s 897us/step - loss: 0.1744 - accuracy: 0.9295\n",
      "Epoch 323/1500\n",
      "36/36 [==============================] - 0s 817us/step - loss: 0.1712 - accuracy: 0.9378\n",
      "Epoch 324/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.1797 - accuracy: 0.9300\n",
      "Epoch 325/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 0.1776 - accuracy: 0.9269\n",
      "Epoch 326/1500\n",
      "36/36 [==============================] - 0s 815us/step - loss: 0.1852 - accuracy: 0.9317\n",
      "Epoch 327/1500\n",
      "36/36 [==============================] - 0s 850us/step - loss: 0.1896 - accuracy: 0.9247\n",
      "Epoch 328/1500\n",
      "36/36 [==============================] - 0s 830us/step - loss: 0.1767 - accuracy: 0.9291\n",
      "Epoch 329/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.1830 - accuracy: 0.9261\n",
      "Epoch 330/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.1661 - accuracy: 0.9330\n",
      "Epoch 331/1500\n",
      "36/36 [==============================] - 0s 863us/step - loss: 0.1666 - accuracy: 0.9378\n",
      "Epoch 332/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.1745 - accuracy: 0.9317\n",
      "Epoch 333/1500\n",
      "36/36 [==============================] - 0s 897us/step - loss: 0.1526 - accuracy: 0.9413\n",
      "Epoch 334/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.1720 - accuracy: 0.9278\n",
      "Epoch 335/1500\n",
      "36/36 [==============================] - 0s 869us/step - loss: 0.1841 - accuracy: 0.9304\n",
      "Epoch 336/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.1704 - accuracy: 0.9321\n",
      "Epoch 337/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.1771 - accuracy: 0.9343\n",
      "Epoch 338/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1586 - accuracy: 0.9369\n",
      "Epoch 339/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.1560 - accuracy: 0.9387\n",
      "Epoch 340/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.1588 - accuracy: 0.9408\n",
      "Epoch 341/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.1693 - accuracy: 0.9387\n",
      "Epoch 342/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.1490 - accuracy: 0.9465\n",
      "Epoch 343/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.1767 - accuracy: 0.9226\n",
      "Epoch 344/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1577 - accuracy: 0.9378\n",
      "Epoch 345/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.1583 - accuracy: 0.9387\n",
      "Epoch 346/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.1693 - accuracy: 0.9348\n",
      "Epoch 347/1500\n",
      "36/36 [==============================] - 0s 864us/step - loss: 0.1610 - accuracy: 0.9413\n",
      "Epoch 348/1500\n",
      "36/36 [==============================] - 0s 950us/step - loss: 0.1645 - accuracy: 0.9369\n",
      "Epoch 349/1500\n",
      "36/36 [==============================] - 0s 941us/step - loss: 0.1727 - accuracy: 0.9317\n",
      "Epoch 350/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 0.1669 - accuracy: 0.9369\n",
      "Epoch 351/1500\n",
      "36/36 [==============================] - 0s 825us/step - loss: 0.1589 - accuracy: 0.9400\n",
      "Epoch 352/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 0.1652 - accuracy: 0.9369\n",
      "Epoch 353/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1699 - accuracy: 0.9343\n",
      "Epoch 354/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.1629 - accuracy: 0.9361\n",
      "Epoch 355/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.1698 - accuracy: 0.9365\n",
      "Epoch 356/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.1580 - accuracy: 0.9339\n",
      "Epoch 357/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.1566 - accuracy: 0.9387\n",
      "Epoch 358/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.1679 - accuracy: 0.9365\n",
      "Epoch 359/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 0.1686 - accuracy: 0.9304\n",
      "Epoch 360/1500\n",
      "36/36 [==============================] - 0s 818us/step - loss: 0.1741 - accuracy: 0.9330\n",
      "Epoch 361/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.1480 - accuracy: 0.9400\n",
      "Epoch 362/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.1564 - accuracy: 0.9400\n",
      "Epoch 363/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.1509 - accuracy: 0.9435\n",
      "Epoch 364/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.1607 - accuracy: 0.9382\n",
      "Epoch 365/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.1624 - accuracy: 0.9391\n",
      "Epoch 366/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.1565 - accuracy: 0.9421\n",
      "Epoch 367/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.1734 - accuracy: 0.9308\n",
      "Epoch 368/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.1539 - accuracy: 0.9391\n",
      "Epoch 369/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.1678 - accuracy: 0.9304\n",
      "Epoch 370/1500\n",
      "36/36 [==============================] - 0s 885us/step - loss: 0.1536 - accuracy: 0.9365\n",
      "Epoch 371/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.1596 - accuracy: 0.9387\n",
      "Epoch 372/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 0.1618 - accuracy: 0.9369\n",
      "Epoch 373/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.1656 - accuracy: 0.9339\n",
      "Epoch 374/1500\n",
      "36/36 [==============================] - 0s 835us/step - loss: 0.1585 - accuracy: 0.9369\n",
      "Epoch 375/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.1586 - accuracy: 0.9387\n",
      "Epoch 376/1500\n",
      "36/36 [==============================] - 0s 913us/step - loss: 0.1626 - accuracy: 0.9374\n",
      "Epoch 377/1500\n",
      "36/36 [==============================] - 0s 981us/step - loss: 0.1511 - accuracy: 0.9391\n",
      "Epoch 378/1500\n",
      "36/36 [==============================] - 0s 912us/step - loss: 0.1517 - accuracy: 0.9408\n",
      "Epoch 379/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.1520 - accuracy: 0.9408\n",
      "Epoch 380/1500\n",
      "36/36 [==============================] - 0s 965us/step - loss: 0.1629 - accuracy: 0.9387\n",
      "Epoch 381/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 0.1538 - accuracy: 0.9439\n",
      "Epoch 382/1500\n",
      "36/36 [==============================] - 0s 950us/step - loss: 0.1437 - accuracy: 0.9439\n",
      "Epoch 383/1500\n",
      "36/36 [==============================] - 0s 955us/step - loss: 0.1596 - accuracy: 0.9369\n",
      "Epoch 384/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.1420 - accuracy: 0.9443\n",
      "Epoch 385/1500\n",
      "36/36 [==============================] - 0s 910us/step - loss: 0.1532 - accuracy: 0.9391\n",
      "Epoch 386/1500\n",
      "36/36 [==============================] - 0s 951us/step - loss: 0.1524 - accuracy: 0.9356\n",
      "Epoch 387/1500\n",
      "36/36 [==============================] - 0s 954us/step - loss: 0.1550 - accuracy: 0.9408\n",
      "Epoch 388/1500\n",
      "36/36 [==============================] - 0s 917us/step - loss: 0.1498 - accuracy: 0.9430\n",
      "Epoch 389/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.1572 - accuracy: 0.9387\n",
      "Epoch 390/1500\n",
      "36/36 [==============================] - 0s 839us/step - loss: 0.1565 - accuracy: 0.9400\n",
      "Epoch 391/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.1510 - accuracy: 0.9469\n",
      "Epoch 392/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.1720 - accuracy: 0.9378\n",
      "Epoch 393/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.1598 - accuracy: 0.9435\n",
      "Epoch 394/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.1515 - accuracy: 0.9413\n",
      "Epoch 395/1500\n",
      "36/36 [==============================] - 0s 807us/step - loss: 0.1600 - accuracy: 0.9387\n",
      "Epoch 396/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.1497 - accuracy: 0.9356\n",
      "Epoch 397/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.1570 - accuracy: 0.9435\n",
      "Epoch 398/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.1470 - accuracy: 0.9387\n",
      "Epoch 399/1500\n",
      "36/36 [==============================] - 0s 896us/step - loss: 0.1500 - accuracy: 0.9408\n",
      "Epoch 400/1500\n",
      "36/36 [==============================] - 0s 869us/step - loss: 0.1534 - accuracy: 0.9504\n",
      "Epoch 401/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.1449 - accuracy: 0.9461\n",
      "Epoch 402/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.1478 - accuracy: 0.9469\n",
      "Epoch 403/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.1508 - accuracy: 0.9430\n",
      "Epoch 404/1500\n",
      "36/36 [==============================] - 0s 943us/step - loss: 0.1383 - accuracy: 0.9482\n",
      "Epoch 405/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.1591 - accuracy: 0.9321\n",
      "Epoch 406/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.1412 - accuracy: 0.9448\n",
      "Epoch 407/1500\n",
      "36/36 [==============================] - 0s 797us/step - loss: 0.1608 - accuracy: 0.9435\n",
      "Epoch 408/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1448 - accuracy: 0.9426\n",
      "Epoch 409/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.1408 - accuracy: 0.9461\n",
      "Epoch 410/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.1596 - accuracy: 0.9426\n",
      "Epoch 411/1500\n",
      "36/36 [==============================] - 0s 799us/step - loss: 0.1538 - accuracy: 0.9369\n",
      "Epoch 412/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.1614 - accuracy: 0.9348\n",
      "Epoch 413/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.1400 - accuracy: 0.9482\n",
      "Epoch 414/1500\n",
      "36/36 [==============================] - 0s 874us/step - loss: 0.1561 - accuracy: 0.9395\n",
      "Epoch 415/1500\n",
      "36/36 [==============================] - 0s 837us/step - loss: 0.1387 - accuracy: 0.9443\n",
      "Epoch 416/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.1543 - accuracy: 0.9421\n",
      "Epoch 417/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.1411 - accuracy: 0.9469\n",
      "Epoch 418/1500\n",
      "36/36 [==============================] - 0s 822us/step - loss: 0.1510 - accuracy: 0.9378\n",
      "Epoch 419/1500\n",
      "36/36 [==============================] - 0s 822us/step - loss: 0.1434 - accuracy: 0.9461\n",
      "Epoch 420/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1442 - accuracy: 0.9430\n",
      "Epoch 421/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.1404 - accuracy: 0.9461\n",
      "Epoch 422/1500\n",
      "36/36 [==============================] - 0s 941us/step - loss: 0.1430 - accuracy: 0.9474\n",
      "Epoch 423/1500\n",
      "36/36 [==============================] - 0s 981us/step - loss: 0.1367 - accuracy: 0.9478\n",
      "Epoch 424/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9452\n",
      "Epoch 425/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9482\n",
      "Epoch 426/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9387\n",
      "Epoch 427/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9461\n",
      "Epoch 428/1500\n",
      "36/36 [==============================] - 0s 985us/step - loss: 0.1398 - accuracy: 0.9500\n",
      "Epoch 429/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.9482\n",
      "Epoch 430/1500\n",
      "36/36 [==============================] - 0s 948us/step - loss: 0.1456 - accuracy: 0.9443\n",
      "Epoch 431/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.1422 - accuracy: 0.9487\n",
      "Epoch 432/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.1545 - accuracy: 0.9417\n",
      "Epoch 433/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.1326 - accuracy: 0.9504\n",
      "Epoch 434/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.1461 - accuracy: 0.9452\n",
      "Epoch 435/1500\n",
      "36/36 [==============================] - 0s 940us/step - loss: 0.1465 - accuracy: 0.9417\n",
      "Epoch 436/1500\n",
      "36/36 [==============================] - 0s 932us/step - loss: 0.1439 - accuracy: 0.9465\n",
      "Epoch 437/1500\n",
      "36/36 [==============================] - 0s 946us/step - loss: 0.1347 - accuracy: 0.9508\n",
      "Epoch 438/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.1384 - accuracy: 0.9487\n",
      "Epoch 439/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.1361 - accuracy: 0.9500\n",
      "Epoch 440/1500\n",
      "36/36 [==============================] - 0s 830us/step - loss: 0.1546 - accuracy: 0.9356\n",
      "Epoch 441/1500\n",
      "36/36 [==============================] - 0s 834us/step - loss: 0.1444 - accuracy: 0.9452\n",
      "Epoch 442/1500\n",
      "36/36 [==============================] - 0s 822us/step - loss: 0.1523 - accuracy: 0.9474\n",
      "Epoch 443/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.1533 - accuracy: 0.9421\n",
      "Epoch 444/1500\n",
      "36/36 [==============================] - 0s 839us/step - loss: 0.1301 - accuracy: 0.9526\n",
      "Epoch 445/1500\n",
      "36/36 [==============================] - 0s 825us/step - loss: 0.1430 - accuracy: 0.9474\n",
      "Epoch 446/1500\n",
      "36/36 [==============================] - 0s 844us/step - loss: 0.1420 - accuracy: 0.9443\n",
      "Epoch 447/1500\n",
      "36/36 [==============================] - 0s 816us/step - loss: 0.1427 - accuracy: 0.9508\n",
      "Epoch 448/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1435 - accuracy: 0.9443\n",
      "Epoch 449/1500\n",
      "36/36 [==============================] - 0s 961us/step - loss: 0.1432 - accuracy: 0.9452\n",
      "Epoch 450/1500\n",
      "36/36 [==============================] - 0s 972us/step - loss: 0.1399 - accuracy: 0.9500\n",
      "Epoch 451/1500\n",
      "36/36 [==============================] - 0s 928us/step - loss: 0.1324 - accuracy: 0.9465\n",
      "Epoch 452/1500\n",
      "36/36 [==============================] - 0s 944us/step - loss: 0.1393 - accuracy: 0.9508\n",
      "Epoch 453/1500\n",
      "36/36 [==============================] - 0s 972us/step - loss: 0.1179 - accuracy: 0.9561\n",
      "Epoch 454/1500\n",
      "36/36 [==============================] - 0s 941us/step - loss: 0.1361 - accuracy: 0.9513\n",
      "Epoch 455/1500\n",
      "36/36 [==============================] - 0s 920us/step - loss: 0.1334 - accuracy: 0.9530\n",
      "Epoch 456/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1432 - accuracy: 0.9456\n",
      "Epoch 457/1500\n",
      "36/36 [==============================] - 0s 814us/step - loss: 0.1375 - accuracy: 0.9448\n",
      "Epoch 458/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.1265 - accuracy: 0.9522\n",
      "Epoch 459/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 0.1251 - accuracy: 0.9530\n",
      "Epoch 460/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.1513 - accuracy: 0.9478\n",
      "Epoch 461/1500\n",
      "36/36 [==============================] - 0s 916us/step - loss: 0.1320 - accuracy: 0.9504\n",
      "Epoch 462/1500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.1366 - accuracy: 0.9474\n",
      "Epoch 463/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9561\n",
      "Epoch 464/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.9382\n",
      "Epoch 465/1500\n",
      "36/36 [==============================] - 0s 920us/step - loss: 0.1459 - accuracy: 0.9421\n",
      "Epoch 466/1500\n",
      "36/36 [==============================] - 0s 902us/step - loss: 0.1375 - accuracy: 0.9474\n",
      "Epoch 467/1500\n",
      "36/36 [==============================] - 0s 953us/step - loss: 0.1469 - accuracy: 0.9404\n",
      "Epoch 468/1500\n",
      "36/36 [==============================] - 0s 895us/step - loss: 0.1392 - accuracy: 0.9435\n",
      "Epoch 469/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.1359 - accuracy: 0.9491\n",
      "Epoch 470/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.1337 - accuracy: 0.9487\n",
      "Epoch 471/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.1231 - accuracy: 0.9500\n",
      "Epoch 472/1500\n",
      "36/36 [==============================] - 0s 873us/step - loss: 0.1267 - accuracy: 0.9565\n",
      "Epoch 473/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.1211 - accuracy: 0.9522\n",
      "Epoch 474/1500\n",
      "36/36 [==============================] - 0s 926us/step - loss: 0.1448 - accuracy: 0.9482\n",
      "Epoch 475/1500\n",
      "36/36 [==============================] - 0s 854us/step - loss: 0.1291 - accuracy: 0.9487\n",
      "Epoch 476/1500\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.1303 - accuracy: 0.9530\n",
      "Epoch 477/1500\n",
      "36/36 [==============================] - 0s 867us/step - loss: 0.1404 - accuracy: 0.9469\n",
      "Epoch 478/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.1294 - accuracy: 0.9474\n",
      "Epoch 479/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.1318 - accuracy: 0.9504\n",
      "Epoch 480/1500\n",
      "36/36 [==============================] - 0s 843us/step - loss: 0.1235 - accuracy: 0.9513\n",
      "Epoch 481/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.1140 - accuracy: 0.9556\n",
      "Epoch 482/1500\n",
      "36/36 [==============================] - 0s 895us/step - loss: 0.1431 - accuracy: 0.9465\n",
      "Epoch 483/1500\n",
      "36/36 [==============================] - 0s 920us/step - loss: 0.1381 - accuracy: 0.9491\n",
      "Epoch 484/1500\n",
      "36/36 [==============================] - 0s 913us/step - loss: 0.1266 - accuracy: 0.9535\n",
      "Epoch 485/1500\n",
      "36/36 [==============================] - 0s 837us/step - loss: 0.1368 - accuracy: 0.9508\n",
      "Epoch 486/1500\n",
      "36/36 [==============================] - 0s 801us/step - loss: 0.1356 - accuracy: 0.9504\n",
      "Epoch 487/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.1385 - accuracy: 0.9452\n",
      "Epoch 488/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1326 - accuracy: 0.9530\n",
      "Epoch 489/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1358 - accuracy: 0.9461\n",
      "Epoch 490/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.1295 - accuracy: 0.9474\n",
      "Epoch 491/1500\n",
      "36/36 [==============================] - 0s 865us/step - loss: 0.1191 - accuracy: 0.9504\n",
      "Epoch 492/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.1309 - accuracy: 0.9482\n",
      "Epoch 493/1500\n",
      "36/36 [==============================] - 0s 929us/step - loss: 0.1298 - accuracy: 0.9482\n",
      "Epoch 494/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9548\n",
      "Epoch 495/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.1250 - accuracy: 0.9587\n",
      "Epoch 496/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.1325 - accuracy: 0.9487\n",
      "Epoch 497/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.1350 - accuracy: 0.9504\n",
      "Epoch 498/1500\n",
      "36/36 [==============================] - 0s 848us/step - loss: 0.1156 - accuracy: 0.9539\n",
      "Epoch 499/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.1333 - accuracy: 0.9474\n",
      "Epoch 500/1500\n",
      "36/36 [==============================] - 0s 856us/step - loss: 0.1190 - accuracy: 0.9565\n",
      "Epoch 501/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.1369 - accuracy: 0.9421\n",
      "Epoch 502/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.1187 - accuracy: 0.9556\n",
      "Epoch 503/1500\n",
      "36/36 [==============================] - 0s 840us/step - loss: 0.1259 - accuracy: 0.9526\n",
      "Epoch 504/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.1215 - accuracy: 0.9526\n",
      "Epoch 505/1500\n",
      "36/36 [==============================] - 0s 930us/step - loss: 0.1279 - accuracy: 0.9526\n",
      "Epoch 506/1500\n",
      "36/36 [==============================] - 0s 904us/step - loss: 0.1288 - accuracy: 0.9504\n",
      "Epoch 507/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.1281 - accuracy: 0.9448\n",
      "Epoch 508/1500\n",
      "36/36 [==============================] - 0s 939us/step - loss: 0.1258 - accuracy: 0.9487\n",
      "Epoch 509/1500\n",
      "36/36 [==============================] - 0s 898us/step - loss: 0.1171 - accuracy: 0.9539\n",
      "Epoch 510/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.1395 - accuracy: 0.9443\n",
      "Epoch 511/1500\n",
      " 1/36 [..............................] - ETA: 0s - loss: 0.1042 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 481.\n",
      "36/36 [==============================] - 0s 903us/step - loss: 0.1403 - accuracy: 0.9452\n",
      "Epoch 511: early stopping\n",
      "7/7 [==============================] - 0s 901us/step - loss: 0.9043 - accuracy: 0.7721\n",
      "7/7 [==============================] - 0s 699us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (24/30)\n",
      "Before appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "After appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "Final Test Results - Loss: 0.904304563999176, Accuracy: 0.7720929980278015, Precision: 0.7640184541592991, Recall: 0.7772521812256912, F1 Score: 0.7692645137080332\n",
      "Confusion Matrix:\n",
      " [[122   1  28]\n",
      " [  1   6   0]\n",
      " [ 19   0  38]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "001A    14\n",
      "042A    14\n",
      "059A    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "025A    11\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "005A    10\n",
      "016A    10\n",
      "014B    10\n",
      "071A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "015A     9\n",
      "045A     9\n",
      "022A     9\n",
      "065A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "050A     7\n",
      "099A     7\n",
      "109A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "070A     5\n",
      "021A     5\n",
      "075A     5\n",
      "009A     4\n",
      "104A     4\n",
      "105A     4\n",
      "003A     4\n",
      "113A     3\n",
      "056A     3\n",
      "064A     3\n",
      "060A     3\n",
      "014A     3\n",
      "006A     3\n",
      "025B     2\n",
      "102A     2\n",
      "011A     2\n",
      "061A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "018A     2\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "043A     1\n",
      "115A     1\n",
      "076A     1\n",
      "066A     1\n",
      "026C     1\n",
      "096A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "057A    27\n",
      "055A    20\n",
      "097B    14\n",
      "039A    12\n",
      "040A    10\n",
      "094A     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "023A     6\n",
      "053A     6\n",
      "108A     6\n",
      "023B     5\n",
      "026A     4\n",
      "062A     4\n",
      "052A     4\n",
      "035A     4\n",
      "058A     3\n",
      "012A     3\n",
      "069A     2\n",
      "048A     1\n",
      "088A     1\n",
      "004A     1\n",
      "073A     1\n",
      "041A     1\n",
      "092A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    306\n",
      "X    268\n",
      "F    158\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    94\n",
      "X    80\n",
      "M    31\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 106A, 104A, 059A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 097B, 062A, 039A, 023A, 027A, 069A, 026...\n",
      "kitten                                   [040A, 041A, 048A]\n",
      "senior                 [057A, 055A, 117A, 058A, 094A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 13, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 3, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '020A'\n",
      " '021A' '022A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '012A' '019B' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'022A'}\n",
      "Moved to Test Set:\n",
      "{'022A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A'\n",
      " '009A' '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A'\n",
      " '020A' '021A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '012A' '019B' '022A' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "Length of X_train_val:\n",
      "762\n",
      "Length of y_train_val:\n",
      "762\n",
      "Length of groups_train_val:\n",
      "762\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     466\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     122\n",
      "senior     71\n",
      "kitten     12\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     496\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     92\n",
      "senior    71\n",
      "kitten    12\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 992, 1: 795, 2: 535})\n",
      "Epoch 1/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.2329 - accuracy: 0.4660\n",
      "Epoch 2/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.9477 - accuracy: 0.5857\n",
      "Epoch 3/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.8854 - accuracy: 0.6180\n",
      "Epoch 4/1500\n",
      "37/37 [==============================] - 0s 877us/step - loss: 0.8058 - accuracy: 0.6434\n",
      "Epoch 5/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.7588 - accuracy: 0.6761\n",
      "Epoch 6/1500\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.7315 - accuracy: 0.6908\n",
      "Epoch 7/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.7027 - accuracy: 0.6921\n",
      "Epoch 8/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.6978 - accuracy: 0.6934\n",
      "Epoch 9/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.6822 - accuracy: 0.7110\n",
      "Epoch 10/1500\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.6784 - accuracy: 0.7080\n",
      "Epoch 11/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.6453 - accuracy: 0.7321\n",
      "Epoch 12/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.6353 - accuracy: 0.7278\n",
      "Epoch 13/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.6076 - accuracy: 0.7407\n",
      "Epoch 14/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.5979 - accuracy: 0.7550\n",
      "Epoch 15/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.5868 - accuracy: 0.7593\n",
      "Epoch 16/1500\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.5590 - accuracy: 0.7606\n",
      "Epoch 17/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.5766 - accuracy: 0.7562\n",
      "Epoch 18/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.5630 - accuracy: 0.7644\n",
      "Epoch 19/1500\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.5331 - accuracy: 0.7752\n",
      "Epoch 20/1500\n",
      "37/37 [==============================] - 0s 866us/step - loss: 0.5558 - accuracy: 0.7636\n",
      "Epoch 21/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.5356 - accuracy: 0.7821\n",
      "Epoch 22/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.5309 - accuracy: 0.7808\n",
      "Epoch 23/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.4986 - accuracy: 0.7868\n",
      "Epoch 24/1500\n",
      "37/37 [==============================] - 0s 817us/step - loss: 0.5020 - accuracy: 0.7911\n",
      "Epoch 25/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.5122 - accuracy: 0.7877\n",
      "Epoch 26/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.5141 - accuracy: 0.7842\n",
      "Epoch 27/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.5165 - accuracy: 0.7808\n",
      "Epoch 28/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.4894 - accuracy: 0.7924\n",
      "Epoch 29/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.4844 - accuracy: 0.7933\n",
      "Epoch 30/1500\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.4830 - accuracy: 0.7959\n",
      "Epoch 31/1500\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.4784 - accuracy: 0.7989\n",
      "Epoch 32/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.4832 - accuracy: 0.8015\n",
      "Epoch 33/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.4627 - accuracy: 0.8040\n",
      "Epoch 34/1500\n",
      "37/37 [==============================] - 0s 900us/step - loss: 0.4608 - accuracy: 0.8006\n",
      "Epoch 35/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.4859 - accuracy: 0.8028\n",
      "Epoch 36/1500\n",
      "37/37 [==============================] - 0s 881us/step - loss: 0.4505 - accuracy: 0.8144\n",
      "Epoch 37/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.4474 - accuracy: 0.8062\n",
      "Epoch 38/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.4641 - accuracy: 0.8079\n",
      "Epoch 39/1500\n",
      "37/37 [==============================] - 0s 999us/step - loss: 0.4456 - accuracy: 0.8161\n",
      "Epoch 40/1500\n",
      "37/37 [==============================] - 0s 929us/step - loss: 0.4436 - accuracy: 0.8170\n",
      "Epoch 41/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.8096\n",
      "Epoch 42/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4256 - accuracy: 0.8187\n",
      "Epoch 43/1500\n",
      "37/37 [==============================] - 0s 979us/step - loss: 0.4502 - accuracy: 0.8170\n",
      "Epoch 44/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4350 - accuracy: 0.8165\n",
      "Epoch 45/1500\n",
      "37/37 [==============================] - 0s 944us/step - loss: 0.4322 - accuracy: 0.8165\n",
      "Epoch 46/1500\n",
      "37/37 [==============================] - 0s 908us/step - loss: 0.4333 - accuracy: 0.8217\n",
      "Epoch 47/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4322 - accuracy: 0.8165\n",
      "Epoch 48/1500\n",
      "37/37 [==============================] - 0s 922us/step - loss: 0.4261 - accuracy: 0.8221\n",
      "Epoch 49/1500\n",
      "37/37 [==============================] - 0s 901us/step - loss: 0.4226 - accuracy: 0.8260\n",
      "Epoch 50/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.4474 - accuracy: 0.8243\n",
      "Epoch 51/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.4095 - accuracy: 0.8295\n",
      "Epoch 52/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.4064 - accuracy: 0.8307\n",
      "Epoch 53/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.3831 - accuracy: 0.8424\n",
      "Epoch 54/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.4213 - accuracy: 0.8273\n",
      "Epoch 55/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.4178 - accuracy: 0.8200\n",
      "Epoch 56/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.4133 - accuracy: 0.8273\n",
      "Epoch 57/1500\n",
      "37/37 [==============================] - 0s 881us/step - loss: 0.3978 - accuracy: 0.8368\n",
      "Epoch 58/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.3982 - accuracy: 0.8385\n",
      "Epoch 59/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.4013 - accuracy: 0.8394\n",
      "Epoch 60/1500\n",
      "37/37 [==============================] - 0s 833us/step - loss: 0.4000 - accuracy: 0.8320\n",
      "Epoch 61/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.3884 - accuracy: 0.8359\n",
      "Epoch 62/1500\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.4175 - accuracy: 0.8187\n",
      "Epoch 63/1500\n",
      "37/37 [==============================] - 0s 876us/step - loss: 0.4010 - accuracy: 0.8290\n",
      "Epoch 64/1500\n",
      "37/37 [==============================] - 0s 938us/step - loss: 0.3869 - accuracy: 0.8389\n",
      "Epoch 65/1500\n",
      "37/37 [==============================] - 0s 885us/step - loss: 0.3672 - accuracy: 0.8527\n",
      "Epoch 66/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.3962 - accuracy: 0.8286\n",
      "Epoch 67/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.3869 - accuracy: 0.8411\n",
      "Epoch 68/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.3930 - accuracy: 0.8329\n",
      "Epoch 69/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.3945 - accuracy: 0.8471\n",
      "Epoch 70/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.3743 - accuracy: 0.8463\n",
      "Epoch 71/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.3917 - accuracy: 0.8217\n",
      "Epoch 72/1500\n",
      "37/37 [==============================] - 0s 945us/step - loss: 0.3754 - accuracy: 0.8424\n",
      "Epoch 73/1500\n",
      "37/37 [==============================] - 0s 910us/step - loss: 0.3601 - accuracy: 0.8497\n",
      "Epoch 74/1500\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.3857 - accuracy: 0.8402\n",
      "Epoch 75/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.3693 - accuracy: 0.8441\n",
      "Epoch 76/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.3830 - accuracy: 0.8415\n",
      "Epoch 77/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.3928 - accuracy: 0.8411\n",
      "Epoch 78/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.3579 - accuracy: 0.8562\n",
      "Epoch 79/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.3622 - accuracy: 0.8501\n",
      "Epoch 80/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.3746 - accuracy: 0.8424\n",
      "Epoch 81/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.3513 - accuracy: 0.8523\n",
      "Epoch 82/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.3672 - accuracy: 0.8506\n",
      "Epoch 83/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.3701 - accuracy: 0.8424\n",
      "Epoch 84/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.3581 - accuracy: 0.8540\n",
      "Epoch 85/1500\n",
      "37/37 [==============================] - 0s 938us/step - loss: 0.3558 - accuracy: 0.8575\n",
      "Epoch 86/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.3654 - accuracy: 0.8493\n",
      "Epoch 87/1500\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.3505 - accuracy: 0.8553\n",
      "Epoch 88/1500\n",
      "37/37 [==============================] - 0s 904us/step - loss: 0.3430 - accuracy: 0.8609\n",
      "Epoch 89/1500\n",
      "37/37 [==============================] - 0s 924us/step - loss: 0.3423 - accuracy: 0.8609\n",
      "Epoch 90/1500\n",
      "37/37 [==============================] - 0s 910us/step - loss: 0.3477 - accuracy: 0.8540\n",
      "Epoch 91/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.3585 - accuracy: 0.8557\n",
      "Epoch 92/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.3474 - accuracy: 0.8605\n",
      "Epoch 93/1500\n",
      "37/37 [==============================] - 0s 927us/step - loss: 0.3520 - accuracy: 0.8510\n",
      "Epoch 94/1500\n",
      "37/37 [==============================] - 0s 935us/step - loss: 0.3546 - accuracy: 0.8514\n",
      "Epoch 95/1500\n",
      "37/37 [==============================] - 0s 948us/step - loss: 0.3414 - accuracy: 0.8549\n",
      "Epoch 96/1500\n",
      "37/37 [==============================] - 0s 886us/step - loss: 0.3441 - accuracy: 0.8575\n",
      "Epoch 97/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.3545 - accuracy: 0.8575\n",
      "Epoch 98/1500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.3352 - accuracy: 0.8613\n",
      "Epoch 99/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8648\n",
      "Epoch 100/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3435 - accuracy: 0.8514\n",
      "Epoch 101/1500\n",
      "37/37 [==============================] - 0s 881us/step - loss: 0.3304 - accuracy: 0.8566\n",
      "Epoch 102/1500\n",
      "37/37 [==============================] - 0s 901us/step - loss: 0.3397 - accuracy: 0.8699\n",
      "Epoch 103/1500\n",
      "37/37 [==============================] - 0s 879us/step - loss: 0.3432 - accuracy: 0.8592\n",
      "Epoch 104/1500\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.3266 - accuracy: 0.8695\n",
      "Epoch 105/1500\n",
      "37/37 [==============================] - 0s 939us/step - loss: 0.3286 - accuracy: 0.8635\n",
      "Epoch 106/1500\n",
      "37/37 [==============================] - 0s 915us/step - loss: 0.3326 - accuracy: 0.8609\n",
      "Epoch 107/1500\n",
      "37/37 [==============================] - 0s 926us/step - loss: 0.3516 - accuracy: 0.8570\n",
      "Epoch 108/1500\n",
      "37/37 [==============================] - 0s 905us/step - loss: 0.3335 - accuracy: 0.8566\n",
      "Epoch 109/1500\n",
      "37/37 [==============================] - 0s 862us/step - loss: 0.3296 - accuracy: 0.8669\n",
      "Epoch 110/1500\n",
      "37/37 [==============================] - 0s 884us/step - loss: 0.3195 - accuracy: 0.8652\n",
      "Epoch 111/1500\n",
      "37/37 [==============================] - 0s 894us/step - loss: 0.3134 - accuracy: 0.8730\n",
      "Epoch 112/1500\n",
      "37/37 [==============================] - 0s 806us/step - loss: 0.3220 - accuracy: 0.8717\n",
      "Epoch 113/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.3077 - accuracy: 0.8738\n",
      "Epoch 114/1500\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.3235 - accuracy: 0.8665\n",
      "Epoch 115/1500\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.3251 - accuracy: 0.8596\n",
      "Epoch 116/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.3314 - accuracy: 0.8635\n",
      "Epoch 117/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.3252 - accuracy: 0.8665\n",
      "Epoch 118/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.3092 - accuracy: 0.8661\n",
      "Epoch 119/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.3255 - accuracy: 0.8686\n",
      "Epoch 120/1500\n",
      "37/37 [==============================] - 0s 885us/step - loss: 0.3314 - accuracy: 0.8639\n",
      "Epoch 121/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.3164 - accuracy: 0.8674\n",
      "Epoch 122/1500\n",
      "37/37 [==============================] - 0s 894us/step - loss: 0.3157 - accuracy: 0.8747\n",
      "Epoch 123/1500\n",
      "37/37 [==============================] - 0s 877us/step - loss: 0.3213 - accuracy: 0.8579\n",
      "Epoch 124/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.3167 - accuracy: 0.8678\n",
      "Epoch 125/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.3114 - accuracy: 0.8717\n",
      "Epoch 126/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.3186 - accuracy: 0.8665\n",
      "Epoch 127/1500\n",
      "37/37 [==============================] - 0s 897us/step - loss: 0.2968 - accuracy: 0.8811\n",
      "Epoch 128/1500\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.3244 - accuracy: 0.8695\n",
      "Epoch 129/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.3163 - accuracy: 0.8708\n",
      "Epoch 130/1500\n",
      "37/37 [==============================] - 0s 974us/step - loss: 0.3059 - accuracy: 0.8773\n",
      "Epoch 131/1500\n",
      "37/37 [==============================] - 0s 893us/step - loss: 0.3080 - accuracy: 0.8777\n",
      "Epoch 132/1500\n",
      "37/37 [==============================] - 0s 918us/step - loss: 0.3061 - accuracy: 0.8712\n",
      "Epoch 133/1500\n",
      "37/37 [==============================] - 0s 917us/step - loss: 0.2998 - accuracy: 0.8820\n",
      "Epoch 134/1500\n",
      "37/37 [==============================] - 0s 918us/step - loss: 0.2971 - accuracy: 0.8829\n",
      "Epoch 135/1500\n",
      "37/37 [==============================] - 0s 980us/step - loss: 0.3089 - accuracy: 0.8773\n",
      "Epoch 136/1500\n",
      "37/37 [==============================] - 0s 888us/step - loss: 0.2905 - accuracy: 0.8837\n",
      "Epoch 137/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2832 - accuracy: 0.8893\n",
      "Epoch 138/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.2937 - accuracy: 0.8816\n",
      "Epoch 139/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2903 - accuracy: 0.8820\n",
      "Epoch 140/1500\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.2927 - accuracy: 0.8798\n",
      "Epoch 141/1500\n",
      "37/37 [==============================] - 0s 879us/step - loss: 0.3007 - accuracy: 0.8712\n",
      "Epoch 142/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.2929 - accuracy: 0.8824\n",
      "Epoch 143/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.2880 - accuracy: 0.8824\n",
      "Epoch 144/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.2934 - accuracy: 0.8781\n",
      "Epoch 145/1500\n",
      "37/37 [==============================] - 0s 872us/step - loss: 0.2940 - accuracy: 0.8773\n",
      "Epoch 146/1500\n",
      "37/37 [==============================] - 0s 891us/step - loss: 0.2943 - accuracy: 0.8811\n",
      "Epoch 147/1500\n",
      "37/37 [==============================] - 0s 874us/step - loss: 0.2951 - accuracy: 0.8803\n",
      "Epoch 148/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.2846 - accuracy: 0.8842\n",
      "Epoch 149/1500\n",
      "37/37 [==============================] - 0s 900us/step - loss: 0.2793 - accuracy: 0.8824\n",
      "Epoch 150/1500\n",
      "37/37 [==============================] - 0s 893us/step - loss: 0.2836 - accuracy: 0.8807\n",
      "Epoch 151/1500\n",
      "37/37 [==============================] - 0s 902us/step - loss: 0.2726 - accuracy: 0.8872\n",
      "Epoch 152/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2838 - accuracy: 0.8846\n",
      "Epoch 153/1500\n",
      "37/37 [==============================] - 0s 867us/step - loss: 0.2848 - accuracy: 0.8794\n",
      "Epoch 154/1500\n",
      "37/37 [==============================] - 0s 867us/step - loss: 0.2798 - accuracy: 0.8824\n",
      "Epoch 155/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.2895 - accuracy: 0.8872\n",
      "Epoch 156/1500\n",
      "37/37 [==============================] - 0s 876us/step - loss: 0.2784 - accuracy: 0.8906\n",
      "Epoch 157/1500\n",
      "37/37 [==============================] - 0s 893us/step - loss: 0.2833 - accuracy: 0.8820\n",
      "Epoch 158/1500\n",
      "37/37 [==============================] - 0s 915us/step - loss: 0.2889 - accuracy: 0.8837\n",
      "Epoch 159/1500\n",
      "37/37 [==============================] - 0s 901us/step - loss: 0.2802 - accuracy: 0.8880\n",
      "Epoch 160/1500\n",
      "37/37 [==============================] - 0s 871us/step - loss: 0.2807 - accuracy: 0.8807\n",
      "Epoch 161/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2892 - accuracy: 0.8790\n",
      "Epoch 162/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.2724 - accuracy: 0.8837\n",
      "Epoch 163/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.2718 - accuracy: 0.8880\n",
      "Epoch 164/1500\n",
      "37/37 [==============================] - 0s 874us/step - loss: 0.2629 - accuracy: 0.8941\n",
      "Epoch 165/1500\n",
      "37/37 [==============================] - 0s 807us/step - loss: 0.2760 - accuracy: 0.8893\n",
      "Epoch 166/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.2811 - accuracy: 0.8906\n",
      "Epoch 167/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.2687 - accuracy: 0.8919\n",
      "Epoch 168/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2621 - accuracy: 0.8984\n",
      "Epoch 169/1500\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.2650 - accuracy: 0.8880\n",
      "Epoch 170/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.2716 - accuracy: 0.8979\n",
      "Epoch 171/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.2602 - accuracy: 0.8885\n",
      "Epoch 172/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2555 - accuracy: 0.9014\n",
      "Epoch 173/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2574 - accuracy: 0.8997\n",
      "Epoch 174/1500\n",
      "37/37 [==============================] - 0s 813us/step - loss: 0.2743 - accuracy: 0.8919\n",
      "Epoch 175/1500\n",
      "37/37 [==============================] - 0s 892us/step - loss: 0.2753 - accuracy: 0.8859\n",
      "Epoch 176/1500\n",
      "37/37 [==============================] - 0s 946us/step - loss: 0.2737 - accuracy: 0.8854\n",
      "Epoch 177/1500\n",
      "37/37 [==============================] - 0s 921us/step - loss: 0.2676 - accuracy: 0.8898\n",
      "Epoch 178/1500\n",
      "37/37 [==============================] - 0s 951us/step - loss: 0.2783 - accuracy: 0.8876\n",
      "Epoch 179/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2650 - accuracy: 0.8915\n",
      "Epoch 180/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.8997\n",
      "Epoch 181/1500\n",
      "37/37 [==============================] - 0s 974us/step - loss: 0.2800 - accuracy: 0.8876\n",
      "Epoch 182/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.2621 - accuracy: 0.8928\n",
      "Epoch 183/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.2648 - accuracy: 0.8975\n",
      "Epoch 184/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.2634 - accuracy: 0.8962\n",
      "Epoch 185/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.2610 - accuracy: 0.8984\n",
      "Epoch 186/1500\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.2581 - accuracy: 0.8997\n",
      "Epoch 187/1500\n",
      "37/37 [==============================] - 0s 882us/step - loss: 0.2506 - accuracy: 0.9005\n",
      "Epoch 188/1500\n",
      "37/37 [==============================] - 0s 965us/step - loss: 0.2706 - accuracy: 0.8941\n",
      "Epoch 189/1500\n",
      "37/37 [==============================] - 0s 936us/step - loss: 0.2501 - accuracy: 0.8971\n",
      "Epoch 190/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.2582 - accuracy: 0.8910\n",
      "Epoch 191/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.2487 - accuracy: 0.9005\n",
      "Epoch 192/1500\n",
      "37/37 [==============================] - 0s 921us/step - loss: 0.2517 - accuracy: 0.8936\n",
      "Epoch 193/1500\n",
      "37/37 [==============================] - 0s 957us/step - loss: 0.2477 - accuracy: 0.9031\n",
      "Epoch 194/1500\n",
      "37/37 [==============================] - 0s 978us/step - loss: 0.2648 - accuracy: 0.8906\n",
      "Epoch 195/1500\n",
      "37/37 [==============================] - 0s 979us/step - loss: 0.2508 - accuracy: 0.9057\n",
      "Epoch 196/1500\n",
      "37/37 [==============================] - 0s 937us/step - loss: 0.2400 - accuracy: 0.9027\n",
      "Epoch 197/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2610 - accuracy: 0.8962\n",
      "Epoch 198/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.2369 - accuracy: 0.9109\n",
      "Epoch 199/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.2534 - accuracy: 0.8962\n",
      "Epoch 200/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.2443 - accuracy: 0.9040\n",
      "Epoch 201/1500\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.2411 - accuracy: 0.9040\n",
      "Epoch 202/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2463 - accuracy: 0.8971\n",
      "Epoch 203/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.2563 - accuracy: 0.8910\n",
      "Epoch 204/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2428 - accuracy: 0.9074\n",
      "Epoch 205/1500\n",
      "37/37 [==============================] - 0s 871us/step - loss: 0.2231 - accuracy: 0.9139\n",
      "Epoch 206/1500\n",
      "37/37 [==============================] - 0s 950us/step - loss: 0.2549 - accuracy: 0.9044\n",
      "Epoch 207/1500\n",
      "37/37 [==============================] - 0s 889us/step - loss: 0.2414 - accuracy: 0.9091\n",
      "Epoch 208/1500\n",
      "37/37 [==============================] - 0s 919us/step - loss: 0.2337 - accuracy: 0.9078\n",
      "Epoch 209/1500\n",
      "37/37 [==============================] - 0s 954us/step - loss: 0.2405 - accuracy: 0.9065\n",
      "Epoch 210/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.2207 - accuracy: 0.9096\n",
      "Epoch 211/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.2362 - accuracy: 0.9053\n",
      "Epoch 212/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.2362 - accuracy: 0.9048\n",
      "Epoch 213/1500\n",
      "37/37 [==============================] - 0s 802us/step - loss: 0.2342 - accuracy: 0.9027\n",
      "Epoch 214/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.2178 - accuracy: 0.9190\n",
      "Epoch 215/1500\n",
      "37/37 [==============================] - 0s 926us/step - loss: 0.2188 - accuracy: 0.9109\n",
      "Epoch 216/1500\n",
      "37/37 [==============================] - 0s 944us/step - loss: 0.2293 - accuracy: 0.9117\n",
      "Epoch 217/1500\n",
      "37/37 [==============================] - 0s 934us/step - loss: 0.2333 - accuracy: 0.9087\n",
      "Epoch 218/1500\n",
      "37/37 [==============================] - 0s 930us/step - loss: 0.2379 - accuracy: 0.9031\n",
      "Epoch 219/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2468 - accuracy: 0.8928\n",
      "Epoch 220/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.2465 - accuracy: 0.9044\n",
      "Epoch 221/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.2201 - accuracy: 0.9134\n",
      "Epoch 222/1500\n",
      "37/37 [==============================] - 0s 867us/step - loss: 0.2290 - accuracy: 0.9096\n",
      "Epoch 223/1500\n",
      "37/37 [==============================] - 0s 939us/step - loss: 0.2271 - accuracy: 0.9143\n",
      "Epoch 224/1500\n",
      "37/37 [==============================] - 0s 923us/step - loss: 0.2349 - accuracy: 0.9040\n",
      "Epoch 225/1500\n",
      "37/37 [==============================] - 0s 891us/step - loss: 0.2314 - accuracy: 0.9044\n",
      "Epoch 226/1500\n",
      "37/37 [==============================] - 0s 898us/step - loss: 0.2308 - accuracy: 0.9083\n",
      "Epoch 227/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.2231 - accuracy: 0.9109\n",
      "Epoch 228/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.2242 - accuracy: 0.9096\n",
      "Epoch 229/1500\n",
      "37/37 [==============================] - 0s 879us/step - loss: 0.2221 - accuracy: 0.9121\n",
      "Epoch 230/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.2303 - accuracy: 0.9053\n",
      "Epoch 231/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.2362 - accuracy: 0.9070\n",
      "Epoch 232/1500\n",
      "37/37 [==============================] - 0s 917us/step - loss: 0.2278 - accuracy: 0.9074\n",
      "Epoch 233/1500\n",
      "37/37 [==============================] - 0s 887us/step - loss: 0.2237 - accuracy: 0.9057\n",
      "Epoch 234/1500\n",
      "37/37 [==============================] - 0s 900us/step - loss: 0.2599 - accuracy: 0.8941\n",
      "Epoch 235/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.2237 - accuracy: 0.9121\n",
      "Epoch 236/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2354 - accuracy: 0.9074\n",
      "Epoch 237/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.2377 - accuracy: 0.9061\n",
      "Epoch 238/1500\n",
      "37/37 [==============================] - 0s 924us/step - loss: 0.2142 - accuracy: 0.9160\n",
      "Epoch 239/1500\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.2271 - accuracy: 0.9126\n",
      "Epoch 240/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.2052 - accuracy: 0.9169\n",
      "Epoch 241/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.2271 - accuracy: 0.9091\n",
      "Epoch 242/1500\n",
      "37/37 [==============================] - 0s 916us/step - loss: 0.2160 - accuracy: 0.9134\n",
      "Epoch 243/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.2342 - accuracy: 0.9035\n",
      "Epoch 244/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.2278 - accuracy: 0.9078\n",
      "Epoch 245/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.2355 - accuracy: 0.9035\n",
      "Epoch 246/1500\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.2278 - accuracy: 0.9096\n",
      "Epoch 247/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2127 - accuracy: 0.9165\n",
      "Epoch 248/1500\n",
      "37/37 [==============================] - 0s 913us/step - loss: 0.2145 - accuracy: 0.9152\n",
      "Epoch 249/1500\n",
      "37/37 [==============================] - 0s 969us/step - loss: 0.2190 - accuracy: 0.9126\n",
      "Epoch 250/1500\n",
      "37/37 [==============================] - 0s 903us/step - loss: 0.1960 - accuracy: 0.9216\n",
      "Epoch 251/1500\n",
      "37/37 [==============================] - 0s 886us/step - loss: 0.2221 - accuracy: 0.9156\n",
      "Epoch 252/1500\n",
      "37/37 [==============================] - 0s 898us/step - loss: 0.2291 - accuracy: 0.9091\n",
      "Epoch 253/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.2079 - accuracy: 0.9152\n",
      "Epoch 254/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.2143 - accuracy: 0.9152\n",
      "Epoch 255/1500\n",
      "37/37 [==============================] - 0s 918us/step - loss: 0.2126 - accuracy: 0.9134\n",
      "Epoch 256/1500\n",
      "37/37 [==============================] - 0s 911us/step - loss: 0.2020 - accuracy: 0.9233\n",
      "Epoch 257/1500\n",
      "37/37 [==============================] - 0s 934us/step - loss: 0.2062 - accuracy: 0.9165\n",
      "Epoch 258/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.2293 - accuracy: 0.9134\n",
      "Epoch 259/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2102 - accuracy: 0.9199\n",
      "Epoch 260/1500\n",
      "37/37 [==============================] - 0s 905us/step - loss: 0.2097 - accuracy: 0.9186\n",
      "Epoch 261/1500\n",
      "37/37 [==============================] - 0s 896us/step - loss: 0.2068 - accuracy: 0.9143\n",
      "Epoch 262/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.2050 - accuracy: 0.9147\n",
      "Epoch 263/1500\n",
      "37/37 [==============================] - 0s 963us/step - loss: 0.2107 - accuracy: 0.9203\n",
      "Epoch 264/1500\n",
      "37/37 [==============================] - 0s 989us/step - loss: 0.2160 - accuracy: 0.9100\n",
      "Epoch 265/1500\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.2269 - accuracy: 0.9044\n",
      "Epoch 266/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.2115 - accuracy: 0.9152\n",
      "Epoch 267/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.2044 - accuracy: 0.9182\n",
      "Epoch 268/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.2121 - accuracy: 0.9147\n",
      "Epoch 269/1500\n",
      "37/37 [==============================] - 0s 862us/step - loss: 0.2123 - accuracy: 0.9165\n",
      "Epoch 270/1500\n",
      "37/37 [==============================] - 0s 863us/step - loss: 0.2035 - accuracy: 0.9242\n",
      "Epoch 271/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.1983 - accuracy: 0.9233\n",
      "Epoch 272/1500\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.2013 - accuracy: 0.9156\n",
      "Epoch 273/1500\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.2104 - accuracy: 0.9169\n",
      "Epoch 274/1500\n",
      "37/37 [==============================] - 0s 815us/step - loss: 0.2192 - accuracy: 0.9057\n",
      "Epoch 275/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.2085 - accuracy: 0.9152\n",
      "Epoch 276/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.1973 - accuracy: 0.9208\n",
      "Epoch 277/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.2314 - accuracy: 0.9109\n",
      "Epoch 278/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.1792 - accuracy: 0.9320\n",
      "Epoch 279/1500\n",
      "37/37 [==============================] - 0s 833us/step - loss: 0.2014 - accuracy: 0.9242\n",
      "Epoch 280/1500\n",
      "37/37 [==============================] - 0s 876us/step - loss: 0.1932 - accuracy: 0.9195\n",
      "Epoch 281/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.2018 - accuracy: 0.9251\n",
      "Epoch 282/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2062 - accuracy: 0.9152\n",
      "Epoch 283/1500\n",
      "37/37 [==============================] - 0s 867us/step - loss: 0.1964 - accuracy: 0.9276\n",
      "Epoch 284/1500\n",
      "37/37 [==============================] - 0s 897us/step - loss: 0.1865 - accuracy: 0.9268\n",
      "Epoch 285/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.1953 - accuracy: 0.9233\n",
      "Epoch 286/1500\n",
      "37/37 [==============================] - 0s 948us/step - loss: 0.1970 - accuracy: 0.9225\n",
      "Epoch 287/1500\n",
      "37/37 [==============================] - 0s 903us/step - loss: 0.1979 - accuracy: 0.9229\n",
      "Epoch 288/1500\n",
      "37/37 [==============================] - 0s 993us/step - loss: 0.1973 - accuracy: 0.9225\n",
      "Epoch 289/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.2013 - accuracy: 0.9251\n",
      "Epoch 290/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.2055 - accuracy: 0.9242\n",
      "Epoch 291/1500\n",
      "37/37 [==============================] - 0s 866us/step - loss: 0.2083 - accuracy: 0.9147\n",
      "Epoch 292/1500\n",
      "37/37 [==============================] - 0s 860us/step - loss: 0.1899 - accuracy: 0.9264\n",
      "Epoch 293/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.1891 - accuracy: 0.9251\n",
      "Epoch 294/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.2195 - accuracy: 0.9096\n",
      "Epoch 295/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.1976 - accuracy: 0.9212\n",
      "Epoch 296/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.1974 - accuracy: 0.9229\n",
      "Epoch 297/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2101 - accuracy: 0.9139\n",
      "Epoch 298/1500\n",
      "37/37 [==============================] - 0s 828us/step - loss: 0.1844 - accuracy: 0.9242\n",
      "Epoch 299/1500\n",
      "37/37 [==============================] - 0s 889us/step - loss: 0.1908 - accuracy: 0.9251\n",
      "Epoch 300/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.1822 - accuracy: 0.9268\n",
      "Epoch 301/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.2003 - accuracy: 0.9220\n",
      "Epoch 302/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.1954 - accuracy: 0.9216\n",
      "Epoch 303/1500\n",
      "37/37 [==============================] - 0s 922us/step - loss: 0.1988 - accuracy: 0.9182\n",
      "Epoch 304/1500\n",
      "37/37 [==============================] - 0s 885us/step - loss: 0.1957 - accuracy: 0.9255\n",
      "Epoch 305/1500\n",
      "37/37 [==============================] - 0s 872us/step - loss: 0.1910 - accuracy: 0.9242\n",
      "Epoch 306/1500\n",
      "37/37 [==============================] - 0s 894us/step - loss: 0.1913 - accuracy: 0.9268\n",
      "Epoch 307/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.1805 - accuracy: 0.9307\n",
      "Epoch 308/1500\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.3174 - accuracy: 0.8750Restoring model weights from the end of the best epoch: 278.\n",
      "37/37 [==============================] - 0s 971us/step - loss: 0.1912 - accuracy: 0.9259\n",
      "Epoch 308: early stopping\n",
      "6/6 [==============================] - 0s 913us/step - loss: 0.9880 - accuracy: 0.6629\n",
      "6/6 [==============================] - 0s 709us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (22/28)\n",
      "Before appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "After appending - Cat IDs: 854, Predictions: 854, Actuals: 854, Gender: 854\n",
      "Final Test Results - Loss: 0.9880028367042542, Accuracy: 0.6628571152687073, Precision: 0.6426675094816688, Recall: 0.7181397564128734, F1 Score: 0.6500602561335347\n",
      "Confusion Matrix:\n",
      " [[75  5 12]\n",
      " [ 1 11  0]\n",
      " [37  4 30]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.657406578993071\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8137785643339157\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7177841365337372\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6606832376793228\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6900438297951684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[1]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2845ad-17c1-494a-8bd0-971aefca9f01",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3910db95-6772-4098-bef1-fb5857901e5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 854, Predictions: 854, Actuals: 854, Gender: 854\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d543c7c2-c11b-4511-ba5a-c52969a61a62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b6f2173-89a8-40ba-afa6-410005c2dcb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (82/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa8d9ba5-9d96-4aee-b3e2-ba87553d1899",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b4ff1f7-d22d-4409-98d4-49e9a82bcb58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[kitten, adult, adult, kitten, adult, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, adult, kitten, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, adult, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, kitten, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, senior, adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[adult, adult, kitten, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [senior, adult, senior, senior, senior, adult,...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A            [kitten, adult, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [adult, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "100   105A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, senior, ...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, adult, senior, ...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C              [senior, senior, adult, adult, adult]         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "15    014A                             [senior, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, adult, senior]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                      [adult, senior, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, senior, senior, adult, adult, ...         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                     [senior, adult, senior, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [kitten, adult, adult, kitten, adult, adult, k...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "48    042A  [kitten, adult, kitten, adult, adult, adult, a...         adult           kitten                  False\n",
       "69    063A  [adult, senior, senior, adult, senior, senior,...        senior            adult                  False\n",
       "42    036A  [adult, senior, senior, senior, senior, adult,...        senior            adult                  False\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, s...        senior            adult                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "103   109A      [adult, adult, kitten, kitten, kitten, adult]         adult           kitten                  False\n",
       "102   108A     [kitten, adult, kitten, kitten, adult, kitten]        kitten           senior                  False\n",
       "101   106A  [adult, senior, adult, senior, senior, adult, ...         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "52    047A  [adult, adult, adult, adult, adult, kitten, ad...         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "76    070A             [adult, senior, adult, senior, senior]        senior            adult                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "57    051B  [adult, adult, kitten, adult, senior, adult, a...         adult           senior                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "18    016A  [senior, adult, adult, senior, senior, adult, ...         adult           senior                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "109   117A  [adult, adult, adult, adult, adult, adult, adult]         adult           senior                  False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ec47984-bc63-4f3f-a7a4-d3979dbd4c52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     61\n",
      "kitten    10\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "952d49d2-180d-4f36-85b0-6e2b96610559",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             61  83.561644\n",
      "1           kitten           15             10  66.666667\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a94de06-f9df-49f6-99e7-abc9024f3dc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885dd133-e156-4b01-8e84-4d94546e0eca",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd0127df-e0fb-4862-9ac4-2113bd346e78",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          558            440  78.853047\n",
      "1           kitten          118             80  67.796610\n",
      "2           senior          178             94  52.808989\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a0df220-5e5c-4c46-8474-ac4ecf291071",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a7528-860f-45b0-8c00-d0ddf10a0d8f",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2924eee7-fd8b-4cee-9e43-bb0352e4c43b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    222      162     72.97\n",
      "1          M    337      259     76.85\n",
      "2          X    295      193     65.42\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7b2139e-f100-44e6-aff4-807f96753499",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58f49-cac6-40b1-8422-e3d95576c453",
   "metadata": {},
   "source": [
    "# RANDOM SEED 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf9d1b64-575e-47ca-bf37-e8916438d506",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[2]))\n",
    "np.random.seed(int(random_seeds[2]))\n",
    "tf.random.set_seed(int(random_seeds[2]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91d5db1d-6c9f-4047-97b9-5a0eef4fbaaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e435598c-cabc-4129-8504-68aac9cc06bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29b526-5098-4ce6-bc77-a93d1e6e1397",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79a922e6-198c-4c43-a6f2-90dc580ae875",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "071A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "033A     9\n",
      "072A     9\n",
      "015A     9\n",
      "022A     9\n",
      "051B     9\n",
      "065A     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "108A     6\n",
      "109A     6\n",
      "037A     6\n",
      "007A     6\n",
      "008A     6\n",
      "025C     5\n",
      "070A     5\n",
      "021A     5\n",
      "034A     5\n",
      "075A     5\n",
      "023B     5\n",
      "035A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "058A     3\n",
      "014A     3\n",
      "011A     2\n",
      "061A     2\n",
      "032A     2\n",
      "093A     2\n",
      "025B     2\n",
      "087A     2\n",
      "069A     2\n",
      "073A     1\n",
      "115A     1\n",
      "088A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "019B     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "005A    10\n",
      "045A     9\n",
      "117A     7\n",
      "053A     6\n",
      "023A     6\n",
      "044A     5\n",
      "105A     4\n",
      "052A     4\n",
      "104A     4\n",
      "018A     2\n",
      "054A     2\n",
      "038A     2\n",
      "102A     2\n",
      "091A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    265\n",
      "M    256\n",
      "F    198\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    83\n",
      "M    81\n",
      "F    54\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 103A, 071A, 028A, 067...\n",
      "kitten    [014B, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [001A, 097B, 019A, 074A, 029A, 005A, 091A, 023...\n",
      "kitten                             [044A, 111A, 045A, 110A]\n",
      "senior                             [104A, 054A, 117A, 051A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "Length of X_train_val:\n",
      "719\n",
      "Length of y_train_val:\n",
      "719\n",
      "Length of groups_train_val:\n",
      "719\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 846, 2: 765, 1: 715})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.0576 - accuracy: 0.5310\n",
      "Epoch 2/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.8775 - accuracy: 0.6165\n",
      "Epoch 3/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.7899 - accuracy: 0.6720\n",
      "Epoch 4/1500\n",
      "37/37 [==============================] - 0s 922us/step - loss: 0.7558 - accuracy: 0.6801\n",
      "Epoch 5/1500\n",
      "37/37 [==============================] - 0s 914us/step - loss: 0.7112 - accuracy: 0.7085\n",
      "Epoch 6/1500\n",
      "37/37 [==============================] - 0s 926us/step - loss: 0.6921 - accuracy: 0.7158\n",
      "Epoch 7/1500\n",
      "37/37 [==============================] - 0s 843us/step - loss: 0.6745 - accuracy: 0.7180\n",
      "Epoch 8/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.6432 - accuracy: 0.7377\n",
      "Epoch 9/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.6519 - accuracy: 0.7347\n",
      "Epoch 10/1500\n",
      "37/37 [==============================] - 0s 889us/step - loss: 0.6141 - accuracy: 0.7575\n",
      "Epoch 11/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.6166 - accuracy: 0.7524\n",
      "Epoch 12/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.6092 - accuracy: 0.7438\n",
      "Epoch 13/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.5681 - accuracy: 0.7739\n",
      "Epoch 14/1500\n",
      "37/37 [==============================] - 0s 826us/step - loss: 0.5733 - accuracy: 0.7709\n",
      "Epoch 15/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.5538 - accuracy: 0.7704\n",
      "Epoch 16/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.5627 - accuracy: 0.7691\n",
      "Epoch 17/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.5542 - accuracy: 0.7640\n",
      "Epoch 18/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.5403 - accuracy: 0.7726\n",
      "Epoch 19/1500\n",
      "37/37 [==============================] - 0s 875us/step - loss: 0.5361 - accuracy: 0.7799\n",
      "Epoch 20/1500\n",
      "37/37 [==============================] - 0s 863us/step - loss: 0.5278 - accuracy: 0.7825\n",
      "Epoch 21/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.5098 - accuracy: 0.7872\n",
      "Epoch 22/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.4995 - accuracy: 0.7949\n",
      "Epoch 23/1500\n",
      "37/37 [==============================] - 0s 929us/step - loss: 0.4910 - accuracy: 0.7966\n",
      "Epoch 24/1500\n",
      "37/37 [==============================] - 0s 982us/step - loss: 0.5054 - accuracy: 0.7880\n",
      "Epoch 25/1500\n",
      "37/37 [==============================] - 0s 954us/step - loss: 0.4894 - accuracy: 0.8005\n",
      "Epoch 26/1500\n",
      "37/37 [==============================] - 0s 889us/step - loss: 0.4815 - accuracy: 0.8070\n",
      "Epoch 27/1500\n",
      "37/37 [==============================] - 0s 902us/step - loss: 0.4802 - accuracy: 0.8009\n",
      "Epoch 28/1500\n",
      "37/37 [==============================] - 0s 928us/step - loss: 0.4683 - accuracy: 0.8186\n",
      "Epoch 29/1500\n",
      "37/37 [==============================] - 0s 874us/step - loss: 0.4779 - accuracy: 0.8022\n",
      "Epoch 30/1500\n",
      "37/37 [==============================] - 0s 888us/step - loss: 0.4545 - accuracy: 0.8104\n",
      "Epoch 31/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.4570 - accuracy: 0.8173\n",
      "Epoch 32/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.4489 - accuracy: 0.8147\n",
      "Epoch 33/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.4618 - accuracy: 0.8078\n",
      "Epoch 34/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.4630 - accuracy: 0.8048\n",
      "Epoch 35/1500\n",
      "37/37 [==============================] - 0s 977us/step - loss: 0.4475 - accuracy: 0.8160\n",
      "Epoch 36/1500\n",
      "37/37 [==============================] - 0s 941us/step - loss: 0.4317 - accuracy: 0.8207\n",
      "Epoch 37/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.4384 - accuracy: 0.8181\n",
      "Epoch 38/1500\n",
      "37/37 [==============================] - 0s 930us/step - loss: 0.4242 - accuracy: 0.8276\n",
      "Epoch 39/1500\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.4407 - accuracy: 0.8138\n",
      "Epoch 40/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.4221 - accuracy: 0.8328\n",
      "Epoch 41/1500\n",
      "37/37 [==============================] - 0s 903us/step - loss: 0.4280 - accuracy: 0.8203\n",
      "Epoch 42/1500\n",
      "37/37 [==============================] - 0s 894us/step - loss: 0.4189 - accuracy: 0.8306\n",
      "Epoch 43/1500\n",
      "37/37 [==============================] - 0s 871us/step - loss: 0.4212 - accuracy: 0.8263\n",
      "Epoch 44/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.4177 - accuracy: 0.8336\n",
      "Epoch 45/1500\n",
      "37/37 [==============================] - 0s 921us/step - loss: 0.4035 - accuracy: 0.8302\n",
      "Epoch 46/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.4065 - accuracy: 0.8353\n",
      "Epoch 47/1500\n",
      "37/37 [==============================] - 0s 913us/step - loss: 0.4017 - accuracy: 0.8405\n",
      "Epoch 48/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4062 - accuracy: 0.8388\n",
      "Epoch 49/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4029 - accuracy: 0.8388\n",
      "Epoch 50/1500\n",
      "37/37 [==============================] - 0s 964us/step - loss: 0.3873 - accuracy: 0.8465\n",
      "Epoch 51/1500\n",
      "37/37 [==============================] - 0s 905us/step - loss: 0.3829 - accuracy: 0.8469\n",
      "Epoch 52/1500\n",
      "37/37 [==============================] - 0s 945us/step - loss: 0.3824 - accuracy: 0.8461\n",
      "Epoch 53/1500\n",
      "37/37 [==============================] - 0s 952us/step - loss: 0.3976 - accuracy: 0.8340\n",
      "Epoch 54/1500\n",
      "37/37 [==============================] - 0s 946us/step - loss: 0.3873 - accuracy: 0.8418\n",
      "Epoch 55/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3799 - accuracy: 0.8426\n",
      "Epoch 56/1500\n",
      "37/37 [==============================] - 0s 957us/step - loss: 0.3833 - accuracy: 0.8383\n",
      "Epoch 57/1500\n",
      "37/37 [==============================] - 0s 983us/step - loss: 0.3937 - accuracy: 0.8401\n",
      "Epoch 58/1500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3754 - accuracy: 0.8560\n",
      "Epoch 59/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.3854 - accuracy: 0.8392\n",
      "Epoch 60/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.3888 - accuracy: 0.8396\n",
      "Epoch 61/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.3737 - accuracy: 0.8512\n",
      "Epoch 62/1500\n",
      "37/37 [==============================] - 0s 916us/step - loss: 0.3679 - accuracy: 0.8426\n",
      "Epoch 63/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.3780 - accuracy: 0.8491\n",
      "Epoch 64/1500\n",
      "37/37 [==============================] - 0s 882us/step - loss: 0.3605 - accuracy: 0.8577\n",
      "Epoch 65/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.3605 - accuracy: 0.8534\n",
      "Epoch 66/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.3620 - accuracy: 0.8525\n",
      "Epoch 67/1500\n",
      "37/37 [==============================] - 0s 862us/step - loss: 0.3528 - accuracy: 0.8607\n",
      "Epoch 68/1500\n",
      "37/37 [==============================] - 0s 843us/step - loss: 0.3740 - accuracy: 0.8444\n",
      "Epoch 69/1500\n",
      "37/37 [==============================] - 0s 952us/step - loss: 0.3494 - accuracy: 0.8616\n",
      "Epoch 70/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.3525 - accuracy: 0.8534\n",
      "Epoch 71/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.3432 - accuracy: 0.8620\n",
      "Epoch 72/1500\n",
      "37/37 [==============================] - 0s 866us/step - loss: 0.3534 - accuracy: 0.8508\n",
      "Epoch 73/1500\n",
      "37/37 [==============================] - 0s 967us/step - loss: 0.3343 - accuracy: 0.8577\n",
      "Epoch 74/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.3580 - accuracy: 0.8530\n",
      "Epoch 75/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.3398 - accuracy: 0.8663\n",
      "Epoch 76/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.3444 - accuracy: 0.8538\n",
      "Epoch 77/1500\n",
      "37/37 [==============================] - 0s 860us/step - loss: 0.3273 - accuracy: 0.8702\n",
      "Epoch 78/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.3434 - accuracy: 0.8564\n",
      "Epoch 79/1500\n",
      "37/37 [==============================] - 0s 828us/step - loss: 0.3439 - accuracy: 0.8573\n",
      "Epoch 80/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.3442 - accuracy: 0.8551\n",
      "Epoch 81/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.3459 - accuracy: 0.8598\n",
      "Epoch 82/1500\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.3248 - accuracy: 0.8672\n",
      "Epoch 83/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.3232 - accuracy: 0.8706\n",
      "Epoch 84/1500\n",
      "37/37 [==============================] - 0s 887us/step - loss: 0.3174 - accuracy: 0.8792\n",
      "Epoch 85/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.3463 - accuracy: 0.8555\n",
      "Epoch 86/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.3404 - accuracy: 0.8629\n",
      "Epoch 87/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.3412 - accuracy: 0.8581\n",
      "Epoch 88/1500\n",
      "37/37 [==============================] - 0s 887us/step - loss: 0.3213 - accuracy: 0.8732\n",
      "Epoch 89/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.3316 - accuracy: 0.8680\n",
      "Epoch 90/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.3305 - accuracy: 0.8581\n",
      "Epoch 91/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.3147 - accuracy: 0.8680\n",
      "Epoch 92/1500\n",
      "37/37 [==============================] - 0s 879us/step - loss: 0.3213 - accuracy: 0.8753\n",
      "Epoch 93/1500\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.3310 - accuracy: 0.8598\n",
      "Epoch 94/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.3111 - accuracy: 0.8689\n",
      "Epoch 95/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.3034 - accuracy: 0.8723\n",
      "Epoch 96/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.3188 - accuracy: 0.8736\n",
      "Epoch 97/1500\n",
      "37/37 [==============================] - 0s 866us/step - loss: 0.3235 - accuracy: 0.8693\n",
      "Epoch 98/1500\n",
      "37/37 [==============================] - 0s 895us/step - loss: 0.3322 - accuracy: 0.8616\n",
      "Epoch 99/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.3221 - accuracy: 0.8650\n",
      "Epoch 100/1500\n",
      "37/37 [==============================] - 0s 886us/step - loss: 0.3136 - accuracy: 0.8770\n",
      "Epoch 101/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.3133 - accuracy: 0.8710\n",
      "Epoch 102/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.3119 - accuracy: 0.8745\n",
      "Epoch 103/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.3021 - accuracy: 0.8826\n",
      "Epoch 104/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.3085 - accuracy: 0.8775\n",
      "Epoch 105/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.3055 - accuracy: 0.8732\n",
      "Epoch 106/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.2999 - accuracy: 0.8801\n",
      "Epoch 107/1500\n",
      "37/37 [==============================] - 0s 901us/step - loss: 0.2858 - accuracy: 0.8917\n",
      "Epoch 108/1500\n",
      "37/37 [==============================] - 0s 894us/step - loss: 0.3020 - accuracy: 0.8809\n",
      "Epoch 109/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.2923 - accuracy: 0.8895\n",
      "Epoch 110/1500\n",
      "37/37 [==============================] - 0s 908us/step - loss: 0.3085 - accuracy: 0.8783\n",
      "Epoch 111/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.2838 - accuracy: 0.8831\n",
      "Epoch 112/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.2939 - accuracy: 0.8809\n",
      "Epoch 113/1500\n",
      "37/37 [==============================] - 0s 894us/step - loss: 0.2918 - accuracy: 0.8826\n",
      "Epoch 114/1500\n",
      "37/37 [==============================] - 0s 893us/step - loss: 0.2850 - accuracy: 0.8887\n",
      "Epoch 115/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.2927 - accuracy: 0.8908\n",
      "Epoch 116/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2887 - accuracy: 0.8869\n",
      "Epoch 117/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.2900 - accuracy: 0.8887\n",
      "Epoch 118/1500\n",
      "37/37 [==============================] - 0s 885us/step - loss: 0.2925 - accuracy: 0.8878\n",
      "Epoch 119/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2976 - accuracy: 0.8839\n",
      "Epoch 120/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.2829 - accuracy: 0.8869\n",
      "Epoch 121/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.2971 - accuracy: 0.8826\n",
      "Epoch 122/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.2874 - accuracy: 0.8882\n",
      "Epoch 123/1500\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.2834 - accuracy: 0.8801\n",
      "Epoch 124/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2960 - accuracy: 0.8809\n",
      "Epoch 125/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.2720 - accuracy: 0.8891\n",
      "Epoch 126/1500\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.2907 - accuracy: 0.8861\n",
      "Epoch 127/1500\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.2907 - accuracy: 0.8809\n",
      "Epoch 128/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2701 - accuracy: 0.8960\n",
      "Epoch 129/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.2896 - accuracy: 0.8844\n",
      "Epoch 130/1500\n",
      "37/37 [==============================] - 0s 828us/step - loss: 0.2733 - accuracy: 0.8887\n",
      "Epoch 131/1500\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.2772 - accuracy: 0.8887\n",
      "Epoch 132/1500\n",
      "37/37 [==============================] - 0s 861us/step - loss: 0.2714 - accuracy: 0.8925\n",
      "Epoch 133/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.2796 - accuracy: 0.8852\n",
      "Epoch 134/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.2700 - accuracy: 0.8934\n",
      "Epoch 135/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.2745 - accuracy: 0.8951\n",
      "Epoch 136/1500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.2643 - accuracy: 0.8934\n",
      "Epoch 137/1500\n",
      "37/37 [==============================] - 0s 976us/step - loss: 0.2888 - accuracy: 0.8921\n",
      "Epoch 138/1500\n",
      "37/37 [==============================] - 0s 935us/step - loss: 0.2827 - accuracy: 0.8826\n",
      "Epoch 139/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.2771 - accuracy: 0.8856\n",
      "Epoch 140/1500\n",
      "37/37 [==============================] - 0s 876us/step - loss: 0.2607 - accuracy: 0.8934\n",
      "Epoch 141/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.2610 - accuracy: 0.8938\n",
      "Epoch 142/1500\n",
      "37/37 [==============================] - 0s 860us/step - loss: 0.2615 - accuracy: 0.8955\n",
      "Epoch 143/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.2626 - accuracy: 0.8981\n",
      "Epoch 144/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.2604 - accuracy: 0.8960\n",
      "Epoch 145/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2609 - accuracy: 0.8929\n",
      "Epoch 146/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.2706 - accuracy: 0.8891\n",
      "Epoch 147/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.2677 - accuracy: 0.8955\n",
      "Epoch 148/1500\n",
      "37/37 [==============================] - 0s 879us/step - loss: 0.2607 - accuracy: 0.8938\n",
      "Epoch 149/1500\n",
      "37/37 [==============================] - 0s 914us/step - loss: 0.2608 - accuracy: 0.8960\n",
      "Epoch 150/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.2671 - accuracy: 0.8955\n",
      "Epoch 151/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.2540 - accuracy: 0.9054\n",
      "Epoch 152/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.2615 - accuracy: 0.8985\n",
      "Epoch 153/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.2507 - accuracy: 0.9046\n",
      "Epoch 154/1500\n",
      "37/37 [==============================] - 0s 769us/step - loss: 0.2531 - accuracy: 0.9024\n",
      "Epoch 155/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.2735 - accuracy: 0.8813\n",
      "Epoch 156/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.2448 - accuracy: 0.9011\n",
      "Epoch 157/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.2466 - accuracy: 0.8968\n",
      "Epoch 158/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.2488 - accuracy: 0.9020\n",
      "Epoch 159/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.2507 - accuracy: 0.9024\n",
      "Epoch 160/1500\n",
      "37/37 [==============================] - 0s 897us/step - loss: 0.2487 - accuracy: 0.9020\n",
      "Epoch 161/1500\n",
      "37/37 [==============================] - 0s 888us/step - loss: 0.2480 - accuracy: 0.9003\n",
      "Epoch 162/1500\n",
      "37/37 [==============================] - 0s 833us/step - loss: 0.2433 - accuracy: 0.9093\n",
      "Epoch 163/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.2386 - accuracy: 0.9063\n",
      "Epoch 164/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2404 - accuracy: 0.9084\n",
      "Epoch 165/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.2514 - accuracy: 0.8977\n",
      "Epoch 166/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2386 - accuracy: 0.9089\n",
      "Epoch 167/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.2471 - accuracy: 0.9097\n",
      "Epoch 168/1500\n",
      "37/37 [==============================] - 0s 855us/step - loss: 0.2379 - accuracy: 0.9093\n",
      "Epoch 169/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.2590 - accuracy: 0.9007\n",
      "Epoch 170/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2356 - accuracy: 0.9050\n",
      "Epoch 171/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.2315 - accuracy: 0.9063\n",
      "Epoch 172/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.2577 - accuracy: 0.8981\n",
      "Epoch 173/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.2367 - accuracy: 0.9101\n",
      "Epoch 174/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.2461 - accuracy: 0.9007\n",
      "Epoch 175/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.2474 - accuracy: 0.9063\n",
      "Epoch 176/1500\n",
      "37/37 [==============================] - 0s 869us/step - loss: 0.2306 - accuracy: 0.9033\n",
      "Epoch 177/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.2335 - accuracy: 0.9084\n",
      "Epoch 178/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.2240 - accuracy: 0.9106\n",
      "Epoch 179/1500\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.2243 - accuracy: 0.9110\n",
      "Epoch 180/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2327 - accuracy: 0.9089\n",
      "Epoch 181/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2384 - accuracy: 0.9063\n",
      "Epoch 182/1500\n",
      "37/37 [==============================] - 0s 971us/step - loss: 0.2394 - accuracy: 0.9071\n",
      "Epoch 183/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2339 - accuracy: 0.9114\n",
      "Epoch 184/1500\n",
      "37/37 [==============================] - 0s 909us/step - loss: 0.2377 - accuracy: 0.9106\n",
      "Epoch 185/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.2421 - accuracy: 0.9101\n",
      "Epoch 186/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.2307 - accuracy: 0.9084\n",
      "Epoch 187/1500\n",
      "37/37 [==============================] - 0s 862us/step - loss: 0.2198 - accuracy: 0.9144\n",
      "Epoch 188/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2249 - accuracy: 0.9119\n",
      "Epoch 189/1500\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.2241 - accuracy: 0.9140\n",
      "Epoch 190/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.2316 - accuracy: 0.9050\n",
      "Epoch 191/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.2349 - accuracy: 0.9067\n",
      "Epoch 192/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.2355 - accuracy: 0.9093\n",
      "Epoch 193/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.2395 - accuracy: 0.9011\n",
      "Epoch 194/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.2159 - accuracy: 0.9166\n",
      "Epoch 195/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.2204 - accuracy: 0.9110\n",
      "Epoch 196/1500\n",
      "37/37 [==============================] - 0s 813us/step - loss: 0.2365 - accuracy: 0.9097\n",
      "Epoch 197/1500\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.2211 - accuracy: 0.9110\n",
      "Epoch 198/1500\n",
      "37/37 [==============================] - 0s 843us/step - loss: 0.2320 - accuracy: 0.9093\n",
      "Epoch 199/1500\n",
      "37/37 [==============================] - 0s 881us/step - loss: 0.2270 - accuracy: 0.9149\n",
      "Epoch 200/1500\n",
      "37/37 [==============================] - 0s 847us/step - loss: 0.2212 - accuracy: 0.9123\n",
      "Epoch 201/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.2314 - accuracy: 0.9106\n",
      "Epoch 202/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.2265 - accuracy: 0.9127\n",
      "Epoch 203/1500\n",
      "37/37 [==============================] - 0s 866us/step - loss: 0.2256 - accuracy: 0.9127\n",
      "Epoch 204/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.2307 - accuracy: 0.9132\n",
      "Epoch 205/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.2179 - accuracy: 0.9119\n",
      "Epoch 206/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.2028 - accuracy: 0.9170\n",
      "Epoch 207/1500\n",
      "37/37 [==============================] - 0s 843us/step - loss: 0.2295 - accuracy: 0.9093\n",
      "Epoch 208/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.2163 - accuracy: 0.9132\n",
      "Epoch 209/1500\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.2271 - accuracy: 0.9114\n",
      "Epoch 210/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.2242 - accuracy: 0.9123\n",
      "Epoch 211/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.2155 - accuracy: 0.9157\n",
      "Epoch 212/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.2354 - accuracy: 0.9093\n",
      "Epoch 213/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.2223 - accuracy: 0.9144\n",
      "Epoch 214/1500\n",
      "37/37 [==============================] - 0s 826us/step - loss: 0.2131 - accuracy: 0.9166\n",
      "Epoch 215/1500\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.2060 - accuracy: 0.9261\n",
      "Epoch 216/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2109 - accuracy: 0.9166\n",
      "Epoch 217/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2160 - accuracy: 0.9162\n",
      "Epoch 218/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.2239 - accuracy: 0.9132\n",
      "Epoch 219/1500\n",
      "37/37 [==============================] - 0s 861us/step - loss: 0.2292 - accuracy: 0.9071\n",
      "Epoch 220/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.2172 - accuracy: 0.9162\n",
      "Epoch 221/1500\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.2144 - accuracy: 0.9200\n",
      "Epoch 222/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.2162 - accuracy: 0.9127\n",
      "Epoch 223/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.2057 - accuracy: 0.9196\n",
      "Epoch 224/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2154 - accuracy: 0.9123\n",
      "Epoch 225/1500\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.2044 - accuracy: 0.9205\n",
      "Epoch 226/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.1984 - accuracy: 0.9261\n",
      "Epoch 227/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.1973 - accuracy: 0.9273\n",
      "Epoch 228/1500\n",
      "37/37 [==============================] - 0s 824us/step - loss: 0.2194 - accuracy: 0.9097\n",
      "Epoch 229/1500\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.2071 - accuracy: 0.9218\n",
      "Epoch 230/1500\n",
      "37/37 [==============================] - 0s 827us/step - loss: 0.2205 - accuracy: 0.9119\n",
      "Epoch 231/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.2104 - accuracy: 0.9230\n",
      "Epoch 232/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.2225 - accuracy: 0.9149\n",
      "Epoch 233/1500\n",
      "37/37 [==============================] - 0s 863us/step - loss: 0.2079 - accuracy: 0.9114\n",
      "Epoch 234/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.2240 - accuracy: 0.9110\n",
      "Epoch 235/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.1972 - accuracy: 0.9252\n",
      "Epoch 236/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2280 - accuracy: 0.9071\n",
      "Epoch 237/1500\n",
      "37/37 [==============================] - 0s 876us/step - loss: 0.1931 - accuracy: 0.9252\n",
      "Epoch 238/1500\n",
      "37/37 [==============================] - 0s 860us/step - loss: 0.2078 - accuracy: 0.9166\n",
      "Epoch 239/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.1942 - accuracy: 0.9243\n",
      "Epoch 240/1500\n",
      "37/37 [==============================] - 0s 908us/step - loss: 0.2017 - accuracy: 0.9205\n",
      "Epoch 241/1500\n",
      "37/37 [==============================] - 0s 826us/step - loss: 0.1979 - accuracy: 0.9213\n",
      "Epoch 242/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.2018 - accuracy: 0.9239\n",
      "Epoch 243/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.2180 - accuracy: 0.9101\n",
      "Epoch 244/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.1970 - accuracy: 0.9230\n",
      "Epoch 245/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2020 - accuracy: 0.9218\n",
      "Epoch 246/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.1909 - accuracy: 0.9295\n",
      "Epoch 247/1500\n",
      "37/37 [==============================] - 0s 820us/step - loss: 0.2077 - accuracy: 0.9248\n",
      "Epoch 248/1500\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.1958 - accuracy: 0.9179\n",
      "Epoch 249/1500\n",
      "37/37 [==============================] - 0s 840us/step - loss: 0.1975 - accuracy: 0.9175\n",
      "Epoch 250/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.2067 - accuracy: 0.9192\n",
      "Epoch 251/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.1915 - accuracy: 0.9291\n",
      "Epoch 252/1500\n",
      "37/37 [==============================] - 0s 864us/step - loss: 0.1989 - accuracy: 0.9265\n",
      "Epoch 253/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.1825 - accuracy: 0.9325\n",
      "Epoch 254/1500\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.1879 - accuracy: 0.9278\n",
      "Epoch 255/1500\n",
      "37/37 [==============================] - 0s 863us/step - loss: 0.1672 - accuracy: 0.9347\n",
      "Epoch 256/1500\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.2088 - accuracy: 0.9200\n",
      "Epoch 257/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.1969 - accuracy: 0.9235\n",
      "Epoch 258/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.1919 - accuracy: 0.9321\n",
      "Epoch 259/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.1857 - accuracy: 0.9278\n",
      "Epoch 260/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.1816 - accuracy: 0.9295\n",
      "Epoch 261/1500\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.1827 - accuracy: 0.9243\n",
      "Epoch 262/1500\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.1812 - accuracy: 0.9338\n",
      "Epoch 263/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.1988 - accuracy: 0.9235\n",
      "Epoch 264/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.1839 - accuracy: 0.9209\n",
      "Epoch 265/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.2023 - accuracy: 0.9200\n",
      "Epoch 266/1500\n",
      "37/37 [==============================] - 0s 846us/step - loss: 0.1955 - accuracy: 0.9222\n",
      "Epoch 267/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.1732 - accuracy: 0.9390\n",
      "Epoch 268/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.1891 - accuracy: 0.9261\n",
      "Epoch 269/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.2174 - accuracy: 0.9114\n",
      "Epoch 270/1500\n",
      "37/37 [==============================] - 0s 831us/step - loss: 0.2002 - accuracy: 0.9179\n",
      "Epoch 271/1500\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.1849 - accuracy: 0.9342\n",
      "Epoch 272/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.2014 - accuracy: 0.9218\n",
      "Epoch 273/1500\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.1887 - accuracy: 0.9282\n",
      "Epoch 274/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.1834 - accuracy: 0.9321\n",
      "Epoch 275/1500\n",
      "37/37 [==============================] - 0s 839us/step - loss: 0.1896 - accuracy: 0.9273\n",
      "Epoch 276/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.1787 - accuracy: 0.9321\n",
      "Epoch 277/1500\n",
      "37/37 [==============================] - 0s 824us/step - loss: 0.1827 - accuracy: 0.9291\n",
      "Epoch 278/1500\n",
      "37/37 [==============================] - 0s 865us/step - loss: 0.1965 - accuracy: 0.9222\n",
      "Epoch 279/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.1903 - accuracy: 0.9269\n",
      "Epoch 280/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.1835 - accuracy: 0.9338\n",
      "Epoch 281/1500\n",
      "37/37 [==============================] - 0s 830us/step - loss: 0.1969 - accuracy: 0.9222\n",
      "Epoch 282/1500\n",
      "37/37 [==============================] - 0s 826us/step - loss: 0.1906 - accuracy: 0.9278\n",
      "Epoch 283/1500\n",
      "37/37 [==============================] - 0s 844us/step - loss: 0.1763 - accuracy: 0.9312\n",
      "Epoch 284/1500\n",
      "37/37 [==============================] - 0s 3ms/step - loss: 0.1801 - accuracy: 0.9261\n",
      "Epoch 285/1500\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.2078 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 255.\n",
      "37/37 [==============================] - 0s 973us/step - loss: 0.1710 - accuracy: 0.9342\n",
      "Epoch 285: early stopping\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 1.0085 - accuracy: 0.6284\n",
      "7/7 [==============================] - 0s 638us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.60 (15/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "Final Test Results - Loss: 1.0085262060165405, Accuracy: 0.6284403800964355, Precision: 0.5704861111111111, Recall: 0.596046176046176, F1 Score: 0.5646449611973775\n",
      "Confusion Matrix:\n",
      " [[106   8  51]\n",
      " [  6  22   0]\n",
      " [ 16   0   9]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "072A     9\n",
      "033A     9\n",
      "045A     9\n",
      "022A     9\n",
      "094A     8\n",
      "013B     8\n",
      "010A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "007A     6\n",
      "037A     6\n",
      "053A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "021A     5\n",
      "035A     4\n",
      "003A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "009A     4\n",
      "105A     4\n",
      "058A     3\n",
      "064A     3\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "014A     3\n",
      "093A     2\n",
      "025B     2\n",
      "038A     2\n",
      "087A     2\n",
      "102A     2\n",
      "032A     2\n",
      "054A     2\n",
      "018A     2\n",
      "115A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "110A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "088A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "042A    14\n",
      "106A    14\n",
      "116A    12\n",
      "039A    12\n",
      "063A    11\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "014B    10\n",
      "065A     9\n",
      "015A     9\n",
      "051B     9\n",
      "095A     8\n",
      "070A     5\n",
      "075A     5\n",
      "023B     5\n",
      "062A     4\n",
      "060A     3\n",
      "011A     2\n",
      "069A     2\n",
      "061A     2\n",
      "043A     1\n",
      "092A     1\n",
      "076A     1\n",
      "004A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    241\n",
      "F    181\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "X    75\n",
      "F    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 001A, 103A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 111A, 046A, 047A, 109A, 050A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 104A, 055A, 059A, 113A, 054...\n",
      "Name: cat_id, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 071A, 062A, 002B, 095A, 065A, 039...\n",
      "kitten                             [014B, 040A, 042A, 043A]\n",
      "senior                 [106A, 116A, 051B, 016A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 12, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 4, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '018A' '019A' '020A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '004A' '011A' '014B' '015A' '016A' '019B' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'020A'}\n",
      "Moved to Test Set:\n",
      "{'020A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '018A' '019A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '011A' '014B' '015A' '016A' '019B' '020A' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "Length of X_train_val:\n",
      "711\n",
      "Length of y_train_val:\n",
      "711\n",
      "Length of groups_train_val:\n",
      "711\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     430\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     158\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     446\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     142\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 892, 1: 680, 2: 645})\n",
      "Epoch 1/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 1.1216 - accuracy: 0.4867\n",
      "Epoch 2/1500\n",
      "35/35 [==============================] - 0s 992us/step - loss: 0.9112 - accuracy: 0.5900\n",
      "Epoch 3/1500\n",
      "35/35 [==============================] - 0s 942us/step - loss: 0.8576 - accuracy: 0.6247\n",
      "Epoch 4/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.8044 - accuracy: 0.6581\n",
      "Epoch 5/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.7711 - accuracy: 0.6631\n",
      "Epoch 6/1500\n",
      "35/35 [==============================] - 0s 796us/step - loss: 0.7270 - accuracy: 0.6955\n",
      "Epoch 7/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.7294 - accuracy: 0.6861\n",
      "Epoch 8/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.6901 - accuracy: 0.7104\n",
      "Epoch 9/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.6587 - accuracy: 0.7163\n",
      "Epoch 10/1500\n",
      "35/35 [==============================] - 0s 827us/step - loss: 0.6676 - accuracy: 0.7068\n",
      "Epoch 11/1500\n",
      "35/35 [==============================] - 0s 826us/step - loss: 0.6638 - accuracy: 0.7145\n",
      "Epoch 12/1500\n",
      "35/35 [==============================] - 0s 878us/step - loss: 0.6245 - accuracy: 0.7226\n",
      "Epoch 13/1500\n",
      "35/35 [==============================] - 0s 911us/step - loss: 0.6031 - accuracy: 0.7510\n",
      "Epoch 14/1500\n",
      "35/35 [==============================] - 0s 890us/step - loss: 0.5994 - accuracy: 0.7415\n",
      "Epoch 15/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.5923 - accuracy: 0.7501\n",
      "Epoch 16/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.5878 - accuracy: 0.7524\n",
      "Epoch 17/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.5728 - accuracy: 0.7569\n",
      "Epoch 18/1500\n",
      "35/35 [==============================] - 0s 884us/step - loss: 0.5551 - accuracy: 0.7659\n",
      "Epoch 19/1500\n",
      "35/35 [==============================] - 0s 845us/step - loss: 0.5523 - accuracy: 0.7673\n",
      "Epoch 20/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.5677 - accuracy: 0.7600\n",
      "Epoch 21/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.5333 - accuracy: 0.7668\n",
      "Epoch 22/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.5393 - accuracy: 0.7686\n",
      "Epoch 23/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.5275 - accuracy: 0.7718\n",
      "Epoch 24/1500\n",
      "35/35 [==============================] - 0s 871us/step - loss: 0.5089 - accuracy: 0.7826\n",
      "Epoch 25/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.5218 - accuracy: 0.7794\n",
      "Epoch 26/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.4968 - accuracy: 0.7903\n",
      "Epoch 27/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.5042 - accuracy: 0.7939\n",
      "Epoch 28/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.4916 - accuracy: 0.7866\n",
      "Epoch 29/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.4819 - accuracy: 0.8060\n",
      "Epoch 30/1500\n",
      "35/35 [==============================] - 0s 811us/step - loss: 0.4844 - accuracy: 0.7948\n",
      "Epoch 31/1500\n",
      "35/35 [==============================] - 0s 867us/step - loss: 0.4633 - accuracy: 0.8056\n",
      "Epoch 32/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.4741 - accuracy: 0.7943\n",
      "Epoch 33/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.4675 - accuracy: 0.8056\n",
      "Epoch 34/1500\n",
      "35/35 [==============================] - 0s 840us/step - loss: 0.4654 - accuracy: 0.8088\n",
      "Epoch 35/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.4609 - accuracy: 0.8024\n",
      "Epoch 36/1500\n",
      "35/35 [==============================] - 0s 892us/step - loss: 0.4442 - accuracy: 0.8173\n",
      "Epoch 37/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.4638 - accuracy: 0.8069\n",
      "Epoch 38/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.4464 - accuracy: 0.8088\n",
      "Epoch 39/1500\n",
      "35/35 [==============================] - 0s 864us/step - loss: 0.4330 - accuracy: 0.8214\n",
      "Epoch 40/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.4276 - accuracy: 0.8173\n",
      "Epoch 41/1500\n",
      "35/35 [==============================] - 0s 890us/step - loss: 0.4203 - accuracy: 0.8182\n",
      "Epoch 42/1500\n",
      "35/35 [==============================] - 0s 888us/step - loss: 0.4298 - accuracy: 0.8137\n",
      "Epoch 43/1500\n",
      "35/35 [==============================] - 0s 876us/step - loss: 0.4357 - accuracy: 0.8263\n",
      "Epoch 44/1500\n",
      "35/35 [==============================] - 0s 855us/step - loss: 0.4109 - accuracy: 0.8259\n",
      "Epoch 45/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.4121 - accuracy: 0.8236\n",
      "Epoch 46/1500\n",
      "35/35 [==============================] - 0s 841us/step - loss: 0.4243 - accuracy: 0.8223\n",
      "Epoch 47/1500\n",
      "35/35 [==============================] - 0s 849us/step - loss: 0.4131 - accuracy: 0.8295\n",
      "Epoch 48/1500\n",
      "35/35 [==============================] - 0s 923us/step - loss: 0.4085 - accuracy: 0.8372\n",
      "Epoch 49/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.3952 - accuracy: 0.8286\n",
      "Epoch 50/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.3933 - accuracy: 0.8358\n",
      "Epoch 51/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.3882 - accuracy: 0.8358\n",
      "Epoch 52/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.3860 - accuracy: 0.8435\n",
      "Epoch 53/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.3916 - accuracy: 0.8403\n",
      "Epoch 54/1500\n",
      "35/35 [==============================] - 0s 905us/step - loss: 0.3756 - accuracy: 0.8462\n",
      "Epoch 55/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.3831 - accuracy: 0.8358\n",
      "Epoch 56/1500\n",
      "35/35 [==============================] - 0s 872us/step - loss: 0.3739 - accuracy: 0.8498\n",
      "Epoch 57/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.3803 - accuracy: 0.8439\n",
      "Epoch 58/1500\n",
      "35/35 [==============================] - 0s 831us/step - loss: 0.3749 - accuracy: 0.8426\n",
      "Epoch 59/1500\n",
      "35/35 [==============================] - 0s 849us/step - loss: 0.3880 - accuracy: 0.8376\n",
      "Epoch 60/1500\n",
      "35/35 [==============================] - 0s 940us/step - loss: 0.3687 - accuracy: 0.8484\n",
      "Epoch 61/1500\n",
      "35/35 [==============================] - 0s 950us/step - loss: 0.3909 - accuracy: 0.8408\n",
      "Epoch 62/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3636 - accuracy: 0.8466\n",
      "Epoch 63/1500\n",
      "35/35 [==============================] - 0s 988us/step - loss: 0.3620 - accuracy: 0.8471\n",
      "Epoch 64/1500\n",
      "35/35 [==============================] - 0s 922us/step - loss: 0.3581 - accuracy: 0.8507\n",
      "Epoch 65/1500\n",
      "35/35 [==============================] - 0s 954us/step - loss: 0.3695 - accuracy: 0.8412\n",
      "Epoch 66/1500\n",
      "35/35 [==============================] - 0s 875us/step - loss: 0.3577 - accuracy: 0.8512\n",
      "Epoch 67/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.3585 - accuracy: 0.8525\n",
      "Epoch 68/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.3507 - accuracy: 0.8548\n",
      "Epoch 69/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.3553 - accuracy: 0.8475\n",
      "Epoch 70/1500\n",
      "35/35 [==============================] - 0s 867us/step - loss: 0.3483 - accuracy: 0.8471\n",
      "Epoch 71/1500\n",
      "35/35 [==============================] - 0s 982us/step - loss: 0.3418 - accuracy: 0.8557\n",
      "Epoch 72/1500\n",
      "35/35 [==============================] - 0s 926us/step - loss: 0.3466 - accuracy: 0.8566\n",
      "Epoch 73/1500\n",
      "35/35 [==============================] - 0s 875us/step - loss: 0.3441 - accuracy: 0.8539\n",
      "Epoch 74/1500\n",
      "35/35 [==============================] - 0s 870us/step - loss: 0.3451 - accuracy: 0.8579\n",
      "Epoch 75/1500\n",
      "35/35 [==============================] - 0s 881us/step - loss: 0.3428 - accuracy: 0.8588\n",
      "Epoch 76/1500\n",
      "35/35 [==============================] - 0s 947us/step - loss: 0.3456 - accuracy: 0.8548\n",
      "Epoch 77/1500\n",
      "35/35 [==============================] - 0s 887us/step - loss: 0.3434 - accuracy: 0.8552\n",
      "Epoch 78/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.3513 - accuracy: 0.8548\n",
      "Epoch 79/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.3241 - accuracy: 0.8629\n",
      "Epoch 80/1500\n",
      "35/35 [==============================] - 0s 872us/step - loss: 0.3365 - accuracy: 0.8611\n",
      "Epoch 81/1500\n",
      "35/35 [==============================] - 0s 875us/step - loss: 0.3193 - accuracy: 0.8751\n",
      "Epoch 82/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.3369 - accuracy: 0.8687\n",
      "Epoch 83/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.3179 - accuracy: 0.8746\n",
      "Epoch 84/1500\n",
      "35/35 [==============================] - 0s 891us/step - loss: 0.3278 - accuracy: 0.8602\n",
      "Epoch 85/1500\n",
      "35/35 [==============================] - 0s 866us/step - loss: 0.3284 - accuracy: 0.8665\n",
      "Epoch 86/1500\n",
      "35/35 [==============================] - 0s 923us/step - loss: 0.3132 - accuracy: 0.8755\n",
      "Epoch 87/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.3350 - accuracy: 0.8624\n",
      "Epoch 88/1500\n",
      "35/35 [==============================] - 0s 889us/step - loss: 0.3109 - accuracy: 0.8764\n",
      "Epoch 89/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.3267 - accuracy: 0.8584\n",
      "Epoch 90/1500\n",
      "35/35 [==============================] - 0s 910us/step - loss: 0.3217 - accuracy: 0.8606\n",
      "Epoch 91/1500\n",
      "35/35 [==============================] - 0s 878us/step - loss: 0.3132 - accuracy: 0.8678\n",
      "Epoch 92/1500\n",
      "35/35 [==============================] - 0s 890us/step - loss: 0.3113 - accuracy: 0.8769\n",
      "Epoch 93/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.3094 - accuracy: 0.8683\n",
      "Epoch 94/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.3102 - accuracy: 0.8773\n",
      "Epoch 95/1500\n",
      "35/35 [==============================] - 0s 872us/step - loss: 0.3175 - accuracy: 0.8660\n",
      "Epoch 96/1500\n",
      "35/35 [==============================] - 0s 905us/step - loss: 0.2935 - accuracy: 0.8809\n",
      "Epoch 97/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.3016 - accuracy: 0.8773\n",
      "Epoch 98/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.3102 - accuracy: 0.8719\n",
      "Epoch 99/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.3148 - accuracy: 0.8728\n",
      "Epoch 100/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.3099 - accuracy: 0.8791\n",
      "Epoch 101/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.2889 - accuracy: 0.8886\n",
      "Epoch 102/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.3159 - accuracy: 0.8651\n",
      "Epoch 103/1500\n",
      "35/35 [==============================] - 0s 904us/step - loss: 0.3044 - accuracy: 0.8719\n",
      "Epoch 104/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.2984 - accuracy: 0.8809\n",
      "Epoch 105/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.3007 - accuracy: 0.8710\n",
      "Epoch 106/1500\n",
      "35/35 [==============================] - 0s 826us/step - loss: 0.2915 - accuracy: 0.8868\n",
      "Epoch 107/1500\n",
      "35/35 [==============================] - 0s 866us/step - loss: 0.3017 - accuracy: 0.8841\n",
      "Epoch 108/1500\n",
      "35/35 [==============================] - 0s 887us/step - loss: 0.3006 - accuracy: 0.8782\n",
      "Epoch 109/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.3034 - accuracy: 0.8710\n",
      "Epoch 110/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.2995 - accuracy: 0.8827\n",
      "Epoch 111/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.2817 - accuracy: 0.8945\n",
      "Epoch 112/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.2944 - accuracy: 0.8796\n",
      "Epoch 113/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.2917 - accuracy: 0.8877\n",
      "Epoch 114/1500\n",
      "35/35 [==============================] - 0s 849us/step - loss: 0.2607 - accuracy: 0.8949\n",
      "Epoch 115/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.2795 - accuracy: 0.8913\n",
      "Epoch 116/1500\n",
      "35/35 [==============================] - 0s 855us/step - loss: 0.2958 - accuracy: 0.8769\n",
      "Epoch 117/1500\n",
      "35/35 [==============================] - 0s 841us/step - loss: 0.2687 - accuracy: 0.8931\n",
      "Epoch 118/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.2825 - accuracy: 0.8872\n",
      "Epoch 119/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.2719 - accuracy: 0.8935\n",
      "Epoch 120/1500\n",
      "35/35 [==============================] - 0s 824us/step - loss: 0.2770 - accuracy: 0.8868\n",
      "Epoch 121/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.2846 - accuracy: 0.8832\n",
      "Epoch 122/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.2767 - accuracy: 0.8832\n",
      "Epoch 123/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.2656 - accuracy: 0.8899\n",
      "Epoch 124/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.2676 - accuracy: 0.8899\n",
      "Epoch 125/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.2702 - accuracy: 0.8926\n",
      "Epoch 126/1500\n",
      "35/35 [==============================] - 0s 877us/step - loss: 0.2836 - accuracy: 0.8755\n",
      "Epoch 127/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.2776 - accuracy: 0.8908\n",
      "Epoch 128/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.2762 - accuracy: 0.8926\n",
      "Epoch 129/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.2691 - accuracy: 0.8913\n",
      "Epoch 130/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.2699 - accuracy: 0.8913\n",
      "Epoch 131/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.2629 - accuracy: 0.8922\n",
      "Epoch 132/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.2756 - accuracy: 0.8922\n",
      "Epoch 133/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.2690 - accuracy: 0.8972\n",
      "Epoch 134/1500\n",
      "35/35 [==============================] - 0s 845us/step - loss: 0.2632 - accuracy: 0.8913\n",
      "Epoch 135/1500\n",
      "35/35 [==============================] - 0s 881us/step - loss: 0.2707 - accuracy: 0.8945\n",
      "Epoch 136/1500\n",
      "35/35 [==============================] - 0s 911us/step - loss: 0.2609 - accuracy: 0.8940\n",
      "Epoch 137/1500\n",
      "35/35 [==============================] - 0s 864us/step - loss: 0.2597 - accuracy: 0.8999\n",
      "Epoch 138/1500\n",
      "35/35 [==============================] - 0s 854us/step - loss: 0.2458 - accuracy: 0.9021\n",
      "Epoch 139/1500\n",
      "35/35 [==============================] - 0s 852us/step - loss: 0.2583 - accuracy: 0.8985\n",
      "Epoch 140/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.2547 - accuracy: 0.8940\n",
      "Epoch 141/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.2514 - accuracy: 0.9026\n",
      "Epoch 142/1500\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.2486 - accuracy: 0.9026\n",
      "Epoch 143/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.9026\n",
      "Epoch 144/1500\n",
      "35/35 [==============================] - 0s 974us/step - loss: 0.2454 - accuracy: 0.9048\n",
      "Epoch 145/1500\n",
      "35/35 [==============================] - 0s 854us/step - loss: 0.2595 - accuracy: 0.8963\n",
      "Epoch 146/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.2384 - accuracy: 0.9134\n",
      "Epoch 147/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.2494 - accuracy: 0.8990\n",
      "Epoch 148/1500\n",
      "35/35 [==============================] - 0s 912us/step - loss: 0.2551 - accuracy: 0.9008\n",
      "Epoch 149/1500\n",
      "35/35 [==============================] - 0s 959us/step - loss: 0.2395 - accuracy: 0.9053\n",
      "Epoch 150/1500\n",
      "35/35 [==============================] - 0s 862us/step - loss: 0.2438 - accuracy: 0.9003\n",
      "Epoch 151/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.2438 - accuracy: 0.8985\n",
      "Epoch 152/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.2438 - accuracy: 0.9084\n",
      "Epoch 153/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.2404 - accuracy: 0.9012\n",
      "Epoch 154/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.2515 - accuracy: 0.9044\n",
      "Epoch 155/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.2419 - accuracy: 0.9053\n",
      "Epoch 156/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.2481 - accuracy: 0.9089\n",
      "Epoch 157/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.2370 - accuracy: 0.9044\n",
      "Epoch 158/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.2434 - accuracy: 0.9008\n",
      "Epoch 159/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.2368 - accuracy: 0.9017\n",
      "Epoch 160/1500\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.2433 - accuracy: 0.9026\n",
      "Epoch 161/1500\n",
      "35/35 [==============================] - 0s 855us/step - loss: 0.2388 - accuracy: 0.9035\n",
      "Epoch 162/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.2403 - accuracy: 0.9080\n",
      "Epoch 163/1500\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.2339 - accuracy: 0.9053\n",
      "Epoch 164/1500\n",
      "35/35 [==============================] - 0s 862us/step - loss: 0.2307 - accuracy: 0.9166\n",
      "Epoch 165/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.2503 - accuracy: 0.8999\n",
      "Epoch 166/1500\n",
      "35/35 [==============================] - 0s 816us/step - loss: 0.2300 - accuracy: 0.9111\n",
      "Epoch 167/1500\n",
      "35/35 [==============================] - 0s 870us/step - loss: 0.2262 - accuracy: 0.9111\n",
      "Epoch 168/1500\n",
      "35/35 [==============================] - 0s 938us/step - loss: 0.2496 - accuracy: 0.9008\n",
      "Epoch 169/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.2285 - accuracy: 0.9080\n",
      "Epoch 170/1500\n",
      "35/35 [==============================] - 0s 810us/step - loss: 0.2358 - accuracy: 0.9075\n",
      "Epoch 171/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.2364 - accuracy: 0.9030\n",
      "Epoch 172/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.2290 - accuracy: 0.9017\n",
      "Epoch 173/1500\n",
      "35/35 [==============================] - 0s 922us/step - loss: 0.2477 - accuracy: 0.9017\n",
      "Epoch 174/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.2388 - accuracy: 0.9080\n",
      "Epoch 175/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.2312 - accuracy: 0.9098\n",
      "Epoch 176/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.2303 - accuracy: 0.9066\n",
      "Epoch 177/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.2266 - accuracy: 0.9093\n",
      "Epoch 178/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.2358 - accuracy: 0.8994\n",
      "Epoch 179/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.2163 - accuracy: 0.9184\n",
      "Epoch 180/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.2259 - accuracy: 0.9044\n",
      "Epoch 181/1500\n",
      "35/35 [==============================] - 0s 858us/step - loss: 0.2271 - accuracy: 0.9048\n",
      "Epoch 182/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.2249 - accuracy: 0.9107\n",
      "Epoch 183/1500\n",
      "35/35 [==============================] - 0s 905us/step - loss: 0.2130 - accuracy: 0.9116\n",
      "Epoch 184/1500\n",
      "35/35 [==============================] - 0s 847us/step - loss: 0.2230 - accuracy: 0.9120\n",
      "Epoch 185/1500\n",
      "35/35 [==============================] - 0s 817us/step - loss: 0.2215 - accuracy: 0.9206\n",
      "Epoch 186/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.2196 - accuracy: 0.9179\n",
      "Epoch 187/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.2309 - accuracy: 0.9075\n",
      "Epoch 188/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.2281 - accuracy: 0.9107\n",
      "Epoch 189/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.2204 - accuracy: 0.9129\n",
      "Epoch 190/1500\n",
      "35/35 [==============================] - 0s 827us/step - loss: 0.1992 - accuracy: 0.9197\n",
      "Epoch 191/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.2208 - accuracy: 0.9116\n",
      "Epoch 192/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.2135 - accuracy: 0.9161\n",
      "Epoch 193/1500\n",
      "35/35 [==============================] - 0s 847us/step - loss: 0.2183 - accuracy: 0.9161\n",
      "Epoch 194/1500\n",
      "35/35 [==============================] - 0s 826us/step - loss: 0.2268 - accuracy: 0.9039\n",
      "Epoch 195/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.2288 - accuracy: 0.9057\n",
      "Epoch 196/1500\n",
      "35/35 [==============================] - 0s 969us/step - loss: 0.2125 - accuracy: 0.9175\n",
      "Epoch 197/1500\n",
      "35/35 [==============================] - 0s 942us/step - loss: 0.2041 - accuracy: 0.9197\n",
      "Epoch 198/1500\n",
      "35/35 [==============================] - 0s 876us/step - loss: 0.2243 - accuracy: 0.9107\n",
      "Epoch 199/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.2008 - accuracy: 0.9211\n",
      "Epoch 200/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.2243 - accuracy: 0.9084\n",
      "Epoch 201/1500\n",
      "35/35 [==============================] - 0s 932us/step - loss: 0.2094 - accuracy: 0.9233\n",
      "Epoch 202/1500\n",
      "35/35 [==============================] - 0s 910us/step - loss: 0.2279 - accuracy: 0.9084\n",
      "Epoch 203/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2051 - accuracy: 0.9188\n",
      "Epoch 204/1500\n",
      "35/35 [==============================] - 0s 930us/step - loss: 0.2007 - accuracy: 0.9197\n",
      "Epoch 205/1500\n",
      "35/35 [==============================] - 0s 906us/step - loss: 0.2246 - accuracy: 0.9129\n",
      "Epoch 206/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2215 - accuracy: 0.9071\n",
      "Epoch 207/1500\n",
      "35/35 [==============================] - 0s 904us/step - loss: 0.2228 - accuracy: 0.9147\n",
      "Epoch 208/1500\n",
      "35/35 [==============================] - 0s 903us/step - loss: 0.2064 - accuracy: 0.9184\n",
      "Epoch 209/1500\n",
      "35/35 [==============================] - 0s 912us/step - loss: 0.2022 - accuracy: 0.9224\n",
      "Epoch 210/1500\n",
      "35/35 [==============================] - 0s 927us/step - loss: 0.2327 - accuracy: 0.9129\n",
      "Epoch 211/1500\n",
      "35/35 [==============================] - 0s 972us/step - loss: 0.2003 - accuracy: 0.9188\n",
      "Epoch 212/1500\n",
      "35/35 [==============================] - 0s 914us/step - loss: 0.2053 - accuracy: 0.9138\n",
      "Epoch 213/1500\n",
      "35/35 [==============================] - 0s 917us/step - loss: 0.2076 - accuracy: 0.9175\n",
      "Epoch 214/1500\n",
      "35/35 [==============================] - 0s 875us/step - loss: 0.2128 - accuracy: 0.9129\n",
      "Epoch 215/1500\n",
      "35/35 [==============================] - 0s 894us/step - loss: 0.1905 - accuracy: 0.9274\n",
      "Epoch 216/1500\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.2004 - accuracy: 0.9260\n",
      "Epoch 217/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.2093 - accuracy: 0.9197\n",
      "Epoch 218/1500\n",
      "35/35 [==============================] - 0s 827us/step - loss: 0.1953 - accuracy: 0.9238\n",
      "Epoch 219/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1964 - accuracy: 0.9238\n",
      "Epoch 220/1500\n",
      "35/35 [==============================] - 0s 890us/step - loss: 0.1945 - accuracy: 0.9260\n",
      "Epoch 221/1500\n",
      "35/35 [==============================] - 0s 966us/step - loss: 0.1929 - accuracy: 0.9197\n",
      "Epoch 222/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9206\n",
      "Epoch 223/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9202\n",
      "Epoch 224/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9229\n",
      "Epoch 225/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1935 - accuracy: 0.9242\n",
      "Epoch 226/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9089\n",
      "Epoch 227/1500\n",
      "35/35 [==============================] - 0s 909us/step - loss: 0.2092 - accuracy: 0.9134\n",
      "Epoch 228/1500\n",
      "35/35 [==============================] - 0s 941us/step - loss: 0.1931 - accuracy: 0.9247\n",
      "Epoch 229/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.9251\n",
      "Epoch 230/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.9224\n",
      "Epoch 231/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1964 - accuracy: 0.9256\n",
      "Epoch 232/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9328\n",
      "Epoch 233/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1940 - accuracy: 0.9274\n",
      "Epoch 234/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1898 - accuracy: 0.9260\n",
      "Epoch 235/1500\n",
      "35/35 [==============================] - 0s 996us/step - loss: 0.1901 - accuracy: 0.9211\n",
      "Epoch 236/1500\n",
      "35/35 [==============================] - 0s 999us/step - loss: 0.1798 - accuracy: 0.9314\n",
      "Epoch 237/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1896 - accuracy: 0.9323\n",
      "Epoch 238/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.9256\n",
      "Epoch 239/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.9175\n",
      "Epoch 240/1500\n",
      "35/35 [==============================] - 0s 951us/step - loss: 0.1881 - accuracy: 0.9238\n",
      "Epoch 241/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9224\n",
      "Epoch 242/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.9314\n",
      "Epoch 243/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1883 - accuracy: 0.9283\n",
      "Epoch 244/1500\n",
      "35/35 [==============================] - 0s 903us/step - loss: 0.1861 - accuracy: 0.9278\n",
      "Epoch 245/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.1891 - accuracy: 0.9274\n",
      "Epoch 246/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.1916 - accuracy: 0.9242\n",
      "Epoch 247/1500\n",
      "35/35 [==============================] - 0s 755us/step - loss: 0.2093 - accuracy: 0.9125\n",
      "Epoch 248/1500\n",
      "35/35 [==============================] - 0s 891us/step - loss: 0.1926 - accuracy: 0.9256\n",
      "Epoch 249/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1892 - accuracy: 0.9242\n",
      "Epoch 250/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.9206\n",
      "Epoch 251/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9278\n",
      "Epoch 252/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9251\n",
      "Epoch 253/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9247\n",
      "Epoch 254/1500\n",
      "35/35 [==============================] - 0s 887us/step - loss: 0.1694 - accuracy: 0.9310\n",
      "Epoch 255/1500\n",
      "35/35 [==============================] - 0s 907us/step - loss: 0.1939 - accuracy: 0.9247\n",
      "Epoch 256/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.9265\n",
      "Epoch 257/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9292\n",
      "Epoch 258/1500\n",
      "35/35 [==============================] - 0s 956us/step - loss: 0.1730 - accuracy: 0.9296\n",
      "Epoch 259/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9405\n",
      "Epoch 260/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1890 - accuracy: 0.9220\n",
      "Epoch 261/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9296\n",
      "Epoch 262/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9369\n",
      "Epoch 263/1500\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 0.1768 - accuracy: 0.9310\n",
      "Epoch 264/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9319\n",
      "Epoch 265/1500\n",
      "35/35 [==============================] - 0s 908us/step - loss: 0.1848 - accuracy: 0.9319\n",
      "Epoch 266/1500\n",
      "35/35 [==============================] - 0s 900us/step - loss: 0.1741 - accuracy: 0.9332\n",
      "Epoch 267/1500\n",
      "35/35 [==============================] - 0s 899us/step - loss: 0.1763 - accuracy: 0.9215\n",
      "Epoch 268/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.1746 - accuracy: 0.9359\n",
      "Epoch 269/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9260\n",
      "Epoch 270/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9314\n",
      "Epoch 271/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.1801 - accuracy: 0.9292\n",
      "Epoch 272/1500\n",
      "35/35 [==============================] - 0s 873us/step - loss: 0.1873 - accuracy: 0.9242\n",
      "Epoch 273/1500\n",
      "35/35 [==============================] - 0s 985us/step - loss: 0.1825 - accuracy: 0.9337\n",
      "Epoch 274/1500\n",
      "35/35 [==============================] - 0s 919us/step - loss: 0.1763 - accuracy: 0.9283\n",
      "Epoch 275/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.1677 - accuracy: 0.9369\n",
      "Epoch 276/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.1652 - accuracy: 0.9328\n",
      "Epoch 277/1500\n",
      "35/35 [==============================] - 0s 861us/step - loss: 0.1652 - accuracy: 0.9405\n",
      "Epoch 278/1500\n",
      "35/35 [==============================] - 0s 871us/step - loss: 0.1740 - accuracy: 0.9341\n",
      "Epoch 279/1500\n",
      "35/35 [==============================] - 0s 927us/step - loss: 0.1627 - accuracy: 0.9391\n",
      "Epoch 280/1500\n",
      "35/35 [==============================] - 0s 946us/step - loss: 0.1754 - accuracy: 0.9305\n",
      "Epoch 281/1500\n",
      "35/35 [==============================] - 0s 930us/step - loss: 0.1717 - accuracy: 0.9350\n",
      "Epoch 282/1500\n",
      "35/35 [==============================] - 0s 974us/step - loss: 0.1695 - accuracy: 0.9323\n",
      "Epoch 283/1500\n",
      "35/35 [==============================] - 0s 941us/step - loss: 0.1840 - accuracy: 0.9310\n",
      "Epoch 284/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.1811 - accuracy: 0.9314\n",
      "Epoch 285/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.1629 - accuracy: 0.9405\n",
      "Epoch 286/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.1929 - accuracy: 0.9238\n",
      "Epoch 287/1500\n",
      "35/35 [==============================] - 0s 854us/step - loss: 0.1740 - accuracy: 0.9337\n",
      "Epoch 288/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.1583 - accuracy: 0.9373\n",
      "Epoch 289/1500\n",
      "35/35 [==============================] - 0s 826us/step - loss: 0.1692 - accuracy: 0.9328\n",
      "Epoch 290/1500\n",
      "35/35 [==============================] - 0s 868us/step - loss: 0.1518 - accuracy: 0.9432\n",
      "Epoch 291/1500\n",
      "35/35 [==============================] - 0s 825us/step - loss: 0.1670 - accuracy: 0.9323\n",
      "Epoch 292/1500\n",
      "35/35 [==============================] - 0s 831us/step - loss: 0.1728 - accuracy: 0.9328\n",
      "Epoch 293/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.1636 - accuracy: 0.9364\n",
      "Epoch 294/1500\n",
      "35/35 [==============================] - 0s 883us/step - loss: 0.1612 - accuracy: 0.9400\n",
      "Epoch 295/1500\n",
      "35/35 [==============================] - 0s 885us/step - loss: 0.1667 - accuracy: 0.9414\n",
      "Epoch 296/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.1589 - accuracy: 0.9387\n",
      "Epoch 297/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.1676 - accuracy: 0.9369\n",
      "Epoch 298/1500\n",
      "35/35 [==============================] - 0s 801us/step - loss: 0.1590 - accuracy: 0.9400\n",
      "Epoch 299/1500\n",
      "35/35 [==============================] - 0s 765us/step - loss: 0.1550 - accuracy: 0.9373\n",
      "Epoch 300/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.1591 - accuracy: 0.9409\n",
      "Epoch 301/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.1573 - accuracy: 0.9427\n",
      "Epoch 302/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.1554 - accuracy: 0.9432\n",
      "Epoch 303/1500\n",
      "35/35 [==============================] - 0s 824us/step - loss: 0.1756 - accuracy: 0.9310\n",
      "Epoch 304/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.1545 - accuracy: 0.9409\n",
      "Epoch 305/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.1718 - accuracy: 0.9359\n",
      "Epoch 306/1500\n",
      "35/35 [==============================] - 0s 797us/step - loss: 0.1701 - accuracy: 0.9319\n",
      "Epoch 307/1500\n",
      "35/35 [==============================] - 0s 819us/step - loss: 0.1634 - accuracy: 0.9355\n",
      "Epoch 308/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.1589 - accuracy: 0.9418\n",
      "Epoch 309/1500\n",
      "35/35 [==============================] - 0s 816us/step - loss: 0.1527 - accuracy: 0.9414\n",
      "Epoch 310/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.1588 - accuracy: 0.9423\n",
      "Epoch 311/1500\n",
      "35/35 [==============================] - 0s 841us/step - loss: 0.1569 - accuracy: 0.9387\n",
      "Epoch 312/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.1612 - accuracy: 0.9364\n",
      "Epoch 313/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.1665 - accuracy: 0.9387\n",
      "Epoch 314/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.1841 - accuracy: 0.9256\n",
      "Epoch 315/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.1651 - accuracy: 0.9369\n",
      "Epoch 316/1500\n",
      "35/35 [==============================] - 0s 867us/step - loss: 0.1565 - accuracy: 0.9414\n",
      "Epoch 317/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.1751 - accuracy: 0.9310\n",
      "Epoch 318/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.1658 - accuracy: 0.9337\n",
      "Epoch 319/1500\n",
      "35/35 [==============================] - 0s 824us/step - loss: 0.1637 - accuracy: 0.9319\n",
      "Epoch 320/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.1395 - accuracy: 0.9490\n",
      "Epoch 321/1500\n",
      "35/35 [==============================] - 0s 791us/step - loss: 0.1465 - accuracy: 0.9454\n",
      "Epoch 322/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.1611 - accuracy: 0.9414\n",
      "Epoch 323/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.1521 - accuracy: 0.9409\n",
      "Epoch 324/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.1602 - accuracy: 0.9369\n",
      "Epoch 325/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.1553 - accuracy: 0.9414\n",
      "Epoch 326/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.1651 - accuracy: 0.9337\n",
      "Epoch 327/1500\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.1540 - accuracy: 0.9436\n",
      "Epoch 328/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.1565 - accuracy: 0.9468\n",
      "Epoch 329/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.1685 - accuracy: 0.9350\n",
      "Epoch 330/1500\n",
      "35/35 [==============================] - 0s 862us/step - loss: 0.1583 - accuracy: 0.9391\n",
      "Epoch 331/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.1484 - accuracy: 0.9450\n",
      "Epoch 332/1500\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.1529 - accuracy: 0.9427\n",
      "Epoch 333/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.1540 - accuracy: 0.9427\n",
      "Epoch 334/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.1558 - accuracy: 0.9391\n",
      "Epoch 335/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.1700 - accuracy: 0.9337\n",
      "Epoch 336/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.1704 - accuracy: 0.9364\n",
      "Epoch 337/1500\n",
      "35/35 [==============================] - 0s 825us/step - loss: 0.1399 - accuracy: 0.9454\n",
      "Epoch 338/1500\n",
      "35/35 [==============================] - 0s 811us/step - loss: 0.1480 - accuracy: 0.9418\n",
      "Epoch 339/1500\n",
      "35/35 [==============================] - 0s 806us/step - loss: 0.1419 - accuracy: 0.9522\n",
      "Epoch 340/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.1497 - accuracy: 0.9396\n",
      "Epoch 341/1500\n",
      "35/35 [==============================] - 0s 800us/step - loss: 0.1498 - accuracy: 0.9409\n",
      "Epoch 342/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.1559 - accuracy: 0.9414\n",
      "Epoch 343/1500\n",
      "35/35 [==============================] - 0s 795us/step - loss: 0.1305 - accuracy: 0.9504\n",
      "Epoch 344/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.1492 - accuracy: 0.9432\n",
      "Epoch 345/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.1444 - accuracy: 0.9382\n",
      "Epoch 346/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.1429 - accuracy: 0.9472\n",
      "Epoch 347/1500\n",
      "35/35 [==============================] - 0s 834us/step - loss: 0.1549 - accuracy: 0.9409\n",
      "Epoch 348/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.1433 - accuracy: 0.9454\n",
      "Epoch 349/1500\n",
      "35/35 [==============================] - 0s 813us/step - loss: 0.1546 - accuracy: 0.9373\n",
      "Epoch 350/1500\n",
      "35/35 [==============================] - 0s 812us/step - loss: 0.1392 - accuracy: 0.9472\n",
      "Epoch 351/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.1505 - accuracy: 0.9414\n",
      "Epoch 352/1500\n",
      "35/35 [==============================] - 0s 789us/step - loss: 0.1549 - accuracy: 0.9418\n",
      "Epoch 353/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.1506 - accuracy: 0.9445\n",
      "Epoch 354/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.1422 - accuracy: 0.9432\n",
      "Epoch 355/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.1510 - accuracy: 0.9432\n",
      "Epoch 356/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.1488 - accuracy: 0.9423\n",
      "Epoch 357/1500\n",
      "35/35 [==============================] - 0s 797us/step - loss: 0.1373 - accuracy: 0.9423\n",
      "Epoch 358/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.1439 - accuracy: 0.9454\n",
      "Epoch 359/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.1357 - accuracy: 0.9522\n",
      "Epoch 360/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.1424 - accuracy: 0.9427\n",
      "Epoch 361/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.1476 - accuracy: 0.9436\n",
      "Epoch 362/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.1521 - accuracy: 0.9364\n",
      "Epoch 363/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.1513 - accuracy: 0.9373\n",
      "Epoch 364/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.1454 - accuracy: 0.9463\n",
      "Epoch 365/1500\n",
      "35/35 [==============================] - 0s 813us/step - loss: 0.1583 - accuracy: 0.9405\n",
      "Epoch 366/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.1558 - accuracy: 0.9423\n",
      "Epoch 367/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.1437 - accuracy: 0.9409\n",
      "Epoch 368/1500\n",
      "35/35 [==============================] - 0s 800us/step - loss: 0.1314 - accuracy: 0.9504\n",
      "Epoch 369/1500\n",
      "35/35 [==============================] - 0s 815us/step - loss: 0.1468 - accuracy: 0.9427\n",
      "Epoch 370/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.1368 - accuracy: 0.9490\n",
      "Epoch 371/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.1318 - accuracy: 0.9504\n",
      "Epoch 372/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.1327 - accuracy: 0.9517\n",
      "Epoch 373/1500\n",
      " 1/35 [..............................] - ETA: 0s - loss: 0.1205 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 343.\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.1446 - accuracy: 0.9427\n",
      "Epoch 373: early stopping\n",
      "8/8 [==============================] - 0s 820us/step - loss: 0.9011 - accuracy: 0.7566\n",
      "8/8 [==============================] - 0s 594us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.71 (20/28)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "After appending - Cat IDs: 444, Predictions: 444, Actuals: 444, Gender: 444\n",
      "Final Test Results - Loss: 0.9010636210441589, Accuracy: 0.7566371560096741, Precision: 0.7305316170987813, Recall: 0.7513461722717256, F1 Score: 0.739904620339403\n",
      "Confusion Matrix:\n",
      " [[112   4  26]\n",
      " [  1  32   2]\n",
      " [ 21   1  27]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "042A    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "040A    10\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "015A     9\n",
      "045A     9\n",
      "095A     8\n",
      "117A     7\n",
      "099A     7\n",
      "031A     7\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "012A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "060A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "038A     2\n",
      "093A     2\n",
      "018A     2\n",
      "054A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "091A     1\n",
      "019B     1\n",
      "092A     1\n",
      "004A     1\n",
      "049A     1\n",
      "076A     1\n",
      "043A     1\n",
      "026C     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "020A    23\n",
      "067A    19\n",
      "097A    16\n",
      "059A    14\n",
      "022A     9\n",
      "072A     9\n",
      "010A     8\n",
      "094A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "037A     6\n",
      "109A     6\n",
      "008A     6\n",
      "009A     4\n",
      "003A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "025B     2\n",
      "087A     2\n",
      "032A     2\n",
      "066A     1\n",
      "048A     1\n",
      "041A     1\n",
      "115A     1\n",
      "096A     1\n",
      "100A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    243\n",
      "X    229\n",
      "F    226\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    119\n",
      "M     94\n",
      "F     26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 043A, 049...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [067A, 020A, 022A, 072A, 009A, 027A, 013B, 014...\n",
      "kitten                 [046A, 109A, 050A, 041A, 048A, 115A]\n",
      "senior                       [097A, 059A, 058A, 094A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '044A' '045A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '046A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'044A'}\n",
      "Moved to Test Set:\n",
      "{'044A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '045A' '046A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '044A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "Length of X_train_val:\n",
      "756\n",
      "Length of y_train_val:\n",
      "756\n",
      "Length of groups_train_val:\n",
      "756\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     470\n",
      "senior    136\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     118\n",
      "kitten     79\n",
      "senior     42\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     470\n",
      "kitten    150\n",
      "senior    136\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     118\n",
      "senior     42\n",
      "kitten     21\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 940, 1: 750, 2: 680})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.0713 - accuracy: 0.4890\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.8646 - accuracy: 0.6203\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.7830 - accuracy: 0.6544\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.7730 - accuracy: 0.6519\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.7365 - accuracy: 0.6675\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.7154 - accuracy: 0.6751\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.7050 - accuracy: 0.7030\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.6658 - accuracy: 0.7110\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.6829 - accuracy: 0.7000\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.6785 - accuracy: 0.6958\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.6566 - accuracy: 0.7169\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.6415 - accuracy: 0.7190\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.6295 - accuracy: 0.7342\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.6223 - accuracy: 0.7359\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.5923 - accuracy: 0.7354\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.6045 - accuracy: 0.7262\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.5866 - accuracy: 0.7380\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6037 - accuracy: 0.7295\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5943 - accuracy: 0.7430\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5822 - accuracy: 0.7443\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.5677 - accuracy: 0.7460\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.5568 - accuracy: 0.7574\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.5552 - accuracy: 0.7603\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.5640 - accuracy: 0.7591\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.5614 - accuracy: 0.7460\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.5307 - accuracy: 0.7684\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.5380 - accuracy: 0.7603\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.5310 - accuracy: 0.7591\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.5150 - accuracy: 0.7700\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.5256 - accuracy: 0.7637\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.5254 - accuracy: 0.7717\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.5168 - accuracy: 0.7738\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.5060 - accuracy: 0.7734\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.5171 - accuracy: 0.7684\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.5100 - accuracy: 0.7726\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.4922 - accuracy: 0.7861\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.4996 - accuracy: 0.7916\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.4868 - accuracy: 0.8025\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.4940 - accuracy: 0.7878\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.4782 - accuracy: 0.7873\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.4885 - accuracy: 0.7789\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4928 - accuracy: 0.7911\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.4943 - accuracy: 0.7785\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.4857 - accuracy: 0.7869\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.4832 - accuracy: 0.7802\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.4659 - accuracy: 0.7932\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.4860 - accuracy: 0.7852\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.4676 - accuracy: 0.7954\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.4792 - accuracy: 0.7819\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.4674 - accuracy: 0.8034\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.4532 - accuracy: 0.7992\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.4563 - accuracy: 0.7970\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.4485 - accuracy: 0.8038\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.4575 - accuracy: 0.7954\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.4505 - accuracy: 0.7992\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.4493 - accuracy: 0.8042\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.4554 - accuracy: 0.7937\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.4364 - accuracy: 0.8068\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.4378 - accuracy: 0.8046\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.4452 - accuracy: 0.7958\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.4409 - accuracy: 0.8021\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.4428 - accuracy: 0.7996\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.4301 - accuracy: 0.8068\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.4322 - accuracy: 0.8038\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.4226 - accuracy: 0.8135\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.4251 - accuracy: 0.8194\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.4355 - accuracy: 0.8038\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.4353 - accuracy: 0.8101\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.4236 - accuracy: 0.8135\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.4268 - accuracy: 0.8122\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4227 - accuracy: 0.8207\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 954us/step - loss: 0.4331 - accuracy: 0.8080\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 984us/step - loss: 0.4211 - accuracy: 0.8169\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.4191 - accuracy: 0.8203\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 910us/step - loss: 0.4115 - accuracy: 0.8186\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.4211 - accuracy: 0.8181\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.4190 - accuracy: 0.8186\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 966us/step - loss: 0.4253 - accuracy: 0.8160\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.4222 - accuracy: 0.8186\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 0.4044 - accuracy: 0.8224\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.4137 - accuracy: 0.8165\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.4088 - accuracy: 0.8262\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.3925 - accuracy: 0.8304\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.4205 - accuracy: 0.8131\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.4094 - accuracy: 0.8215\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.3970 - accuracy: 0.8342\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.4078 - accuracy: 0.8241\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.3994 - accuracy: 0.8198\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.8316\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 986us/step - loss: 0.4003 - accuracy: 0.8287\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3847 - accuracy: 0.8295\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3883 - accuracy: 0.8350\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 993us/step - loss: 0.3889 - accuracy: 0.8325\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 951us/step - loss: 0.3904 - accuracy: 0.8316\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 934us/step - loss: 0.3961 - accuracy: 0.8198\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.3708 - accuracy: 0.8443\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3902 - accuracy: 0.8257\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.3752 - accuracy: 0.8405\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.3798 - accuracy: 0.8333\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.3804 - accuracy: 0.8338\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 986us/step - loss: 0.3845 - accuracy: 0.8333\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3748 - accuracy: 0.8371\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3802 - accuracy: 0.8439\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3748 - accuracy: 0.8329\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 956us/step - loss: 0.3777 - accuracy: 0.8262\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 936us/step - loss: 0.3903 - accuracy: 0.8338\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.3681 - accuracy: 0.8401\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 989us/step - loss: 0.3596 - accuracy: 0.8502\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.3734 - accuracy: 0.8388\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.3702 - accuracy: 0.8363\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.3727 - accuracy: 0.8380\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.3791 - accuracy: 0.8392\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3789 - accuracy: 0.8359\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.3726 - accuracy: 0.8371\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3509 - accuracy: 0.8447\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.3678 - accuracy: 0.8384\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.3650 - accuracy: 0.8397\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.3816 - accuracy: 0.8295\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.3606 - accuracy: 0.8447\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.3572 - accuracy: 0.8435\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.3601 - accuracy: 0.8451\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 980us/step - loss: 0.3654 - accuracy: 0.8388\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3752 - accuracy: 0.8392\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3604 - accuracy: 0.8405\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3518 - accuracy: 0.8506\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3554 - accuracy: 0.8451\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3494 - accuracy: 0.8481\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3538 - accuracy: 0.8544\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.3647 - accuracy: 0.8392\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3478 - accuracy: 0.8494\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.3551 - accuracy: 0.8502\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.3514 - accuracy: 0.8494\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3501 - accuracy: 0.8549\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.3551 - accuracy: 0.8477\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3466 - accuracy: 0.8620\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.3398 - accuracy: 0.8603\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.3397 - accuracy: 0.8553\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.3374 - accuracy: 0.8654\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.3336 - accuracy: 0.8506\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3470 - accuracy: 0.8515\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.3369 - accuracy: 0.8565\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.3394 - accuracy: 0.8582\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.3533 - accuracy: 0.8380\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.3578 - accuracy: 0.8519\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3382 - accuracy: 0.8515\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3401 - accuracy: 0.8540\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3395 - accuracy: 0.8536\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3228 - accuracy: 0.8633\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.3337 - accuracy: 0.8603\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.3348 - accuracy: 0.8553\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.8603\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3373 - accuracy: 0.8561\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.3462 - accuracy: 0.8494\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3343 - accuracy: 0.8549\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 993us/step - loss: 0.3221 - accuracy: 0.8675\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.3225 - accuracy: 0.8667\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 947us/step - loss: 0.3384 - accuracy: 0.8561\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3261 - accuracy: 0.8722\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3308 - accuracy: 0.8586\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3268 - accuracy: 0.8646\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 936us/step - loss: 0.3290 - accuracy: 0.8650\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.3274 - accuracy: 0.8662\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3285 - accuracy: 0.8654\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.3241 - accuracy: 0.8650\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.3283 - accuracy: 0.8565\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3300 - accuracy: 0.8641\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.3081 - accuracy: 0.8751\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.3249 - accuracy: 0.8595\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3144 - accuracy: 0.8633\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3088 - accuracy: 0.8658\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3072 - accuracy: 0.8671\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 996us/step - loss: 0.3141 - accuracy: 0.8755\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3255 - accuracy: 0.8654\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3216 - accuracy: 0.8675\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3176 - accuracy: 0.8637\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.3225 - accuracy: 0.8658\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.3169 - accuracy: 0.8637\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3123 - accuracy: 0.8662\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.3111 - accuracy: 0.8705\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.3128 - accuracy: 0.8633\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.3055 - accuracy: 0.8662\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3131 - accuracy: 0.8726\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.3047 - accuracy: 0.8684\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.3053 - accuracy: 0.8785\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.3124 - accuracy: 0.8688\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.3157 - accuracy: 0.8734\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.3174 - accuracy: 0.8722\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 990us/step - loss: 0.3041 - accuracy: 0.8776\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 0.3214 - accuracy: 0.8620\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.3100 - accuracy: 0.8688\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3321 - accuracy: 0.8684\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.3176 - accuracy: 0.8654\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.3047 - accuracy: 0.8722\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3045 - accuracy: 0.8781\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2970 - accuracy: 0.8705\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.3038 - accuracy: 0.8764\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.3056 - accuracy: 0.8722\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2907 - accuracy: 0.8814\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.3080 - accuracy: 0.8705\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2998 - accuracy: 0.8781\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 941us/step - loss: 0.2914 - accuracy: 0.8730\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.3005 - accuracy: 0.8772\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.3067 - accuracy: 0.8705\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2943 - accuracy: 0.8738\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.2932 - accuracy: 0.8776\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2889 - accuracy: 0.8785\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2986 - accuracy: 0.8776\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.3001 - accuracy: 0.8709\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.3104 - accuracy: 0.8717\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.2891 - accuracy: 0.8814\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2905 - accuracy: 0.8776\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 909us/step - loss: 0.2875 - accuracy: 0.8831\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 985us/step - loss: 0.2964 - accuracy: 0.8747\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2858 - accuracy: 0.8793\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2919 - accuracy: 0.8768\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2918 - accuracy: 0.8768\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.2847 - accuracy: 0.8781\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.3019 - accuracy: 0.8734\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.2786 - accuracy: 0.8827\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.3024 - accuracy: 0.8759\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2810 - accuracy: 0.8861\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.2825 - accuracy: 0.8747\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.2770 - accuracy: 0.8928\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2717 - accuracy: 0.8911\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2761 - accuracy: 0.8899\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2845 - accuracy: 0.8831\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2907 - accuracy: 0.8755\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2884 - accuracy: 0.8840\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2891 - accuracy: 0.8827\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2755 - accuracy: 0.8844\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2852 - accuracy: 0.8802\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2762 - accuracy: 0.8831\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2738 - accuracy: 0.8895\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.2742 - accuracy: 0.8903\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.2799 - accuracy: 0.8907\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.2767 - accuracy: 0.8819\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.2668 - accuracy: 0.8911\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2831 - accuracy: 0.8831\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2878 - accuracy: 0.8802\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2863 - accuracy: 0.8776\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2753 - accuracy: 0.8937\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2879 - accuracy: 0.8852\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.2679 - accuracy: 0.8924\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2882 - accuracy: 0.8810\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2733 - accuracy: 0.8869\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2672 - accuracy: 0.8907\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2759 - accuracy: 0.8852\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 959us/step - loss: 0.2620 - accuracy: 0.8970\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2707 - accuracy: 0.8848\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.2736 - accuracy: 0.8861\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2725 - accuracy: 0.8911\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2728 - accuracy: 0.8819\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2759 - accuracy: 0.8928\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2862 - accuracy: 0.8747\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2702 - accuracy: 0.8835\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2744 - accuracy: 0.8895\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.2600 - accuracy: 0.9004\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2606 - accuracy: 0.8895\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.2632 - accuracy: 0.8844\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 928us/step - loss: 0.2731 - accuracy: 0.8920\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2452 - accuracy: 0.8983\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2640 - accuracy: 0.8970\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2703 - accuracy: 0.8949\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2529 - accuracy: 0.9004\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2597 - accuracy: 0.8970\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2589 - accuracy: 0.8932\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2742 - accuracy: 0.8890\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2646 - accuracy: 0.8924\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.2684 - accuracy: 0.8945\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.2622 - accuracy: 0.8954\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.2641 - accuracy: 0.8890\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.2616 - accuracy: 0.8954\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.2595 - accuracy: 0.8924\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.2582 - accuracy: 0.8920\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2572 - accuracy: 0.8983\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.2663 - accuracy: 0.8916\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2667 - accuracy: 0.8869\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2537 - accuracy: 0.8992\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2520 - accuracy: 0.9038\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2611 - accuracy: 0.8932\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2502 - accuracy: 0.8966\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2724 - accuracy: 0.8873\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2559 - accuracy: 0.8979\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2536 - accuracy: 0.8983\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2631 - accuracy: 0.8983\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.2420 - accuracy: 0.8979\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.2644 - accuracy: 0.8882\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.2604 - accuracy: 0.8996\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2575 - accuracy: 0.8949\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2482 - accuracy: 0.8987\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2575 - accuracy: 0.8886\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2475 - accuracy: 0.8992\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2627 - accuracy: 0.8869\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9008\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2625 - accuracy: 0.8873\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2438 - accuracy: 0.9008\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2429 - accuracy: 0.9008\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2541 - accuracy: 0.9004\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2499 - accuracy: 0.8979\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2399 - accuracy: 0.9046\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2271 - accuracy: 0.9097\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2454 - accuracy: 0.9025\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2417 - accuracy: 0.9017\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2496 - accuracy: 0.9017\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2633 - accuracy: 0.8932\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2585 - accuracy: 0.8924\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.8954\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2474 - accuracy: 0.9042\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2406 - accuracy: 0.8983\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.2159 - accuracy: 0.9203\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2454 - accuracy: 0.9004\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2472 - accuracy: 0.8996\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.2395 - accuracy: 0.8996\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2472 - accuracy: 0.9025\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2386 - accuracy: 0.9042\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2375 - accuracy: 0.9051\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2458 - accuracy: 0.8975\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2413 - accuracy: 0.9021\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.2344 - accuracy: 0.9089\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2289 - accuracy: 0.9055\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2287 - accuracy: 0.9093\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2473 - accuracy: 0.8970\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2455 - accuracy: 0.9025\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2344 - accuracy: 0.9004\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2418 - accuracy: 0.9084\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2334 - accuracy: 0.9068\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2323 - accuracy: 0.9030\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2391 - accuracy: 0.9017\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2318 - accuracy: 0.9114\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2311 - accuracy: 0.9051\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2183 - accuracy: 0.9118\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.2357 - accuracy: 0.9072\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2466 - accuracy: 0.9084\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2341 - accuracy: 0.9038\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2392 - accuracy: 0.9038\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2217 - accuracy: 0.9089\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2454 - accuracy: 0.8996\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.2445 - accuracy: 0.9076\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2314 - accuracy: 0.9068\n",
      "Epoch 340/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.1973 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 310.\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.2269 - accuracy: 0.9068\n",
      "Epoch 340: early stopping\n",
      "6/6 [==============================] - 0s 963us/step - loss: 0.4490 - accuracy: 0.8011\n",
      "6/6 [==============================] - 0s 689us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.87 (26/30)\n",
      "Before appending - Cat IDs: 444, Predictions: 444, Actuals: 444, Gender: 444\n",
      "After appending - Cat IDs: 625, Predictions: 625, Actuals: 625, Gender: 625\n",
      "Final Test Results - Loss: 0.44897574186325073, Accuracy: 0.8011049628257751, Precision: 0.7783575975294638, Recall: 0.7195318805488298, F1 Score: 0.7424402679467361\n",
      "Confusion Matrix:\n",
      " [[103   3  12]\n",
      " [  8  12   1]\n",
      " [ 12   0  30]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "116A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "016A    10\n",
      "005A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "050A     7\n",
      "117A     7\n",
      "037A     6\n",
      "053A     6\n",
      "008A     6\n",
      "109A     6\n",
      "023A     6\n",
      "044A     5\n",
      "023B     5\n",
      "070A     5\n",
      "075A     5\n",
      "009A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "060A     3\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "025B     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "032A     2\n",
      "018A     2\n",
      "069A     2\n",
      "092A     1\n",
      "100A     1\n",
      "096A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "041A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "066A     1\n",
      "091A     1\n",
      "076A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "055A    20\n",
      "101A    15\n",
      "028A    13\n",
      "002A    13\n",
      "025A    11\n",
      "033A     9\n",
      "031A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "034A     5\n",
      "025C     5\n",
      "021A     5\n",
      "026A     4\n",
      "035A     4\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "093A     2\n",
      "049A     1\n",
      "026C     1\n",
      "073A     1\n",
      "088A     1\n",
      "090A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    277\n",
      "M    271\n",
      "F    151\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    101\n",
      "X     71\n",
      "M     66\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 071A, 097B, 019A, 074A, 067...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 050...\n",
      "senior    [097A, 106A, 104A, 059A, 116A, 051B, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 103A, 028A, 101A, 034A, 002A, 099...\n",
      "kitten                                         [047A, 049A]\n",
      "senior           [093A, 057A, 055A, 113A, 056A, 108A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 14, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 2, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "Length of X_train_val:\n",
      "699\n",
      "Length of y_train_val:\n",
      "699\n",
      "Length of groups_train_val:\n",
      "699\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 882, 1: 710, 2: 580})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.3169 - accuracy: 0.3867\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.0442 - accuracy: 0.5249\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.9227 - accuracy: 0.5824\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 927us/step - loss: 0.8587 - accuracy: 0.6294\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.8157 - accuracy: 0.6464\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.7947 - accuracy: 0.6510\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 935us/step - loss: 0.7575 - accuracy: 0.6731\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.7394 - accuracy: 0.6828\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.7135 - accuracy: 0.6934\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.6892 - accuracy: 0.7113\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.6705 - accuracy: 0.7099\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.6680 - accuracy: 0.7136\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.6502 - accuracy: 0.7247\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.6270 - accuracy: 0.7284\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.6130 - accuracy: 0.7330\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.5747 - accuracy: 0.7569\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.5791 - accuracy: 0.7551\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 924us/step - loss: 0.5895 - accuracy: 0.7569\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5714 - accuracy: 0.7620\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5426 - accuracy: 0.7703\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.5582 - accuracy: 0.7592\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 948us/step - loss: 0.5560 - accuracy: 0.7657\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5362 - accuracy: 0.7753\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 962us/step - loss: 0.5507 - accuracy: 0.7716\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.5341 - accuracy: 0.7799\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.5197 - accuracy: 0.7735\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.5299 - accuracy: 0.7767\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.5213 - accuracy: 0.7680\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5076 - accuracy: 0.7799\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.5260 - accuracy: 0.7753\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.5098 - accuracy: 0.7868\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 872us/step - loss: 0.4956 - accuracy: 0.7896\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.4841 - accuracy: 0.8006\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.5048 - accuracy: 0.7831\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.4832 - accuracy: 0.7901\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.4595 - accuracy: 0.8020\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.4653 - accuracy: 0.8039\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.4838 - accuracy: 0.7993\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.4641 - accuracy: 0.8103\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.4558 - accuracy: 0.8085\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.4505 - accuracy: 0.8071\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.4521 - accuracy: 0.8071\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.4397 - accuracy: 0.8154\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.4599 - accuracy: 0.8089\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.4492 - accuracy: 0.8172\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.4555 - accuracy: 0.8099\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.4427 - accuracy: 0.8099\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.4258 - accuracy: 0.8223\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.4443 - accuracy: 0.8080\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.4441 - accuracy: 0.8177\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.4142 - accuracy: 0.8292\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.4289 - accuracy: 0.8200\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.4318 - accuracy: 0.8191\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.4427 - accuracy: 0.8131\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.4264 - accuracy: 0.8241\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.4243 - accuracy: 0.8181\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.4106 - accuracy: 0.8352\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.3974 - accuracy: 0.8264\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.4016 - accuracy: 0.8356\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.3998 - accuracy: 0.8260\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.3936 - accuracy: 0.8352\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.4214 - accuracy: 0.8186\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.4028 - accuracy: 0.8283\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.4189 - accuracy: 0.8191\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.4000 - accuracy: 0.8297\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.4093 - accuracy: 0.8375\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.3870 - accuracy: 0.8471\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3955 - accuracy: 0.8237\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3843 - accuracy: 0.8366\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3822 - accuracy: 0.8435\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3953 - accuracy: 0.8352\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3798 - accuracy: 0.8393\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3764 - accuracy: 0.8476\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.3895 - accuracy: 0.8347\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.3908 - accuracy: 0.8389\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3755 - accuracy: 0.8476\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.3724 - accuracy: 0.8458\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 970us/step - loss: 0.3760 - accuracy: 0.8416\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.3799 - accuracy: 0.8379\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.3638 - accuracy: 0.8490\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.3662 - accuracy: 0.8448\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.3585 - accuracy: 0.8587\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.3564 - accuracy: 0.8504\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.3684 - accuracy: 0.8536\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.3751 - accuracy: 0.8435\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.3565 - accuracy: 0.8541\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.3764 - accuracy: 0.8430\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.3627 - accuracy: 0.8490\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.3680 - accuracy: 0.8462\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3525 - accuracy: 0.8499\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 982us/step - loss: 0.3481 - accuracy: 0.8504\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 958us/step - loss: 0.3640 - accuracy: 0.8416\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 949us/step - loss: 0.3469 - accuracy: 0.8476\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 957us/step - loss: 0.3608 - accuracy: 0.8490\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.3486 - accuracy: 0.8591\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.3407 - accuracy: 0.8531\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.3376 - accuracy: 0.8646\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3411 - accuracy: 0.8536\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.3426 - accuracy: 0.8564\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.3420 - accuracy: 0.8554\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.3482 - accuracy: 0.8582\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3329 - accuracy: 0.8669\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.3408 - accuracy: 0.8522\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3346 - accuracy: 0.8610\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.3389 - accuracy: 0.8656\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.3356 - accuracy: 0.8591\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.3492 - accuracy: 0.8531\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.3301 - accuracy: 0.8665\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.3310 - accuracy: 0.8651\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.3388 - accuracy: 0.8605\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.3271 - accuracy: 0.8651\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.3275 - accuracy: 0.8651\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.3234 - accuracy: 0.8692\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.3262 - accuracy: 0.8669\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.3269 - accuracy: 0.8660\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.3159 - accuracy: 0.8738\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.3098 - accuracy: 0.8692\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3292 - accuracy: 0.8669\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.3225 - accuracy: 0.8646\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3365 - accuracy: 0.8623\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3182 - accuracy: 0.8674\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 886us/step - loss: 0.3144 - accuracy: 0.8757\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.3195 - accuracy: 0.8656\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.3267 - accuracy: 0.8734\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.3165 - accuracy: 0.8683\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.3066 - accuracy: 0.8771\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.3159 - accuracy: 0.8720\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.3116 - accuracy: 0.8738\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.3059 - accuracy: 0.8697\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.3059 - accuracy: 0.8752\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.3123 - accuracy: 0.8725\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.3020 - accuracy: 0.8757\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.3125 - accuracy: 0.8762\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.2852 - accuracy: 0.8854\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.3202 - accuracy: 0.8752\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.3159 - accuracy: 0.8725\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.2973 - accuracy: 0.8762\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2903 - accuracy: 0.8785\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.2920 - accuracy: 0.8775\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.3000 - accuracy: 0.8757\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.2903 - accuracy: 0.8844\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.2882 - accuracy: 0.8803\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.2883 - accuracy: 0.8789\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2892 - accuracy: 0.8877\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.2985 - accuracy: 0.8738\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.3046 - accuracy: 0.8780\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2956 - accuracy: 0.8720\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.2909 - accuracy: 0.8803\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.2882 - accuracy: 0.8803\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.2955 - accuracy: 0.8854\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.2948 - accuracy: 0.8775\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.2962 - accuracy: 0.8780\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.2712 - accuracy: 0.8900\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2971 - accuracy: 0.8738\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2744 - accuracy: 0.8840\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2964 - accuracy: 0.8697\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2751 - accuracy: 0.8849\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2933 - accuracy: 0.8798\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.2795 - accuracy: 0.8877\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 715us/step - loss: 0.2911 - accuracy: 0.8821\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.2861 - accuracy: 0.8789\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2912 - accuracy: 0.8812\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.2661 - accuracy: 0.8904\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.2773 - accuracy: 0.8858\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2810 - accuracy: 0.8785\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.2684 - accuracy: 0.8913\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.2813 - accuracy: 0.8780\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2706 - accuracy: 0.8849\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.2811 - accuracy: 0.8886\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2770 - accuracy: 0.8867\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 713us/step - loss: 0.2761 - accuracy: 0.8881\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2695 - accuracy: 0.8923\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2740 - accuracy: 0.8890\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 974us/step - loss: 0.2589 - accuracy: 0.9015\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.2670 - accuracy: 0.8867\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2680 - accuracy: 0.8918\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.2800 - accuracy: 0.8886\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.2700 - accuracy: 0.8886\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2748 - accuracy: 0.8835\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 706us/step - loss: 0.2730 - accuracy: 0.8932\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2496 - accuracy: 0.9001\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2734 - accuracy: 0.8895\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2583 - accuracy: 0.9001\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2687 - accuracy: 0.8918\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.2705 - accuracy: 0.8877\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.2515 - accuracy: 0.8959\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.2679 - accuracy: 0.8890\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2665 - accuracy: 0.8955\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2664 - accuracy: 0.8881\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.2565 - accuracy: 0.8983\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.2419 - accuracy: 0.9047\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.2627 - accuracy: 0.8936\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.2370 - accuracy: 0.9047\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.2656 - accuracy: 0.8881\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.2534 - accuracy: 0.8904\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2609 - accuracy: 0.8909\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.2546 - accuracy: 0.9010\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.2457 - accuracy: 0.8983\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2509 - accuracy: 0.8983\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2471 - accuracy: 0.8987\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.2554 - accuracy: 0.8950\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.2610 - accuracy: 0.8890\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.2441 - accuracy: 0.9047\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.2567 - accuracy: 0.9006\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2586 - accuracy: 0.8932\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.2482 - accuracy: 0.8950\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2552 - accuracy: 0.9015\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.2400 - accuracy: 0.9075\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 730us/step - loss: 0.2499 - accuracy: 0.8955\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.2469 - accuracy: 0.9029\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.2347 - accuracy: 0.9042\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 705us/step - loss: 0.2443 - accuracy: 0.9047\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.2452 - accuracy: 0.8946\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2512 - accuracy: 0.8946\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.2345 - accuracy: 0.9102\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.2361 - accuracy: 0.8978\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.2217 - accuracy: 0.9125\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.2403 - accuracy: 0.9038\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2240 - accuracy: 0.9134\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.2486 - accuracy: 0.8946\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.2490 - accuracy: 0.9001\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.2204 - accuracy: 0.9056\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.2297 - accuracy: 0.9093\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2336 - accuracy: 0.9065\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.2371 - accuracy: 0.9079\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 734us/step - loss: 0.2309 - accuracy: 0.9102\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 721us/step - loss: 0.2258 - accuracy: 0.9047\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2280 - accuracy: 0.9111\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 711us/step - loss: 0.2215 - accuracy: 0.9088\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 727us/step - loss: 0.2382 - accuracy: 0.9024\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 733us/step - loss: 0.2350 - accuracy: 0.9079\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2244 - accuracy: 0.9088\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2284 - accuracy: 0.9134\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.2449 - accuracy: 0.9015\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2240 - accuracy: 0.9157\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.2342 - accuracy: 0.9088\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.2215 - accuracy: 0.9111\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.2419 - accuracy: 0.8969\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.2293 - accuracy: 0.9079\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2174 - accuracy: 0.9116\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.2125 - accuracy: 0.9185\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.2289 - accuracy: 0.9065\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2172 - accuracy: 0.9148\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2164 - accuracy: 0.9121\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2234 - accuracy: 0.9111\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2303 - accuracy: 0.9019\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 718us/step - loss: 0.2312 - accuracy: 0.9148\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 723us/step - loss: 0.2264 - accuracy: 0.9116\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 727us/step - loss: 0.2317 - accuracy: 0.9075\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.2141 - accuracy: 0.9185\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 731us/step - loss: 0.2231 - accuracy: 0.9102\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2074 - accuracy: 0.9194\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2345 - accuracy: 0.9070\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2267 - accuracy: 0.9084\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2110 - accuracy: 0.9134\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.2077 - accuracy: 0.9190\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 720us/step - loss: 0.2082 - accuracy: 0.9176\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.2152 - accuracy: 0.9116\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 720us/step - loss: 0.2168 - accuracy: 0.9121\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.2193 - accuracy: 0.9148\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 734us/step - loss: 0.2053 - accuracy: 0.9222\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.2060 - accuracy: 0.9162\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.2209 - accuracy: 0.9144\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2039 - accuracy: 0.9245\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 720us/step - loss: 0.2186 - accuracy: 0.9199\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 733us/step - loss: 0.2109 - accuracy: 0.9185\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2125 - accuracy: 0.9121\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.2292 - accuracy: 0.9121\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.2225 - accuracy: 0.9130\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.2056 - accuracy: 0.9180\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2023 - accuracy: 0.9208\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.2070 - accuracy: 0.9199\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 708us/step - loss: 0.2022 - accuracy: 0.9185\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 739us/step - loss: 0.2037 - accuracy: 0.9176\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.1948 - accuracy: 0.9217\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.2021 - accuracy: 0.9162\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2125 - accuracy: 0.9153\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1990 - accuracy: 0.9227\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.2178 - accuracy: 0.9144\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1925 - accuracy: 0.9185\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.2191 - accuracy: 0.9121\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.2080 - accuracy: 0.9227\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.1956 - accuracy: 0.9148\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.2027 - accuracy: 0.9199\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.2141 - accuracy: 0.9153\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2101 - accuracy: 0.9144\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1917 - accuracy: 0.9273\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 965us/step - loss: 0.2049 - accuracy: 0.9162\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 960us/step - loss: 0.2127 - accuracy: 0.9125\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.2109 - accuracy: 0.9130\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 745us/step - loss: 0.1997 - accuracy: 0.9139\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.2025 - accuracy: 0.9217\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.2238 - accuracy: 0.9121\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.2113 - accuracy: 0.9180\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.1932 - accuracy: 0.9250\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.2012 - accuracy: 0.9231\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.2045 - accuracy: 0.9194\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.1986 - accuracy: 0.9231\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.2065 - accuracy: 0.9217\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.2004 - accuracy: 0.9171\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.1962 - accuracy: 0.9213\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.2035 - accuracy: 0.9185\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.2069 - accuracy: 0.9171\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 923us/step - loss: 0.1929 - accuracy: 0.9203\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2080 - accuracy: 0.9167\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 731us/step - loss: 0.1757 - accuracy: 0.9314\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 707us/step - loss: 0.1804 - accuracy: 0.9314\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 746us/step - loss: 0.2053 - accuracy: 0.9213\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1870 - accuracy: 0.9296\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.2001 - accuracy: 0.9185\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 953us/step - loss: 0.1852 - accuracy: 0.9309\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 977us/step - loss: 0.1899 - accuracy: 0.9259\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 991us/step - loss: 0.1893 - accuracy: 0.9254\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 998us/step - loss: 0.1953 - accuracy: 0.9199\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.1819 - accuracy: 0.9346\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.2162 - accuracy: 0.9134\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1876 - accuracy: 0.9314\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1880 - accuracy: 0.9240\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 995us/step - loss: 0.1806 - accuracy: 0.9282\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.1822 - accuracy: 0.9254\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1629 - accuracy: 0.9351\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 927us/step - loss: 0.1942 - accuracy: 0.9222\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 958us/step - loss: 0.1859 - accuracy: 0.9245\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.2014 - accuracy: 0.9185\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.1830 - accuracy: 0.9273\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.1876 - accuracy: 0.9263\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.1751 - accuracy: 0.9282\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.1905 - accuracy: 0.9203\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.1956 - accuracy: 0.9332\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.1746 - accuracy: 0.9282\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1772 - accuracy: 0.9365\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.1882 - accuracy: 0.9263\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.1744 - accuracy: 0.9369\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.1827 - accuracy: 0.9286\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.1868 - accuracy: 0.9240\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.1803 - accuracy: 0.9300\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 711us/step - loss: 0.1898 - accuracy: 0.9227\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1834 - accuracy: 0.9309\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.1856 - accuracy: 0.9282\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1702 - accuracy: 0.9401\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.1817 - accuracy: 0.9319\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.1981 - accuracy: 0.9199\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.1854 - accuracy: 0.9277\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.1758 - accuracy: 0.9300\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.1824 - accuracy: 0.9314\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.1704 - accuracy: 0.9355\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.1648 - accuracy: 0.9351\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.1719 - accuracy: 0.9360\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.1655 - accuracy: 0.9406\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 723us/step - loss: 0.1744 - accuracy: 0.9309\n",
      "Epoch 351/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.1664 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 321.\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.1777 - accuracy: 0.9282\n",
      "Epoch 351: early stopping\n",
      "8/8 [==============================] - 0s 777us/step - loss: 0.9520 - accuracy: 0.6597\n",
      "8/8 [==============================] - 0s 584us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (23/29)\n",
      "Before appending - Cat IDs: 625, Predictions: 625, Actuals: 625, Gender: 625\n",
      "After appending - Cat IDs: 863, Predictions: 863, Actuals: 863, Gender: 863\n",
      "Final Test Results - Loss: 0.951958179473877, Accuracy: 0.6596638560295105, Precision: 0.6781842818428184, Recall: 0.6456430551456771, F1 Score: 0.6586378251973107\n",
      "Confusion Matrix:\n",
      " [[115   0  32]\n",
      " [  3  26   0]\n",
      " [ 46   0  16]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6764069186702067\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8276309370994568\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7114615887403488\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6893899018955436\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6781418210031022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[2]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be692ae4-6d3f-4353-b5bf-554d20da4df3",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8da9a092-ed2e-4397-a6c8-2c4888735265",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 863, Predictions: 863, Actuals: 863, Gender: 863\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51cf386a-c49e-4716-ba15-aa3b7930419a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8d43ac5-d50e-430d-98a1-ff4f45006bae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (83/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccc9acb7-bb1b-42a6-bb25-cdf5a3356315",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95e69b27-cae1-4a3a-ba70-5244a11aadf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, adult, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[senior, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, senior, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[adult, adult, kitten, adult, kitten, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "64    058A                           [senior, senior, senior]        senior           senior                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A              [adult, adult, senior, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, senior, adult, adult, senior, ...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "59    053A       [adult, adult, senior, senior, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "57    051B  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "56    051A  [adult, senior, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, senior, senior, ...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "53    048A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "51    045A  [adult, adult, kitten, kitten, kitten, adult, ...        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A    [senior, senior, senior, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                     [senior, adult, adult, senior]         adult            adult                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, adult, senior, adult, se...         adult            adult                   True\n",
       "92    097A  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, senior, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "17    015A  [adult, adult, senior, senior, senior, adult, ...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A       [adult, senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                      [adult, senior, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, senior, senior, adult, adult, ...         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "32    026B                                            [adult]         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "103   109A        [adult, adult, kitten, adult, adult, adult]         adult           kitten                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "101   106A  [adult, senior, senior, senior, adult, adult, ...         adult           senior                  False\n",
       "99    104A                       [adult, adult, adult, adult]         adult           senior                  False\n",
       "42    036A  [adult, senior, senior, senior, senior, senior...        senior            adult                  False\n",
       "50    044A  [adult, adult, kitten, adult, kitten, adult, a...         adult           kitten                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "60    054A                                    [senior, adult]         adult           senior                  False\n",
       "90    095A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "61    055A  [adult, adult, senior, adult, adult, adult, ad...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "87    092A                                           [senior]        senior            adult                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "18    016A  [senior, adult, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "62    056A                              [adult, adult, adult]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "82    076A                                           [kitten]        kitten            adult                  False\n",
       "65    059A  [senior, senior, senior, senior, adult, adult,...         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                    [kitten, adult]         adult           senior                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, a...        senior            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d36b3c54-3377-4249-a774-6d31557e36da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     60\n",
      "kitten    13\n",
      "senior    10\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b36eb8a4-57f3-48c0-b92c-4a8e5a52c59e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             60  82.191781\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22             10  45.454545\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1750e2da-df8c-4f00-b860-539dd822864f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6b34e-2368-4956-8241-8e6ab6e8cf81",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdefd9be-6e6d-4903-a0ec-7824da4313d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          572            436  76.223776\n",
      "1           kitten          113             92  81.415929\n",
      "2           senior          178             82  46.067416\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57576e25-349c-46e0-b57b-8bf3cee3b5de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b69785-d4c1-49ab-ba75-4a733326cdc1",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15c5b6e5-d7b0-4089-ae78-3dafe1cead3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      146     68.54\n",
      "1          M    360      263     73.06\n",
      "2          X    290      201     69.31\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6001f1b-3f01-42e9-8761-4253acbed5e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      146     68.54\n",
      "1          M    360      263     73.06\n",
      "2          X    290      201     69.31\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b5d44-f489-4de0-8785-4bfa52d09797",
   "metadata": {},
   "source": [
    "# RANDOM SEED 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c4439fc4-670c-4009-8ca9-23c419ee99ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[3])) \n",
    "np.random.seed(int(random_seeds[3]))\n",
    "tf.random.set_seed(int(random_seeds[3]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5fe04e3d-fd90-43c1-97af-5eac75e35f85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2ed5261d-2aeb-412e-a017-65b461a2cf5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70aa6-1716-400a-be48-cccb3511542e",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d0248-481e-4807-85dd-407fa88963f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "072A     9\n",
      "015A     9\n",
      "051B     9\n",
      "022A     9\n",
      "033A     9\n",
      "045A     9\n",
      "095A     8\n",
      "013B     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "109A     6\n",
      "023A     6\n",
      "037A     6\n",
      "075A     5\n",
      "070A     5\n",
      "025C     5\n",
      "021A     5\n",
      "034A     5\n",
      "044A     5\n",
      "023B     5\n",
      "003A     4\n",
      "105A     4\n",
      "035A     4\n",
      "026A     4\n",
      "052A     4\n",
      "062A     4\n",
      "012A     3\n",
      "113A     3\n",
      "058A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "025B     2\n",
      "032A     2\n",
      "093A     2\n",
      "054A     2\n",
      "069A     2\n",
      "087A     2\n",
      "038A     2\n",
      "073A     1\n",
      "004A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "091A     1\n",
      "019B     1\n",
      "066A     1\n",
      "048A     1\n",
      "092A     1\n",
      "026C     1\n",
      "076A     1\n",
      "043A     1\n",
      "041A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Group Distribution:\n",
      "019A    17\n",
      "101A    15\n",
      "039A    12\n",
      "063A    11\n",
      "071A    10\n",
      "005A    10\n",
      "065A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "099A     7\n",
      "008A     6\n",
      "009A     4\n",
      "104A     4\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "096A     1\n",
      "049A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    296\n",
      "X    286\n",
      "F    208\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    62\n",
      "F    44\n",
      "M    41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 028...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 019A, 101A, 005A, 065A, 039A, 009A, 063...\n",
      "kitten                                         [050A, 049A]\n",
      "senior                       [104A, 056A, 094A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 14, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 2, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "Length of X_train_val:\n",
      "790\n",
      "Length of y_train_val:\n",
      "790\n",
      "Length of groups_train_val:\n",
      "790\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 936, 1: 815, 2: 795})\n",
      "Epoch 1/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 1.1782 - accuracy: 0.5012\n",
      "Epoch 2/1500\n",
      "40/40 [==============================] - 0s 875us/step - loss: 0.8925 - accuracy: 0.6033\n",
      "Epoch 3/1500\n",
      "40/40 [==============================] - 0s 782us/step - loss: 0.8146 - accuracy: 0.6445\n",
      "Epoch 4/1500\n",
      "40/40 [==============================] - 0s 790us/step - loss: 0.7843 - accuracy: 0.6579\n",
      "Epoch 5/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.7307 - accuracy: 0.6732\n",
      "Epoch 6/1500\n",
      "40/40 [==============================] - 0s 804us/step - loss: 0.6969 - accuracy: 0.6991\n",
      "Epoch 7/1500\n",
      "40/40 [==============================] - 0s 851us/step - loss: 0.6692 - accuracy: 0.7003\n",
      "Epoch 8/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.6687 - accuracy: 0.7011\n",
      "Epoch 9/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6333 - accuracy: 0.7235\n",
      "Epoch 10/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.6102 - accuracy: 0.7290\n",
      "Epoch 11/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.5984 - accuracy: 0.7333\n",
      "Epoch 12/1500\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.5957 - accuracy: 0.7357\n",
      "Epoch 13/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.6027 - accuracy: 0.7451\n",
      "Epoch 14/1500\n",
      "40/40 [==============================] - 0s 835us/step - loss: 0.5753 - accuracy: 0.7482\n",
      "Epoch 15/1500\n",
      "40/40 [==============================] - 0s 917us/step - loss: 0.5626 - accuracy: 0.7533\n",
      "Epoch 16/1500\n",
      "40/40 [==============================] - 0s 929us/step - loss: 0.5594 - accuracy: 0.7581\n",
      "Epoch 17/1500\n",
      "40/40 [==============================] - 0s 884us/step - loss: 0.5592 - accuracy: 0.7651\n",
      "Epoch 18/1500\n",
      "40/40 [==============================] - 0s 837us/step - loss: 0.5296 - accuracy: 0.7710\n",
      "Epoch 19/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.5277 - accuracy: 0.7604\n",
      "Epoch 20/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.5283 - accuracy: 0.7734\n",
      "Epoch 21/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.5187 - accuracy: 0.7722\n",
      "Epoch 22/1500\n",
      "40/40 [==============================] - 0s 823us/step - loss: 0.5134 - accuracy: 0.7730\n",
      "Epoch 23/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.5200 - accuracy: 0.7738\n",
      "Epoch 24/1500\n",
      "40/40 [==============================] - 0s 854us/step - loss: 0.4929 - accuracy: 0.7887\n",
      "Epoch 25/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.5015 - accuracy: 0.7804\n",
      "Epoch 26/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.4822 - accuracy: 0.7828\n",
      "Epoch 27/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.5136 - accuracy: 0.7702\n",
      "Epoch 28/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.4869 - accuracy: 0.7926\n",
      "Epoch 29/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.4896 - accuracy: 0.7852\n",
      "Epoch 30/1500\n",
      "40/40 [==============================] - 0s 875us/step - loss: 0.4792 - accuracy: 0.7895\n",
      "Epoch 31/1500\n",
      "40/40 [==============================] - 0s 823us/step - loss: 0.4776 - accuracy: 0.8056\n",
      "Epoch 32/1500\n",
      "40/40 [==============================] - 0s 802us/step - loss: 0.4690 - accuracy: 0.7950\n",
      "Epoch 33/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.4655 - accuracy: 0.7981\n",
      "Epoch 34/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.4804 - accuracy: 0.7887\n",
      "Epoch 35/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.4503 - accuracy: 0.8052\n",
      "Epoch 36/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.4464 - accuracy: 0.8020\n",
      "Epoch 37/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.4494 - accuracy: 0.8119\n",
      "Epoch 38/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.4467 - accuracy: 0.8064\n",
      "Epoch 39/1500\n",
      "40/40 [==============================] - 0s 808us/step - loss: 0.4616 - accuracy: 0.7938\n",
      "Epoch 40/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.4526 - accuracy: 0.7977\n",
      "Epoch 41/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.4327 - accuracy: 0.8052\n",
      "Epoch 42/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.4377 - accuracy: 0.8123\n",
      "Epoch 43/1500\n",
      "40/40 [==============================] - 0s 754us/step - loss: 0.4348 - accuracy: 0.8087\n",
      "Epoch 44/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.4280 - accuracy: 0.8213\n",
      "Epoch 45/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.4312 - accuracy: 0.8170\n",
      "Epoch 46/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.4360 - accuracy: 0.8154\n",
      "Epoch 47/1500\n",
      "40/40 [==============================] - 0s 813us/step - loss: 0.4196 - accuracy: 0.8236\n",
      "Epoch 48/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.4357 - accuracy: 0.8099\n",
      "Epoch 49/1500\n",
      "40/40 [==============================] - 0s 853us/step - loss: 0.4243 - accuracy: 0.8107\n",
      "Epoch 50/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.4377 - accuracy: 0.8028\n",
      "Epoch 51/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.4136 - accuracy: 0.8217\n",
      "Epoch 52/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.4108 - accuracy: 0.8197\n",
      "Epoch 53/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.4041 - accuracy: 0.8244\n",
      "Epoch 54/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.4126 - accuracy: 0.8291\n",
      "Epoch 55/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.4191 - accuracy: 0.8213\n",
      "Epoch 56/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.4158 - accuracy: 0.8209\n",
      "Epoch 57/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.4036 - accuracy: 0.8311\n",
      "Epoch 58/1500\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.4028 - accuracy: 0.8252\n",
      "Epoch 59/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.3988 - accuracy: 0.8319\n",
      "Epoch 60/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.4000 - accuracy: 0.8350\n",
      "Epoch 61/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.3891 - accuracy: 0.8394\n",
      "Epoch 62/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.4052 - accuracy: 0.8221\n",
      "Epoch 63/1500\n",
      "40/40 [==============================] - 0s 808us/step - loss: 0.3986 - accuracy: 0.8319\n",
      "Epoch 64/1500\n",
      "40/40 [==============================] - 0s 881us/step - loss: 0.3846 - accuracy: 0.8346\n",
      "Epoch 65/1500\n",
      "40/40 [==============================] - 0s 928us/step - loss: 0.3850 - accuracy: 0.8362\n",
      "Epoch 66/1500\n",
      "40/40 [==============================] - 0s 927us/step - loss: 0.4090 - accuracy: 0.8229\n",
      "Epoch 67/1500\n",
      "40/40 [==============================] - 0s 865us/step - loss: 0.3893 - accuracy: 0.8374\n",
      "Epoch 68/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.3722 - accuracy: 0.8449\n",
      "Epoch 69/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.3946 - accuracy: 0.8339\n",
      "Epoch 70/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.3812 - accuracy: 0.8386\n",
      "Epoch 71/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.3783 - accuracy: 0.8386\n",
      "Epoch 72/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.3807 - accuracy: 0.8390\n",
      "Epoch 73/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.3699 - accuracy: 0.8397\n",
      "Epoch 74/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.3729 - accuracy: 0.8429\n",
      "Epoch 75/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.3751 - accuracy: 0.8374\n",
      "Epoch 76/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.3751 - accuracy: 0.8409\n",
      "Epoch 77/1500\n",
      "40/40 [==============================] - 0s 835us/step - loss: 0.3775 - accuracy: 0.8397\n",
      "Epoch 78/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.3688 - accuracy: 0.8456\n",
      "Epoch 79/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.3665 - accuracy: 0.8429\n",
      "Epoch 80/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.3656 - accuracy: 0.8413\n",
      "Epoch 81/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.3670 - accuracy: 0.8429\n",
      "Epoch 82/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.3651 - accuracy: 0.8488\n",
      "Epoch 83/1500\n",
      "40/40 [==============================] - 0s 833us/step - loss: 0.3573 - accuracy: 0.8492\n",
      "Epoch 84/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.3625 - accuracy: 0.8456\n",
      "Epoch 85/1500\n",
      "40/40 [==============================] - 0s 889us/step - loss: 0.3633 - accuracy: 0.8476\n",
      "Epoch 86/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.3683 - accuracy: 0.8429\n",
      "Epoch 87/1500\n",
      "40/40 [==============================] - 0s 878us/step - loss: 0.3602 - accuracy: 0.8555\n",
      "Epoch 88/1500\n",
      "40/40 [==============================] - 0s 849us/step - loss: 0.3533 - accuracy: 0.8531\n",
      "Epoch 89/1500\n",
      "40/40 [==============================] - 0s 836us/step - loss: 0.3521 - accuracy: 0.8531\n",
      "Epoch 90/1500\n",
      "40/40 [==============================] - 0s 849us/step - loss: 0.3589 - accuracy: 0.8441\n",
      "Epoch 91/1500\n",
      "40/40 [==============================] - 0s 842us/step - loss: 0.3609 - accuracy: 0.8445\n",
      "Epoch 92/1500\n",
      "40/40 [==============================] - 0s 881us/step - loss: 0.3410 - accuracy: 0.8476\n",
      "Epoch 93/1500\n",
      "40/40 [==============================] - 0s 884us/step - loss: 0.3473 - accuracy: 0.8547\n",
      "Epoch 94/1500\n",
      "40/40 [==============================] - 0s 841us/step - loss: 0.3416 - accuracy: 0.8551\n",
      "Epoch 95/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.3405 - accuracy: 0.8602\n",
      "Epoch 96/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.3268 - accuracy: 0.8559\n",
      "Epoch 97/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.3427 - accuracy: 0.8555\n",
      "Epoch 98/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.3523 - accuracy: 0.8598\n",
      "Epoch 99/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.3457 - accuracy: 0.8551\n",
      "Epoch 100/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.3403 - accuracy: 0.8582\n",
      "Epoch 101/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.3446 - accuracy: 0.8539\n",
      "Epoch 102/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.3423 - accuracy: 0.8555\n",
      "Epoch 103/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.3431 - accuracy: 0.8582\n",
      "Epoch 104/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.3379 - accuracy: 0.8641\n",
      "Epoch 105/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.3313 - accuracy: 0.8610\n",
      "Epoch 106/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.3377 - accuracy: 0.8586\n",
      "Epoch 107/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.3300 - accuracy: 0.8578\n",
      "Epoch 108/1500\n",
      "40/40 [==============================] - 0s 782us/step - loss: 0.3187 - accuracy: 0.8665\n",
      "Epoch 109/1500\n",
      "40/40 [==============================] - 0s 740us/step - loss: 0.3202 - accuracy: 0.8641\n",
      "Epoch 110/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.3270 - accuracy: 0.8653\n",
      "Epoch 111/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.3373 - accuracy: 0.8574\n",
      "Epoch 112/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.3457 - accuracy: 0.8590\n",
      "Epoch 113/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.3266 - accuracy: 0.8610\n",
      "Epoch 114/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.3201 - accuracy: 0.8676\n",
      "Epoch 115/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.3213 - accuracy: 0.8606\n",
      "Epoch 116/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.3378 - accuracy: 0.8566\n",
      "Epoch 117/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.3137 - accuracy: 0.8712\n",
      "Epoch 118/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.3104 - accuracy: 0.8708\n",
      "Epoch 119/1500\n",
      "40/40 [==============================] - 0s 814us/step - loss: 0.3060 - accuracy: 0.8723\n",
      "Epoch 120/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.3306 - accuracy: 0.8680\n",
      "Epoch 121/1500\n",
      "40/40 [==============================] - 0s 801us/step - loss: 0.3245 - accuracy: 0.8676\n",
      "Epoch 122/1500\n",
      "40/40 [==============================] - 0s 909us/step - loss: 0.3427 - accuracy: 0.8606\n",
      "Epoch 123/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.3104 - accuracy: 0.8751\n",
      "Epoch 124/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.3183 - accuracy: 0.8727\n",
      "Epoch 125/1500\n",
      "40/40 [==============================] - 0s 768us/step - loss: 0.3103 - accuracy: 0.8731\n",
      "Epoch 126/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.3188 - accuracy: 0.8712\n",
      "Epoch 127/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.3252 - accuracy: 0.8657\n",
      "Epoch 128/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.3249 - accuracy: 0.8598\n",
      "Epoch 129/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.3225 - accuracy: 0.8625\n",
      "Epoch 130/1500\n",
      "40/40 [==============================] - 0s 732us/step - loss: 0.3216 - accuracy: 0.8641\n",
      "Epoch 131/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.3005 - accuracy: 0.8802\n",
      "Epoch 132/1500\n",
      "40/40 [==============================] - 0s 745us/step - loss: 0.3153 - accuracy: 0.8700\n",
      "Epoch 133/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.3206 - accuracy: 0.8684\n",
      "Epoch 134/1500\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.3137 - accuracy: 0.8731\n",
      "Epoch 135/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.2955 - accuracy: 0.8865\n",
      "Epoch 136/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2978 - accuracy: 0.8798\n",
      "Epoch 137/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.3078 - accuracy: 0.8763\n",
      "Epoch 138/1500\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.2951 - accuracy: 0.8830\n",
      "Epoch 139/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.3015 - accuracy: 0.8743\n",
      "Epoch 140/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.2949 - accuracy: 0.8794\n",
      "Epoch 141/1500\n",
      "40/40 [==============================] - 0s 742us/step - loss: 0.2987 - accuracy: 0.8830\n",
      "Epoch 142/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.2957 - accuracy: 0.8794\n",
      "Epoch 143/1500\n",
      "40/40 [==============================] - 0s 755us/step - loss: 0.3091 - accuracy: 0.8704\n",
      "Epoch 144/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.3058 - accuracy: 0.8672\n",
      "Epoch 145/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.2927 - accuracy: 0.8771\n",
      "Epoch 146/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.2881 - accuracy: 0.8830\n",
      "Epoch 147/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.3023 - accuracy: 0.8767\n",
      "Epoch 148/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.2985 - accuracy: 0.8747\n",
      "Epoch 149/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.3005 - accuracy: 0.8731\n",
      "Epoch 150/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.2852 - accuracy: 0.8861\n",
      "Epoch 151/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.2914 - accuracy: 0.8845\n",
      "Epoch 152/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.2691 - accuracy: 0.8892\n",
      "Epoch 153/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.2833 - accuracy: 0.8861\n",
      "Epoch 154/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.2883 - accuracy: 0.8755\n",
      "Epoch 155/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.2777 - accuracy: 0.8888\n",
      "Epoch 156/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.2883 - accuracy: 0.8849\n",
      "Epoch 157/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.2763 - accuracy: 0.8814\n",
      "Epoch 158/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.2791 - accuracy: 0.8892\n",
      "Epoch 159/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.2813 - accuracy: 0.8841\n",
      "Epoch 160/1500\n",
      "40/40 [==============================] - 0s 773us/step - loss: 0.2817 - accuracy: 0.8830\n",
      "Epoch 161/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.2987 - accuracy: 0.8700\n",
      "Epoch 162/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.2814 - accuracy: 0.8830\n",
      "Epoch 163/1500\n",
      "40/40 [==============================] - 0s 755us/step - loss: 0.2586 - accuracy: 0.8979\n",
      "Epoch 164/1500\n",
      "40/40 [==============================] - 0s 784us/step - loss: 0.2719 - accuracy: 0.8885\n",
      "Epoch 165/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.2723 - accuracy: 0.8916\n",
      "Epoch 166/1500\n",
      "40/40 [==============================] - 0s 761us/step - loss: 0.2845 - accuracy: 0.8869\n",
      "Epoch 167/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.2637 - accuracy: 0.8857\n",
      "Epoch 168/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.2767 - accuracy: 0.8865\n",
      "Epoch 169/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.2823 - accuracy: 0.8849\n",
      "Epoch 170/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.2715 - accuracy: 0.8869\n",
      "Epoch 171/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.2795 - accuracy: 0.8849\n",
      "Epoch 172/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.2664 - accuracy: 0.8885\n",
      "Epoch 173/1500\n",
      "40/40 [==============================] - 0s 826us/step - loss: 0.2679 - accuracy: 0.8888\n",
      "Epoch 174/1500\n",
      "40/40 [==============================] - 0s 865us/step - loss: 0.2741 - accuracy: 0.8885\n",
      "Epoch 175/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.2685 - accuracy: 0.8940\n",
      "Epoch 176/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.2706 - accuracy: 0.8833\n",
      "Epoch 177/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.2735 - accuracy: 0.8873\n",
      "Epoch 178/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.2677 - accuracy: 0.8830\n",
      "Epoch 179/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.2791 - accuracy: 0.8908\n",
      "Epoch 180/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.2731 - accuracy: 0.8900\n",
      "Epoch 181/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.2722 - accuracy: 0.8936\n",
      "Epoch 182/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.2685 - accuracy: 0.8881\n",
      "Epoch 183/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.2662 - accuracy: 0.8896\n",
      "Epoch 184/1500\n",
      "40/40 [==============================] - 0s 928us/step - loss: 0.2570 - accuracy: 0.9014\n",
      "Epoch 185/1500\n",
      "40/40 [==============================] - 0s 947us/step - loss: 0.2660 - accuracy: 0.8881\n",
      "Epoch 186/1500\n",
      "40/40 [==============================] - 0s 901us/step - loss: 0.2614 - accuracy: 0.8963\n",
      "Epoch 187/1500\n",
      "40/40 [==============================] - 0s 898us/step - loss: 0.2701 - accuracy: 0.8936\n",
      "Epoch 188/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.2590 - accuracy: 0.8928\n",
      "Epoch 189/1500\n",
      "40/40 [==============================] - 0s 804us/step - loss: 0.2731 - accuracy: 0.8853\n",
      "Epoch 190/1500\n",
      "40/40 [==============================] - 0s 826us/step - loss: 0.2631 - accuracy: 0.8943\n",
      "Epoch 191/1500\n",
      "40/40 [==============================] - 0s 862us/step - loss: 0.2561 - accuracy: 0.8963\n",
      "Epoch 192/1500\n",
      "40/40 [==============================] - 0s 911us/step - loss: 0.2730 - accuracy: 0.8873\n",
      "Epoch 193/1500\n",
      "40/40 [==============================] - 0s 934us/step - loss: 0.2671 - accuracy: 0.8943\n",
      "Epoch 194/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.2581 - accuracy: 0.8908\n",
      "Epoch 195/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.2566 - accuracy: 0.8936\n",
      "Epoch 196/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2552 - accuracy: 0.8971\n",
      "Epoch 197/1500\n",
      "40/40 [==============================] - 0s 926us/step - loss: 0.2555 - accuracy: 0.8983\n",
      "Epoch 198/1500\n",
      "40/40 [==============================] - 0s 837us/step - loss: 0.2550 - accuracy: 0.9034\n",
      "Epoch 199/1500\n",
      "40/40 [==============================] - 0s 912us/step - loss: 0.2572 - accuracy: 0.8908\n",
      "Epoch 200/1500\n",
      "40/40 [==============================] - 0s 880us/step - loss: 0.2590 - accuracy: 0.8975\n",
      "Epoch 201/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.2584 - accuracy: 0.8943\n",
      "Epoch 202/1500\n",
      "40/40 [==============================] - 0s 919us/step - loss: 0.2500 - accuracy: 0.8995\n",
      "Epoch 203/1500\n",
      "40/40 [==============================] - 0s 903us/step - loss: 0.2603 - accuracy: 0.8936\n",
      "Epoch 204/1500\n",
      "40/40 [==============================] - 0s 922us/step - loss: 0.2390 - accuracy: 0.9053\n",
      "Epoch 205/1500\n",
      "40/40 [==============================] - 0s 853us/step - loss: 0.2439 - accuracy: 0.8987\n",
      "Epoch 206/1500\n",
      "40/40 [==============================] - 0s 901us/step - loss: 0.2391 - accuracy: 0.9046\n",
      "Epoch 207/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.2515 - accuracy: 0.9026\n",
      "Epoch 208/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.2492 - accuracy: 0.8991\n",
      "Epoch 209/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.2405 - accuracy: 0.9057\n",
      "Epoch 210/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.2447 - accuracy: 0.8943\n",
      "Epoch 211/1500\n",
      "40/40 [==============================] - 0s 802us/step - loss: 0.2417 - accuracy: 0.9018\n",
      "Epoch 212/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.2496 - accuracy: 0.8967\n",
      "Epoch 213/1500\n",
      "40/40 [==============================] - 0s 867us/step - loss: 0.2454 - accuracy: 0.8998\n",
      "Epoch 214/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.2449 - accuracy: 0.8924\n",
      "Epoch 215/1500\n",
      "40/40 [==============================] - 0s 866us/step - loss: 0.2499 - accuracy: 0.8963\n",
      "Epoch 216/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.2583 - accuracy: 0.8955\n",
      "Epoch 217/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.2455 - accuracy: 0.8983\n",
      "Epoch 218/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.2403 - accuracy: 0.9034\n",
      "Epoch 219/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.2326 - accuracy: 0.9065\n",
      "Epoch 220/1500\n",
      "40/40 [==============================] - 0s 792us/step - loss: 0.2393 - accuracy: 0.9053\n",
      "Epoch 221/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.2401 - accuracy: 0.9057\n",
      "Epoch 222/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.2341 - accuracy: 0.9069\n",
      "Epoch 223/1500\n",
      "40/40 [==============================] - 0s 882us/step - loss: 0.2483 - accuracy: 0.9038\n",
      "Epoch 224/1500\n",
      "40/40 [==============================] - 0s 902us/step - loss: 0.2464 - accuracy: 0.9042\n",
      "Epoch 225/1500\n",
      "40/40 [==============================] - 0s 836us/step - loss: 0.2349 - accuracy: 0.9038\n",
      "Epoch 226/1500\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.2424 - accuracy: 0.8971\n",
      "Epoch 227/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.2366 - accuracy: 0.9057\n",
      "Epoch 228/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.2336 - accuracy: 0.9018\n",
      "Epoch 229/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.2466 - accuracy: 0.9002\n",
      "Epoch 230/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.2338 - accuracy: 0.9097\n",
      "Epoch 231/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.2443 - accuracy: 0.9026\n",
      "Epoch 232/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.2389 - accuracy: 0.9085\n",
      "Epoch 233/1500\n",
      "40/40 [==============================] - 0s 903us/step - loss: 0.2265 - accuracy: 0.9038\n",
      "Epoch 234/1500\n",
      "40/40 [==============================] - 0s 907us/step - loss: 0.2409 - accuracy: 0.9030\n",
      "Epoch 235/1500\n",
      "40/40 [==============================] - 0s 898us/step - loss: 0.2328 - accuracy: 0.9112\n",
      "Epoch 236/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.2424 - accuracy: 0.9014\n",
      "Epoch 237/1500\n",
      "40/40 [==============================] - 0s 846us/step - loss: 0.2348 - accuracy: 0.9010\n",
      "Epoch 238/1500\n",
      "40/40 [==============================] - 0s 847us/step - loss: 0.2372 - accuracy: 0.9053\n",
      "Epoch 239/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.2316 - accuracy: 0.9049\n",
      "Epoch 240/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.2351 - accuracy: 0.9093\n",
      "Epoch 241/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.2337 - accuracy: 0.9077\n",
      "Epoch 242/1500\n",
      "40/40 [==============================] - 0s 888us/step - loss: 0.2301 - accuracy: 0.9136\n",
      "Epoch 243/1500\n",
      "40/40 [==============================] - 0s 963us/step - loss: 0.2030 - accuracy: 0.9203\n",
      "Epoch 244/1500\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.2360 - accuracy: 0.9057\n",
      "Epoch 245/1500\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.2435 - accuracy: 0.9010\n",
      "Epoch 246/1500\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.2182 - accuracy: 0.9120\n",
      "Epoch 247/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.2237 - accuracy: 0.9108\n",
      "Epoch 248/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.2328 - accuracy: 0.9038\n",
      "Epoch 249/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.2283 - accuracy: 0.9093\n",
      "Epoch 250/1500\n",
      "40/40 [==============================] - 0s 753us/step - loss: 0.2208 - accuracy: 0.9140\n",
      "Epoch 251/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2203 - accuracy: 0.9089\n",
      "Epoch 252/1500\n",
      "40/40 [==============================] - 0s 972us/step - loss: 0.2305 - accuracy: 0.9108\n",
      "Epoch 253/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.2230 - accuracy: 0.9120\n",
      "Epoch 254/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.2186 - accuracy: 0.9132\n",
      "Epoch 255/1500\n",
      "40/40 [==============================] - 0s 734us/step - loss: 0.2234 - accuracy: 0.9144\n",
      "Epoch 256/1500\n",
      "40/40 [==============================] - 0s 746us/step - loss: 0.2165 - accuracy: 0.9152\n",
      "Epoch 257/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.2268 - accuracy: 0.9108\n",
      "Epoch 258/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.2323 - accuracy: 0.9049\n",
      "Epoch 259/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.2228 - accuracy: 0.9124\n",
      "Epoch 260/1500\n",
      "40/40 [==============================] - 0s 891us/step - loss: 0.2290 - accuracy: 0.9034\n",
      "Epoch 261/1500\n",
      "40/40 [==============================] - 0s 898us/step - loss: 0.2288 - accuracy: 0.9077\n",
      "Epoch 262/1500\n",
      "40/40 [==============================] - 0s 897us/step - loss: 0.2180 - accuracy: 0.9195\n",
      "Epoch 263/1500\n",
      "40/40 [==============================] - 0s 883us/step - loss: 0.2120 - accuracy: 0.9159\n",
      "Epoch 264/1500\n",
      "40/40 [==============================] - 0s 892us/step - loss: 0.2073 - accuracy: 0.9183\n",
      "Epoch 265/1500\n",
      "40/40 [==============================] - 0s 802us/step - loss: 0.2108 - accuracy: 0.9218\n",
      "Epoch 266/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.2092 - accuracy: 0.9171\n",
      "Epoch 267/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.2205 - accuracy: 0.9148\n",
      "Epoch 268/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.2190 - accuracy: 0.9116\n",
      "Epoch 269/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.2157 - accuracy: 0.9191\n",
      "Epoch 270/1500\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.2095 - accuracy: 0.9195\n",
      "Epoch 271/1500\n",
      "40/40 [==============================] - 0s 821us/step - loss: 0.2008 - accuracy: 0.9195\n",
      "Epoch 272/1500\n",
      "40/40 [==============================] - 0s 821us/step - loss: 0.2127 - accuracy: 0.9120\n",
      "Epoch 273/1500\n",
      "40/40 [==============================] - 0s 815us/step - loss: 0.2093 - accuracy: 0.9226\n",
      "Epoch 274/1500\n",
      "40/40 [==============================] - 0s 814us/step - loss: 0.2198 - accuracy: 0.9140\n",
      "Epoch 275/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.2069 - accuracy: 0.9156\n",
      "Epoch 276/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.2073 - accuracy: 0.9144\n",
      "Epoch 277/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.2175 - accuracy: 0.9116\n",
      "Epoch 278/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.2060 - accuracy: 0.9199\n",
      "Epoch 279/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.2167 - accuracy: 0.9159\n",
      "Epoch 280/1500\n",
      "40/40 [==============================] - 0s 730us/step - loss: 0.2037 - accuracy: 0.9222\n",
      "Epoch 281/1500\n",
      "40/40 [==============================] - 0s 748us/step - loss: 0.2123 - accuracy: 0.9167\n",
      "Epoch 282/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.2122 - accuracy: 0.9148\n",
      "Epoch 283/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.2009 - accuracy: 0.9179\n",
      "Epoch 284/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.2094 - accuracy: 0.9230\n",
      "Epoch 285/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.2121 - accuracy: 0.9156\n",
      "Epoch 286/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.2040 - accuracy: 0.9187\n",
      "Epoch 287/1500\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.2164 - accuracy: 0.9112\n",
      "Epoch 288/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.2055 - accuracy: 0.9183\n",
      "Epoch 289/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.2032 - accuracy: 0.9226\n",
      "Epoch 290/1500\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.2058 - accuracy: 0.9163\n",
      "Epoch 291/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.2062 - accuracy: 0.9156\n",
      "Epoch 292/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.2080 - accuracy: 0.9159\n",
      "Epoch 293/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.1957 - accuracy: 0.9238\n",
      "Epoch 294/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.1973 - accuracy: 0.9203\n",
      "Epoch 295/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.2167 - accuracy: 0.9120\n",
      "Epoch 296/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.1995 - accuracy: 0.9195\n",
      "Epoch 297/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.1968 - accuracy: 0.9199\n",
      "Epoch 298/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.2028 - accuracy: 0.9187\n",
      "Epoch 299/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.1960 - accuracy: 0.9226\n",
      "Epoch 300/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.2064 - accuracy: 0.9171\n",
      "Epoch 301/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.2065 - accuracy: 0.9171\n",
      "Epoch 302/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.1939 - accuracy: 0.9218\n",
      "Epoch 303/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.1856 - accuracy: 0.9321\n",
      "Epoch 304/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.2024 - accuracy: 0.9254\n",
      "Epoch 305/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.1910 - accuracy: 0.9222\n",
      "Epoch 306/1500\n",
      "40/40 [==============================] - 0s 868us/step - loss: 0.2062 - accuracy: 0.9179\n",
      "Epoch 307/1500\n",
      "40/40 [==============================] - 0s 898us/step - loss: 0.2094 - accuracy: 0.9124\n",
      "Epoch 308/1500\n",
      "40/40 [==============================] - 0s 875us/step - loss: 0.2043 - accuracy: 0.9207\n",
      "Epoch 309/1500\n",
      "40/40 [==============================] - 0s 893us/step - loss: 0.1867 - accuracy: 0.9321\n",
      "Epoch 310/1500\n",
      "40/40 [==============================] - 0s 835us/step - loss: 0.2101 - accuracy: 0.9132\n",
      "Epoch 311/1500\n",
      "40/40 [==============================] - 0s 784us/step - loss: 0.2018 - accuracy: 0.9191\n",
      "Epoch 312/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.2043 - accuracy: 0.9250\n",
      "Epoch 313/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.2012 - accuracy: 0.9230\n",
      "Epoch 314/1500\n",
      "40/40 [==============================] - 0s 860us/step - loss: 0.1990 - accuracy: 0.9211\n",
      "Epoch 315/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.1930 - accuracy: 0.9199\n",
      "Epoch 316/1500\n",
      "40/40 [==============================] - 0s 768us/step - loss: 0.1848 - accuracy: 0.9332\n",
      "Epoch 317/1500\n",
      "40/40 [==============================] - 0s 773us/step - loss: 0.2084 - accuracy: 0.9144\n",
      "Epoch 318/1500\n",
      "40/40 [==============================] - 0s 737us/step - loss: 0.1930 - accuracy: 0.9207\n",
      "Epoch 319/1500\n",
      "40/40 [==============================] - 0s 749us/step - loss: 0.2204 - accuracy: 0.9167\n",
      "Epoch 320/1500\n",
      "40/40 [==============================] - 0s 751us/step - loss: 0.2001 - accuracy: 0.9222\n",
      "Epoch 321/1500\n",
      "40/40 [==============================] - 0s 735us/step - loss: 0.1909 - accuracy: 0.9266\n",
      "Epoch 322/1500\n",
      "40/40 [==============================] - 0s 750us/step - loss: 0.2116 - accuracy: 0.9167\n",
      "Epoch 323/1500\n",
      "40/40 [==============================] - 0s 743us/step - loss: 0.1946 - accuracy: 0.9246\n",
      "Epoch 324/1500\n",
      "40/40 [==============================] - 0s 747us/step - loss: 0.1999 - accuracy: 0.9226\n",
      "Epoch 325/1500\n",
      "40/40 [==============================] - 0s 762us/step - loss: 0.1914 - accuracy: 0.9273\n",
      "Epoch 326/1500\n",
      "40/40 [==============================] - 0s 780us/step - loss: 0.1945 - accuracy: 0.9246\n",
      "Epoch 327/1500\n",
      "40/40 [==============================] - 0s 815us/step - loss: 0.1972 - accuracy: 0.9183\n",
      "Epoch 328/1500\n",
      "40/40 [==============================] - 0s 866us/step - loss: 0.1957 - accuracy: 0.9226\n",
      "Epoch 329/1500\n",
      "40/40 [==============================] - 0s 890us/step - loss: 0.1756 - accuracy: 0.9321\n",
      "Epoch 330/1500\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.1951 - accuracy: 0.9230\n",
      "Epoch 331/1500\n",
      "40/40 [==============================] - 0s 882us/step - loss: 0.1938 - accuracy: 0.9211\n",
      "Epoch 332/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.1971 - accuracy: 0.9258\n",
      "Epoch 333/1500\n",
      "40/40 [==============================] - 0s 902us/step - loss: 0.1977 - accuracy: 0.9234\n",
      "Epoch 334/1500\n",
      "40/40 [==============================] - 0s 813us/step - loss: 0.1937 - accuracy: 0.9281\n",
      "Epoch 335/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.1985 - accuracy: 0.9250\n",
      "Epoch 336/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.1793 - accuracy: 0.9222\n",
      "Epoch 337/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.1763 - accuracy: 0.9317\n",
      "Epoch 338/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.1914 - accuracy: 0.9277\n",
      "Epoch 339/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.1937 - accuracy: 0.9293\n",
      "Epoch 340/1500\n",
      "40/40 [==============================] - 0s 763us/step - loss: 0.1764 - accuracy: 0.9273\n",
      "Epoch 341/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.1960 - accuracy: 0.9207\n",
      "Epoch 342/1500\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.1920 - accuracy: 0.9262\n",
      "Epoch 343/1500\n",
      "40/40 [==============================] - 0s 800us/step - loss: 0.1812 - accuracy: 0.9285\n",
      "Epoch 344/1500\n",
      "40/40 [==============================] - 0s 733us/step - loss: 0.1866 - accuracy: 0.9250\n",
      "Epoch 345/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.1814 - accuracy: 0.9309\n",
      "Epoch 346/1500\n",
      "40/40 [==============================] - 0s 758us/step - loss: 0.1850 - accuracy: 0.9297\n",
      "Epoch 347/1500\n",
      "40/40 [==============================] - 0s 738us/step - loss: 0.1851 - accuracy: 0.9246\n",
      "Epoch 348/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.1887 - accuracy: 0.9246\n",
      "Epoch 349/1500\n",
      "40/40 [==============================] - 0s 741us/step - loss: 0.1831 - accuracy: 0.9293\n",
      "Epoch 350/1500\n",
      "40/40 [==============================] - 0s 744us/step - loss: 0.1776 - accuracy: 0.9328\n",
      "Epoch 351/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.1821 - accuracy: 0.9273\n",
      "Epoch 352/1500\n",
      "40/40 [==============================] - 0s 767us/step - loss: 0.2025 - accuracy: 0.9214\n",
      "Epoch 353/1500\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.1777 - accuracy: 0.9321\n",
      "Epoch 354/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.1858 - accuracy: 0.9250\n",
      "Epoch 355/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.1830 - accuracy: 0.9297\n",
      "Epoch 356/1500\n",
      "40/40 [==============================] - 0s 784us/step - loss: 0.1792 - accuracy: 0.9368\n",
      "Epoch 357/1500\n",
      "40/40 [==============================] - 0s 757us/step - loss: 0.1838 - accuracy: 0.9313\n",
      "Epoch 358/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.1843 - accuracy: 0.9281\n",
      "Epoch 359/1500\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.1264 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 329.\n",
      "40/40 [==============================] - 0s 900us/step - loss: 0.1908 - accuracy: 0.9207\n",
      "Epoch 359: early stopping\n",
      "5/5 [==============================] - 0s 960us/step - loss: 0.6918 - accuracy: 0.7347\n",
      "5/5 [==============================] - 0s 873us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (20/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "Final Test Results - Loss: 0.6918318271636963, Accuracy: 0.7346938848495483, Precision: 0.5873205741626794, Recall: 0.8326023391812866, F1 Score: 0.6394582870571571\n",
      "Confusion Matrix:\n",
      " [[85 10 25]\n",
      " [ 0  8  0]\n",
      " [ 3  1 15]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "042A    14\n",
      "059A    14\n",
      "001A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "025A    11\n",
      "036A    11\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "065A     9\n",
      "051B     9\n",
      "072A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "099A     7\n",
      "008A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "026A     4\n",
      "009A     4\n",
      "105A     4\n",
      "035A     4\n",
      "003A     4\n",
      "104A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "014A     3\n",
      "012A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "018A     2\n",
      "032A     2\n",
      "093A     2\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "048A     1\n",
      "096A     1\n",
      "088A     1\n",
      "076A     1\n",
      "091A     1\n",
      "115A     1\n",
      "110A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "097B    14\n",
      "068A    11\n",
      "015A     9\n",
      "045A     9\n",
      "033A     9\n",
      "109A     6\n",
      "053A     6\n",
      "075A     5\n",
      "025C     5\n",
      "062A     4\n",
      "060A     3\n",
      "025B     2\n",
      "087A     2\n",
      "038A     2\n",
      "054A     2\n",
      "041A     1\n",
      "043A     1\n",
      "026C     1\n",
      "066A     1\n",
      "004A     1\n",
      "019B     1\n",
      "090A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    299\n",
      "F    216\n",
      "M    214\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    123\n",
      "X     49\n",
      "F     36\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 071A, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 050...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 097B, 067A, 020A, 062A, 002B, 029...\n",
      "kitten                             [109A, 043A, 041A, 045A]\n",
      "senior                             [055A, 054A, 090A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "Length of X_train_val:\n",
      "729\n",
      "Length of y_train_val:\n",
      "729\n",
      "Length of groups_train_val:\n",
      "729\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 842, 1: 770, 2: 770})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.1792 - accuracy: 0.4887\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.9641 - accuracy: 0.5684\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.8832 - accuracy: 0.6096\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.8350 - accuracy: 0.6352\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.7809 - accuracy: 0.6553\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.7504 - accuracy: 0.6721\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.7154 - accuracy: 0.6919\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.6978 - accuracy: 0.6919\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.6714 - accuracy: 0.7120\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.6822 - accuracy: 0.7120\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.6562 - accuracy: 0.7242\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.6420 - accuracy: 0.7280\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.6482 - accuracy: 0.7296\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.6109 - accuracy: 0.7477\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.6013 - accuracy: 0.7506\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.5963 - accuracy: 0.7540\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.6035 - accuracy: 0.7385\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.5850 - accuracy: 0.7448\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.5648 - accuracy: 0.7494\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.5805 - accuracy: 0.7473\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.5623 - accuracy: 0.7590\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.5352 - accuracy: 0.7641\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.5432 - accuracy: 0.7636\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.5445 - accuracy: 0.7548\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.5235 - accuracy: 0.7708\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.5153 - accuracy: 0.7750\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5195 - accuracy: 0.7699\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5170 - accuracy: 0.7842\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.5136 - accuracy: 0.7825\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 949us/step - loss: 0.5187 - accuracy: 0.7750\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 950us/step - loss: 0.5011 - accuracy: 0.7775\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.5041 - accuracy: 0.7914\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.5001 - accuracy: 0.7838\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.5022 - accuracy: 0.7863\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.4839 - accuracy: 0.7955\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.4907 - accuracy: 0.7876\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 905us/step - loss: 0.4672 - accuracy: 0.7955\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.5002 - accuracy: 0.7842\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 909us/step - loss: 0.4890 - accuracy: 0.7779\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 917us/step - loss: 0.4745 - accuracy: 0.7985\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.4713 - accuracy: 0.7964\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.4732 - accuracy: 0.7939\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.4509 - accuracy: 0.7997\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.4650 - accuracy: 0.8006\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.4651 - accuracy: 0.7968\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.4570 - accuracy: 0.8039\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.4478 - accuracy: 0.8094\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.4536 - accuracy: 0.8006\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.4366 - accuracy: 0.8073\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.4350 - accuracy: 0.8073\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.4439 - accuracy: 0.8073\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.4484 - accuracy: 0.8065\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.4398 - accuracy: 0.8056\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.4515 - accuracy: 0.8039\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.4403 - accuracy: 0.8044\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.4236 - accuracy: 0.8233\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.4336 - accuracy: 0.8140\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.4298 - accuracy: 0.8178\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.4381 - accuracy: 0.8065\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.4231 - accuracy: 0.8216\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.4325 - accuracy: 0.8119\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.4367 - accuracy: 0.8044\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.4183 - accuracy: 0.8115\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.4303 - accuracy: 0.8132\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.3977 - accuracy: 0.8233\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.4118 - accuracy: 0.8174\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.4079 - accuracy: 0.8321\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.4126 - accuracy: 0.8178\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.4112 - accuracy: 0.8237\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.4145 - accuracy: 0.8182\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.4099 - accuracy: 0.8212\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.4021 - accuracy: 0.8233\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.4006 - accuracy: 0.8195\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 699us/step - loss: 0.4031 - accuracy: 0.8249\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.3882 - accuracy: 0.8262\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.4006 - accuracy: 0.8262\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3966 - accuracy: 0.8283\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.4063 - accuracy: 0.8254\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.3791 - accuracy: 0.8396\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.4033 - accuracy: 0.8291\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.3949 - accuracy: 0.8266\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.3713 - accuracy: 0.8380\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.3874 - accuracy: 0.8338\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3781 - accuracy: 0.8396\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.3885 - accuracy: 0.8262\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.3806 - accuracy: 0.8296\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.3765 - accuracy: 0.8312\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.3815 - accuracy: 0.8342\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.3724 - accuracy: 0.8321\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.3714 - accuracy: 0.8380\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.3635 - accuracy: 0.8384\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3768 - accuracy: 0.8371\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.3614 - accuracy: 0.8476\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.3790 - accuracy: 0.8312\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3543 - accuracy: 0.8396\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.3572 - accuracy: 0.8463\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.3614 - accuracy: 0.8505\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.3604 - accuracy: 0.8434\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.3584 - accuracy: 0.8396\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.3448 - accuracy: 0.8472\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.3621 - accuracy: 0.8396\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.3552 - accuracy: 0.8438\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.3518 - accuracy: 0.8392\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3491 - accuracy: 0.8459\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.3505 - accuracy: 0.8447\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.3542 - accuracy: 0.8463\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.3366 - accuracy: 0.8552\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.3514 - accuracy: 0.8463\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.3516 - accuracy: 0.8493\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.3390 - accuracy: 0.8543\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.3422 - accuracy: 0.8493\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3435 - accuracy: 0.8526\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.3450 - accuracy: 0.8543\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.3431 - accuracy: 0.8463\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.3465 - accuracy: 0.8589\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.3478 - accuracy: 0.8493\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.3459 - accuracy: 0.8518\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.3442 - accuracy: 0.8426\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3354 - accuracy: 0.8535\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.3381 - accuracy: 0.8535\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.3311 - accuracy: 0.8577\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.3487 - accuracy: 0.8510\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3289 - accuracy: 0.8568\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.3410 - accuracy: 0.8497\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.3069 - accuracy: 0.8640\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.3416 - accuracy: 0.8535\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.3354 - accuracy: 0.8594\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.3222 - accuracy: 0.8623\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.3136 - accuracy: 0.8694\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3234 - accuracy: 0.8640\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.3249 - accuracy: 0.8615\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.3327 - accuracy: 0.8652\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.3236 - accuracy: 0.8631\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.3283 - accuracy: 0.8552\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.3254 - accuracy: 0.8673\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.3146 - accuracy: 0.8652\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.3107 - accuracy: 0.8715\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.3165 - accuracy: 0.8669\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.3167 - accuracy: 0.8669\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.3070 - accuracy: 0.8707\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.3090 - accuracy: 0.8661\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.3249 - accuracy: 0.8631\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.3065 - accuracy: 0.8694\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.3174 - accuracy: 0.8589\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.3107 - accuracy: 0.8640\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.3038 - accuracy: 0.8728\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.2992 - accuracy: 0.8757\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2976 - accuracy: 0.8766\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2980 - accuracy: 0.8732\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2941 - accuracy: 0.8753\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.3004 - accuracy: 0.8724\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.3045 - accuracy: 0.8715\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.3033 - accuracy: 0.8715\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.3037 - accuracy: 0.8715\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.3010 - accuracy: 0.8745\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.3109 - accuracy: 0.8732\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2961 - accuracy: 0.8720\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.3123 - accuracy: 0.8699\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2844 - accuracy: 0.8816\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.3054 - accuracy: 0.8694\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.2940 - accuracy: 0.8778\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2807 - accuracy: 0.8866\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2873 - accuracy: 0.8783\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2833 - accuracy: 0.8753\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2924 - accuracy: 0.8707\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2874 - accuracy: 0.8745\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.3005 - accuracy: 0.8741\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.2852 - accuracy: 0.8795\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.3005 - accuracy: 0.8728\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2796 - accuracy: 0.8816\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2899 - accuracy: 0.8736\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.2893 - accuracy: 0.8753\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2817 - accuracy: 0.8808\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2792 - accuracy: 0.8862\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2967 - accuracy: 0.8778\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2719 - accuracy: 0.8778\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2862 - accuracy: 0.8804\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2775 - accuracy: 0.8795\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2781 - accuracy: 0.8866\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.2784 - accuracy: 0.8829\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2768 - accuracy: 0.8917\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2716 - accuracy: 0.8841\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2794 - accuracy: 0.8766\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2857 - accuracy: 0.8728\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2792 - accuracy: 0.8837\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2745 - accuracy: 0.8850\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2672 - accuracy: 0.8829\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.2727 - accuracy: 0.8816\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2576 - accuracy: 0.8934\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2666 - accuracy: 0.8908\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2744 - accuracy: 0.8766\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2666 - accuracy: 0.8913\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2650 - accuracy: 0.8934\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2535 - accuracy: 0.8934\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2681 - accuracy: 0.8854\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2939 - accuracy: 0.8745\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.2658 - accuracy: 0.8917\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2601 - accuracy: 0.8929\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2698 - accuracy: 0.8862\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2572 - accuracy: 0.8980\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2547 - accuracy: 0.8971\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2615 - accuracy: 0.8925\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2681 - accuracy: 0.8925\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.2654 - accuracy: 0.8908\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2674 - accuracy: 0.8854\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2614 - accuracy: 0.8904\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2639 - accuracy: 0.8883\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2606 - accuracy: 0.8921\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2512 - accuracy: 0.8955\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.2506 - accuracy: 0.8984\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 732us/step - loss: 0.2565 - accuracy: 0.8946\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2476 - accuracy: 0.8938\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2490 - accuracy: 0.8929\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2486 - accuracy: 0.8976\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2557 - accuracy: 0.8913\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.2577 - accuracy: 0.8963\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.2454 - accuracy: 0.8988\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.2614 - accuracy: 0.8913\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2344 - accuracy: 0.8971\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.2517 - accuracy: 0.8959\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2451 - accuracy: 0.8984\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2498 - accuracy: 0.8976\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2417 - accuracy: 0.8984\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2460 - accuracy: 0.8980\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2464 - accuracy: 0.8988\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2441 - accuracy: 0.9018\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2486 - accuracy: 0.8929\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2416 - accuracy: 0.8980\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.2427 - accuracy: 0.9051\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.2454 - accuracy: 0.9026\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2324 - accuracy: 0.9026\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2396 - accuracy: 0.9055\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2419 - accuracy: 0.9009\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2341 - accuracy: 0.9060\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2264 - accuracy: 0.9043\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2318 - accuracy: 0.9106\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2401 - accuracy: 0.9026\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2357 - accuracy: 0.9085\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2225 - accuracy: 0.9093\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2324 - accuracy: 0.9034\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2292 - accuracy: 0.9034\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2466 - accuracy: 0.8967\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2228 - accuracy: 0.9106\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2428 - accuracy: 0.9051\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.2306 - accuracy: 0.9051\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2297 - accuracy: 0.9060\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2417 - accuracy: 0.9005\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.2158 - accuracy: 0.9144\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.9022\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.2287 - accuracy: 0.9068\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2294 - accuracy: 0.9139\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2307 - accuracy: 0.9026\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2367 - accuracy: 0.9022\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2236 - accuracy: 0.9081\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2358 - accuracy: 0.9085\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2262 - accuracy: 0.9097\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2322 - accuracy: 0.9034\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2289 - accuracy: 0.8988\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.2263 - accuracy: 0.9097\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 945us/step - loss: 0.2190 - accuracy: 0.9076\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.2230 - accuracy: 0.9076\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2235 - accuracy: 0.9102\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2368 - accuracy: 0.8955\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2224 - accuracy: 0.9055\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 998us/step - loss: 0.2207 - accuracy: 0.9110\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9114\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.2449 - accuracy: 0.8963\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.2210 - accuracy: 0.9123\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2232 - accuracy: 0.9068\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2113 - accuracy: 0.9110\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2313 - accuracy: 0.9068\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.2097 - accuracy: 0.9118\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.2203 - accuracy: 0.9144\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2210 - accuracy: 0.9055\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2277 - accuracy: 0.9102\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.2111 - accuracy: 0.9148\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.2107 - accuracy: 0.9186\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 940us/step - loss: 0.2143 - accuracy: 0.9106\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.2184 - accuracy: 0.9085\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2219 - accuracy: 0.9072\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 951us/step - loss: 0.2163 - accuracy: 0.9181\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2228 - accuracy: 0.9123\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2107 - accuracy: 0.9139\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2098 - accuracy: 0.9232\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2087 - accuracy: 0.9186\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2100 - accuracy: 0.9169\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2176 - accuracy: 0.9072\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2111 - accuracy: 0.9127\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2007 - accuracy: 0.9181\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.2031 - accuracy: 0.9194\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2048 - accuracy: 0.9156\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.1977 - accuracy: 0.9232\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1952 - accuracy: 0.9274\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.2034 - accuracy: 0.9211\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2233 - accuracy: 0.9106\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2137 - accuracy: 0.9106\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 907us/step - loss: 0.1967 - accuracy: 0.9186\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.1997 - accuracy: 0.9219\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.2016 - accuracy: 0.9156\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2144 - accuracy: 0.9131\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.2096 - accuracy: 0.9135\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2148 - accuracy: 0.9144\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.2063 - accuracy: 0.9148\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2129 - accuracy: 0.9207\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.2054 - accuracy: 0.9228\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2109 - accuracy: 0.9198\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2050 - accuracy: 0.9156\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.2145 - accuracy: 0.9085\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.2021 - accuracy: 0.9211\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2149 - accuracy: 0.9089\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.1944 - accuracy: 0.9211\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2018 - accuracy: 0.9173\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1990 - accuracy: 0.9211\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2056 - accuracy: 0.9186\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2002 - accuracy: 0.9198\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2133 - accuracy: 0.9181\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2104 - accuracy: 0.9160\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.2034 - accuracy: 0.9160\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2009 - accuracy: 0.9186\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.1984 - accuracy: 0.9215\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2036 - accuracy: 0.9228\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1966 - accuracy: 0.9223\n",
      "Epoch 323/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.2174 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 293.\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2077 - accuracy: 0.9194\n",
      "Epoch 323: early stopping\n",
      "7/7 [==============================] - 0s 749us/step - loss: 0.5779 - accuracy: 0.7596\n",
      "7/7 [==============================] - 0s 610us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "After appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "Final Test Results - Loss: 0.5778680443763733, Accuracy: 0.7596153616905212, Precision: 0.6506018893387314, Recall: 0.8469091229306094, F1 Score: 0.6971572294392726\n",
      "Confusion Matrix:\n",
      " [[121  10  36]\n",
      " [  1  16   0]\n",
      " [  3   0  21]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "016A    10\n",
      "071A    10\n",
      "065A     9\n",
      "045A     9\n",
      "022A     9\n",
      "015A     9\n",
      "033A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "031A     7\n",
      "117A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "053A     6\n",
      "075A     5\n",
      "044A     5\n",
      "021A     5\n",
      "025C     5\n",
      "034A     5\n",
      "023B     5\n",
      "009A     4\n",
      "003A     4\n",
      "104A     4\n",
      "062A     4\n",
      "113A     3\n",
      "014A     3\n",
      "056A     3\n",
      "060A     3\n",
      "064A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "018A     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "011A     2\n",
      "100A     1\n",
      "090A     1\n",
      "115A     1\n",
      "088A     1\n",
      "024A     1\n",
      "019B     1\n",
      "096A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "043A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "116A    12\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "037A     6\n",
      "023A     6\n",
      "070A     5\n",
      "052A     4\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "012A     3\n",
      "006A     3\n",
      "058A     3\n",
      "032A     2\n",
      "076A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    235\n",
      "X    186\n",
      "F    169\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    162\n",
      "M    102\n",
      "F     83\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 071A, 097B, 028A, 019A, 074A, 067...\n",
      "kitten    [044A, 047A, 109A, 050A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 095A, 072A, 023A, 027...\n",
      "kitten                 [014B, 111A, 040A, 046A, 042A, 110A]\n",
      "senior                       [057A, 106A, 116A, 051B, 058A]\n",
      "Name: cat_id, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A' '010A'\n",
      " '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '031A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '047A' '048A'\n",
      " '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A' '073A'\n",
      " '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A' '096A'\n",
      " '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '109A' '113A'\n",
      " '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A'\n",
      " '032A' '035A' '037A' '040A' '042A' '046A' '051B' '052A' '057A' '058A'\n",
      " '070A' '072A' '076A' '095A' '103A' '105A' '106A' '110A' '111A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A', '000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'109A', '031A'}\n",
      "Moved to Test Set:\n",
      "{'109A', '031A'}\n",
      "Removed from Test Set\n",
      "{'046A', '000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A'\n",
      " '010A' '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A'\n",
      " '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A'\n",
      " '073A' '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '096A' '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '113A'\n",
      " '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A' '031A'\n",
      " '032A' '035A' '037A' '040A' '042A' '051B' '052A' '057A' '058A' '070A'\n",
      " '072A' '076A' '095A' '103A' '105A' '106A' '109A' '110A' '111A' '116A']\n",
      "Length of X_train_val:\n",
      "679\n",
      "Length of y_train_val:\n",
      "679\n",
      "Length of groups_train_val:\n",
      "679\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     417\n",
      "senior    113\n",
      "kitten     60\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     171\n",
      "kitten    111\n",
      "senior     65\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     449\n",
      "kitten    117\n",
      "senior    113\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     139\n",
      "senior     65\n",
      "kitten     54\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 898, 1: 585, 2: 565})\n",
      "Epoch 1/1500\n",
      "32/32 [==============================] - 0s 969us/step - loss: 1.1738 - accuracy: 0.4521\n",
      "Epoch 2/1500\n",
      "32/32 [==============================] - 0s 935us/step - loss: 0.9742 - accuracy: 0.5566\n",
      "Epoch 3/1500\n",
      "32/32 [==============================] - 0s 881us/step - loss: 0.8616 - accuracy: 0.6172\n",
      "Epoch 4/1500\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.8320 - accuracy: 0.6416\n",
      "Epoch 5/1500\n",
      "32/32 [==============================] - 0s 779us/step - loss: 0.7739 - accuracy: 0.6685\n",
      "Epoch 6/1500\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.7315 - accuracy: 0.6870\n",
      "Epoch 7/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.7306 - accuracy: 0.6821\n",
      "Epoch 8/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.6864 - accuracy: 0.7124\n",
      "Epoch 9/1500\n",
      "32/32 [==============================] - 0s 782us/step - loss: 0.6781 - accuracy: 0.7202\n",
      "Epoch 10/1500\n",
      "32/32 [==============================] - 0s 874us/step - loss: 0.6637 - accuracy: 0.7212\n",
      "Epoch 11/1500\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.6449 - accuracy: 0.7153\n",
      "Epoch 12/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.6136 - accuracy: 0.7329\n",
      "Epoch 13/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.6290 - accuracy: 0.7407\n",
      "Epoch 14/1500\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.6216 - accuracy: 0.7402\n",
      "Epoch 15/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.5706 - accuracy: 0.7515\n",
      "Epoch 16/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.5868 - accuracy: 0.7485\n",
      "Epoch 17/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.5741 - accuracy: 0.7656\n",
      "Epoch 18/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.5594 - accuracy: 0.7666\n",
      "Epoch 19/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.5404 - accuracy: 0.7783\n",
      "Epoch 20/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.5510 - accuracy: 0.7598\n",
      "Epoch 21/1500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.5263 - accuracy: 0.7837\n",
      "Epoch 22/1500\n",
      "32/32 [==============================] - 0s 818us/step - loss: 0.5167 - accuracy: 0.7793\n",
      "Epoch 23/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.5169 - accuracy: 0.7886\n",
      "Epoch 24/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.5353 - accuracy: 0.7812\n",
      "Epoch 25/1500\n",
      "32/32 [==============================] - 0s 816us/step - loss: 0.5165 - accuracy: 0.7896\n",
      "Epoch 26/1500\n",
      "32/32 [==============================] - 0s 795us/step - loss: 0.5034 - accuracy: 0.7905\n",
      "Epoch 27/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.5024 - accuracy: 0.7910\n",
      "Epoch 28/1500\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.4999 - accuracy: 0.7983\n",
      "Epoch 29/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.4767 - accuracy: 0.8071\n",
      "Epoch 30/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4805 - accuracy: 0.8130\n",
      "Epoch 31/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.4845 - accuracy: 0.8086\n",
      "Epoch 32/1500\n",
      "32/32 [==============================] - 0s 691us/step - loss: 0.4795 - accuracy: 0.8057\n",
      "Epoch 33/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.4733 - accuracy: 0.8013\n",
      "Epoch 34/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.4799 - accuracy: 0.8071\n",
      "Epoch 35/1500\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.4667 - accuracy: 0.7925\n",
      "Epoch 36/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.4660 - accuracy: 0.8062\n",
      "Epoch 37/1500\n",
      "32/32 [==============================] - 0s 817us/step - loss: 0.4650 - accuracy: 0.8105\n",
      "Epoch 38/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.4460 - accuracy: 0.8218\n",
      "Epoch 39/1500\n",
      "32/32 [==============================] - 0s 759us/step - loss: 0.4399 - accuracy: 0.8208\n",
      "Epoch 40/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.4546 - accuracy: 0.8086\n",
      "Epoch 41/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.4498 - accuracy: 0.8120\n",
      "Epoch 42/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.4371 - accuracy: 0.8223\n",
      "Epoch 43/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.4368 - accuracy: 0.8237\n",
      "Epoch 44/1500\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.4262 - accuracy: 0.8213\n",
      "Epoch 45/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.4327 - accuracy: 0.8271\n",
      "Epoch 46/1500\n",
      "32/32 [==============================] - 0s 795us/step - loss: 0.4227 - accuracy: 0.8228\n",
      "Epoch 47/1500\n",
      "32/32 [==============================] - 0s 782us/step - loss: 0.4225 - accuracy: 0.8330\n",
      "Epoch 48/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.4208 - accuracy: 0.8306\n",
      "Epoch 49/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.4228 - accuracy: 0.8237\n",
      "Epoch 50/1500\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.4195 - accuracy: 0.8301\n",
      "Epoch 51/1500\n",
      "32/32 [==============================] - 0s 779us/step - loss: 0.4116 - accuracy: 0.8291\n",
      "Epoch 52/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.4295 - accuracy: 0.8169\n",
      "Epoch 53/1500\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.4090 - accuracy: 0.8364\n",
      "Epoch 54/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.3919 - accuracy: 0.8345\n",
      "Epoch 55/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.3989 - accuracy: 0.8413\n",
      "Epoch 56/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.4116 - accuracy: 0.8301\n",
      "Epoch 57/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.3867 - accuracy: 0.8413\n",
      "Epoch 58/1500\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.3838 - accuracy: 0.8438\n",
      "Epoch 59/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.4164 - accuracy: 0.8291\n",
      "Epoch 60/1500\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.4021 - accuracy: 0.8369\n",
      "Epoch 61/1500\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.3987 - accuracy: 0.8369\n",
      "Epoch 62/1500\n",
      "32/32 [==============================] - 0s 816us/step - loss: 0.3859 - accuracy: 0.8442\n",
      "Epoch 63/1500\n",
      "32/32 [==============================] - 0s 828us/step - loss: 0.4010 - accuracy: 0.8379\n",
      "Epoch 64/1500\n",
      "32/32 [==============================] - 0s 795us/step - loss: 0.4080 - accuracy: 0.8379\n",
      "Epoch 65/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.3851 - accuracy: 0.8447\n",
      "Epoch 66/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.3876 - accuracy: 0.8408\n",
      "Epoch 67/1500\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.3766 - accuracy: 0.8540\n",
      "Epoch 68/1500\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.3737 - accuracy: 0.8423\n",
      "Epoch 69/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.3929 - accuracy: 0.8418\n",
      "Epoch 70/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.3740 - accuracy: 0.8540\n",
      "Epoch 71/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.3642 - accuracy: 0.8481\n",
      "Epoch 72/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.3819 - accuracy: 0.8457\n",
      "Epoch 73/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.3664 - accuracy: 0.8530\n",
      "Epoch 74/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3544 - accuracy: 0.8540\n",
      "Epoch 75/1500\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.3586 - accuracy: 0.8535\n",
      "Epoch 76/1500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.3585 - accuracy: 0.8442\n",
      "Epoch 77/1500\n",
      "32/32 [==============================] - 0s 917us/step - loss: 0.3635 - accuracy: 0.8545\n",
      "Epoch 78/1500\n",
      "32/32 [==============================] - 0s 896us/step - loss: 0.3538 - accuracy: 0.8569\n",
      "Epoch 79/1500\n",
      "32/32 [==============================] - 0s 962us/step - loss: 0.3643 - accuracy: 0.8467\n",
      "Epoch 80/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3547 - accuracy: 0.8594\n",
      "Epoch 81/1500\n",
      "32/32 [==============================] - 0s 915us/step - loss: 0.3509 - accuracy: 0.8540\n",
      "Epoch 82/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.3554 - accuracy: 0.8579\n",
      "Epoch 83/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.3407 - accuracy: 0.8662\n",
      "Epoch 84/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.3517 - accuracy: 0.8555\n",
      "Epoch 85/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.3488 - accuracy: 0.8574\n",
      "Epoch 86/1500\n",
      "32/32 [==============================] - 0s 921us/step - loss: 0.3371 - accuracy: 0.8667\n",
      "Epoch 87/1500\n",
      "32/32 [==============================] - 0s 866us/step - loss: 0.3447 - accuracy: 0.8579\n",
      "Epoch 88/1500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.3452 - accuracy: 0.8643\n",
      "Epoch 89/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.3641 - accuracy: 0.8486\n",
      "Epoch 90/1500\n",
      "32/32 [==============================] - 0s 814us/step - loss: 0.3641 - accuracy: 0.8555\n",
      "Epoch 91/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.3336 - accuracy: 0.8594\n",
      "Epoch 92/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.3311 - accuracy: 0.8604\n",
      "Epoch 93/1500\n",
      "32/32 [==============================] - 0s 800us/step - loss: 0.3281 - accuracy: 0.8633\n",
      "Epoch 94/1500\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.3270 - accuracy: 0.8740\n",
      "Epoch 95/1500\n",
      "32/32 [==============================] - 0s 823us/step - loss: 0.3302 - accuracy: 0.8657\n",
      "Epoch 96/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.3341 - accuracy: 0.8652\n",
      "Epoch 97/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.3588 - accuracy: 0.8555\n",
      "Epoch 98/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3286 - accuracy: 0.8677\n",
      "Epoch 99/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.3211 - accuracy: 0.8706\n",
      "Epoch 100/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.3261 - accuracy: 0.8662\n",
      "Epoch 101/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.3261 - accuracy: 0.8711\n",
      "Epoch 102/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.3378 - accuracy: 0.8643\n",
      "Epoch 103/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.3073 - accuracy: 0.8730\n",
      "Epoch 104/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.3176 - accuracy: 0.8638\n",
      "Epoch 105/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.3179 - accuracy: 0.8740\n",
      "Epoch 106/1500\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.2996 - accuracy: 0.8877\n",
      "Epoch 107/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.3120 - accuracy: 0.8706\n",
      "Epoch 108/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.3144 - accuracy: 0.8726\n",
      "Epoch 109/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.3204 - accuracy: 0.8677\n",
      "Epoch 110/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.3126 - accuracy: 0.8735\n",
      "Epoch 111/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.3051 - accuracy: 0.8794\n",
      "Epoch 112/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.3296 - accuracy: 0.8628\n",
      "Epoch 113/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.3191 - accuracy: 0.8765\n",
      "Epoch 114/1500\n",
      "32/32 [==============================] - 0s 815us/step - loss: 0.2943 - accuracy: 0.8877\n",
      "Epoch 115/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.3032 - accuracy: 0.8740\n",
      "Epoch 116/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.3036 - accuracy: 0.8794\n",
      "Epoch 117/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.3086 - accuracy: 0.8735\n",
      "Epoch 118/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.3025 - accuracy: 0.8750\n",
      "Epoch 119/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.3063 - accuracy: 0.8779\n",
      "Epoch 120/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.3128 - accuracy: 0.8735\n",
      "Epoch 121/1500\n",
      "32/32 [==============================] - 0s 701us/step - loss: 0.3163 - accuracy: 0.8696\n",
      "Epoch 122/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.3108 - accuracy: 0.8765\n",
      "Epoch 123/1500\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.2965 - accuracy: 0.8857\n",
      "Epoch 124/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.3005 - accuracy: 0.8735\n",
      "Epoch 125/1500\n",
      "32/32 [==============================] - 0s 907us/step - loss: 0.2964 - accuracy: 0.8867\n",
      "Epoch 126/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2981 - accuracy: 0.8818\n",
      "Epoch 127/1500\n",
      "32/32 [==============================] - 0s 827us/step - loss: 0.2983 - accuracy: 0.8774\n",
      "Epoch 128/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.2924 - accuracy: 0.8823\n",
      "Epoch 129/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2872 - accuracy: 0.8838\n",
      "Epoch 130/1500\n",
      "32/32 [==============================] - 0s 766us/step - loss: 0.2950 - accuracy: 0.8789\n",
      "Epoch 131/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.2969 - accuracy: 0.8760\n",
      "Epoch 132/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2786 - accuracy: 0.8936\n",
      "Epoch 133/1500\n",
      "32/32 [==============================] - 0s 882us/step - loss: 0.2872 - accuracy: 0.8906\n",
      "Epoch 134/1500\n",
      "32/32 [==============================] - 0s 823us/step - loss: 0.2903 - accuracy: 0.8887\n",
      "Epoch 135/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.2993 - accuracy: 0.8799\n",
      "Epoch 136/1500\n",
      "32/32 [==============================] - 0s 902us/step - loss: 0.2835 - accuracy: 0.8838\n",
      "Epoch 137/1500\n",
      "32/32 [==============================] - 0s 879us/step - loss: 0.2922 - accuracy: 0.8848\n",
      "Epoch 138/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.2787 - accuracy: 0.8921\n",
      "Epoch 139/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.2825 - accuracy: 0.8911\n",
      "Epoch 140/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.2754 - accuracy: 0.8950\n",
      "Epoch 141/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.2864 - accuracy: 0.8823\n",
      "Epoch 142/1500\n",
      "32/32 [==============================] - 0s 898us/step - loss: 0.2609 - accuracy: 0.9019\n",
      "Epoch 143/1500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.2845 - accuracy: 0.8901\n",
      "Epoch 144/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2800 - accuracy: 0.8906\n",
      "Epoch 145/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.2715 - accuracy: 0.8950\n",
      "Epoch 146/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.2593 - accuracy: 0.8950\n",
      "Epoch 147/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.2612 - accuracy: 0.8833\n",
      "Epoch 148/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.2758 - accuracy: 0.8994\n",
      "Epoch 149/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.2598 - accuracy: 0.8984\n",
      "Epoch 150/1500\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.2644 - accuracy: 0.8843\n",
      "Epoch 151/1500\n",
      "32/32 [==============================] - 0s 715us/step - loss: 0.2734 - accuracy: 0.8975\n",
      "Epoch 152/1500\n",
      "32/32 [==============================] - 0s 800us/step - loss: 0.2657 - accuracy: 0.8931\n",
      "Epoch 153/1500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.2560 - accuracy: 0.9038\n",
      "Epoch 154/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2533 - accuracy: 0.9111\n",
      "Epoch 155/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.2592 - accuracy: 0.8911\n",
      "Epoch 156/1500\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.2583 - accuracy: 0.8970\n",
      "Epoch 157/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.2668 - accuracy: 0.8911\n",
      "Epoch 158/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.2477 - accuracy: 0.9014\n",
      "Epoch 159/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.2711 - accuracy: 0.8955\n",
      "Epoch 160/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.2516 - accuracy: 0.9004\n",
      "Epoch 161/1500\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.2539 - accuracy: 0.9097\n",
      "Epoch 162/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.2569 - accuracy: 0.8984\n",
      "Epoch 163/1500\n",
      "32/32 [==============================] - 0s 779us/step - loss: 0.2547 - accuracy: 0.9033\n",
      "Epoch 164/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.2543 - accuracy: 0.9058\n",
      "Epoch 165/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2449 - accuracy: 0.9019\n",
      "Epoch 166/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2604 - accuracy: 0.8921\n",
      "Epoch 167/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.2355 - accuracy: 0.9146\n",
      "Epoch 168/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2557 - accuracy: 0.9038\n",
      "Epoch 169/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.2343 - accuracy: 0.9082\n",
      "Epoch 170/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2366 - accuracy: 0.9033\n",
      "Epoch 171/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.2448 - accuracy: 0.9048\n",
      "Epoch 172/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.2523 - accuracy: 0.9004\n",
      "Epoch 173/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.2350 - accuracy: 0.9033\n",
      "Epoch 174/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2417 - accuracy: 0.9009\n",
      "Epoch 175/1500\n",
      "32/32 [==============================] - 0s 688us/step - loss: 0.2516 - accuracy: 0.9023\n",
      "Epoch 176/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.2405 - accuracy: 0.9067\n",
      "Epoch 177/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.2520 - accuracy: 0.8975\n",
      "Epoch 178/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.2307 - accuracy: 0.9058\n",
      "Epoch 179/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.2267 - accuracy: 0.9092\n",
      "Epoch 180/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.2323 - accuracy: 0.9072\n",
      "Epoch 181/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2467 - accuracy: 0.8984\n",
      "Epoch 182/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.2421 - accuracy: 0.9048\n",
      "Epoch 183/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.2431 - accuracy: 0.9023\n",
      "Epoch 184/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.2408 - accuracy: 0.9102\n",
      "Epoch 185/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.2308 - accuracy: 0.9155\n",
      "Epoch 186/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.2405 - accuracy: 0.9082\n",
      "Epoch 187/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.2389 - accuracy: 0.9058\n",
      "Epoch 188/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.2368 - accuracy: 0.9033\n",
      "Epoch 189/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.2248 - accuracy: 0.9126\n",
      "Epoch 190/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.2461 - accuracy: 0.8994\n",
      "Epoch 191/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.2404 - accuracy: 0.9033\n",
      "Epoch 192/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.2348 - accuracy: 0.9072\n",
      "Epoch 193/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.2324 - accuracy: 0.9082\n",
      "Epoch 194/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.2370 - accuracy: 0.9038\n",
      "Epoch 195/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2258 - accuracy: 0.9102\n",
      "Epoch 196/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.2319 - accuracy: 0.9111\n",
      "Epoch 197/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.2402 - accuracy: 0.9058\n",
      "Epoch 198/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.2247 - accuracy: 0.9106\n",
      "Epoch 199/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.2216 - accuracy: 0.9111\n",
      "Epoch 200/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.2170 - accuracy: 0.9214\n",
      "Epoch 201/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.2110 - accuracy: 0.9194\n",
      "Epoch 202/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.2216 - accuracy: 0.9146\n",
      "Epoch 203/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.2282 - accuracy: 0.9106\n",
      "Epoch 204/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.2299 - accuracy: 0.9067\n",
      "Epoch 205/1500\n",
      "32/32 [==============================] - 0s 715us/step - loss: 0.2251 - accuracy: 0.9170\n",
      "Epoch 206/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.2013 - accuracy: 0.9229\n",
      "Epoch 207/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.2260 - accuracy: 0.9126\n",
      "Epoch 208/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.2204 - accuracy: 0.9141\n",
      "Epoch 209/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.2432 - accuracy: 0.9043\n",
      "Epoch 210/1500\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.2183 - accuracy: 0.9180\n",
      "Epoch 211/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.2060 - accuracy: 0.9268\n",
      "Epoch 212/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.2151 - accuracy: 0.9141\n",
      "Epoch 213/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.2176 - accuracy: 0.9150\n",
      "Epoch 214/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.2246 - accuracy: 0.9097\n",
      "Epoch 215/1500\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.2119 - accuracy: 0.9097\n",
      "Epoch 216/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.2225 - accuracy: 0.9106\n",
      "Epoch 217/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.2054 - accuracy: 0.9229\n",
      "Epoch 218/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.2133 - accuracy: 0.9175\n",
      "Epoch 219/1500\n",
      "32/32 [==============================] - 0s 687us/step - loss: 0.2089 - accuracy: 0.9189\n",
      "Epoch 220/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.2204 - accuracy: 0.9146\n",
      "Epoch 221/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.2248 - accuracy: 0.9131\n",
      "Epoch 222/1500\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.1952 - accuracy: 0.9229\n",
      "Epoch 223/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.1990 - accuracy: 0.9233\n",
      "Epoch 224/1500\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.2157 - accuracy: 0.9136\n",
      "Epoch 225/1500\n",
      "32/32 [==============================] - 0s 810us/step - loss: 0.2147 - accuracy: 0.9165\n",
      "Epoch 226/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.2081 - accuracy: 0.9136\n",
      "Epoch 227/1500\n",
      "32/32 [==============================] - 0s 849us/step - loss: 0.1862 - accuracy: 0.9258\n",
      "Epoch 228/1500\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.2006 - accuracy: 0.9170\n",
      "Epoch 229/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.1886 - accuracy: 0.9287\n",
      "Epoch 230/1500\n",
      "32/32 [==============================] - 0s 799us/step - loss: 0.1890 - accuracy: 0.9248\n",
      "Epoch 231/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.1973 - accuracy: 0.9258\n",
      "Epoch 232/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.1976 - accuracy: 0.9160\n",
      "Epoch 233/1500\n",
      "32/32 [==============================] - 0s 813us/step - loss: 0.1910 - accuracy: 0.9287\n",
      "Epoch 234/1500\n",
      "32/32 [==============================] - 0s 874us/step - loss: 0.1944 - accuracy: 0.9248\n",
      "Epoch 235/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.2238 - accuracy: 0.9092\n",
      "Epoch 236/1500\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.2089 - accuracy: 0.9160\n",
      "Epoch 237/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.1996 - accuracy: 0.9258\n",
      "Epoch 238/1500\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.1987 - accuracy: 0.9297\n",
      "Epoch 239/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.1939 - accuracy: 0.9268\n",
      "Epoch 240/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.1978 - accuracy: 0.9253\n",
      "Epoch 241/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1985 - accuracy: 0.9238\n",
      "Epoch 242/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.1758 - accuracy: 0.9331\n",
      "Epoch 243/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.1933 - accuracy: 0.9263\n",
      "Epoch 244/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2010 - accuracy: 0.9204\n",
      "Epoch 245/1500\n",
      "32/32 [==============================] - 0s 766us/step - loss: 0.1971 - accuracy: 0.9233\n",
      "Epoch 246/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.1862 - accuracy: 0.9253\n",
      "Epoch 247/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.2116 - accuracy: 0.9155\n",
      "Epoch 248/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.1904 - accuracy: 0.9287\n",
      "Epoch 249/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.1912 - accuracy: 0.9268\n",
      "Epoch 250/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.1913 - accuracy: 0.9199\n",
      "Epoch 251/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.1859 - accuracy: 0.9365\n",
      "Epoch 252/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.1979 - accuracy: 0.9219\n",
      "Epoch 253/1500\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.1931 - accuracy: 0.9224\n",
      "Epoch 254/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.1836 - accuracy: 0.9268\n",
      "Epoch 255/1500\n",
      "32/32 [==============================] - 0s 703us/step - loss: 0.1875 - accuracy: 0.9282\n",
      "Epoch 256/1500\n",
      "32/32 [==============================] - 0s 694us/step - loss: 0.1866 - accuracy: 0.9287\n",
      "Epoch 257/1500\n",
      "32/32 [==============================] - 0s 786us/step - loss: 0.1927 - accuracy: 0.9292\n",
      "Epoch 258/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.2039 - accuracy: 0.9185\n",
      "Epoch 259/1500\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.1741 - accuracy: 0.9321\n",
      "Epoch 260/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.1788 - accuracy: 0.9355\n",
      "Epoch 261/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.1835 - accuracy: 0.9258\n",
      "Epoch 262/1500\n",
      "32/32 [==============================] - 0s 711us/step - loss: 0.1813 - accuracy: 0.9380\n",
      "Epoch 263/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.1863 - accuracy: 0.9277\n",
      "Epoch 264/1500\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.1836 - accuracy: 0.9272\n",
      "Epoch 265/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.1731 - accuracy: 0.9277\n",
      "Epoch 266/1500\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.1906 - accuracy: 0.9253\n",
      "Epoch 267/1500\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.1843 - accuracy: 0.9282\n",
      "Epoch 268/1500\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.1765 - accuracy: 0.9341\n",
      "Epoch 269/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.1695 - accuracy: 0.9385\n",
      "Epoch 270/1500\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.1712 - accuracy: 0.9385\n",
      "Epoch 271/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.1840 - accuracy: 0.9272\n",
      "Epoch 272/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.1901 - accuracy: 0.9238\n",
      "Epoch 273/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.1748 - accuracy: 0.9302\n",
      "Epoch 274/1500\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.1774 - accuracy: 0.9277\n",
      "Epoch 275/1500\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.1912 - accuracy: 0.9312\n",
      "Epoch 276/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.1739 - accuracy: 0.9321\n",
      "Epoch 277/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.1721 - accuracy: 0.9380\n",
      "Epoch 278/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.1701 - accuracy: 0.9399\n",
      "Epoch 279/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.1738 - accuracy: 0.9370\n",
      "Epoch 280/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.1701 - accuracy: 0.9390\n",
      "Epoch 281/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.1830 - accuracy: 0.9263\n",
      "Epoch 282/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.1811 - accuracy: 0.9312\n",
      "Epoch 283/1500\n",
      "32/32 [==============================] - 0s 900us/step - loss: 0.1666 - accuracy: 0.9370\n",
      "Epoch 284/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.1776 - accuracy: 0.9312\n",
      "Epoch 285/1500\n",
      "32/32 [==============================] - 0s 900us/step - loss: 0.1595 - accuracy: 0.9380\n",
      "Epoch 286/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.1782 - accuracy: 0.9258\n",
      "Epoch 287/1500\n",
      "32/32 [==============================] - 0s 715us/step - loss: 0.1649 - accuracy: 0.9385\n",
      "Epoch 288/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.1920 - accuracy: 0.9233\n",
      "Epoch 289/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.1864 - accuracy: 0.9277\n",
      "Epoch 290/1500\n",
      "32/32 [==============================] - 0s 714us/step - loss: 0.1813 - accuracy: 0.9341\n",
      "Epoch 291/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.1665 - accuracy: 0.9346\n",
      "Epoch 292/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.1690 - accuracy: 0.9385\n",
      "Epoch 293/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.1929 - accuracy: 0.9243\n",
      "Epoch 294/1500\n",
      "32/32 [==============================] - 0s 751us/step - loss: 0.1612 - accuracy: 0.9390\n",
      "Epoch 295/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.1778 - accuracy: 0.9287\n",
      "Epoch 296/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1628 - accuracy: 0.9365\n",
      "Epoch 297/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1898 - accuracy: 0.9238\n",
      "Epoch 298/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.1640 - accuracy: 0.9365\n",
      "Epoch 299/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1702 - accuracy: 0.9355\n",
      "Epoch 300/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.1752 - accuracy: 0.9331\n",
      "Epoch 301/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.1670 - accuracy: 0.9395\n",
      "Epoch 302/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.1623 - accuracy: 0.9370\n",
      "Epoch 303/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.1547 - accuracy: 0.9438\n",
      "Epoch 304/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.1745 - accuracy: 0.9331\n",
      "Epoch 305/1500\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.1632 - accuracy: 0.9380\n",
      "Epoch 306/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.1610 - accuracy: 0.9409\n",
      "Epoch 307/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.1719 - accuracy: 0.9316\n",
      "Epoch 308/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.1906 - accuracy: 0.9272\n",
      "Epoch 309/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.1646 - accuracy: 0.9395\n",
      "Epoch 310/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.1596 - accuracy: 0.9429\n",
      "Epoch 311/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.1554 - accuracy: 0.9375\n",
      "Epoch 312/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.1758 - accuracy: 0.9331\n",
      "Epoch 313/1500\n",
      "32/32 [==============================] - 0s 686us/step - loss: 0.1602 - accuracy: 0.9390\n",
      "Epoch 314/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1652 - accuracy: 0.9365\n",
      "Epoch 315/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.1668 - accuracy: 0.9316\n",
      "Epoch 316/1500\n",
      "32/32 [==============================] - 0s 695us/step - loss: 0.1645 - accuracy: 0.9390\n",
      "Epoch 317/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.1612 - accuracy: 0.9341\n",
      "Epoch 318/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.1606 - accuracy: 0.9375\n",
      "Epoch 319/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.1569 - accuracy: 0.9419\n",
      "Epoch 320/1500\n",
      "32/32 [==============================] - 0s 699us/step - loss: 0.1566 - accuracy: 0.9390\n",
      "Epoch 321/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.1496 - accuracy: 0.9414\n",
      "Epoch 322/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.1525 - accuracy: 0.9414\n",
      "Epoch 323/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.1472 - accuracy: 0.9438\n",
      "Epoch 324/1500\n",
      "32/32 [==============================] - 0s 689us/step - loss: 0.1659 - accuracy: 0.9297\n",
      "Epoch 325/1500\n",
      "32/32 [==============================] - 0s 710us/step - loss: 0.1471 - accuracy: 0.9429\n",
      "Epoch 326/1500\n",
      "32/32 [==============================] - 0s 707us/step - loss: 0.1501 - accuracy: 0.9355\n",
      "Epoch 327/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.1540 - accuracy: 0.9380\n",
      "Epoch 328/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.1545 - accuracy: 0.9409\n",
      "Epoch 329/1500\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.1639 - accuracy: 0.9346\n",
      "Epoch 330/1500\n",
      "32/32 [==============================] - 0s 696us/step - loss: 0.1654 - accuracy: 0.9370\n",
      "Epoch 331/1500\n",
      "32/32 [==============================] - 0s 738us/step - loss: 0.1517 - accuracy: 0.9390\n",
      "Epoch 332/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.1509 - accuracy: 0.9448\n",
      "Epoch 333/1500\n",
      "32/32 [==============================] - 0s 699us/step - loss: 0.1540 - accuracy: 0.9404\n",
      "Epoch 334/1500\n",
      "32/32 [==============================] - 0s 693us/step - loss: 0.1603 - accuracy: 0.9380\n",
      "Epoch 335/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.1391 - accuracy: 0.9512\n",
      "Epoch 336/1500\n",
      "32/32 [==============================] - 0s 724us/step - loss: 0.1517 - accuracy: 0.9414\n",
      "Epoch 337/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.1595 - accuracy: 0.9395\n",
      "Epoch 338/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.1623 - accuracy: 0.9385\n",
      "Epoch 339/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.1417 - accuracy: 0.9463\n",
      "Epoch 340/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1648 - accuracy: 0.9331\n",
      "Epoch 341/1500\n",
      "32/32 [==============================] - 0s 712us/step - loss: 0.1705 - accuracy: 0.9336\n",
      "Epoch 342/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1493 - accuracy: 0.9463\n",
      "Epoch 343/1500\n",
      "32/32 [==============================] - 0s 685us/step - loss: 0.1674 - accuracy: 0.9429\n",
      "Epoch 344/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.1455 - accuracy: 0.9443\n",
      "Epoch 345/1500\n",
      "32/32 [==============================] - 0s 717us/step - loss: 0.1479 - accuracy: 0.9434\n",
      "Epoch 346/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.1546 - accuracy: 0.9424\n",
      "Epoch 347/1500\n",
      "32/32 [==============================] - 0s 704us/step - loss: 0.1443 - accuracy: 0.9473\n",
      "Epoch 348/1500\n",
      "32/32 [==============================] - 0s 742us/step - loss: 0.1571 - accuracy: 0.9399\n",
      "Epoch 349/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.1650 - accuracy: 0.9424\n",
      "Epoch 350/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1478 - accuracy: 0.9443\n",
      "Epoch 351/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.1301 - accuracy: 0.9492\n",
      "Epoch 352/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.1527 - accuracy: 0.9438\n",
      "Epoch 353/1500\n",
      "32/32 [==============================] - 0s 766us/step - loss: 0.1477 - accuracy: 0.9443\n",
      "Epoch 354/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.1496 - accuracy: 0.9399\n",
      "Epoch 355/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.1559 - accuracy: 0.9380\n",
      "Epoch 356/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1354 - accuracy: 0.9492\n",
      "Epoch 357/1500\n",
      "32/32 [==============================] - 0s 890us/step - loss: 0.1584 - accuracy: 0.9365\n",
      "Epoch 358/1500\n",
      "32/32 [==============================] - 0s 754us/step - loss: 0.1543 - accuracy: 0.9370\n",
      "Epoch 359/1500\n",
      "32/32 [==============================] - 0s 748us/step - loss: 0.1496 - accuracy: 0.9473\n",
      "Epoch 360/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.1418 - accuracy: 0.9434\n",
      "Epoch 361/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.1515 - accuracy: 0.9341\n",
      "Epoch 362/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.1428 - accuracy: 0.9497\n",
      "Epoch 363/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.1445 - accuracy: 0.9419\n",
      "Epoch 364/1500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.1390 - accuracy: 0.9497\n",
      "Epoch 365/1500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.1511 - accuracy: 0.9385\n",
      "Epoch 366/1500\n",
      "32/32 [==============================] - 0s 884us/step - loss: 0.1374 - accuracy: 0.9424\n",
      "Epoch 367/1500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.1484 - accuracy: 0.9414\n",
      "Epoch 368/1500\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.1572 - accuracy: 0.9414\n",
      "Epoch 369/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.1328 - accuracy: 0.9502\n",
      "Epoch 370/1500\n",
      "32/32 [==============================] - 0s 796us/step - loss: 0.1459 - accuracy: 0.9453\n",
      "Epoch 371/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.1475 - accuracy: 0.9473\n",
      "Epoch 372/1500\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.1630 - accuracy: 0.9346\n",
      "Epoch 373/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.1532 - accuracy: 0.9438\n",
      "Epoch 374/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.1393 - accuracy: 0.9463\n",
      "Epoch 375/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.1279 - accuracy: 0.9482\n",
      "Epoch 376/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1536 - accuracy: 0.9482\n",
      "Epoch 377/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.1323 - accuracy: 0.9536\n",
      "Epoch 378/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.1515 - accuracy: 0.9375\n",
      "Epoch 379/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.1302 - accuracy: 0.9526\n",
      "Epoch 380/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.1290 - accuracy: 0.9492\n",
      "Epoch 381/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.1461 - accuracy: 0.9448\n",
      "Epoch 382/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.1293 - accuracy: 0.9487\n",
      "Epoch 383/1500\n",
      "32/32 [==============================] - 0s 803us/step - loss: 0.1376 - accuracy: 0.9463\n",
      "Epoch 384/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.1324 - accuracy: 0.9521\n",
      "Epoch 385/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.1457 - accuracy: 0.9448\n",
      "Epoch 386/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.1379 - accuracy: 0.9482\n",
      "Epoch 387/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.1448 - accuracy: 0.9434\n",
      "Epoch 388/1500\n",
      "32/32 [==============================] - 0s 718us/step - loss: 0.1379 - accuracy: 0.9531\n",
      "Epoch 389/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.1385 - accuracy: 0.9487\n",
      "Epoch 390/1500\n",
      "32/32 [==============================] - 0s 733us/step - loss: 0.1317 - accuracy: 0.9482\n",
      "Epoch 391/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.1320 - accuracy: 0.9448\n",
      "Epoch 392/1500\n",
      "32/32 [==============================] - 0s 719us/step - loss: 0.1421 - accuracy: 0.9468\n",
      "Epoch 393/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1427 - accuracy: 0.9478\n",
      "Epoch 394/1500\n",
      "32/32 [==============================] - 0s 745us/step - loss: 0.1292 - accuracy: 0.9482\n",
      "Epoch 395/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.1437 - accuracy: 0.9424\n",
      "Epoch 396/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.1363 - accuracy: 0.9468\n",
      "Epoch 397/1500\n",
      "32/32 [==============================] - 0s 723us/step - loss: 0.1383 - accuracy: 0.9443\n",
      "Epoch 398/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.1414 - accuracy: 0.9448\n",
      "Epoch 399/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1435 - accuracy: 0.9448\n",
      "Epoch 400/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.1503 - accuracy: 0.9453\n",
      "Epoch 401/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.1298 - accuracy: 0.9487\n",
      "Epoch 402/1500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.1392 - accuracy: 0.9453\n",
      "Epoch 403/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.1259 - accuracy: 0.9536\n",
      "Epoch 404/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.1282 - accuracy: 0.9487\n",
      "Epoch 405/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.1411 - accuracy: 0.9487\n",
      "Epoch 406/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1458 - accuracy: 0.9458\n",
      "Epoch 407/1500\n",
      "32/32 [==============================] - 0s 729us/step - loss: 0.1396 - accuracy: 0.9468\n",
      "Epoch 408/1500\n",
      "32/32 [==============================] - 0s 711us/step - loss: 0.1225 - accuracy: 0.9570\n",
      "Epoch 409/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.1341 - accuracy: 0.9463\n",
      "Epoch 410/1500\n",
      "32/32 [==============================] - 0s 746us/step - loss: 0.1367 - accuracy: 0.9526\n",
      "Epoch 411/1500\n",
      "32/32 [==============================] - 0s 731us/step - loss: 0.1279 - accuracy: 0.9507\n",
      "Epoch 412/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.1420 - accuracy: 0.9438\n",
      "Epoch 413/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.1406 - accuracy: 0.9438\n",
      "Epoch 414/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1358 - accuracy: 0.9478\n",
      "Epoch 415/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.1032 - accuracy: 0.9644\n",
      "Epoch 416/1500\n",
      "32/32 [==============================] - 0s 756us/step - loss: 0.1340 - accuracy: 0.9487\n",
      "Epoch 417/1500\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.1439 - accuracy: 0.9414\n",
      "Epoch 418/1500\n",
      "32/32 [==============================] - 0s 793us/step - loss: 0.1271 - accuracy: 0.9551\n",
      "Epoch 419/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.1263 - accuracy: 0.9531\n",
      "Epoch 420/1500\n",
      "32/32 [==============================] - 0s 800us/step - loss: 0.1233 - accuracy: 0.9492\n",
      "Epoch 421/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1375 - accuracy: 0.9478\n",
      "Epoch 422/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1347 - accuracy: 0.9434\n",
      "Epoch 423/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.1551 - accuracy: 0.9443\n",
      "Epoch 424/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.1368 - accuracy: 0.9453\n",
      "Epoch 425/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.1360 - accuracy: 0.9492\n",
      "Epoch 426/1500\n",
      "32/32 [==============================] - 0s 716us/step - loss: 0.1339 - accuracy: 0.9492\n",
      "Epoch 427/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1426 - accuracy: 0.9536\n",
      "Epoch 428/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.1228 - accuracy: 0.9570\n",
      "Epoch 429/1500\n",
      "32/32 [==============================] - 0s 728us/step - loss: 0.1468 - accuracy: 0.9404\n",
      "Epoch 430/1500\n",
      "32/32 [==============================] - 0s 732us/step - loss: 0.1288 - accuracy: 0.9507\n",
      "Epoch 431/1500\n",
      "32/32 [==============================] - 0s 709us/step - loss: 0.1395 - accuracy: 0.9482\n",
      "Epoch 432/1500\n",
      "32/32 [==============================] - 0s 734us/step - loss: 0.1303 - accuracy: 0.9502\n",
      "Epoch 433/1500\n",
      "32/32 [==============================] - 0s 692us/step - loss: 0.1234 - accuracy: 0.9561\n",
      "Epoch 434/1500\n",
      "32/32 [==============================] - 0s 705us/step - loss: 0.1366 - accuracy: 0.9502\n",
      "Epoch 435/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.1283 - accuracy: 0.9478\n",
      "Epoch 436/1500\n",
      "32/32 [==============================] - 0s 725us/step - loss: 0.1239 - accuracy: 0.9546\n",
      "Epoch 437/1500\n",
      "32/32 [==============================] - 0s 721us/step - loss: 0.1245 - accuracy: 0.9546\n",
      "Epoch 438/1500\n",
      "32/32 [==============================] - 0s 701us/step - loss: 0.1150 - accuracy: 0.9580\n",
      "Epoch 439/1500\n",
      "32/32 [==============================] - 0s 722us/step - loss: 0.1246 - accuracy: 0.9512\n",
      "Epoch 440/1500\n",
      "32/32 [==============================] - 0s 701us/step - loss: 0.1167 - accuracy: 0.9634\n",
      "Epoch 441/1500\n",
      "32/32 [==============================] - 0s 708us/step - loss: 0.1252 - accuracy: 0.9565\n",
      "Epoch 442/1500\n",
      "32/32 [==============================] - 0s 706us/step - loss: 0.1175 - accuracy: 0.9497\n",
      "Epoch 443/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.1254 - accuracy: 0.9526\n",
      "Epoch 444/1500\n",
      "32/32 [==============================] - 0s 713us/step - loss: 0.1228 - accuracy: 0.9561\n",
      "Epoch 445/1500\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.1306 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 415.\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.1249 - accuracy: 0.9473\n",
      "Epoch 445: early stopping\n",
      "9/9 [==============================] - 0s 740us/step - loss: 0.9752 - accuracy: 0.6977\n",
      "9/9 [==============================] - 0s 569us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (24/30)\n",
      "Before appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "After appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "Final Test Results - Loss: 0.9751850366592407, Accuracy: 0.6976743936538696, Precision: 0.7169987546699875, Recall: 0.6792439552631399, F1 Score: 0.694951664876477\n",
      "Confusion Matrix:\n",
      " [[105   3  31]\n",
      " [ 10  41   3]\n",
      " [ 31   0  34]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "097B    14\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "068A    11\n",
      "025A    11\n",
      "063A    11\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "005A    10\n",
      "072A     9\n",
      "065A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "099A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "062A     4\n",
      "009A     4\n",
      "012A     3\n",
      "058A     3\n",
      "060A     3\n",
      "006A     3\n",
      "056A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "032A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "066A     1\n",
      "026C     1\n",
      "076A     1\n",
      "096A     1\n",
      "041A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "047A    28\n",
      "074A    25\n",
      "000B    19\n",
      "097A    16\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "036A    11\n",
      "016A    10\n",
      "022A     9\n",
      "117A     7\n",
      "031A     7\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "003A     4\n",
      "064A     3\n",
      "113A     3\n",
      "093A     2\n",
      "069A     2\n",
      "073A     1\n",
      "091A     1\n",
      "092A     1\n",
      "048A     1\n",
      "115A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    266\n",
      "F    163\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    89\n",
      "X    75\n",
      "M    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [014B, 111A, 040A, 046A, 042A, 109A, 050A, 043...\n",
      "senior    [057A, 106A, 104A, 055A, 116A, 051B, 054A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [028A, 074A, 022A, 034A, 091A, 002A, 007A, 069...\n",
      "kitten                             [044A, 047A, 048A, 115A]\n",
      "senior     [093A, 097A, 059A, 113A, 117A, 051A, 016A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "Length of X_train_val:\n",
      "702\n",
      "Length of y_train_val:\n",
      "702\n",
      "Length of groups_train_val:\n",
      "702\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 916, 1: 680, 2: 540})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.1177 - accuracy: 0.5070\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.9836 - accuracy: 0.5730\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.8872 - accuracy: 0.6189\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.8825 - accuracy: 0.6259\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 995us/step - loss: 0.8262 - accuracy: 0.6428\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.8024 - accuracy: 0.6512\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.7647 - accuracy: 0.6709\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.7216 - accuracy: 0.6868\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.7371 - accuracy: 0.6854\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 922us/step - loss: 0.7052 - accuracy: 0.6845\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.6991 - accuracy: 0.6934\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 954us/step - loss: 0.6908 - accuracy: 0.6919\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.7016 - accuracy: 0.7022\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.6660 - accuracy: 0.7083\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.6443 - accuracy: 0.7228\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.6432 - accuracy: 0.7182\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.6385 - accuracy: 0.7322\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.6159 - accuracy: 0.7191\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.6212 - accuracy: 0.7360\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.6224 - accuracy: 0.7257\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.6039 - accuracy: 0.7336\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.5997 - accuracy: 0.7360\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.5813 - accuracy: 0.7467\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.5966 - accuracy: 0.7444\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.5795 - accuracy: 0.7519\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.5781 - accuracy: 0.7519\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.5517 - accuracy: 0.7650\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.5354 - accuracy: 0.7664\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.5655 - accuracy: 0.7430\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.5525 - accuracy: 0.7669\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.5208 - accuracy: 0.7772\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.5238 - accuracy: 0.7814\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.5133 - accuracy: 0.7781\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.5137 - accuracy: 0.7711\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.5375 - accuracy: 0.7575\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.5117 - accuracy: 0.7706\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.5096 - accuracy: 0.7772\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.4962 - accuracy: 0.7860\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.5104 - accuracy: 0.7743\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.5048 - accuracy: 0.7720\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.4947 - accuracy: 0.7870\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.5028 - accuracy: 0.7832\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.4956 - accuracy: 0.7814\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.4863 - accuracy: 0.7832\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.4870 - accuracy: 0.7870\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.4878 - accuracy: 0.7884\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.4806 - accuracy: 0.7870\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.4669 - accuracy: 0.8010\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.4692 - accuracy: 0.8001\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.4675 - accuracy: 0.7954\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.4647 - accuracy: 0.7935\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.4545 - accuracy: 0.7992\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.4682 - accuracy: 0.7903\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.4730 - accuracy: 0.7968\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.4438 - accuracy: 0.8020\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.4696 - accuracy: 0.8006\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.4329 - accuracy: 0.8057\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.4484 - accuracy: 0.8071\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.4630 - accuracy: 0.8066\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.4234 - accuracy: 0.8202\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.4287 - accuracy: 0.8230\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.4289 - accuracy: 0.8179\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.4450 - accuracy: 0.8048\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.4122 - accuracy: 0.8169\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.4054 - accuracy: 0.8268\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.4304 - accuracy: 0.8179\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.4399 - accuracy: 0.8048\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.4157 - accuracy: 0.8160\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.4158 - accuracy: 0.8151\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.4097 - accuracy: 0.8235\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.4184 - accuracy: 0.8230\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.4177 - accuracy: 0.8207\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.4222 - accuracy: 0.8141\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.4199 - accuracy: 0.8202\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.4023 - accuracy: 0.8333\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.3956 - accuracy: 0.8291\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.4059 - accuracy: 0.8282\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.3852 - accuracy: 0.8357\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.3946 - accuracy: 0.8263\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3829 - accuracy: 0.8352\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.4084 - accuracy: 0.8268\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3920 - accuracy: 0.8329\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3881 - accuracy: 0.8272\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3875 - accuracy: 0.8385\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.3970 - accuracy: 0.8347\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.3798 - accuracy: 0.8324\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.3809 - accuracy: 0.8333\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.4130 - accuracy: 0.8230\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.3785 - accuracy: 0.8422\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.3761 - accuracy: 0.8361\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.3787 - accuracy: 0.8385\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.3710 - accuracy: 0.8488\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.3682 - accuracy: 0.8413\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.3695 - accuracy: 0.8282\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.3666 - accuracy: 0.8460\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.3666 - accuracy: 0.8404\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.3819 - accuracy: 0.8296\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.3793 - accuracy: 0.8394\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3672 - accuracy: 0.8432\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.3474 - accuracy: 0.8511\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.3611 - accuracy: 0.8474\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.3732 - accuracy: 0.8343\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.3502 - accuracy: 0.8581\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.3680 - accuracy: 0.8469\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3493 - accuracy: 0.8436\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.3589 - accuracy: 0.8474\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3478 - accuracy: 0.8460\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.3604 - accuracy: 0.8464\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.3669 - accuracy: 0.8455\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.3592 - accuracy: 0.8390\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.3581 - accuracy: 0.8366\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.3482 - accuracy: 0.8577\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.3561 - accuracy: 0.8511\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.3484 - accuracy: 0.8563\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3390 - accuracy: 0.8586\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.3490 - accuracy: 0.8511\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 738us/step - loss: 0.3569 - accuracy: 0.8460\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.3620 - accuracy: 0.8441\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.3402 - accuracy: 0.8600\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.3481 - accuracy: 0.8446\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.3397 - accuracy: 0.8553\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3458 - accuracy: 0.8525\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.3243 - accuracy: 0.8647\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3409 - accuracy: 0.8535\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.3299 - accuracy: 0.8619\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.3133 - accuracy: 0.8652\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.3305 - accuracy: 0.8535\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.3451 - accuracy: 0.8497\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.3212 - accuracy: 0.8680\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.3344 - accuracy: 0.8614\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.3266 - accuracy: 0.8567\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.3199 - accuracy: 0.8600\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.3258 - accuracy: 0.8628\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.3195 - accuracy: 0.8680\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.3297 - accuracy: 0.8633\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.3181 - accuracy: 0.8596\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.3196 - accuracy: 0.8680\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.3107 - accuracy: 0.8713\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.3142 - accuracy: 0.8647\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.3056 - accuracy: 0.8755\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.3072 - accuracy: 0.8764\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3094 - accuracy: 0.8727\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.3218 - accuracy: 0.8647\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.3071 - accuracy: 0.8764\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3153 - accuracy: 0.8666\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.3083 - accuracy: 0.8661\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.3344 - accuracy: 0.8633\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.3132 - accuracy: 0.8684\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.3111 - accuracy: 0.8708\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.3053 - accuracy: 0.8684\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.3227 - accuracy: 0.8628\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.3200 - accuracy: 0.8596\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.3158 - accuracy: 0.8675\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.3044 - accuracy: 0.8722\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.3188 - accuracy: 0.8624\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.3135 - accuracy: 0.8736\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2956 - accuracy: 0.8722\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.3018 - accuracy: 0.8787\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.2949 - accuracy: 0.8769\n",
      "Epoch 160/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.2817 - accuracy: 0.8438"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[3]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0522ad-89f9-479c-b073-c5e720fc6221",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecac477e-680c-4a82-abcb-2b3a2dbfbea4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce33a298-d7a3-40ec-8f78-00c109118230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d083a1-d61a-4fce-9b56-667a9930bfe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431e9ecf-be90-466a-a1b7-dd98f7fb2a9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e4a5c0-56dd-4771-820f-bcecaf391f0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2e5ff6-b761-4b71-b7cd-9eb24aae0ebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff925f9d-efd5-46a0-89a0-1fb3da66a46a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f08817-c78c-4654-b195-911f2a9ccb2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6f284-fe35-4071-8054-5e5a086b4ed9",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4394c6ec-6da8-42de-aaaa-bc1b17bbf74b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d541c77-590d-4b79-b526-85520ea5c4a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c98d75-f483-4819-b292-975bd229cfd2",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7325b11-ab06-45bd-8e77-d498737b7552",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777ae0eb-dab5-47c0-86a7-0d3c6b01b9c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40270db-853d-46a1-acb4-8dab8a4f9c81",
   "metadata": {},
   "source": [
    "# RANDOM SEED 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588e6cfb-6ee5-4b51-a9ee-79dcbfbb208c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[4]))\n",
    "np.random.seed(int(random_seeds[4]))\n",
    "tf.random.set_seed(int(random_seeds[4]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a9515-e7f1-482c-bdaf-73cb07a9132e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8231e441-4472-4d9b-8ddf-dc285c07923b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9ed6b-6d2f-491e-95d8-73d33edeef6a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd63258-8a3b-4bc0-9ac6-ae8aa4b515b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[4]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90d80b-198d-4293-a1a0-73a65f6588d0",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd5399-64d4-435b-9e73-cb31f16d042d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9869f0-e23f-4453-a329-395fa44f1208",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f81185-7b51-497f-98cb-80c4b8f35388",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd2ca0-3dea-4b42-bfad-93138cb1ae30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ea4fe7-6ee1-4185-a009-b864d9aa6b85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03366c5-a807-4159-b0c1-8dc88c04d91e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae8f86b-0e19-49df-a07c-f64383bce76b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6350c1b2-050d-4bf3-98f1-5a80b3078d3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ea2c7-0827-4ffa-9777-ad2891e5f49b",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fbe612-c62e-4abe-8ec3-d75940a993cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e84f5a-909e-47ca-93ca-2e92abf7ded1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ac46e-4cc6-4237-925c-524d47e83b46",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1743599c-0d3f-4b10-a095-02c36218c679",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8546267-aa7a-4e4b-b60e-810f836ebf8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613636d-22ae-4629-ac86-daf482925194",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
