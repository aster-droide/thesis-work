{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17da2043-9a5b-40fe-b3cb-f755b9a98deb",
   "metadata": {},
   "source": [
    "# VGGIsh Multi Seed Validation\n",
    "#### 5 Random (but reproducible) seeds are selected for 5 different runs of train/test\n",
    "#### Models have been optimised and verified on validation sets thoroughly, running a final train/test evaluation here\n",
    "#### The setup:\n",
    "\n",
    "- 4 fold StratifiedGroupKFold for stratification and ensuring each cat_id group only appears in one set at a time\n",
    "- Final scores averaged over the 4 folds\n",
    "- For each seed run we will explore the cat_id predictions through majority voting\n",
    "- For each run we will explore the potential impact of gender\n",
    "\n",
    "The dataset is highly unbalanced, resources for an unbiased estimate have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "ac1e09ae-387c-4347-b6f5-8d09dd1bf3f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, SGD, RMSprop, AdamW\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import shap\n",
    "from keras.regularizers import l1, l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "d005cfd2-3eda-44c4-bbb5-af343e979bc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [7270  860 5390 5191 5734]\n"
     ]
    }
   ],
   "source": [
    "# Set an initial seed for reproducibility\n",
    "np.random.seed(42)  \n",
    "\n",
    "# Generate a list of 5 random seeds\n",
    "random_seeds = np.random.randint(0, 10000, size=5)\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae122a-9f9c-47a3-8835-2d46d2f8e2f9",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "d5097e68-5153-440c-9294-dfa9e6061652",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_initial_group_split(groups_train, groups_test):\n",
    "    \"\"\"\n",
    "    Check if any group is present in both the train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - groups_train: Array of group identifiers for the train set\n",
    "    - groups_test: Array of group identifiers for the test set\n",
    "\n",
    "    Returns:\n",
    "    - Prints out any groups found in both sets and the count of such groups\n",
    "    \"\"\"\n",
    "    train_groups = set(groups_train)\n",
    "    test_groups = set(groups_test)\n",
    "    common_groups = train_groups.intersection(test_groups)\n",
    "\n",
    "    if common_groups:\n",
    "        print(f\"Warning: Found {len(common_groups)} common groups in both train/validation and test sets: {common_groups}\")\n",
    "    else:\n",
    "        print(\"No common groups found between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "88dfe2de-bb1e-4d49-9260-e056c39627bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to perform the swaps based on cat_id, ensuring swaps within the same age_group\n",
    "def swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids):\n",
    "    for cat_id in specific_cat_ids:\n",
    "        # Check if the specific cat_id is not in the training set\n",
    "        if cat_id not in dataframe.iloc[train_val_idx]['cat_id'].values:\n",
    "            # Get the age_group of this cat_id\n",
    "            age_group = dataframe[dataframe['cat_id'] == cat_id]['age_group'].iloc[0]\n",
    "                \n",
    "            # Find a different cat_id within the same age_group in the train set that is not in the test set\n",
    "            other_cat_ids_in_age_group = dataframe[(dataframe['age_group'] == age_group) & \n",
    "                                                   (dataframe['cat_id'] != cat_id) &\n",
    "                                                   (~dataframe['cat_id'].isin(dataframe.iloc[test_idx]['cat_id']))]['cat_id'].unique()\n",
    "            \n",
    "            # Choose one other cat_id for swapping\n",
    "            if len(other_cat_ids_in_age_group) > 0:\n",
    "                other_cat_id = np.random.choice(other_cat_ids_in_age_group)\n",
    "\n",
    "                # Find all instances of the other_cat_id in the train set\n",
    "                other_cat_id_train_val_indices = train_val_idx[dataframe.iloc[train_val_idx]['cat_id'] == other_cat_id]\n",
    "                \n",
    "                # Find all instances of the specific cat_id in the test set\n",
    "                cat_id_test_indices = test_idx[dataframe.iloc[test_idx]['cat_id'] == cat_id]\n",
    "                \n",
    "                # Swap the indices\n",
    "                train_val_idx = np.setdiff1d(train_val_idx, other_cat_id_train_val_indices, assume_unique=True)\n",
    "                test_idx = np.setdiff1d(test_idx, cat_id_test_indices, assume_unique=True)\n",
    "\n",
    "                train_val_idx = np.concatenate((train_val_idx, cat_id_test_indices))\n",
    "                test_idx = np.concatenate((test_idx, other_cat_id_train_val_indices))\n",
    "            else:\n",
    "                print(f\"No alternative cat_id found in the same age_group as {cat_id} for swapping.\")\n",
    "                \n",
    "    return train_val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "2e6c2d87-cfab-493e-bca8-b3b8976a2bda",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify differences in groups\n",
    "def find_group_differences(original, new):\n",
    "    # Convert numpy arrays to sets for easy difference computation\n",
    "    original_set = set(original)\n",
    "    new_set = set(new)\n",
    "    # Find differences\n",
    "    moved_to_new = new_set - original_set\n",
    "    moved_to_original = original_set - new_set\n",
    "    return moved_to_new, moved_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "6164b1c3-75dd-4549-aafd-cb6106297941",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create custom logger function for local logs & stored in a .txt\n",
    "def logger(message, file=None):\n",
    "    print(message)\n",
    "    if file is not None:\n",
    "        with open(file, \"a\") as log_file:\n",
    "            log_file.write(message + \"\\n\")\n",
    "\n",
    "log_file_path = \"multi-seed-val-D13.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "899fb535-0e25-4c7b-b6a5-13231e26c502",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "# Define a custom color palette\n",
    "colors = [\"#6aabd1\", \"#b6e2d3\", \"#dac292\"] \n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "# Function to create bar plots with enhanced style\n",
    "def styled_barplot(data, x, y, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bar_plot = sns.barplot(x=x, y=y, data=data, errorbar=None, width=0.5)  \n",
    "    plt.title(title, fontsize=16, fontweight='bold', color=\"#333333\")\n",
    "    plt.xlabel(xlabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.ylabel(ylabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.xticks(fontsize=12, color=\"#333333\")\n",
    "    plt.yticks(fontsize=12, color=\"#333333\")\n",
    "    plt.ylim(0, 100) \n",
    "\n",
    "    # Adding value labels on top of each bar\n",
    "    for p in bar_plot.patches:\n",
    "        height = p.get_height()\n",
    "        # Annotate the height value on the bar\n",
    "        bar_plot.annotate(f'{height:.1f}', \n",
    "                          (p.get_x() + p.get_width() / 2., height), \n",
    "                          ha='center', va='center', \n",
    "                          xytext=(0, 9), \n",
    "                          textcoords='offset points', fontsize=12, color=\"#333333\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936a24a-13bd-4bc3-be13-0b210a09b5da",
   "metadata": {},
   "source": [
    "# RANDOM SEED 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70136a37-0507-4c2e-8307-c2dd905432e8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "14bb802b-f4bd-4464-a5fd-1977438428b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[0])) \n",
    "np.random.seed(int(random_seeds[0]))\n",
    "tf.random.set_seed(int(random_seeds[0]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_13.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "6ce97c28-4210-4150-a60b-acdbf0e7716c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "c26f17c3-4d21-4537-a090-9ca00f2be51a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91a9c3-3474-4e83-8a5b-c5eb4a9a9223",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "f4605c0b-f99d-48c3-8c2a-d87dd22c8946",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "005A    10\n",
      "071A    10\n",
      "040A    10\n",
      "014B    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "022A     9\n",
      "072A     9\n",
      "095A     8\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "031A     7\n",
      "027A     7\n",
      "050A     7\n",
      "109A     6\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "037A     6\n",
      "075A     5\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "026A     4\n",
      "035A     4\n",
      "105A     4\n",
      "052A     4\n",
      "003A     4\n",
      "062A     4\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "090A     1\n",
      "100A     1\n",
      "115A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "049A     1\n",
      "048A     1\n",
      "066A     1\n",
      "096A     1\n",
      "026C     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "055A    20\n",
      "001A    14\n",
      "025A    11\n",
      "016A    10\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "117A     7\n",
      "008A     6\n",
      "053A     6\n",
      "104A     4\n",
      "009A     4\n",
      "060A     3\n",
      "056A     3\n",
      "058A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "073A     1\n",
      "076A     1\n",
      "092A     1\n",
      "091A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    325\n",
      "M    253\n",
      "F    197\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    84\n",
      "F    55\n",
      "X    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [097A, 057A, 106A, 059A, 113A, 116A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 001A, 103A, 091A, 009A, 025A, 069A, 032...\n",
      "kitten                                         [045A, 110A]\n",
      "senior    [093A, 104A, 055A, 117A, 056A, 058A, 016A, 094...\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 14, 'senior': 13}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 2, 'senior': 9}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "Length of X_train_val:\n",
      "775\n",
      "Length of y_train_val:\n",
      "775\n",
      "Length of groups_train_val:\n",
      "775\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 988, 1: 805, 2: 600})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.1768 - accuracy: 0.5027\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.9201 - accuracy: 0.5942\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.8533 - accuracy: 0.6335\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.7790 - accuracy: 0.6640\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.7624 - accuracy: 0.6812\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.7473 - accuracy: 0.6778\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.7159 - accuracy: 0.7054\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.6762 - accuracy: 0.7238\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 886us/step - loss: 0.6631 - accuracy: 0.7238\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.6703 - accuracy: 0.7066\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.6300 - accuracy: 0.7275\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.6452 - accuracy: 0.7229\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.6344 - accuracy: 0.7296\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.6160 - accuracy: 0.7346\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 944us/step - loss: 0.5967 - accuracy: 0.7447\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 963us/step - loss: 0.5806 - accuracy: 0.7543\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.5822 - accuracy: 0.7589\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 913us/step - loss: 0.5819 - accuracy: 0.7534\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.5790 - accuracy: 0.7514\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.5652 - accuracy: 0.7568\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.5381 - accuracy: 0.7681\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.5448 - accuracy: 0.7651\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.5295 - accuracy: 0.7806\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 936us/step - loss: 0.5223 - accuracy: 0.7814\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 931us/step - loss: 0.5131 - accuracy: 0.7823\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 944us/step - loss: 0.5085 - accuracy: 0.7869\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 917us/step - loss: 0.5174 - accuracy: 0.7768\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.5131 - accuracy: 0.7856\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.5001 - accuracy: 0.7852\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.5053 - accuracy: 0.7860\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.4834 - accuracy: 0.7990\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.4933 - accuracy: 0.7831\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.4945 - accuracy: 0.7881\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.4978 - accuracy: 0.7827\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.4669 - accuracy: 0.8074\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 949us/step - loss: 0.4701 - accuracy: 0.7977\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.4691 - accuracy: 0.8044\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.4554 - accuracy: 0.7998\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.4789 - accuracy: 0.8019\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 958us/step - loss: 0.4647 - accuracy: 0.8023\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4633 - accuracy: 0.7986\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 976us/step - loss: 0.4518 - accuracy: 0.8136\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4479 - accuracy: 0.8078\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4502 - accuracy: 0.8078\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4494 - accuracy: 0.8065\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 1000us/step - loss: 0.4462 - accuracy: 0.8078\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 979us/step - loss: 0.4502 - accuracy: 0.8053\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 987us/step - loss: 0.4186 - accuracy: 0.8249\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.4212 - accuracy: 0.8274\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.4284 - accuracy: 0.8207\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.4236 - accuracy: 0.8266\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.4168 - accuracy: 0.8228\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.8270\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.4264 - accuracy: 0.8203\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.4192 - accuracy: 0.8207\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.4278 - accuracy: 0.8257\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.4159 - accuracy: 0.8207\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.4117 - accuracy: 0.8366\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.4052 - accuracy: 0.8274\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3976 - accuracy: 0.8383\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.3967 - accuracy: 0.8295\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.3972 - accuracy: 0.8333\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.3831 - accuracy: 0.8374\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.4023 - accuracy: 0.8278\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.3973 - accuracy: 0.8333\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.3898 - accuracy: 0.8320\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.3845 - accuracy: 0.8383\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.3992 - accuracy: 0.8237\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.3811 - accuracy: 0.8429\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.3724 - accuracy: 0.8458\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.3890 - accuracy: 0.8379\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3741 - accuracy: 0.8416\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 958us/step - loss: 0.3806 - accuracy: 0.8366\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 963us/step - loss: 0.3724 - accuracy: 0.8445\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 944us/step - loss: 0.3584 - accuracy: 0.8571\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3662 - accuracy: 0.8429\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.3666 - accuracy: 0.8479\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.3673 - accuracy: 0.8433\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.3619 - accuracy: 0.8479\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.3551 - accuracy: 0.8554\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.3523 - accuracy: 0.8525\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3623 - accuracy: 0.8504\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.3534 - accuracy: 0.8496\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.3778 - accuracy: 0.8416\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3517 - accuracy: 0.8500\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.3510 - accuracy: 0.8546\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3563 - accuracy: 0.8462\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3456 - accuracy: 0.8558\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 974us/step - loss: 0.3452 - accuracy: 0.8533\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 979us/step - loss: 0.3552 - accuracy: 0.8487\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.3449 - accuracy: 0.8650\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 962us/step - loss: 0.3491 - accuracy: 0.8558\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.3461 - accuracy: 0.8546\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.3344 - accuracy: 0.8613\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 835us/step - loss: 0.3352 - accuracy: 0.8684\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3324 - accuracy: 0.8692\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.3386 - accuracy: 0.8613\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3223 - accuracy: 0.8634\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.3289 - accuracy: 0.8663\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.3269 - accuracy: 0.8675\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.3277 - accuracy: 0.8575\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.3282 - accuracy: 0.8596\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.3221 - accuracy: 0.8642\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.3319 - accuracy: 0.8642\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.3183 - accuracy: 0.8642\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.3173 - accuracy: 0.8621\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.3184 - accuracy: 0.8621\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.3132 - accuracy: 0.8675\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3083 - accuracy: 0.8763\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3253 - accuracy: 0.8546\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 974us/step - loss: 0.3135 - accuracy: 0.8709\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.3249 - accuracy: 0.8596\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.3086 - accuracy: 0.8688\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.3017 - accuracy: 0.8734\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.3046 - accuracy: 0.8767\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.3201 - accuracy: 0.8646\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.3052 - accuracy: 0.8730\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2956 - accuracy: 0.8700\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.3084 - accuracy: 0.8696\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2977 - accuracy: 0.8792\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.3163 - accuracy: 0.8700\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.3115 - accuracy: 0.8717\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.3013 - accuracy: 0.8746\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3137 - accuracy: 0.8709\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2892 - accuracy: 0.8734\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.3016 - accuracy: 0.8705\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2949 - accuracy: 0.8730\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2871 - accuracy: 0.8834\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2939 - accuracy: 0.8792\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.3048 - accuracy: 0.8684\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2954 - accuracy: 0.8809\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.2918 - accuracy: 0.8738\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.3026 - accuracy: 0.8776\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 957us/step - loss: 0.2789 - accuracy: 0.8918\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.2914 - accuracy: 0.8809\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 988us/step - loss: 0.2731 - accuracy: 0.8893\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 931us/step - loss: 0.2841 - accuracy: 0.8809\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 970us/step - loss: 0.2941 - accuracy: 0.8851\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.2769 - accuracy: 0.8859\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 968us/step - loss: 0.2720 - accuracy: 0.8822\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 981us/step - loss: 0.2697 - accuracy: 0.8884\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 936us/step - loss: 0.2658 - accuracy: 0.9014\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 989us/step - loss: 0.2822 - accuracy: 0.8897\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2790 - accuracy: 0.8788\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2758 - accuracy: 0.8884\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2862 - accuracy: 0.8771\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 934us/step - loss: 0.2868 - accuracy: 0.8809\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.2793 - accuracy: 0.8872\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2572 - accuracy: 0.8897\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.2728 - accuracy: 0.8918\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 990us/step - loss: 0.2854 - accuracy: 0.8805\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2718 - accuracy: 0.8901\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2609 - accuracy: 0.8951\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.2678 - accuracy: 0.8893\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2674 - accuracy: 0.8901\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2611 - accuracy: 0.8972\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 979us/step - loss: 0.2886 - accuracy: 0.8809\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.2643 - accuracy: 0.8918\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.2590 - accuracy: 0.8955\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 959us/step - loss: 0.2607 - accuracy: 0.8922\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.2674 - accuracy: 0.8884\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 983us/step - loss: 0.2691 - accuracy: 0.8913\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.2592 - accuracy: 0.8939\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.2637 - accuracy: 0.8909\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2629 - accuracy: 0.8964\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2731 - accuracy: 0.8955\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.2553 - accuracy: 0.8976\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 965us/step - loss: 0.2598 - accuracy: 0.8955\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2598 - accuracy: 0.8989\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 998us/step - loss: 0.2427 - accuracy: 0.9072\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2408 - accuracy: 0.9001\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2436 - accuracy: 0.9026\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2483 - accuracy: 0.8980\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2517 - accuracy: 0.9010\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2364 - accuracy: 0.9043\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 910us/step - loss: 0.2537 - accuracy: 0.8993\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 966us/step - loss: 0.2523 - accuracy: 0.8955\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 967us/step - loss: 0.2474 - accuracy: 0.9056\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 0.9026\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 990us/step - loss: 0.2405 - accuracy: 0.8997\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 978us/step - loss: 0.2379 - accuracy: 0.9035\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2394 - accuracy: 0.9047\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2300 - accuracy: 0.9076\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.2345 - accuracy: 0.9122\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.2309 - accuracy: 0.9152\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.2357 - accuracy: 0.8989\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2390 - accuracy: 0.9068\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2431 - accuracy: 0.9026\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.2375 - accuracy: 0.9014\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2329 - accuracy: 0.9089\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.2193 - accuracy: 0.9152\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 957us/step - loss: 0.2346 - accuracy: 0.9039\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 986us/step - loss: 0.2205 - accuracy: 0.9102\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 913us/step - loss: 0.2312 - accuracy: 0.9081\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 972us/step - loss: 0.2218 - accuracy: 0.9081\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2154 - accuracy: 0.9206\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.2349 - accuracy: 0.9064\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 964us/step - loss: 0.2327 - accuracy: 0.9081\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 959us/step - loss: 0.2361 - accuracy: 0.9010\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 976us/step - loss: 0.2396 - accuracy: 0.9127\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 954us/step - loss: 0.2299 - accuracy: 0.9106\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.2336 - accuracy: 0.9085\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2245 - accuracy: 0.9127\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.2203 - accuracy: 0.9081\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2337 - accuracy: 0.9026\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2267 - accuracy: 0.9085\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2116 - accuracy: 0.9177\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2267 - accuracy: 0.9102\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.2386 - accuracy: 0.9031\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.2213 - accuracy: 0.9093\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 936us/step - loss: 0.2255 - accuracy: 0.9122\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2291 - accuracy: 0.9110\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2163 - accuracy: 0.9131\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2005 - accuracy: 0.9239\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2283 - accuracy: 0.9047\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.2227 - accuracy: 0.9122\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.2176 - accuracy: 0.9156\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2283 - accuracy: 0.9047\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2056 - accuracy: 0.9214\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2152 - accuracy: 0.9193\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2093 - accuracy: 0.9168\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2189 - accuracy: 0.9085\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.2197 - accuracy: 0.9135\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2173 - accuracy: 0.9131\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.2151 - accuracy: 0.9152\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.2073 - accuracy: 0.9198\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2170 - accuracy: 0.9131\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2121 - accuracy: 0.9139\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2121 - accuracy: 0.9173\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2164 - accuracy: 0.9085\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2237 - accuracy: 0.9160\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 957us/step - loss: 0.2013 - accuracy: 0.9227\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.1983 - accuracy: 0.9269\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2156 - accuracy: 0.9110\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.2150 - accuracy: 0.9173\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.2014 - accuracy: 0.9168\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 963us/step - loss: 0.2121 - accuracy: 0.9114\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 944us/step - loss: 0.2046 - accuracy: 0.9235\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2114 - accuracy: 0.9173\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 931us/step - loss: 0.2103 - accuracy: 0.9198\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.1971 - accuracy: 0.9210\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.2203 - accuracy: 0.9102\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.1868 - accuracy: 0.9277\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2099 - accuracy: 0.9198\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2097 - accuracy: 0.9127\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2090 - accuracy: 0.9269\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.2069 - accuracy: 0.9227\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.2039 - accuracy: 0.9189\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.1980 - accuracy: 0.9290\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 954us/step - loss: 0.2057 - accuracy: 0.9177\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 954us/step - loss: 0.2029 - accuracy: 0.9219\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1900 - accuracy: 0.9244\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 966us/step - loss: 0.1914 - accuracy: 0.9214\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1869 - accuracy: 0.9265\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 861us/step - loss: 0.2006 - accuracy: 0.9164\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.2068 - accuracy: 0.9131\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.1987 - accuracy: 0.9235\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1981 - accuracy: 0.9235\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2026 - accuracy: 0.9206\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2108 - accuracy: 0.9156\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1861 - accuracy: 0.9269\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.1907 - accuracy: 0.9302\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2118 - accuracy: 0.9168\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.1930 - accuracy: 0.9223\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.2059 - accuracy: 0.9265\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.9336\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.1872 - accuracy: 0.9252\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 942us/step - loss: 0.1779 - accuracy: 0.9310\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.2001 - accuracy: 0.9210\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.1872 - accuracy: 0.9256\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 913us/step - loss: 0.1839 - accuracy: 0.9348\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.1980 - accuracy: 0.9198\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.1902 - accuracy: 0.9248\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 947us/step - loss: 0.1868 - accuracy: 0.9290\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.1961 - accuracy: 0.9260\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.1908 - accuracy: 0.9210\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.1793 - accuracy: 0.9294\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1922 - accuracy: 0.9227\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1783 - accuracy: 0.9340\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 969us/step - loss: 0.2051 - accuracy: 0.9139\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.1731 - accuracy: 0.9319\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.1736 - accuracy: 0.9331\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 936us/step - loss: 0.1840 - accuracy: 0.9294\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1806 - accuracy: 0.9302\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.1998 - accuracy: 0.9223\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.1858 - accuracy: 0.9269\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.1883 - accuracy: 0.9269\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.1767 - accuracy: 0.9319\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.1820 - accuracy: 0.9290\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.1838 - accuracy: 0.9336\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 985us/step - loss: 0.1840 - accuracy: 0.9235\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.9256\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1879 - accuracy: 0.9319\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1743 - accuracy: 0.9327\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 962us/step - loss: 0.1928 - accuracy: 0.9248\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.1852 - accuracy: 0.9235\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.1830 - accuracy: 0.9369\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.1773 - accuracy: 0.9294\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.1813 - accuracy: 0.9256\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.1740 - accuracy: 0.9310\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.1707 - accuracy: 0.9348\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.1704 - accuracy: 0.9331\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1831 - accuracy: 0.9290\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.1724 - accuracy: 0.9319\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.1672 - accuracy: 0.9377\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1874 - accuracy: 0.9239\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.1716 - accuracy: 0.9382\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1619 - accuracy: 0.9344\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.1737 - accuracy: 0.9352\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1743 - accuracy: 0.9277\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1631 - accuracy: 0.9377\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.1664 - accuracy: 0.9365\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.1713 - accuracy: 0.9315\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.1761 - accuracy: 0.9294\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.9269\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9252\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.1716 - accuracy: 0.9348\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.9327\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 968us/step - loss: 0.1698 - accuracy: 0.9382\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.1694 - accuracy: 0.9365\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1558 - accuracy: 0.9369\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1830 - accuracy: 0.9256\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1671 - accuracy: 0.9348\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.1648 - accuracy: 0.9386\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1663 - accuracy: 0.9377\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.1709 - accuracy: 0.9344\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 945us/step - loss: 0.1655 - accuracy: 0.9386\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 991us/step - loss: 0.1602 - accuracy: 0.9377\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 978us/step - loss: 0.1671 - accuracy: 0.9398\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 0.1651 - accuracy: 0.9369\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.1633 - accuracy: 0.9369\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 957us/step - loss: 0.1547 - accuracy: 0.9369\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 944us/step - loss: 0.1624 - accuracy: 0.9306\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.1689 - accuracy: 0.9365\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 967us/step - loss: 0.1679 - accuracy: 0.9398\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.1653 - accuracy: 0.9373\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 986us/step - loss: 0.1775 - accuracy: 0.9348\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 987us/step - loss: 0.1643 - accuracy: 0.9377\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 961us/step - loss: 0.1492 - accuracy: 0.9461\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.1645 - accuracy: 0.9323\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.1637 - accuracy: 0.9340\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 956us/step - loss: 0.1675 - accuracy: 0.9390\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.1675 - accuracy: 0.9336\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1583 - accuracy: 0.9373\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.1714 - accuracy: 0.9394\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.1638 - accuracy: 0.9398\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1695 - accuracy: 0.9398\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.1666 - accuracy: 0.9373\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1581 - accuracy: 0.9394\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.1661 - accuracy: 0.9402\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1633 - accuracy: 0.9365\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1716 - accuracy: 0.9340\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.1449 - accuracy: 0.9440\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1589 - accuracy: 0.9440\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.1524 - accuracy: 0.9402\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.1509 - accuracy: 0.9407\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1568 - accuracy: 0.9373\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1482 - accuracy: 0.9432\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9432\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1496 - accuracy: 0.9432\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.9319\n",
      "Epoch 362/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9390\n",
      "Epoch 363/1500\n",
      "38/38 [==============================] - 0s 993us/step - loss: 0.1510 - accuracy: 0.9444\n",
      "Epoch 364/1500\n",
      "38/38 [==============================] - 0s 955us/step - loss: 0.1521 - accuracy: 0.9423\n",
      "Epoch 365/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.1606 - accuracy: 0.9402\n",
      "Epoch 366/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1727 - accuracy: 0.9373\n",
      "Epoch 367/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9323\n",
      "Epoch 368/1500\n",
      "38/38 [==============================] - 0s 968us/step - loss: 0.1662 - accuracy: 0.9348\n",
      "Epoch 369/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9394\n",
      "Epoch 370/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9461\n",
      "Epoch 371/1500\n",
      "38/38 [==============================] - 0s 994us/step - loss: 0.1546 - accuracy: 0.9444\n",
      "Epoch 372/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.1506 - accuracy: 0.9373\n",
      "Epoch 373/1500\n",
      "38/38 [==============================] - 0s 888us/step - loss: 0.1490 - accuracy: 0.9448\n",
      "Epoch 374/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.1534 - accuracy: 0.9419\n",
      "Epoch 375/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.1572 - accuracy: 0.9361\n",
      "Epoch 376/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1539 - accuracy: 0.9427\n",
      "Epoch 377/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.1653 - accuracy: 0.9336\n",
      "Epoch 378/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.1501 - accuracy: 0.9386\n",
      "Epoch 379/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.1603 - accuracy: 0.9386\n",
      "Epoch 380/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.1374 - accuracy: 0.9482\n",
      "Epoch 381/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.1552 - accuracy: 0.9402\n",
      "Epoch 382/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.1549 - accuracy: 0.9369\n",
      "Epoch 383/1500\n",
      "38/38 [==============================] - 0s 726us/step - loss: 0.1504 - accuracy: 0.9415\n",
      "Epoch 384/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.1696 - accuracy: 0.9361\n",
      "Epoch 385/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.1603 - accuracy: 0.9361\n",
      "Epoch 386/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.1430 - accuracy: 0.9440\n",
      "Epoch 387/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.1648 - accuracy: 0.9361\n",
      "Epoch 388/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.1676 - accuracy: 0.9394\n",
      "Epoch 389/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.1557 - accuracy: 0.9365\n",
      "Epoch 390/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1460 - accuracy: 0.9457\n",
      "Epoch 391/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.1581 - accuracy: 0.9356\n",
      "Epoch 392/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.1436 - accuracy: 0.9423\n",
      "Epoch 393/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.1462 - accuracy: 0.9398\n",
      "Epoch 394/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1577 - accuracy: 0.9373\n",
      "Epoch 395/1500\n",
      "38/38 [==============================] - 0s 710us/step - loss: 0.1414 - accuracy: 0.9432\n",
      "Epoch 396/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1523 - accuracy: 0.9432\n",
      "Epoch 397/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.1444 - accuracy: 0.9382\n",
      "Epoch 398/1500\n",
      "38/38 [==============================] - 0s 716us/step - loss: 0.1352 - accuracy: 0.9461\n",
      "Epoch 399/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.1495 - accuracy: 0.9415\n",
      "Epoch 400/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.1398 - accuracy: 0.9494\n",
      "Epoch 401/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1553 - accuracy: 0.9407\n",
      "Epoch 402/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1529 - accuracy: 0.9427\n",
      "Epoch 403/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1500 - accuracy: 0.9398\n",
      "Epoch 404/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.1542 - accuracy: 0.9386\n",
      "Epoch 405/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1437 - accuracy: 0.9453\n",
      "Epoch 406/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1448 - accuracy: 0.9448\n",
      "Epoch 407/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1565 - accuracy: 0.9377\n",
      "Epoch 408/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.9427\n",
      "Epoch 409/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9465\n",
      "Epoch 410/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9448\n",
      "Epoch 411/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9415\n",
      "Epoch 412/1500\n",
      "38/38 [==============================] - 0s 986us/step - loss: 0.1487 - accuracy: 0.9423\n",
      "Epoch 413/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1599 - accuracy: 0.9361\n",
      "Epoch 414/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.1567 - accuracy: 0.9390\n",
      "Epoch 415/1500\n",
      "38/38 [==============================] - 0s 984us/step - loss: 0.1460 - accuracy: 0.9444\n",
      "Epoch 416/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9348\n",
      "Epoch 417/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.9361\n",
      "Epoch 418/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9448\n",
      "Epoch 419/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.1387 - accuracy: 0.9511\n",
      "Epoch 420/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.1285 - accuracy: 0.9519\n",
      "Epoch 421/1500\n",
      "38/38 [==============================] - 0s 942us/step - loss: 0.1397 - accuracy: 0.9440\n",
      "Epoch 422/1500\n",
      "38/38 [==============================] - 0s 994us/step - loss: 0.1410 - accuracy: 0.9432\n",
      "Epoch 423/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.1522 - accuracy: 0.9398\n",
      "Epoch 424/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.1415 - accuracy: 0.9461\n",
      "Epoch 425/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1425 - accuracy: 0.9503\n",
      "Epoch 426/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1406 - accuracy: 0.9524\n",
      "Epoch 427/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.1495 - accuracy: 0.9461\n",
      "Epoch 428/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1511 - accuracy: 0.9457\n",
      "Epoch 429/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9549\n",
      "Epoch 430/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1592 - accuracy: 0.9361\n",
      "Epoch 431/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9478\n",
      "Epoch 432/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.9427\n",
      "Epoch 433/1500\n",
      "38/38 [==============================] - 0s 908us/step - loss: 0.1339 - accuracy: 0.9490\n",
      "Epoch 434/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.1435 - accuracy: 0.9453\n",
      "Epoch 435/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.1382 - accuracy: 0.9486\n",
      "Epoch 436/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.9478\n",
      "Epoch 437/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.1464 - accuracy: 0.9407\n",
      "Epoch 438/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.1279 - accuracy: 0.9524\n",
      "Epoch 439/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.1526 - accuracy: 0.9407\n",
      "Epoch 440/1500\n",
      "38/38 [==============================] - 0s 949us/step - loss: 0.1429 - accuracy: 0.9453\n",
      "Epoch 441/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1549 - accuracy: 0.9415\n",
      "Epoch 442/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.1436 - accuracy: 0.9465\n",
      "Epoch 443/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1513 - accuracy: 0.9398\n",
      "Epoch 444/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.1455 - accuracy: 0.9419\n",
      "Epoch 445/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.1200 - accuracy: 0.9578\n",
      "Epoch 446/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.1338 - accuracy: 0.9465\n",
      "Epoch 447/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.1367 - accuracy: 0.9432\n",
      "Epoch 448/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.1403 - accuracy: 0.9473\n",
      "Epoch 449/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.1405 - accuracy: 0.9469\n",
      "Epoch 450/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.1443 - accuracy: 0.9411\n",
      "Epoch 451/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.1307 - accuracy: 0.9519\n",
      "Epoch 452/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.1449 - accuracy: 0.9440\n",
      "Epoch 453/1500\n",
      "38/38 [==============================] - 0s 899us/step - loss: 0.1447 - accuracy: 0.9432\n",
      "Epoch 454/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.1459 - accuracy: 0.9365\n",
      "Epoch 455/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1338 - accuracy: 0.9478\n",
      "Epoch 456/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.1516 - accuracy: 0.9407\n",
      "Epoch 457/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1268 - accuracy: 0.9524\n",
      "Epoch 458/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.1481 - accuracy: 0.9457\n",
      "Epoch 459/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1230 - accuracy: 0.9570\n",
      "Epoch 460/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.1306 - accuracy: 0.9473\n",
      "Epoch 461/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1489 - accuracy: 0.9432\n",
      "Epoch 462/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1275 - accuracy: 0.9532\n",
      "Epoch 463/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.1357 - accuracy: 0.9511\n",
      "Epoch 464/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.1506 - accuracy: 0.9436\n",
      "Epoch 465/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.1307 - accuracy: 0.9536\n",
      "Epoch 466/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.1457 - accuracy: 0.9427\n",
      "Epoch 467/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.1381 - accuracy: 0.9469\n",
      "Epoch 468/1500\n",
      "38/38 [==============================] - 0s 719us/step - loss: 0.1367 - accuracy: 0.9440\n",
      "Epoch 469/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.1412 - accuracy: 0.9419\n",
      "Epoch 470/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.1388 - accuracy: 0.9515\n",
      "Epoch 471/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1446 - accuracy: 0.9419\n",
      "Epoch 472/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1504 - accuracy: 0.9440\n",
      "Epoch 473/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.1300 - accuracy: 0.9511\n",
      "Epoch 474/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.1392 - accuracy: 0.9499\n",
      "Epoch 475/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.1793 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 445.\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.1362 - accuracy: 0.9511\n",
      "Epoch 475: early stopping\n",
      "6/6 [==============================] - 0s 942us/step - loss: 0.9069 - accuracy: 0.7160\n",
      "6/6 [==============================] - 0s 724us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (18/25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "Final Test Results - Loss: 0.9069304466247559, Accuracy: 0.7160493731498718, Precision: 0.7213664516543115, Recall: 0.7774516996820738, F1 Score: 0.7417825101054915\n",
      "Confusion Matrix:\n",
      " [[75  3 16]\n",
      " [ 0 10  0]\n",
      " [27  0 31]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "016A    10\n",
      "033A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "108A     6\n",
      "007A     6\n",
      "021A     5\n",
      "034A     5\n",
      "025C     5\n",
      "023B     5\n",
      "075A     5\n",
      "052A     4\n",
      "104A     4\n",
      "026A     4\n",
      "035A     4\n",
      "009A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "056A     3\n",
      "058A     3\n",
      "113A     3\n",
      "038A     2\n",
      "069A     2\n",
      "093A     2\n",
      "087A     2\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "032A     2\n",
      "018A     2\n",
      "115A     1\n",
      "110A     1\n",
      "019B     1\n",
      "090A     1\n",
      "004A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "076A     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "057A    27\n",
      "000B    19\n",
      "029A    17\n",
      "106A    14\n",
      "028A    13\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "071A    10\n",
      "005A    10\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "037A     6\n",
      "044A     5\n",
      "070A     5\n",
      "105A     4\n",
      "014A     3\n",
      "054A     2\n",
      "025B     2\n",
      "096A     1\n",
      "049A     1\n",
      "048A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    264\n",
      "X    198\n",
      "F    193\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    150\n",
      "M     73\n",
      "F     59\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 019...\n",
      "kitten    [014B, 111A, 040A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 116A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 028A, 022A, 029A, 095A, 005A, 039A, 013...\n",
      "kitten                             [044A, 046A, 049A, 048A]\n",
      "senior                             [057A, 106A, 051B, 054A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '047A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '046A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'047A'}\n",
      "Moved to Test Set:\n",
      "{'047A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '046A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '047A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "Length of X_train_val:\n",
      "690\n",
      "Length of y_train_val:\n",
      "690\n",
      "Length of groups_train_val:\n",
      "690\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    126\n",
      "kitten    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     70\n",
      "senior     52\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "kitten    136\n",
      "senior    126\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "senior     52\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 856, 1: 680, 2: 630})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.1786 - accuracy: 0.4584\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 948us/step - loss: 0.9763 - accuracy: 0.5693\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.8664 - accuracy: 0.6210\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.8409 - accuracy: 0.6362\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.7550 - accuracy: 0.6814\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.7211 - accuracy: 0.6990\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.6756 - accuracy: 0.7230\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.6883 - accuracy: 0.7041\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.6669 - accuracy: 0.7239\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.6493 - accuracy: 0.7387\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.6222 - accuracy: 0.7465\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.5987 - accuracy: 0.7465\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.5891 - accuracy: 0.7553\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.5988 - accuracy: 0.7590\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.5734 - accuracy: 0.7664\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.5694 - accuracy: 0.7645\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.5629 - accuracy: 0.7650\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.5659 - accuracy: 0.7632\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.5409 - accuracy: 0.7756\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.5208 - accuracy: 0.7812\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.5358 - accuracy: 0.7793\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.5320 - accuracy: 0.7802\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.5271 - accuracy: 0.7927\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.5022 - accuracy: 0.7946\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 733us/step - loss: 0.5216 - accuracy: 0.7835\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.4941 - accuracy: 0.8084\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.5112 - accuracy: 0.7835\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.4806 - accuracy: 0.7918\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.4794 - accuracy: 0.8079\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.4835 - accuracy: 0.8061\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.4709 - accuracy: 0.7932\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.4734 - accuracy: 0.8024\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.4608 - accuracy: 0.8139\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.4522 - accuracy: 0.8135\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.4680 - accuracy: 0.8010\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.4567 - accuracy: 0.8158\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 741us/step - loss: 0.4586 - accuracy: 0.8079\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.4424 - accuracy: 0.8232\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.4339 - accuracy: 0.8283\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.4531 - accuracy: 0.8144\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.4429 - accuracy: 0.8139\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.4190 - accuracy: 0.8246\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 741us/step - loss: 0.4180 - accuracy: 0.8333\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.4273 - accuracy: 0.8218\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.4096 - accuracy: 0.8292\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.4109 - accuracy: 0.8310\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.4232 - accuracy: 0.8204\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.3870 - accuracy: 0.8407\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.3990 - accuracy: 0.8241\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.4074 - accuracy: 0.8264\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.4100 - accuracy: 0.8356\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.3950 - accuracy: 0.8444\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.3855 - accuracy: 0.8513\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 745us/step - loss: 0.3917 - accuracy: 0.8389\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.3869 - accuracy: 0.8463\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.3997 - accuracy: 0.8356\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.3789 - accuracy: 0.8370\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.3697 - accuracy: 0.8560\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.3598 - accuracy: 0.8601\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.3865 - accuracy: 0.8481\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.3665 - accuracy: 0.8536\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.3720 - accuracy: 0.8430\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 710us/step - loss: 0.3609 - accuracy: 0.8486\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.3667 - accuracy: 0.8440\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.3755 - accuracy: 0.8472\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.3692 - accuracy: 0.8495\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.3435 - accuracy: 0.8596\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.3772 - accuracy: 0.8426\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.3424 - accuracy: 0.8721\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.3468 - accuracy: 0.8666\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.3549 - accuracy: 0.8564\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.3382 - accuracy: 0.8629\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.3469 - accuracy: 0.8541\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.3365 - accuracy: 0.8606\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.3453 - accuracy: 0.8518\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.3334 - accuracy: 0.8707\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.3265 - accuracy: 0.8652\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.3267 - accuracy: 0.8684\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.3274 - accuracy: 0.8680\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.3386 - accuracy: 0.8666\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.3070 - accuracy: 0.8846\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.3384 - accuracy: 0.8583\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.3293 - accuracy: 0.8680\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.3096 - accuracy: 0.8809\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.3204 - accuracy: 0.8735\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.3239 - accuracy: 0.8698\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.3077 - accuracy: 0.8749\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.3288 - accuracy: 0.8652\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.3218 - accuracy: 0.8717\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.3104 - accuracy: 0.8777\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.3099 - accuracy: 0.8740\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2915 - accuracy: 0.8850\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 724us/step - loss: 0.3159 - accuracy: 0.8786\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2925 - accuracy: 0.8887\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.3029 - accuracy: 0.8850\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2947 - accuracy: 0.8800\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.3031 - accuracy: 0.8772\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.2987 - accuracy: 0.8804\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2990 - accuracy: 0.8846\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.3092 - accuracy: 0.8726\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.3124 - accuracy: 0.8800\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.2950 - accuracy: 0.8818\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2965 - accuracy: 0.8800\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 743us/step - loss: 0.3009 - accuracy: 0.8850\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.2870 - accuracy: 0.8887\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2837 - accuracy: 0.8860\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2884 - accuracy: 0.8864\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.2951 - accuracy: 0.8841\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2794 - accuracy: 0.8897\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2722 - accuracy: 0.8984\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.2794 - accuracy: 0.8869\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2770 - accuracy: 0.8869\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.2767 - accuracy: 0.8869\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2835 - accuracy: 0.8841\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.2829 - accuracy: 0.8915\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.2818 - accuracy: 0.8873\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2779 - accuracy: 0.8897\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.2662 - accuracy: 0.8961\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.2723 - accuracy: 0.8897\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.2718 - accuracy: 0.8920\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.2852 - accuracy: 0.8832\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.2825 - accuracy: 0.8892\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.2515 - accuracy: 0.9040\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2671 - accuracy: 0.8892\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2659 - accuracy: 0.8887\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.2551 - accuracy: 0.9017\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.2573 - accuracy: 0.8994\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.2637 - accuracy: 0.8957\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.2556 - accuracy: 0.9017\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.2492 - accuracy: 0.8980\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 886us/step - loss: 0.2639 - accuracy: 0.8929\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 948us/step - loss: 0.2480 - accuracy: 0.9021\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.2702 - accuracy: 0.8924\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.2645 - accuracy: 0.8892\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.2650 - accuracy: 0.8947\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2557 - accuracy: 0.8989\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 915us/step - loss: 0.2588 - accuracy: 0.9040\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2472 - accuracy: 0.9058\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.2491 - accuracy: 0.8980\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.2528 - accuracy: 0.9058\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2479 - accuracy: 0.9049\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 948us/step - loss: 0.2439 - accuracy: 0.9030\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.2447 - accuracy: 0.9058\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2461 - accuracy: 0.8998\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2366 - accuracy: 0.9040\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.2404 - accuracy: 0.9077\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2497 - accuracy: 0.9007\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2300 - accuracy: 0.9109\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.2419 - accuracy: 0.9030\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2347 - accuracy: 0.9086\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.2342 - accuracy: 0.9072\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.2189 - accuracy: 0.9114\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.2337 - accuracy: 0.9104\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2470 - accuracy: 0.9090\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2299 - accuracy: 0.9086\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.2433 - accuracy: 0.9040\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2456 - accuracy: 0.9049\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.2341 - accuracy: 0.9118\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.2358 - accuracy: 0.9063\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2335 - accuracy: 0.9132\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2395 - accuracy: 0.9035\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.2389 - accuracy: 0.9086\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.2212 - accuracy: 0.9137\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.2325 - accuracy: 0.9054\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2210 - accuracy: 0.9095\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.2394 - accuracy: 0.9049\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.2247 - accuracy: 0.9123\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.2145 - accuracy: 0.9146\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.2153 - accuracy: 0.9252\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2385 - accuracy: 0.9030\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.2324 - accuracy: 0.9109\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.2076 - accuracy: 0.9206\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.2161 - accuracy: 0.9151\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 728us/step - loss: 0.2323 - accuracy: 0.9081\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.2200 - accuracy: 0.9127\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2202 - accuracy: 0.9100\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2286 - accuracy: 0.9155\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.2164 - accuracy: 0.9178\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2146 - accuracy: 0.9174\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 744us/step - loss: 0.2373 - accuracy: 0.9114\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2196 - accuracy: 0.9137\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.2104 - accuracy: 0.9201\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.2268 - accuracy: 0.9058\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2113 - accuracy: 0.9132\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.2103 - accuracy: 0.9197\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.2262 - accuracy: 0.9063\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2228 - accuracy: 0.9127\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2026 - accuracy: 0.9238\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.2203 - accuracy: 0.9123\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.2182 - accuracy: 0.9183\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.2171 - accuracy: 0.9187\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2126 - accuracy: 0.9141\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2188 - accuracy: 0.9187\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.1923 - accuracy: 0.9243\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 745us/step - loss: 0.2238 - accuracy: 0.9104\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.2191 - accuracy: 0.9049\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2005 - accuracy: 0.9238\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1991 - accuracy: 0.9201\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.1934 - accuracy: 0.9252\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.1957 - accuracy: 0.9201\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.2105 - accuracy: 0.9178\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.2027 - accuracy: 0.9229\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2035 - accuracy: 0.9201\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.2056 - accuracy: 0.9201\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.1882 - accuracy: 0.9271\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.2129 - accuracy: 0.9137\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 731us/step - loss: 0.1956 - accuracy: 0.9238\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.2007 - accuracy: 0.9266\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1956 - accuracy: 0.9215\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.2068 - accuracy: 0.9187\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.2010 - accuracy: 0.9178\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.1962 - accuracy: 0.9238\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.1871 - accuracy: 0.9215\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.1886 - accuracy: 0.9298\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 751us/step - loss: 0.1869 - accuracy: 0.9266\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.2082 - accuracy: 0.9220\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.1920 - accuracy: 0.9215\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1851 - accuracy: 0.9294\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 739us/step - loss: 0.1923 - accuracy: 0.9238\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.1904 - accuracy: 0.9261\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.2058 - accuracy: 0.9206\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.1871 - accuracy: 0.9289\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.1918 - accuracy: 0.9280\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.1876 - accuracy: 0.9307\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.1806 - accuracy: 0.9354\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.1832 - accuracy: 0.9335\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.1937 - accuracy: 0.9141\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.1866 - accuracy: 0.9252\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.2007 - accuracy: 0.9197\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.1925 - accuracy: 0.9234\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.1826 - accuracy: 0.9257\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.1682 - accuracy: 0.9317\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.2005 - accuracy: 0.9192\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.1773 - accuracy: 0.9307\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.9307\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 988us/step - loss: 0.1723 - accuracy: 0.9326\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.1979 - accuracy: 0.9220\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.1946 - accuracy: 0.9224\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.1748 - accuracy: 0.9335\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.1747 - accuracy: 0.9386\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.1982 - accuracy: 0.9169\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1748 - accuracy: 0.9303\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 735us/step - loss: 0.1706 - accuracy: 0.9331\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.1902 - accuracy: 0.9275\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.1882 - accuracy: 0.9298\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.1794 - accuracy: 0.9340\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1616 - accuracy: 0.9386\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.1656 - accuracy: 0.9344\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.1848 - accuracy: 0.9266\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1829 - accuracy: 0.9298\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 918us/step - loss: 0.1840 - accuracy: 0.9298\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.1850 - accuracy: 0.9326\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.1979 - accuracy: 0.9192\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.1691 - accuracy: 0.9312\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1767 - accuracy: 0.9344\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.1726 - accuracy: 0.9372\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1729 - accuracy: 0.9363\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.1651 - accuracy: 0.9391\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.1630 - accuracy: 0.9432\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.1847 - accuracy: 0.9243\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.1693 - accuracy: 0.9331\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.1805 - accuracy: 0.9326\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.1706 - accuracy: 0.9294\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.1729 - accuracy: 0.9294\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.9386\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.9238\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.1765 - accuracy: 0.9349\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1765 - accuracy: 0.9252\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9358\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.1744 - accuracy: 0.9344\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.1626 - accuracy: 0.9354\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.1704 - accuracy: 0.9321\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.1603 - accuracy: 0.9349\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1578 - accuracy: 0.9404\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.1784 - accuracy: 0.9358\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.1649 - accuracy: 0.9367\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.1613 - accuracy: 0.9428\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.1704 - accuracy: 0.9234\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.1693 - accuracy: 0.9358\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.1762 - accuracy: 0.9321\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.1710 - accuracy: 0.9344\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.1565 - accuracy: 0.9423\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1718 - accuracy: 0.9363\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.1759 - accuracy: 0.9372\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1717 - accuracy: 0.9331\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1673 - accuracy: 0.9418\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.1601 - accuracy: 0.9409\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.1515 - accuracy: 0.9474\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.1459 - accuracy: 0.9409\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.1631 - accuracy: 0.9381\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.1556 - accuracy: 0.9404\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1557 - accuracy: 0.9414\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.1661 - accuracy: 0.9354\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1653 - accuracy: 0.9363\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.1608 - accuracy: 0.9418\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.1474 - accuracy: 0.9404\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.1550 - accuracy: 0.9460\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.1597 - accuracy: 0.9363\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.1407 - accuracy: 0.9446\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.1470 - accuracy: 0.9460\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.1482 - accuracy: 0.9506\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.1628 - accuracy: 0.9391\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.1551 - accuracy: 0.9409\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.1480 - accuracy: 0.9404\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.1427 - accuracy: 0.9464\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.1701 - accuracy: 0.9317\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.1426 - accuracy: 0.9492\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.1530 - accuracy: 0.9386\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.1519 - accuracy: 0.9432\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.1527 - accuracy: 0.9455\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.1639 - accuracy: 0.9400\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.1532 - accuracy: 0.9395\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1549 - accuracy: 0.9386\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.1613 - accuracy: 0.9367\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.1582 - accuracy: 0.9404\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.1495 - accuracy: 0.9469\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.1462 - accuracy: 0.9460\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.1590 - accuracy: 0.9386\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 935us/step - loss: 0.1421 - accuracy: 0.9464\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.1448 - accuracy: 0.9418\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.1393 - accuracy: 0.9501\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.1545 - accuracy: 0.9377\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 908us/step - loss: 0.1495 - accuracy: 0.9372\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.1470 - accuracy: 0.9441\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.1547 - accuracy: 0.9391\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1472 - accuracy: 0.9395\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.1590 - accuracy: 0.9418\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.1534 - accuracy: 0.9367\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1284 - accuracy: 0.9506\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 922us/step - loss: 0.1435 - accuracy: 0.9497\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.1568 - accuracy: 0.9437\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.1493 - accuracy: 0.9381\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1409 - accuracy: 0.9478\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9437\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.1564 - accuracy: 0.9335\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.1415 - accuracy: 0.9391\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1336 - accuracy: 0.9474\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.1386 - accuracy: 0.9418\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.1371 - accuracy: 0.9501\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.1531 - accuracy: 0.9437\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1494 - accuracy: 0.9386\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 749us/step - loss: 0.1531 - accuracy: 0.9400\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.1507 - accuracy: 0.9446\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1416 - accuracy: 0.9441\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.1271 - accuracy: 0.9511\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.1411 - accuracy: 0.9441\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.1485 - accuracy: 0.9451\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.1394 - accuracy: 0.9492\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.1503 - accuracy: 0.9404\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.1485 - accuracy: 0.9464\n",
      "Epoch 351/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1446 - accuracy: 0.9446\n",
      "Epoch 352/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.1381 - accuracy: 0.9441\n",
      "Epoch 353/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.1594 - accuracy: 0.9307\n",
      "Epoch 354/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.1366 - accuracy: 0.9464\n",
      "Epoch 355/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1435 - accuracy: 0.9474\n",
      "Epoch 356/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.1433 - accuracy: 0.9400\n",
      "Epoch 357/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.1439 - accuracy: 0.9469\n",
      "Epoch 358/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.1535 - accuracy: 0.9478\n",
      "Epoch 359/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1339 - accuracy: 0.9483\n",
      "Epoch 360/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.1403 - accuracy: 0.9441\n",
      "Epoch 361/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.1341 - accuracy: 0.9520\n",
      "Epoch 362/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.1365 - accuracy: 0.9451\n",
      "Epoch 363/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.1373 - accuracy: 0.9423\n",
      "Epoch 364/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.1264 - accuracy: 0.9566\n",
      "Epoch 365/1500\n",
      "34/34 [==============================] - 0s 908us/step - loss: 0.1242 - accuracy: 0.9566\n",
      "Epoch 366/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.1306 - accuracy: 0.9524\n",
      "Epoch 367/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.1453 - accuracy: 0.9441\n",
      "Epoch 368/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1497 - accuracy: 0.9409\n",
      "Epoch 369/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1280 - accuracy: 0.9534\n",
      "Epoch 370/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.1226 - accuracy: 0.9529\n",
      "Epoch 371/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.1382 - accuracy: 0.9478\n",
      "Epoch 372/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.1374 - accuracy: 0.9437\n",
      "Epoch 373/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.1292 - accuracy: 0.9529\n",
      "Epoch 374/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1298 - accuracy: 0.9501\n",
      "Epoch 375/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.1308 - accuracy: 0.9529\n",
      "Epoch 376/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.1306 - accuracy: 0.9534\n",
      "Epoch 377/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.1384 - accuracy: 0.9506\n",
      "Epoch 378/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.1346 - accuracy: 0.9455\n",
      "Epoch 379/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.1590 - accuracy: 0.9381\n",
      "Epoch 380/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.1311 - accuracy: 0.9451\n",
      "Epoch 381/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.1444 - accuracy: 0.9446\n",
      "Epoch 382/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.1407 - accuracy: 0.9404\n",
      "Epoch 383/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.1310 - accuracy: 0.9515\n",
      "Epoch 384/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.1223 - accuracy: 0.9524\n",
      "Epoch 385/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.1351 - accuracy: 0.9455\n",
      "Epoch 386/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.1347 - accuracy: 0.9488\n",
      "Epoch 387/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.1242 - accuracy: 0.9543\n",
      "Epoch 388/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.1305 - accuracy: 0.9520\n",
      "Epoch 389/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.1279 - accuracy: 0.9492\n",
      "Epoch 390/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.1259 - accuracy: 0.9520\n",
      "Epoch 391/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.1332 - accuracy: 0.9446\n",
      "Epoch 392/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.1304 - accuracy: 0.9506\n",
      "Epoch 393/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1266 - accuracy: 0.9566\n",
      "Epoch 394/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.1394 - accuracy: 0.9428\n",
      "Epoch 395/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.1215 - accuracy: 0.9617\n",
      "Epoch 396/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1460 - accuracy: 0.9441\n",
      "Epoch 397/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1404 - accuracy: 0.9432\n",
      "Epoch 398/1500\n",
      "34/34 [==============================] - 0s 939us/step - loss: 0.1233 - accuracy: 0.9534\n",
      "Epoch 399/1500\n",
      "34/34 [==============================] - 0s 914us/step - loss: 0.1294 - accuracy: 0.9483\n",
      "Epoch 400/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9488\n",
      "Epoch 401/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.1369 - accuracy: 0.9492\n",
      "Epoch 402/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.1390 - accuracy: 0.9483\n",
      "Epoch 403/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.1313 - accuracy: 0.9483\n",
      "Epoch 404/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.1316 - accuracy: 0.9497\n",
      "Epoch 405/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.1286 - accuracy: 0.9492\n",
      "Epoch 406/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.1344 - accuracy: 0.9464\n",
      "Epoch 407/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.1327 - accuracy: 0.9483\n",
      "Epoch 408/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.1407 - accuracy: 0.9451\n",
      "Epoch 409/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.1204 - accuracy: 0.9524\n",
      "Epoch 410/1500\n",
      "34/34 [==============================] - 0s 872us/step - loss: 0.1242 - accuracy: 0.9543\n",
      "Epoch 411/1500\n",
      "34/34 [==============================] - 0s 852us/step - loss: 0.1301 - accuracy: 0.9566\n",
      "Epoch 412/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1270 - accuracy: 0.9501\n",
      "Epoch 413/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.1292 - accuracy: 0.9534\n",
      "Epoch 414/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1191 - accuracy: 0.9598\n",
      "Epoch 415/1500\n",
      "34/34 [==============================] - 0s 885us/step - loss: 0.1237 - accuracy: 0.9589\n",
      "Epoch 416/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1237 - accuracy: 0.9538\n",
      "Epoch 417/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.1447 - accuracy: 0.9432\n",
      "Epoch 418/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.1136 - accuracy: 0.9635\n",
      "Epoch 419/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.1106 - accuracy: 0.9598\n",
      "Epoch 420/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.1469 - accuracy: 0.9428\n",
      "Epoch 421/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.1131 - accuracy: 0.9584\n",
      "Epoch 422/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1224 - accuracy: 0.9552\n",
      "Epoch 423/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.1392 - accuracy: 0.9451\n",
      "Epoch 424/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.1206 - accuracy: 0.9548\n",
      "Epoch 425/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.1103 - accuracy: 0.9575\n",
      "Epoch 426/1500\n",
      "34/34 [==============================] - 0s 940us/step - loss: 0.1211 - accuracy: 0.9575\n",
      "Epoch 427/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.1111 - accuracy: 0.9589\n",
      "Epoch 428/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.1264 - accuracy: 0.9548\n",
      "Epoch 429/1500\n",
      "34/34 [==============================] - 0s 946us/step - loss: 0.1205 - accuracy: 0.9506\n",
      "Epoch 430/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9520\n",
      "Epoch 431/1500\n",
      "34/34 [==============================] - 0s 941us/step - loss: 0.1134 - accuracy: 0.9603\n",
      "Epoch 432/1500\n",
      "34/34 [==============================] - 0s 989us/step - loss: 0.1220 - accuracy: 0.9492\n",
      "Epoch 433/1500\n",
      "34/34 [==============================] - 0s 950us/step - loss: 0.1257 - accuracy: 0.9501\n",
      "Epoch 434/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.1181 - accuracy: 0.9571\n",
      "Epoch 435/1500\n",
      "34/34 [==============================] - 0s 979us/step - loss: 0.1356 - accuracy: 0.9478\n",
      "Epoch 436/1500\n",
      "34/34 [==============================] - 0s 948us/step - loss: 0.1204 - accuracy: 0.9534\n",
      "Epoch 437/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.1240 - accuracy: 0.9511\n",
      "Epoch 438/1500\n",
      "34/34 [==============================] - 0s 958us/step - loss: 0.1265 - accuracy: 0.9506\n",
      "Epoch 439/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9488\n",
      "Epoch 440/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9520\n",
      "Epoch 441/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.1201 - accuracy: 0.9538\n",
      "Epoch 442/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.1289 - accuracy: 0.9501\n",
      "Epoch 443/1500\n",
      "34/34 [==============================] - 0s 872us/step - loss: 0.1134 - accuracy: 0.9598\n",
      "Epoch 444/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9548\n",
      "Epoch 445/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9497\n",
      "Epoch 446/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9524\n",
      "Epoch 447/1500\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.1241 - accuracy: 0.9520\n",
      "Epoch 448/1500\n",
      "34/34 [==============================] - 0s 3ms/step - loss: 0.1171 - accuracy: 0.9571\n",
      "Epoch 449/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.2116 - accuracy: 0.9062Restoring model weights from the end of the best epoch: 419.\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9529\n",
      "Epoch 449: early stopping\n",
      "8/8 [==============================] - 0s 844us/step - loss: 1.0189 - accuracy: 0.6478\n",
      "8/8 [==============================] - 0s 672us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.69 (20/29)\n",
      "Before appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "After appending - Cat IDs: 409, Predictions: 409, Actuals: 409, Gender: 409\n",
      "Final Test Results - Loss: 1.0188928842544556, Accuracy: 0.647773265838623, Precision: 0.6740896947509029, Recall: 0.6522664835164835, F1 Score: 0.651598426784994\n",
      "Confusion Matrix:\n",
      " [[105   3  52]\n",
      " [  8  26   1]\n",
      " [ 23   0  29]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "106A    14\n",
      "042A    14\n",
      "028A    13\n",
      "111A    13\n",
      "039A    12\n",
      "068A    11\n",
      "063A    11\n",
      "036A    11\n",
      "025A    11\n",
      "005A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "051B     9\n",
      "022A     9\n",
      "065A     9\n",
      "045A     9\n",
      "015A     9\n",
      "010A     8\n",
      "095A     8\n",
      "094A     8\n",
      "013B     8\n",
      "031A     7\n",
      "117A     7\n",
      "053A     6\n",
      "108A     6\n",
      "008A     6\n",
      "109A     6\n",
      "007A     6\n",
      "037A     6\n",
      "070A     5\n",
      "021A     5\n",
      "044A     5\n",
      "023B     5\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "062A     4\n",
      "009A     4\n",
      "104A     4\n",
      "058A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "056A     3\n",
      "113A     3\n",
      "025B     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "054A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "032A     2\n",
      "069A     2\n",
      "049A     1\n",
      "088A     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "004A     1\n",
      "092A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "073A     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "059A    14\n",
      "097B    14\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "072A     9\n",
      "033A     9\n",
      "027A     7\n",
      "099A     7\n",
      "050A     7\n",
      "023A     6\n",
      "034A     5\n",
      "025C     5\n",
      "075A     5\n",
      "052A     4\n",
      "003A     4\n",
      "012A     3\n",
      "006A     3\n",
      "018A     2\n",
      "026C     1\n",
      "019B     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    250\n",
      "F    202\n",
      "M    180\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    157\n",
      "X     98\n",
      "F     50\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 103A, 071A, 028A, 062A, 101...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 043...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 097B, 019A, 074A, 067A, 020A, 002...\n",
      "kitten                                   [047A, 050A, 115A]\n",
      "senior                       [097A, 059A, 116A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 52, 'kitten': 13, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 22, 'kitten': 3, 'senior': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "632\n",
      "Length of y_train_val:\n",
      "632\n",
      "Length of groups_train_val:\n",
      "632\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 748, 1: 675, 2: 615})\n",
      "Epoch 1/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1506 - accuracy: 0.4642\n",
      "Epoch 2/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.9305 - accuracy: 0.5819\n",
      "Epoch 3/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.8391 - accuracy: 0.6232\n",
      "Epoch 4/1500\n",
      "32/32 [==============================] - 0s 945us/step - loss: 0.8297 - accuracy: 0.6413\n",
      "Epoch 5/1500\n",
      "32/32 [==============================] - 0s 923us/step - loss: 0.7671 - accuracy: 0.6580\n",
      "Epoch 6/1500\n",
      "32/32 [==============================] - 0s 945us/step - loss: 0.7679 - accuracy: 0.6771\n",
      "Epoch 7/1500\n",
      "32/32 [==============================] - 0s 889us/step - loss: 0.7204 - accuracy: 0.6801\n",
      "Epoch 8/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.7305 - accuracy: 0.6732\n",
      "Epoch 9/1500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.6954 - accuracy: 0.6914\n",
      "Epoch 10/1500\n",
      "32/32 [==============================] - 0s 811us/step - loss: 0.6749 - accuracy: 0.7012\n",
      "Epoch 11/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.6711 - accuracy: 0.6938\n",
      "Epoch 12/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.6499 - accuracy: 0.7144\n",
      "Epoch 13/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.6452 - accuracy: 0.7134\n",
      "Epoch 14/1500\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.6498 - accuracy: 0.7090\n",
      "Epoch 15/1500\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.6281 - accuracy: 0.7203\n",
      "Epoch 16/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.6021 - accuracy: 0.7390\n",
      "Epoch 17/1500\n",
      "32/32 [==============================] - 0s 766us/step - loss: 0.6279 - accuracy: 0.7213\n",
      "Epoch 18/1500\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.6131 - accuracy: 0.7306\n",
      "Epoch 19/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.5951 - accuracy: 0.7341\n",
      "Epoch 20/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.5797 - accuracy: 0.7478\n",
      "Epoch 21/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.5921 - accuracy: 0.7306\n",
      "Epoch 22/1500\n",
      "32/32 [==============================] - 0s 793us/step - loss: 0.5902 - accuracy: 0.7316\n",
      "Epoch 23/1500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.5767 - accuracy: 0.7404\n",
      "Epoch 24/1500\n",
      "32/32 [==============================] - 0s 864us/step - loss: 0.5624 - accuracy: 0.7586\n",
      "Epoch 25/1500\n",
      "32/32 [==============================] - 0s 905us/step - loss: 0.5647 - accuracy: 0.7493\n",
      "Epoch 26/1500\n",
      "32/32 [==============================] - 0s 900us/step - loss: 0.5826 - accuracy: 0.7463\n",
      "Epoch 27/1500\n",
      "32/32 [==============================] - 0s 890us/step - loss: 0.5357 - accuracy: 0.7689\n",
      "Epoch 28/1500\n",
      "32/32 [==============================] - 0s 814us/step - loss: 0.5392 - accuracy: 0.7601\n",
      "Epoch 29/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.5471 - accuracy: 0.7493\n",
      "Epoch 30/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.5354 - accuracy: 0.7669\n",
      "Epoch 31/1500\n",
      "32/32 [==============================] - 0s 802us/step - loss: 0.5297 - accuracy: 0.7659\n",
      "Epoch 32/1500\n",
      "32/32 [==============================] - 0s 918us/step - loss: 0.5182 - accuracy: 0.7758\n",
      "Epoch 33/1500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.5313 - accuracy: 0.7664\n",
      "Epoch 34/1500\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.5153 - accuracy: 0.7689\n",
      "Epoch 35/1500\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.5167 - accuracy: 0.7753\n",
      "Epoch 36/1500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.5240 - accuracy: 0.7704\n",
      "Epoch 37/1500\n",
      "32/32 [==============================] - 0s 903us/step - loss: 0.4996 - accuracy: 0.7836\n",
      "Epoch 38/1500\n",
      "32/32 [==============================] - 0s 939us/step - loss: 0.5143 - accuracy: 0.7694\n",
      "Epoch 39/1500\n",
      "32/32 [==============================] - 0s 934us/step - loss: 0.4858 - accuracy: 0.7920\n",
      "Epoch 40/1500\n",
      "32/32 [==============================] - 0s 866us/step - loss: 0.5127 - accuracy: 0.7767\n",
      "Epoch 41/1500\n",
      "32/32 [==============================] - 0s 896us/step - loss: 0.4913 - accuracy: 0.7826\n",
      "Epoch 42/1500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.4867 - accuracy: 0.7875\n",
      "Epoch 43/1500\n",
      "32/32 [==============================] - 0s 974us/step - loss: 0.4767 - accuracy: 0.7983\n",
      "Epoch 44/1500\n",
      "32/32 [==============================] - 0s 934us/step - loss: 0.4735 - accuracy: 0.7920\n",
      "Epoch 45/1500\n",
      "32/32 [==============================] - 0s 934us/step - loss: 0.4749 - accuracy: 0.7856\n",
      "Epoch 46/1500\n",
      "32/32 [==============================] - 0s 928us/step - loss: 0.4617 - accuracy: 0.7929\n",
      "Epoch 47/1500\n",
      "32/32 [==============================] - 0s 953us/step - loss: 0.4562 - accuracy: 0.8047\n",
      "Epoch 48/1500\n",
      "32/32 [==============================] - 0s 889us/step - loss: 0.4644 - accuracy: 0.7969\n",
      "Epoch 49/1500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.4476 - accuracy: 0.8062\n",
      "Epoch 50/1500\n",
      "32/32 [==============================] - 0s 864us/step - loss: 0.4708 - accuracy: 0.7880\n",
      "Epoch 51/1500\n",
      "32/32 [==============================] - 0s 915us/step - loss: 0.4650 - accuracy: 0.7944\n",
      "Epoch 52/1500\n",
      "32/32 [==============================] - 0s 880us/step - loss: 0.4645 - accuracy: 0.7929\n",
      "Epoch 53/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.4494 - accuracy: 0.7969\n",
      "Epoch 54/1500\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.4154 - accuracy: 0.8258\n",
      "Epoch 55/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.4378 - accuracy: 0.8135\n",
      "Epoch 56/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.4468 - accuracy: 0.8008\n",
      "Epoch 57/1500\n",
      "32/32 [==============================] - 0s 885us/step - loss: 0.4490 - accuracy: 0.8008\n",
      "Epoch 58/1500\n",
      "32/32 [==============================] - 0s 879us/step - loss: 0.4290 - accuracy: 0.8135\n",
      "Epoch 59/1500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.4185 - accuracy: 0.8229\n",
      "Epoch 60/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.4337 - accuracy: 0.8111\n",
      "Epoch 61/1500\n",
      "32/32 [==============================] - 0s 787us/step - loss: 0.4420 - accuracy: 0.8091\n",
      "Epoch 62/1500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.4500 - accuracy: 0.8072\n",
      "Epoch 63/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.4493 - accuracy: 0.7949\n",
      "Epoch 64/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.4412 - accuracy: 0.8072\n",
      "Epoch 65/1500\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.4255 - accuracy: 0.8165\n",
      "Epoch 66/1500\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.4159 - accuracy: 0.8273\n",
      "Epoch 67/1500\n",
      "32/32 [==============================] - 0s 828us/step - loss: 0.4259 - accuracy: 0.8160\n",
      "Epoch 68/1500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.4117 - accuracy: 0.8224\n",
      "Epoch 69/1500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.4184 - accuracy: 0.8170\n",
      "Epoch 70/1500\n",
      "32/32 [==============================] - 0s 816us/step - loss: 0.4113 - accuracy: 0.8150\n",
      "Epoch 71/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.4003 - accuracy: 0.8238\n",
      "Epoch 72/1500\n",
      "32/32 [==============================] - 0s 877us/step - loss: 0.4128 - accuracy: 0.8238\n",
      "Epoch 73/1500\n",
      "32/32 [==============================] - 0s 946us/step - loss: 0.4094 - accuracy: 0.8243\n",
      "Epoch 74/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.4105 - accuracy: 0.8317\n",
      "Epoch 75/1500\n",
      "32/32 [==============================] - 0s 876us/step - loss: 0.4091 - accuracy: 0.8322\n",
      "Epoch 76/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.4094 - accuracy: 0.8224\n",
      "Epoch 77/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.3996 - accuracy: 0.8243\n",
      "Epoch 78/1500\n",
      "32/32 [==============================] - 0s 923us/step - loss: 0.4045 - accuracy: 0.8238\n",
      "Epoch 79/1500\n",
      "32/32 [==============================] - 0s 862us/step - loss: 0.4020 - accuracy: 0.8278\n",
      "Epoch 80/1500\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.3844 - accuracy: 0.8386\n",
      "Epoch 81/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.4073 - accuracy: 0.8209\n",
      "Epoch 82/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.3865 - accuracy: 0.8400\n",
      "Epoch 83/1500\n",
      "32/32 [==============================] - 0s 802us/step - loss: 0.4049 - accuracy: 0.8140\n",
      "Epoch 84/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.3882 - accuracy: 0.8391\n",
      "Epoch 85/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.3996 - accuracy: 0.8312\n",
      "Epoch 86/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.4124 - accuracy: 0.8258\n",
      "Epoch 87/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.3826 - accuracy: 0.8386\n",
      "Epoch 88/1500\n",
      "32/32 [==============================] - 0s 741us/step - loss: 0.3904 - accuracy: 0.8376\n",
      "Epoch 89/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.3886 - accuracy: 0.8342\n",
      "Epoch 90/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.3755 - accuracy: 0.8430\n",
      "Epoch 91/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.3811 - accuracy: 0.8312\n",
      "Epoch 92/1500\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.3768 - accuracy: 0.8356\n",
      "Epoch 93/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.3798 - accuracy: 0.8405\n",
      "Epoch 94/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.3796 - accuracy: 0.8376\n",
      "Epoch 95/1500\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.3730 - accuracy: 0.8435\n",
      "Epoch 96/1500\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.3808 - accuracy: 0.8376\n",
      "Epoch 97/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.3681 - accuracy: 0.8445\n",
      "Epoch 98/1500\n",
      "32/32 [==============================] - 0s 786us/step - loss: 0.3595 - accuracy: 0.8440\n",
      "Epoch 99/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.3728 - accuracy: 0.8366\n",
      "Epoch 100/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.3586 - accuracy: 0.8518\n",
      "Epoch 101/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.3676 - accuracy: 0.8518\n",
      "Epoch 102/1500\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.3469 - accuracy: 0.8553\n",
      "Epoch 103/1500\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.3513 - accuracy: 0.8494\n",
      "Epoch 104/1500\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.3687 - accuracy: 0.8337\n",
      "Epoch 105/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.3683 - accuracy: 0.8420\n",
      "Epoch 106/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.3562 - accuracy: 0.8440\n",
      "Epoch 107/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.3622 - accuracy: 0.8528\n",
      "Epoch 108/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.3465 - accuracy: 0.8494\n",
      "Epoch 109/1500\n",
      "32/32 [==============================] - 0s 796us/step - loss: 0.3593 - accuracy: 0.8445\n",
      "Epoch 110/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.3527 - accuracy: 0.8538\n",
      "Epoch 111/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.3559 - accuracy: 0.8469\n",
      "Epoch 112/1500\n",
      "32/32 [==============================] - 0s 790us/step - loss: 0.3689 - accuracy: 0.8371\n",
      "Epoch 113/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.3467 - accuracy: 0.8435\n",
      "Epoch 114/1500\n",
      "32/32 [==============================] - 0s 793us/step - loss: 0.3386 - accuracy: 0.8577\n",
      "Epoch 115/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.3450 - accuracy: 0.8641\n",
      "Epoch 116/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.3448 - accuracy: 0.8494\n",
      "Epoch 117/1500\n",
      "32/32 [==============================] - 0s 795us/step - loss: 0.3475 - accuracy: 0.8503\n",
      "Epoch 118/1500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.3312 - accuracy: 0.8621\n",
      "Epoch 119/1500\n",
      "32/32 [==============================] - 0s 812us/step - loss: 0.3423 - accuracy: 0.8494\n",
      "Epoch 120/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.3554 - accuracy: 0.8494\n",
      "Epoch 121/1500\n",
      "32/32 [==============================] - 0s 800us/step - loss: 0.3432 - accuracy: 0.8606\n",
      "Epoch 122/1500\n",
      "32/32 [==============================] - 0s 777us/step - loss: 0.3285 - accuracy: 0.8606\n",
      "Epoch 123/1500\n",
      "32/32 [==============================] - 0s 817us/step - loss: 0.3364 - accuracy: 0.8567\n",
      "Epoch 124/1500\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.3393 - accuracy: 0.8533\n",
      "Epoch 125/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8597\n",
      "Epoch 126/1500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.3260 - accuracy: 0.8685\n",
      "Epoch 127/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.3290 - accuracy: 0.8538\n",
      "Epoch 128/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.3290 - accuracy: 0.8621\n",
      "Epoch 129/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.3290 - accuracy: 0.8651\n",
      "Epoch 130/1500\n",
      "32/32 [==============================] - 0s 749us/step - loss: 0.3213 - accuracy: 0.8597\n",
      "Epoch 131/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.3361 - accuracy: 0.8543\n",
      "Epoch 132/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.3405 - accuracy: 0.8602\n",
      "Epoch 133/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.3230 - accuracy: 0.8651\n",
      "Epoch 134/1500\n",
      "32/32 [==============================] - 0s 810us/step - loss: 0.3170 - accuracy: 0.8656\n",
      "Epoch 135/1500\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.3340 - accuracy: 0.8587\n",
      "Epoch 136/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.3257 - accuracy: 0.8597\n",
      "Epoch 137/1500\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.3383 - accuracy: 0.8616\n",
      "Epoch 138/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.3152 - accuracy: 0.8611\n",
      "Epoch 139/1500\n",
      "32/32 [==============================] - 0s 803us/step - loss: 0.3214 - accuracy: 0.8700\n",
      "Epoch 140/1500\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.3169 - accuracy: 0.8739\n",
      "Epoch 141/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.3278 - accuracy: 0.8616\n",
      "Epoch 142/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.3174 - accuracy: 0.8680\n",
      "Epoch 143/1500\n",
      "32/32 [==============================] - 0s 755us/step - loss: 0.3214 - accuracy: 0.8670\n",
      "Epoch 144/1500\n",
      "32/32 [==============================] - 0s 813us/step - loss: 0.3174 - accuracy: 0.8611\n",
      "Epoch 145/1500\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.3239 - accuracy: 0.8616\n",
      "Epoch 146/1500\n",
      "32/32 [==============================] - 0s 816us/step - loss: 0.3025 - accuracy: 0.8734\n",
      "Epoch 147/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.3060 - accuracy: 0.8803\n",
      "Epoch 148/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.3108 - accuracy: 0.8803\n",
      "Epoch 149/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.3130 - accuracy: 0.8675\n",
      "Epoch 150/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.3021 - accuracy: 0.8705\n",
      "Epoch 151/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.3060 - accuracy: 0.8705\n",
      "Epoch 152/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.2996 - accuracy: 0.8744\n",
      "Epoch 153/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.3106 - accuracy: 0.8719\n",
      "Epoch 154/1500\n",
      "32/32 [==============================] - 0s 803us/step - loss: 0.3044 - accuracy: 0.8724\n",
      "Epoch 155/1500\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.3124 - accuracy: 0.8656\n",
      "Epoch 156/1500\n",
      "32/32 [==============================] - 0s 810us/step - loss: 0.3012 - accuracy: 0.8660\n",
      "Epoch 157/1500\n",
      "32/32 [==============================] - 0s 795us/step - loss: 0.2942 - accuracy: 0.8754\n",
      "Epoch 158/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.3086 - accuracy: 0.8724\n",
      "Epoch 159/1500\n",
      "32/32 [==============================] - 0s 786us/step - loss: 0.3229 - accuracy: 0.8665\n",
      "Epoch 160/1500\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.3001 - accuracy: 0.8763\n",
      "Epoch 161/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.3019 - accuracy: 0.8808\n",
      "Epoch 162/1500\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.3157 - accuracy: 0.8729\n",
      "Epoch 163/1500\n",
      "32/32 [==============================] - 0s 803us/step - loss: 0.3030 - accuracy: 0.8744\n",
      "Epoch 164/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.2992 - accuracy: 0.8763\n",
      "Epoch 165/1500\n",
      "32/32 [==============================] - 0s 752us/step - loss: 0.3221 - accuracy: 0.8651\n",
      "Epoch 166/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.3080 - accuracy: 0.8754\n",
      "Epoch 167/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2972 - accuracy: 0.8734\n",
      "Epoch 168/1500\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2880 - accuracy: 0.8793\n",
      "Epoch 169/1500\n",
      "32/32 [==============================] - 0s 931us/step - loss: 0.3004 - accuracy: 0.8724\n",
      "Epoch 170/1500\n",
      "32/32 [==============================] - 0s 790us/step - loss: 0.2900 - accuracy: 0.8680\n",
      "Epoch 171/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2858 - accuracy: 0.8857\n",
      "Epoch 172/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.2840 - accuracy: 0.8788\n",
      "Epoch 173/1500\n",
      "32/32 [==============================] - 0s 796us/step - loss: 0.2786 - accuracy: 0.8837\n",
      "Epoch 174/1500\n",
      "32/32 [==============================] - 0s 796us/step - loss: 0.2927 - accuracy: 0.8734\n",
      "Epoch 175/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2879 - accuracy: 0.8822\n",
      "Epoch 176/1500\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.2871 - accuracy: 0.8837\n",
      "Epoch 177/1500\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.2874 - accuracy: 0.8798\n",
      "Epoch 178/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.3022 - accuracy: 0.8724\n",
      "Epoch 179/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.2779 - accuracy: 0.8896\n",
      "Epoch 180/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.2855 - accuracy: 0.8798\n",
      "Epoch 181/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.2801 - accuracy: 0.8832\n",
      "Epoch 182/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.2816 - accuracy: 0.8817\n",
      "Epoch 183/1500\n",
      "32/32 [==============================] - 0s 796us/step - loss: 0.2819 - accuracy: 0.8867\n",
      "Epoch 184/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2798 - accuracy: 0.8817\n",
      "Epoch 185/1500\n",
      "32/32 [==============================] - 0s 840us/step - loss: 0.2758 - accuracy: 0.8847\n",
      "Epoch 186/1500\n",
      "32/32 [==============================] - 0s 928us/step - loss: 0.2824 - accuracy: 0.8886\n",
      "Epoch 187/1500\n",
      "32/32 [==============================] - 0s 904us/step - loss: 0.2760 - accuracy: 0.8803\n",
      "Epoch 188/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2825 - accuracy: 0.8876\n",
      "Epoch 189/1500\n",
      "32/32 [==============================] - 0s 972us/step - loss: 0.2805 - accuracy: 0.8808\n",
      "Epoch 190/1500\n",
      "32/32 [==============================] - 0s 989us/step - loss: 0.2916 - accuracy: 0.8837\n",
      "Epoch 191/1500\n",
      "32/32 [==============================] - 0s 955us/step - loss: 0.2772 - accuracy: 0.8867\n",
      "Epoch 192/1500\n",
      "32/32 [==============================] - 0s 967us/step - loss: 0.2598 - accuracy: 0.8871\n",
      "Epoch 193/1500\n",
      "32/32 [==============================] - 0s 963us/step - loss: 0.2785 - accuracy: 0.8881\n",
      "Epoch 194/1500\n",
      "32/32 [==============================] - 0s 956us/step - loss: 0.2767 - accuracy: 0.8759\n",
      "Epoch 195/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2642 - accuracy: 0.8921\n",
      "Epoch 196/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2714 - accuracy: 0.8960\n",
      "Epoch 197/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.8862\n",
      "Epoch 198/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2667 - accuracy: 0.8945\n",
      "Epoch 199/1500\n",
      "32/32 [==============================] - 0s 986us/step - loss: 0.2676 - accuracy: 0.8852\n",
      "Epoch 200/1500\n",
      "32/32 [==============================] - 0s 990us/step - loss: 0.2662 - accuracy: 0.8945\n",
      "Epoch 201/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2715 - accuracy: 0.8803\n",
      "Epoch 202/1500\n",
      "32/32 [==============================] - 0s 939us/step - loss: 0.2646 - accuracy: 0.8906\n",
      "Epoch 203/1500\n",
      "32/32 [==============================] - 0s 942us/step - loss: 0.2592 - accuracy: 0.8906\n",
      "Epoch 204/1500\n",
      "32/32 [==============================] - 0s 964us/step - loss: 0.2720 - accuracy: 0.8965\n",
      "Epoch 205/1500\n",
      "32/32 [==============================] - 0s 960us/step - loss: 0.2621 - accuracy: 0.8940\n",
      "Epoch 206/1500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.2503 - accuracy: 0.8970\n",
      "Epoch 207/1500\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.2635 - accuracy: 0.8876\n",
      "Epoch 208/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.2619 - accuracy: 0.8906\n",
      "Epoch 209/1500\n",
      "32/32 [==============================] - 0s 940us/step - loss: 0.2504 - accuracy: 0.8979\n",
      "Epoch 210/1500\n",
      "32/32 [==============================] - 0s 970us/step - loss: 0.2626 - accuracy: 0.8901\n",
      "Epoch 211/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2570 - accuracy: 0.8955\n",
      "Epoch 212/1500\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.2637 - accuracy: 0.9024\n",
      "Epoch 213/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.2672 - accuracy: 0.8871\n",
      "Epoch 214/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2699 - accuracy: 0.8901\n",
      "Epoch 215/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2514 - accuracy: 0.8891\n",
      "Epoch 216/1500\n",
      "32/32 [==============================] - 0s 891us/step - loss: 0.2508 - accuracy: 0.8974\n",
      "Epoch 217/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2593 - accuracy: 0.8950\n",
      "Epoch 218/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2528 - accuracy: 0.8965\n",
      "Epoch 219/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2395 - accuracy: 0.9087\n",
      "Epoch 220/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2732 - accuracy: 0.8842\n",
      "Epoch 221/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2487 - accuracy: 0.9024\n",
      "Epoch 222/1500\n",
      "32/32 [==============================] - 0s 940us/step - loss: 0.2561 - accuracy: 0.8921\n",
      "Epoch 223/1500\n",
      "32/32 [==============================] - 0s 925us/step - loss: 0.2308 - accuracy: 0.9087\n",
      "Epoch 224/1500\n",
      "32/32 [==============================] - 0s 822us/step - loss: 0.2540 - accuracy: 0.8984\n",
      "Epoch 225/1500\n",
      "32/32 [==============================] - 0s 953us/step - loss: 0.2382 - accuracy: 0.9014\n",
      "Epoch 226/1500\n",
      "32/32 [==============================] - 0s 982us/step - loss: 0.2450 - accuracy: 0.9019\n",
      "Epoch 227/1500\n",
      "32/32 [==============================] - 0s 972us/step - loss: 0.2536 - accuracy: 0.8955\n",
      "Epoch 228/1500\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.2584 - accuracy: 0.8945\n",
      "Epoch 229/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2451 - accuracy: 0.9014\n",
      "Epoch 230/1500\n",
      "32/32 [==============================] - 0s 852us/step - loss: 0.2398 - accuracy: 0.9127\n",
      "Epoch 231/1500\n",
      "32/32 [==============================] - 0s 814us/step - loss: 0.2617 - accuracy: 0.8925\n",
      "Epoch 232/1500\n",
      "32/32 [==============================] - 0s 803us/step - loss: 0.2606 - accuracy: 0.8921\n",
      "Epoch 233/1500\n",
      "32/32 [==============================] - 0s 800us/step - loss: 0.2556 - accuracy: 0.8955\n",
      "Epoch 234/1500\n",
      "32/32 [==============================] - 0s 904us/step - loss: 0.2442 - accuracy: 0.9004\n",
      "Epoch 235/1500\n",
      "32/32 [==============================] - 0s 975us/step - loss: 0.2530 - accuracy: 0.8999\n",
      "Epoch 236/1500\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.2554 - accuracy: 0.9009\n",
      "Epoch 237/1500\n",
      "32/32 [==============================] - 0s 982us/step - loss: 0.2510 - accuracy: 0.9033\n",
      "Epoch 238/1500\n",
      "32/32 [==============================] - 0s 962us/step - loss: 0.2504 - accuracy: 0.9028\n",
      "Epoch 239/1500\n",
      "32/32 [==============================] - 0s 865us/step - loss: 0.2506 - accuracy: 0.8970\n",
      "Epoch 240/1500\n",
      "32/32 [==============================] - 0s 821us/step - loss: 0.2302 - accuracy: 0.9127\n",
      "Epoch 241/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.2614 - accuracy: 0.8955\n",
      "Epoch 242/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.2360 - accuracy: 0.8999\n",
      "Epoch 243/1500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.2392 - accuracy: 0.9087\n",
      "Epoch 244/1500\n",
      "32/32 [==============================] - 0s 938us/step - loss: 0.2439 - accuracy: 0.8999\n",
      "Epoch 245/1500\n",
      "32/32 [==============================] - 0s 977us/step - loss: 0.2458 - accuracy: 0.9019\n",
      "Epoch 246/1500\n",
      "32/32 [==============================] - 0s 979us/step - loss: 0.2366 - accuracy: 0.9009\n",
      "Epoch 247/1500\n",
      "32/32 [==============================] - 0s 963us/step - loss: 0.2386 - accuracy: 0.9038\n",
      "Epoch 248/1500\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.2492 - accuracy: 0.8989\n",
      "Epoch 249/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.2279 - accuracy: 0.9117\n",
      "Epoch 250/1500\n",
      "32/32 [==============================] - 0s 932us/step - loss: 0.2453 - accuracy: 0.8955\n",
      "Epoch 251/1500\n",
      "32/32 [==============================] - 0s 918us/step - loss: 0.2238 - accuracy: 0.9073\n",
      "Epoch 252/1500\n",
      "32/32 [==============================] - 0s 973us/step - loss: 0.2375 - accuracy: 0.9048\n",
      "Epoch 253/1500\n",
      "32/32 [==============================] - 0s 923us/step - loss: 0.2279 - accuracy: 0.9117\n",
      "Epoch 254/1500\n",
      "32/32 [==============================] - 0s 826us/step - loss: 0.2234 - accuracy: 0.9107\n",
      "Epoch 255/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.2262 - accuracy: 0.9053\n",
      "Epoch 256/1500\n",
      "32/32 [==============================] - 0s 799us/step - loss: 0.2297 - accuracy: 0.9019\n",
      "Epoch 257/1500\n",
      "32/32 [==============================] - 0s 952us/step - loss: 0.2225 - accuracy: 0.9053\n",
      "Epoch 258/1500\n",
      "32/32 [==============================] - 0s 902us/step - loss: 0.2267 - accuracy: 0.9161\n",
      "Epoch 259/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2355 - accuracy: 0.9014\n",
      "Epoch 260/1500\n",
      "32/32 [==============================] - 0s 950us/step - loss: 0.2255 - accuracy: 0.9073\n",
      "Epoch 261/1500\n",
      "32/32 [==============================] - 0s 906us/step - loss: 0.2362 - accuracy: 0.9048\n",
      "Epoch 262/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.2475 - accuracy: 0.9009\n",
      "Epoch 263/1500\n",
      "32/32 [==============================] - 0s 762us/step - loss: 0.2580 - accuracy: 0.8989\n",
      "Epoch 264/1500\n",
      "32/32 [==============================] - 0s 826us/step - loss: 0.2377 - accuracy: 0.9033\n",
      "Epoch 265/1500\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.2219 - accuracy: 0.9112\n",
      "Epoch 266/1500\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.2228 - accuracy: 0.9161\n",
      "Epoch 267/1500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.2303 - accuracy: 0.9102\n",
      "Epoch 268/1500\n",
      "32/32 [==============================] - 0s 813us/step - loss: 0.2322 - accuracy: 0.9073\n",
      "Epoch 269/1500\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.2265 - accuracy: 0.9112\n",
      "Epoch 270/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2331 - accuracy: 0.9053\n",
      "Epoch 271/1500\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.2413 - accuracy: 0.9019\n",
      "Epoch 272/1500\n",
      "32/32 [==============================] - 0s 782us/step - loss: 0.2262 - accuracy: 0.9063\n",
      "Epoch 273/1500\n",
      "32/32 [==============================] - 0s 912us/step - loss: 0.2190 - accuracy: 0.9127\n",
      "Epoch 274/1500\n",
      "32/32 [==============================] - 0s 798us/step - loss: 0.2335 - accuracy: 0.9136\n",
      "Epoch 275/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.2151 - accuracy: 0.9073\n",
      "Epoch 276/1500\n",
      "32/32 [==============================] - 0s 830us/step - loss: 0.2239 - accuracy: 0.9136\n",
      "Epoch 277/1500\n",
      "32/32 [==============================] - 0s 769us/step - loss: 0.2275 - accuracy: 0.9092\n",
      "Epoch 278/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2169 - accuracy: 0.9171\n",
      "Epoch 279/1500\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.2187 - accuracy: 0.9122\n",
      "Epoch 280/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.2197 - accuracy: 0.9156\n",
      "Epoch 281/1500\n",
      "32/32 [==============================] - 0s 787us/step - loss: 0.2328 - accuracy: 0.9014\n",
      "Epoch 282/1500\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.2190 - accuracy: 0.9117\n",
      "Epoch 283/1500\n",
      "32/32 [==============================] - 0s 796us/step - loss: 0.2246 - accuracy: 0.9058\n",
      "Epoch 284/1500\n",
      "32/32 [==============================] - 0s 986us/step - loss: 0.2269 - accuracy: 0.9082\n",
      "Epoch 285/1500\n",
      "32/32 [==============================] - 0s 971us/step - loss: 0.2096 - accuracy: 0.9200\n",
      "Epoch 286/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.2191 - accuracy: 0.9146\n",
      "Epoch 287/1500\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.2172 - accuracy: 0.9117\n",
      "Epoch 288/1500\n",
      "32/32 [==============================] - 0s 750us/step - loss: 0.2219 - accuracy: 0.9132\n",
      "Epoch 289/1500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.2158 - accuracy: 0.9132\n",
      "Epoch 290/1500\n",
      "32/32 [==============================] - 0s 809us/step - loss: 0.2109 - accuracy: 0.9136\n",
      "Epoch 291/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2153 - accuracy: 0.9112\n",
      "Epoch 292/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.2221 - accuracy: 0.9082\n",
      "Epoch 293/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.2054 - accuracy: 0.9136\n",
      "Epoch 294/1500\n",
      "32/32 [==============================] - 0s 730us/step - loss: 0.2248 - accuracy: 0.9171\n",
      "Epoch 295/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.2206 - accuracy: 0.9161\n",
      "Epoch 296/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2230 - accuracy: 0.9117\n",
      "Epoch 297/1500\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2006 - accuracy: 0.9259\n",
      "Epoch 298/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.2088 - accuracy: 0.9210\n",
      "Epoch 299/1500\n",
      "32/32 [==============================] - 0s 793us/step - loss: 0.2348 - accuracy: 0.9082\n",
      "Epoch 300/1500\n",
      "32/32 [==============================] - 0s 736us/step - loss: 0.2151 - accuracy: 0.9171\n",
      "Epoch 301/1500\n",
      "32/32 [==============================] - 0s 799us/step - loss: 0.1940 - accuracy: 0.9185\n",
      "Epoch 302/1500\n",
      "32/32 [==============================] - 0s 760us/step - loss: 0.2152 - accuracy: 0.9220\n",
      "Epoch 303/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.2174 - accuracy: 0.9117\n",
      "Epoch 304/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.2173 - accuracy: 0.9127\n",
      "Epoch 305/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.2151 - accuracy: 0.9161\n",
      "Epoch 306/1500\n",
      "32/32 [==============================] - 0s 891us/step - loss: 0.2157 - accuracy: 0.9127\n",
      "Epoch 307/1500\n",
      "32/32 [==============================] - 0s 796us/step - loss: 0.2061 - accuracy: 0.9215\n",
      "Epoch 308/1500\n",
      "32/32 [==============================] - 0s 787us/step - loss: 0.2064 - accuracy: 0.9205\n",
      "Epoch 309/1500\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.2133 - accuracy: 0.9141\n",
      "Epoch 310/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1952 - accuracy: 0.9185\n",
      "Epoch 311/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2165 - accuracy: 0.9063\n",
      "Epoch 312/1500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.2172 - accuracy: 0.9102\n",
      "Epoch 313/1500\n",
      "32/32 [==============================] - 0s 765us/step - loss: 0.1999 - accuracy: 0.9200\n",
      "Epoch 314/1500\n",
      "32/32 [==============================] - 0s 808us/step - loss: 0.2055 - accuracy: 0.9146\n",
      "Epoch 315/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.2239 - accuracy: 0.9087\n",
      "Epoch 316/1500\n",
      "32/32 [==============================] - 0s 810us/step - loss: 0.2141 - accuracy: 0.9161\n",
      "Epoch 317/1500\n",
      "32/32 [==============================] - 0s 740us/step - loss: 0.1957 - accuracy: 0.9185\n",
      "Epoch 318/1500\n",
      "32/32 [==============================] - 0s 767us/step - loss: 0.2033 - accuracy: 0.9161\n",
      "Epoch 319/1500\n",
      "32/32 [==============================] - 0s 739us/step - loss: 0.2122 - accuracy: 0.9141\n",
      "Epoch 320/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.2078 - accuracy: 0.9151\n",
      "Epoch 321/1500\n",
      "32/32 [==============================] - 0s 737us/step - loss: 0.2003 - accuracy: 0.9171\n",
      "Epoch 322/1500\n",
      "32/32 [==============================] - 0s 726us/step - loss: 0.2045 - accuracy: 0.9190\n",
      "Epoch 323/1500\n",
      "32/32 [==============================] - 0s 743us/step - loss: 0.2063 - accuracy: 0.9161\n",
      "Epoch 324/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2007 - accuracy: 0.9151\n",
      "Epoch 325/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.2286 - accuracy: 0.9048\n",
      "Epoch 326/1500\n",
      "32/32 [==============================] - 0s 735us/step - loss: 0.2058 - accuracy: 0.9195\n",
      "Epoch 327/1500\n",
      "32/32 [==============================] - 0s 720us/step - loss: 0.1973 - accuracy: 0.9117\n",
      "Epoch 328/1500\n",
      "32/32 [==============================] - 0s 727us/step - loss: 0.1959 - accuracy: 0.9274\n",
      "Epoch 329/1500\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.2014 - accuracy: 0.9235\n",
      "Epoch 330/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.2028 - accuracy: 0.9161\n",
      "Epoch 331/1500\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.1172 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 301.\n",
      "32/32 [==============================] - 0s 821us/step - loss: 0.1938 - accuracy: 0.9239\n",
      "Epoch 331: early stopping\n",
      "10/10 [==============================] - 0s 742us/step - loss: 0.5631 - accuracy: 0.7770\n",
      "10/10 [==============================] - 0s 564us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (25/30)\n",
      "Before appending - Cat IDs: 409, Predictions: 409, Actuals: 409, Gender: 409\n",
      "After appending - Cat IDs: 714, Predictions: 714, Actuals: 714, Gender: 714\n",
      "Final Test Results - Loss: 0.5631164312362671, Accuracy: 0.77704918384552, Precision: 0.7444976076555023, Recall: 0.788618269926681, F1 Score: 0.7603017058462603\n",
      "Confusion Matrix:\n",
      " [[168   4  42]\n",
      " [  2  34   0]\n",
      " [ 20   0  35]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "071A    10\n",
      "016A    10\n",
      "072A     9\n",
      "022A     9\n",
      "045A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "094A     8\n",
      "095A     8\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "037A     6\n",
      "008A     6\n",
      "023A     6\n",
      "053A     6\n",
      "025C     5\n",
      "075A     5\n",
      "044A     5\n",
      "034A     5\n",
      "070A     5\n",
      "009A     4\n",
      "052A     4\n",
      "105A     4\n",
      "104A     4\n",
      "003A     4\n",
      "060A     3\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "025B     2\n",
      "073A     1\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "048A     1\n",
      "019B     1\n",
      "088A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "065A     9\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "021A     5\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "113A     3\n",
      "064A     3\n",
      "087A     2\n",
      "038A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "043A     1\n",
      "041A     1\n",
      "066A     1\n",
      "004A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    314\n",
      "X    271\n",
      "F    164\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    88\n",
      "X    77\n",
      "M    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 097B, 028...\n",
      "kitten    [044A, 046A, 047A, 050A, 049A, 045A, 048A, 115...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 062A, 101A, 065A, 063A, 038A, 007A, 087...\n",
      "kitten           [014B, 111A, 040A, 042A, 109A, 043A, 041A]\n",
      "senior                             [113A, 108A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 9, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 7, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'037A'}\n",
      "Moved to Test Set:\n",
      "{'037A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A'\n",
      " '029A' '032A' '033A' '034A' '036A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A' '037A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "782\n",
      "Length of y_train_val:\n",
      "782\n",
      "Length of groups_train_val:\n",
      "782\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "kitten     55\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     501\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     87\n",
      "kitten    55\n",
      "senior    13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1002, 2: 825, 1: 580})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 1.2446 - accuracy: 0.4695\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 964us/step - loss: 1.0062 - accuracy: 0.5613\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.9231 - accuracy: 0.6045\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.8859 - accuracy: 0.6244\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.8486 - accuracy: 0.6485\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.8091 - accuracy: 0.6485\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.7858 - accuracy: 0.6660\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.7681 - accuracy: 0.6705\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.7378 - accuracy: 0.7067\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.7355 - accuracy: 0.6946\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.7010 - accuracy: 0.6971\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.7007 - accuracy: 0.6988\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.6729 - accuracy: 0.7142\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.6733 - accuracy: 0.7142\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.6417 - accuracy: 0.7287\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.6708 - accuracy: 0.7146\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.6420 - accuracy: 0.7383\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.6326 - accuracy: 0.7383\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.6227 - accuracy: 0.7424\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.6014 - accuracy: 0.7449\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.6207 - accuracy: 0.7416\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.6021 - accuracy: 0.7403\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.5707 - accuracy: 0.7632\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.5794 - accuracy: 0.7511\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.5794 - accuracy: 0.7416\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.5728 - accuracy: 0.7536\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.5586 - accuracy: 0.7640\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.5629 - accuracy: 0.7599\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.5654 - accuracy: 0.7644\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.5481 - accuracy: 0.7711\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.5342 - accuracy: 0.7740\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.5506 - accuracy: 0.7698\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.5277 - accuracy: 0.7732\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.5221 - accuracy: 0.7757\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.5415 - accuracy: 0.7740\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.5005 - accuracy: 0.7927\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.5058 - accuracy: 0.7894\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.5147 - accuracy: 0.7761\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.4982 - accuracy: 0.7898\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.5009 - accuracy: 0.7881\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.4975 - accuracy: 0.7906\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.4980 - accuracy: 0.7852\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.4898 - accuracy: 0.7894\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.4962 - accuracy: 0.7856\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.4829 - accuracy: 0.7927\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.4863 - accuracy: 0.7956\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.4589 - accuracy: 0.8006\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.4590 - accuracy: 0.8002\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.4732 - accuracy: 0.7956\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.4719 - accuracy: 0.7977\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.4651 - accuracy: 0.8093\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.4510 - accuracy: 0.8118\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.4433 - accuracy: 0.8043\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.4623 - accuracy: 0.8068\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.4424 - accuracy: 0.8147\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.4439 - accuracy: 0.8081\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.4547 - accuracy: 0.8043\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.4477 - accuracy: 0.8076\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.4344 - accuracy: 0.8139\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.4335 - accuracy: 0.8164\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.4282 - accuracy: 0.8151\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.4315 - accuracy: 0.8122\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.4267 - accuracy: 0.8122\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.4306 - accuracy: 0.8143\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.4168 - accuracy: 0.8222\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.4450 - accuracy: 0.8114\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.4154 - accuracy: 0.8259\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.4241 - accuracy: 0.8230\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.4133 - accuracy: 0.8226\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.4146 - accuracy: 0.8255\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.4040 - accuracy: 0.8280\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.4013 - accuracy: 0.8338\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.4110 - accuracy: 0.8280\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.4028 - accuracy: 0.8330\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.4045 - accuracy: 0.8288\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.4056 - accuracy: 0.8288\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.4079 - accuracy: 0.8326\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.3921 - accuracy: 0.8305\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.3987 - accuracy: 0.8351\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.3933 - accuracy: 0.8371\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.3907 - accuracy: 0.8351\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.3841 - accuracy: 0.8346\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.3874 - accuracy: 0.8413\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.3960 - accuracy: 0.8251\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.3864 - accuracy: 0.8351\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.3693 - accuracy: 0.8504\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 990us/step - loss: 0.3846 - accuracy: 0.8322\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 968us/step - loss: 0.3817 - accuracy: 0.8434\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.3712 - accuracy: 0.8421\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3637 - accuracy: 0.8492\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.3701 - accuracy: 0.8509\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 970us/step - loss: 0.3670 - accuracy: 0.8459\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.3740 - accuracy: 0.8430\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 917us/step - loss: 0.3648 - accuracy: 0.8496\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 918us/step - loss: 0.3774 - accuracy: 0.8405\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.3766 - accuracy: 0.8450\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.3761 - accuracy: 0.8425\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.3665 - accuracy: 0.8463\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.3569 - accuracy: 0.8509\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.3551 - accuracy: 0.8529\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.3574 - accuracy: 0.8446\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.3484 - accuracy: 0.8533\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.3638 - accuracy: 0.8459\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.3647 - accuracy: 0.8488\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.3566 - accuracy: 0.8488\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.3425 - accuracy: 0.8592\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.3522 - accuracy: 0.8492\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.3518 - accuracy: 0.8492\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.3483 - accuracy: 0.8558\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.3408 - accuracy: 0.8517\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.3470 - accuracy: 0.8517\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 864us/step - loss: 0.3417 - accuracy: 0.8587\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.3300 - accuracy: 0.8691\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.3437 - accuracy: 0.8579\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.3375 - accuracy: 0.8658\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3307 - accuracy: 0.8671\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3455 - accuracy: 0.8587\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.3211 - accuracy: 0.8654\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3432 - accuracy: 0.8575\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3347 - accuracy: 0.8612\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.3322 - accuracy: 0.8571\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.3378 - accuracy: 0.8671\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.3228 - accuracy: 0.8687\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.3299 - accuracy: 0.8554\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.3324 - accuracy: 0.8641\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.3201 - accuracy: 0.8695\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.3152 - accuracy: 0.8712\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 908us/step - loss: 0.3140 - accuracy: 0.8733\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 913us/step - loss: 0.3284 - accuracy: 0.8671\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.3164 - accuracy: 0.8679\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.3108 - accuracy: 0.8737\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.3089 - accuracy: 0.8745\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.3048 - accuracy: 0.8725\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.3245 - accuracy: 0.8612\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2882 - accuracy: 0.8812\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.3168 - accuracy: 0.8691\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.3252 - accuracy: 0.8617\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.3128 - accuracy: 0.8671\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.3103 - accuracy: 0.8704\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.3075 - accuracy: 0.8737\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2879 - accuracy: 0.8841\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.3108 - accuracy: 0.8725\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.3150 - accuracy: 0.8671\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.3029 - accuracy: 0.8762\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2913 - accuracy: 0.8808\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.3110 - accuracy: 0.8745\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.3111 - accuracy: 0.8658\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.3033 - accuracy: 0.8758\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2892 - accuracy: 0.8808\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.3020 - accuracy: 0.8725\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.3020 - accuracy: 0.8695\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2814 - accuracy: 0.8862\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2997 - accuracy: 0.8845\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2967 - accuracy: 0.8741\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2870 - accuracy: 0.8791\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2798 - accuracy: 0.8849\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.3078 - accuracy: 0.8791\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2935 - accuracy: 0.8837\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.2970 - accuracy: 0.8737\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2901 - accuracy: 0.8791\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 733us/step - loss: 0.2794 - accuracy: 0.8907\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2686 - accuracy: 0.8932\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2980 - accuracy: 0.8816\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2847 - accuracy: 0.8837\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.2837 - accuracy: 0.8845\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2790 - accuracy: 0.8916\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2793 - accuracy: 0.8833\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2753 - accuracy: 0.8899\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2781 - accuracy: 0.8841\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2857 - accuracy: 0.8903\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2804 - accuracy: 0.8803\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2821 - accuracy: 0.8907\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2823 - accuracy: 0.8928\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2807 - accuracy: 0.8816\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.2749 - accuracy: 0.8870\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2651 - accuracy: 0.8903\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2614 - accuracy: 0.8970\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2818 - accuracy: 0.8833\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.2550 - accuracy: 0.8990\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2614 - accuracy: 0.8887\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2724 - accuracy: 0.8866\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2715 - accuracy: 0.8895\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2623 - accuracy: 0.8857\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2560 - accuracy: 0.9015\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.2576 - accuracy: 0.8916\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2767 - accuracy: 0.8878\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2626 - accuracy: 0.8986\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2581 - accuracy: 0.8936\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2681 - accuracy: 0.8912\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2748 - accuracy: 0.8878\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2491 - accuracy: 0.9003\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.2458 - accuracy: 0.8995\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2607 - accuracy: 0.8970\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.2661 - accuracy: 0.8845\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2598 - accuracy: 0.8916\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2578 - accuracy: 0.8953\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2402 - accuracy: 0.9053\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2399 - accuracy: 0.9107\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2404 - accuracy: 0.9061\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2381 - accuracy: 0.9024\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2478 - accuracy: 0.9032\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2431 - accuracy: 0.8974\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2512 - accuracy: 0.8928\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.2618 - accuracy: 0.8957\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.2416 - accuracy: 0.9024\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2547 - accuracy: 0.8978\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.2418 - accuracy: 0.9053\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2362 - accuracy: 0.9107\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2499 - accuracy: 0.9040\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2343 - accuracy: 0.9032\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2358 - accuracy: 0.9111\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2435 - accuracy: 0.9020\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.2452 - accuracy: 0.9007\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2376 - accuracy: 0.9024\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2521 - accuracy: 0.9024\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.2373 - accuracy: 0.9074\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.2410 - accuracy: 0.8982\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2492 - accuracy: 0.8974\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2487 - accuracy: 0.8953\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2403 - accuracy: 0.9090\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.2397 - accuracy: 0.9069\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2372 - accuracy: 0.9090\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 850us/step - loss: 0.2329 - accuracy: 0.9107\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.2435 - accuracy: 0.9024\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2404 - accuracy: 0.9040\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2327 - accuracy: 0.9103\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2415 - accuracy: 0.9082\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2363 - accuracy: 0.9057\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.2217 - accuracy: 0.9182\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.2359 - accuracy: 0.9049\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2373 - accuracy: 0.9015\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.2304 - accuracy: 0.9094\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.2323 - accuracy: 0.9069\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2243 - accuracy: 0.9057\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2261 - accuracy: 0.9128\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2358 - accuracy: 0.9107\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2413 - accuracy: 0.9036\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.2197 - accuracy: 0.9177\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 749us/step - loss: 0.2302 - accuracy: 0.9074\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.2318 - accuracy: 0.9057\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2279 - accuracy: 0.9115\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2218 - accuracy: 0.9152\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.2400 - accuracy: 0.9015\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.2224 - accuracy: 0.9111\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2205 - accuracy: 0.9140\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2313 - accuracy: 0.9090\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.2113 - accuracy: 0.9136\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2239 - accuracy: 0.9107\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2222 - accuracy: 0.9123\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2223 - accuracy: 0.9140\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.2210 - accuracy: 0.9094\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 737us/step - loss: 0.2172 - accuracy: 0.9119\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.2261 - accuracy: 0.9090\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2253 - accuracy: 0.9123\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2088 - accuracy: 0.9115\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2254 - accuracy: 0.9086\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2184 - accuracy: 0.9152\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.1992 - accuracy: 0.9223\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2145 - accuracy: 0.9103\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2239 - accuracy: 0.9157\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2088 - accuracy: 0.9190\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2188 - accuracy: 0.9082\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2086 - accuracy: 0.9231\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2130 - accuracy: 0.9211\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.2027 - accuracy: 0.9186\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2385 - accuracy: 0.9086\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2112 - accuracy: 0.9190\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2160 - accuracy: 0.9152\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2094 - accuracy: 0.9115\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 725us/step - loss: 0.2159 - accuracy: 0.9078\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2064 - accuracy: 0.9215\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2078 - accuracy: 0.9211\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2126 - accuracy: 0.9194\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2087 - accuracy: 0.9215\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2052 - accuracy: 0.9123\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2069 - accuracy: 0.9231\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2165 - accuracy: 0.9094\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.2104 - accuracy: 0.9140\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.2054 - accuracy: 0.9169\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2080 - accuracy: 0.9152\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2046 - accuracy: 0.9177\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.2154 - accuracy: 0.9148\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.1965 - accuracy: 0.9236\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.2097 - accuracy: 0.9190\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.1990 - accuracy: 0.9240\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1993 - accuracy: 0.9206\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.1928 - accuracy: 0.9319\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.2103 - accuracy: 0.9169\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2204 - accuracy: 0.9119\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.2079 - accuracy: 0.9194\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2270 - accuracy: 0.9140\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.1994 - accuracy: 0.9227\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2065 - accuracy: 0.9148\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.1997 - accuracy: 0.9206\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2013 - accuracy: 0.9265\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.2014 - accuracy: 0.9198\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1900 - accuracy: 0.9223\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2202 - accuracy: 0.9115\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2122 - accuracy: 0.9157\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2026 - accuracy: 0.9256\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.1925 - accuracy: 0.9231\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.1956 - accuracy: 0.9260\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1962 - accuracy: 0.9244\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2018 - accuracy: 0.9173\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.2043 - accuracy: 0.9198\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.1946 - accuracy: 0.9223\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.2049 - accuracy: 0.9211\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1892 - accuracy: 0.9273\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.2088 - accuracy: 0.9144\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.1917 - accuracy: 0.9323\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2144 - accuracy: 0.9140\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.1947 - accuracy: 0.9236\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1960 - accuracy: 0.9206\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.1870 - accuracy: 0.9285\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.1934 - accuracy: 0.9265\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2012 - accuracy: 0.9182\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1858 - accuracy: 0.9252\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.1978 - accuracy: 0.9219\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.1704 - accuracy: 0.9356\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.1937 - accuracy: 0.9177\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.1873 - accuracy: 0.9285\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.1922 - accuracy: 0.9215\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.1892 - accuracy: 0.9277\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1923 - accuracy: 0.9219\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.1925 - accuracy: 0.9265\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.1841 - accuracy: 0.9240\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.1872 - accuracy: 0.9256\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.1864 - accuracy: 0.9277\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.1811 - accuracy: 0.9310\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1958 - accuracy: 0.9211\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1924 - accuracy: 0.9306\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.1928 - accuracy: 0.9240\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.1805 - accuracy: 0.9335\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1968 - accuracy: 0.9231\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.1925 - accuracy: 0.9269\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1815 - accuracy: 0.9323\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.1781 - accuracy: 0.9327\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.1909 - accuracy: 0.9294\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.2014 - accuracy: 0.9198\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.1914 - accuracy: 0.9281\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.1812 - accuracy: 0.9281\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.1880 - accuracy: 0.9327\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1833 - accuracy: 0.9281\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2058 - accuracy: 0.9206\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2160 - accuracy: 0.9182\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.1829 - accuracy: 0.9269\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.1654 - accuracy: 0.9352\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.1795 - accuracy: 0.9314\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.1880 - accuracy: 0.9260\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.1846 - accuracy: 0.9302\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.1837 - accuracy: 0.9290\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1766 - accuracy: 0.9314\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1848 - accuracy: 0.9290\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.1876 - accuracy: 0.9294\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 741us/step - loss: 0.1808 - accuracy: 0.9302\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.1699 - accuracy: 0.9335\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.1767 - accuracy: 0.9360\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1924 - accuracy: 0.9260\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1823 - accuracy: 0.9298\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.1790 - accuracy: 0.9327\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.1834 - accuracy: 0.9252\n",
      "Epoch 362/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.1823 - accuracy: 0.9290\n",
      "Epoch 363/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.1786 - accuracy: 0.9310\n",
      "Epoch 364/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1809 - accuracy: 0.9285\n",
      "Epoch 365/1500\n",
      "38/38 [==============================] - 0s 908us/step - loss: 0.1857 - accuracy: 0.9236\n",
      "Epoch 366/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.1691 - accuracy: 0.9373\n",
      "Epoch 367/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.1599 - accuracy: 0.9427\n",
      "Epoch 368/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.1850 - accuracy: 0.9302\n",
      "Epoch 369/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1690 - accuracy: 0.9369\n",
      "Epoch 370/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1841 - accuracy: 0.9331\n",
      "Epoch 371/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.1844 - accuracy: 0.9269\n",
      "Epoch 372/1500\n",
      "38/38 [==============================] - 0s 844us/step - loss: 0.1822 - accuracy: 0.9273\n",
      "Epoch 373/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.1638 - accuracy: 0.9364\n",
      "Epoch 374/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.1805 - accuracy: 0.9306\n",
      "Epoch 375/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.1820 - accuracy: 0.9298\n",
      "Epoch 376/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.1627 - accuracy: 0.9356\n",
      "Epoch 377/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.1779 - accuracy: 0.9344\n",
      "Epoch 378/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.1740 - accuracy: 0.9314\n",
      "Epoch 379/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1670 - accuracy: 0.9327\n",
      "Epoch 380/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.1599 - accuracy: 0.9335\n",
      "Epoch 381/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1810 - accuracy: 0.9310\n",
      "Epoch 382/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1688 - accuracy: 0.9381\n",
      "Epoch 383/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.1741 - accuracy: 0.9281\n",
      "Epoch 384/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.1759 - accuracy: 0.9356\n",
      "Epoch 385/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.1759 - accuracy: 0.9314\n",
      "Epoch 386/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.1673 - accuracy: 0.9385\n",
      "Epoch 387/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.1682 - accuracy: 0.9414\n",
      "Epoch 388/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1773 - accuracy: 0.9327\n",
      "Epoch 389/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.1770 - accuracy: 0.9319\n",
      "Epoch 390/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.1712 - accuracy: 0.9381\n",
      "Epoch 391/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1705 - accuracy: 0.9356\n",
      "Epoch 392/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1747 - accuracy: 0.9393\n",
      "Epoch 393/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.1718 - accuracy: 0.9352\n",
      "Epoch 394/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.1659 - accuracy: 0.9381\n",
      "Epoch 395/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.1642 - accuracy: 0.9364\n",
      "Epoch 396/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.1474 - accuracy: 0.9431\n",
      "Epoch 397/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.1657 - accuracy: 0.9352\n",
      "Epoch 398/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.1749 - accuracy: 0.9323\n",
      "Epoch 399/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.1521 - accuracy: 0.9447\n",
      "Epoch 400/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.1715 - accuracy: 0.9331\n",
      "Epoch 401/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.1586 - accuracy: 0.9364\n",
      "Epoch 402/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.1697 - accuracy: 0.9385\n",
      "Epoch 403/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.1664 - accuracy: 0.9369\n",
      "Epoch 404/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.1739 - accuracy: 0.9310\n",
      "Epoch 405/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.1630 - accuracy: 0.9414\n",
      "Epoch 406/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.1591 - accuracy: 0.9369\n",
      "Epoch 407/1500\n",
      "38/38 [==============================] - 0s 742us/step - loss: 0.1639 - accuracy: 0.9393\n",
      "Epoch 408/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1639 - accuracy: 0.9381\n",
      "Epoch 409/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.1609 - accuracy: 0.9402\n",
      "Epoch 410/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.1654 - accuracy: 0.9356\n",
      "Epoch 411/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.1785 - accuracy: 0.9323\n",
      "Epoch 412/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.1662 - accuracy: 0.9418\n",
      "Epoch 413/1500\n",
      "38/38 [==============================] - 0s 949us/step - loss: 0.1698 - accuracy: 0.9290\n",
      "Epoch 414/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.1766 - accuracy: 0.9323\n",
      "Epoch 415/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.1742 - accuracy: 0.9319\n",
      "Epoch 416/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.1657 - accuracy: 0.9385\n",
      "Epoch 417/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1907 - accuracy: 0.9227\n",
      "Epoch 418/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.1687 - accuracy: 0.9352\n",
      "Epoch 419/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.1631 - accuracy: 0.9352\n",
      "Epoch 420/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.1559 - accuracy: 0.9398\n",
      "Epoch 421/1500\n",
      "38/38 [==============================] - 0s 731us/step - loss: 0.1567 - accuracy: 0.9364\n",
      "Epoch 422/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.1577 - accuracy: 0.9402\n",
      "Epoch 423/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1619 - accuracy: 0.9393\n",
      "Epoch 424/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.1616 - accuracy: 0.9389\n",
      "Epoch 425/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1526 - accuracy: 0.9423\n",
      "Epoch 426/1500\n",
      "11/38 [=======>......................] - ETA: 0s - loss: 0.1525 - accuracy: 0.9361Restoring model weights from the end of the best epoch: 396.\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1523 - accuracy: 0.9385\n",
      "Epoch 426: early stopping\n",
      "5/5 [==============================] - 0s 942us/step - loss: 0.5217 - accuracy: 0.7742\n",
      "5/5 [==============================] - 0s 732us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 714, Predictions: 714, Actuals: 714, Gender: 714\n",
      "After appending - Cat IDs: 869, Predictions: 869, Actuals: 869, Gender: 869\n",
      "Final Test Results - Loss: 0.5217323899269104, Accuracy: 0.774193525314331, Precision: 0.6699143206854346, Recall: 0.6887120542292956, F1 Score: 0.674114597669511\n",
      "Confusion Matrix:\n",
      " [[70  5 12]\n",
      " [10 44  1]\n",
      " [ 6  1  6]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.7069493101015643\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.7526680380105972\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7287663370370865\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.7024670186865378\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7267621268386335\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[0]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79e793-c694-4bc7-8cc6-05e124ddf71f",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "2c3c239d-6215-4589-a583-b37a18ca22cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 869, Predictions: 869, Actuals: 869, Gender: 869\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "99978976-e4a9-4c04-a6b2-6b14a3111277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "189b812b-d37f-462f-a99f-6de8e4a7899f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (82/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "907d957f-7340-4a76-b3fa-7cf166f43049",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "7016fa20-094f-409a-9a2f-b6d6793aed03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, adult, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, senior, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, kitten]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, kitten, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, adult, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[kitten, kitten, adult, adult, adult, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, adult...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, adult, kitten,...        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, senior, adult, senior, a...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "68    062A                      [kitten, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [adult, adult, senior, adult, senior, senior, ...        senior           senior                   True\n",
       "63    057A  [senior, adult, adult, senior, senior, adult, ...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [senior, adult, senior, senior, senior, senior...        senior           senior                   True\n",
       "59    053A        [adult, adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "58    052A                     [adult, senior, senior, adult]         adult            adult                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "53    048A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A           [kitten, senior, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, kitten, adult, adult, ad...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, adult,...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A    [adult, senior, senior, senior, senior, kitten]        senior           senior                   True\n",
       "101   106A  [adult, senior, senior, senior, senior, adult,...        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "98    103A  [adult, adult, adult, adult, adult, senior, se...         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, kitten, ...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, senior, senior, senior...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, adult, senior, senior, adult,...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "24    022A  [kitten, kitten, adult, adult, adult, adult, k...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, senior, senior, adult, adult, a...         adult            adult                   True\n",
       "20    019A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, adult, s...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C              [senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "2     002A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "43    037A  [senior, senior, adult, adult, adult, adult, a...         adult            adult                   True\n",
       "41    035A                     [senior, adult, senior, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "106   113A                             [adult, senior, adult]         adult           senior                  False\n",
       "71    065A  [senior, adult, kitten, adult, adult, senior, ...        senior            adult                  False\n",
       "42    036A  [adult, senior, senior, senior, senior, senior...        senior            adult                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "6     005A  [senior, senior, senior, adult, senior, senior...        senior            adult                  False\n",
       "7     006A                            [senior, senior, adult]        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "99    104A                     [adult, senior, adult, senior]         adult           senior                  False\n",
       "56    051A  [adult, senior, adult, adult, senior, senior, ...         adult           senior                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "57    051B  [senior, adult, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "12    011A                                    [adult, senior]         adult           senior                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "74    068A  [senior, senior, senior, senior, senior, adult...        senior            adult                  False\n",
       "90    095A  [senior, adult, senior, senior, senior, senior...        senior            adult                  False\n",
       "34    027A  [adult, senior, senior, senior, senior, senior...        senior            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "36    029A  [senior, adult, adult, adult, senior, senior, ...        senior            adult                  False\n",
       "18    016A  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "64    058A                             [senior, adult, adult]         adult           senior                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "109   117A  [senior, senior, adult, adult, adult, adult, a...         adult           senior                  False"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "994531fd-e6fb-4491-9eab-86e41ac1ed3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     58\n",
      "kitten    14\n",
      "senior    10\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "229af3e2-6a64-4f3d-a594-e04108fce87f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             58  79.452055\n",
      "1           kitten           15             14  93.333333\n",
      "2           senior           22             10  45.454545\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "3960450a-db52-4464-a965-00f1e600bda5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABnUElEQVR4nO3dd3iN9//H8edJRMgQEYLYW1M1gxStPWtWqzpUqaC2qm9bVbSotkbtUatWrdbeWmomVO2KmCERu1YGMs7vj1y5fzmSkEXCeT2uy3Xl3Pd97vt9H+c+53U+9+f+3Caz2WxGRERERMRK2GR0ASIiIiIiz5ICsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWETkORYVFZXRJaS7F3GfRCRzyZLRBYgkV0REBE2aNCEsLAyAMmXKsGjRogyuStLi7NmzTJkyhSNHjhAWFkauXLmoXbs2n3/+eZLP8fLysnicI0cO/vjjD2xsLH/P//DDDyxfvtxi2tChQ2nRokWqaj1w4ADdu3cHIH/+/KxduzZV60mJYcOGsW7dOgB8fHzo1q2bxfwtW7awfPlyZs6cma7bffjwIY0bN+bevXsAfPTRR/Tq1SvJ5Zs3b86VK1cA6NKli/E6pdS9e/f4+eefyZkzJx9//HGq1pHe1q5dyzfffANA5cqV+fnnnzO0nm+++cbivbd48WJKlSqVgRUl3507d1i/fj3bt2/n0qVL3Lp1iyxZspAnTx7KlStH8+bNqVatWkaXKVZCLcDy3Ni6dasRfgECAgL4999/M7AiSYvIyEh69OjBzp07uXPnDlFRUVy7do2rV6+maD13797F398/wfT9+/enV6mZzo0bN/Dx8WHQoEFG8ExPWbNmpX79+sbjrVu3Jrns8ePHLWpo2rRpqra5fft23nzzTRYvXqwW4CSEhYXxxx9/WExbsWJFBlWTMrt376Zdu3aMGzeOQ4cOce3aNSIjI4mIiODixYts2LCBHj16MGjQIB4+fJjR5YoVUAuwPDdWr16dYNrKlSt5+eWXM6AaSauzZ89y8+ZN43HTpk3JmTMn5cuXT/G69u/fb/E+uHbtGhcuXEiXOuPky5ePjh07AuDs7Jyu605KrVq1cHNzA6BixYrG9MDAQA4dOvRUt92kSRNWrVoFwKVLl/j3338TPdb+/PNP429PT0+KFCmSqu3t2LGDW7dupeq51mLr1q1ERERYTNu4cSN9+/YlW7ZsGVTVk23bto3//e9/xmMHBweqV69O/vz5uX37Nvv27TM+C7Zs2YKjoyNfffVVRpUrVkIBWJ4LgYGBHDlyBIg95X337l0g9sOyf//+ODo6ZmR5kgrxW/Pd3d0ZPnx4iteRLVs27t+/z/79++nUqZMxPX7rb/bs2ROEhtQoWLAgvXv3TvN6UqJBgwY0aNDgmW4zTpUqVcibN6/RIr9169ZEA/C2bduMv5s0afLM6rNG8RsB4j4HQ0ND2bJlCy1btszAypIWHBxsdCEBqFatGiNHjsTV1dWY9vDhQ4YPH87GjRsBWLVqFR988EGqf0yJJIcCsDwX4n/wv/322/j5+fHvv/8SHh7Opk2baNu2bZLPPXnyJAsWLODgwYPcvn2bXLlyUaJECdq3b0+NGjUSLB8aGsqiRYvYvn07wcHB2NnZ4eHhQaNGjXj77bdxcHAwln1cH83H9RmN68fq5ubGzJkzGTZsGP7+/uTIkYP//e9/1K9fn4cPH7Jo0SK2bt1KUFAQDx48wNHRkWLFitG2bVveeOONVNfeuXNnjh49CkC/fv344IMPLNazePFixo4dC8S2Qo4fPz7J1zdOVFQUa9euZcOGDZw/f56IiAjy5s1LzZo16dChA+7u7sayLVq04PLly8bja9euGa/JmjVr8PDweOL2AMqXL8/+/fs5evQoDx48wN7eHoC///7bWKZChQr4+fkl+vwbN24we/ZsfH19uXbtGtHR0eTMmRNPT086depk0RqdnD7AW7ZsYc2aNZw+fZp79+7h5uZGtWrV6NChA0WLFrVYdsaMGUbf3S+++IK7d+/y66+/EhERgaenp/G+ePT9FX8awOXLl/Hy8iJ//vx89dVXRl9dFxcXNm/eTJYs//8xHxUVRZMmTbh9+zYA8+fPx9PTM9HXxmQy0bhxY+bPnw/EBuC+fftiMpmMZfz9/bl06RIAtra2NGrUyJh3+/Ztli9fzrZt2wgJCcFsNlOkSBEaNmxIu3btLFosH+3XPXPmTGbOnJngmPrjjz9YtmwZAQEBREdHU6hQIRo2bMh7772XoAU0PDycBQsWsGPHDoKCgnj48CFOTk6UKlWKVq1apbqrxo0bN5g4cSK7d+8mMjKSMmXK0LFjR1577TUAYmJiaNGihfHD4YcffrDoTgIwduxYFi9eDMR+nj2uz3ucs2fPcuzYMeD/z0b88MMPQOyZsMcF4ODgYKZPn46fnx8RERGULVsWHx8fsmXLRpcuXYDYftzDhg2zeF5KXu+kzJs3z/ixmz9/fsaMGWPxGQqxXW6++uor/vvvP9zd3SlRogR2dnbG/OQcK3GOHTvGsmXLOHz4MDdu3MDZ2Zly5crRrl07vL29Lbb7pGM6/ufU9OnTjfdp/GPwp59+wtnZmZ9//pnjx49jZ2dHtWrV6NmzJwULFkzWayQZQwFYMr2oqCjWr19vPG7RogX58uUz+v+uXLkyyQC8bt06hg8fTnR0tDHt6tWrXL16lb1799KrVy8++ugjY96VK1f45JNPCAoKMqbdv3+fgIAAAgIC+PPPP5k+fXqCD/DUun//Pr169SIkJASAmzdvUrp0aWJiYvjqq6/Yvn27xfL37t3j6NGjHD16lODgYItwkJLaW7ZsaQTgLVu2JAjA8ft8Nm/e/In7cfv2bQYMGGC00se5ePEiFy9eZN26dYwePTpB0EmrKlWqsH//fh48eMChQ4eML7gDBw4AULhwYXLnzp3oc2/dukXXrl25ePGixfSbN2+ya9cu9u7dy8SJE6levfoT63jw4AGDBg1ix44dFtMvX77M6tWr2bhxI0OHDqVx48aJPn/FihWcOnXKeJwvX74nbjMx1apVI1++fFy5coU7d+7g5+dHrVq1jPkHDhwwwm/x4sWTDL9xmjZtagTgq1evcvToUSpUqGDMj9/9oWrVqsZr7e/vz4ABA7h27ZrF+vz9/fH392fdunVMmjSJvHnzJnvfEruo8fTp05w+fZo//viDadOm4eLiAsS+77t06WLxmkLsRVgHDhzgwIEDBAcH4+Pjk+ztQ+x7o2PHjhb91A8fPszhw4f59NNPee+997CxsaF58+bMnj0biD2+4gdgs9ls8bol96LM+I0AzZs3p2nTpowfP54HDx5w7Ngxzpw5Q8mSJRM87+TJk3zyySfGBY0AR44coXfv3rRp0ybJ7aXk9U5KTEyMxRmCtm3bJvnZmS1bNqZMmfLY9cHjj5U5c+Ywffp0YmJijGn//fcfO3fuZOfOnbz77rsMGDDgidtIiZ07d7JmzRqL75itW7eyb98+pk+fTunSpdN1e5J+dBGcZHq7du3iv//+A6BSpUoULFiQRo0akT17diD2Az6xi6DOnTvHyJEjjQ+mUqVK8fbbb1u0AkyePJmAgADj8VdffWUESCcnJ5o3b06rVq2MLhYnTpxg2rRp6bZvYWFhhISE8Nprr9GmTRuqV69OoUKF2L17txF+HR0dadWqFe3bt7f4MP31118xm82pqr1Ro0bGF9GJEycIDg421nPlyhWjpSlHjhy8/vrrT9yPb775xgi/WbJkoW7durRp08YIOPfu3eOzzz4zttO2bVuLMOjo6EjHjh3p2LEjTk5OyX79qlSpYvwd1+p74cIFI6DEn/+oX375xQi/BQoUoH379rz55ptGiIuOjmbJkiXJqmPixIlG+DWZTNSoUYO2bdsap3AfPnzI0KFDjdf1UadOnSJ37ty0a9eOypUrJxmUIbZFPrHXrm3bttjY2FgEqi1btlg8N6U/bEqVKkWJEiUSfT4k3v3h3r17DBw40Ai/OXPmpEWLFjRu3Nh4z507d45PP/3UuNitY8eOFtupUKECHTt2NPo9r1+/3ghjJpOJ119/nbZt2xpnFU6dOsWPP/5oPH/Dhg1GSHJ1daVly5a89957FiMMzJw50+J9nxxx761atWrx5ptvWgT4CRMmEBgYCMSG2riW8t27dxMeHm4sd+TIEeO1Sc6PEIi9YHTDhg3G/jdv3hwnJyeLYJ3YxXAxMTF8/fXXRvi1t7enadOmNGvWDAcHhyQvoEvp652UkJAQ7ty5YzyO3489tZI6VrZt28bUqVON8Fu2bFnefvttKleubDx38eLFLFy4MM01xLdy5Urs7Oxo2rQpTZs2Nc5C3b17l8GDB1t8RkvmohZgyfTit3zEfbk7OjrSoEED45TVihUrElw0sXjxYiIjIwGoU6cO33//vXE6eMSIEaxatQpHR0f2799PmTJlOHLkiBHiHB0dWbhwoXEKq0WLFnTp0gVbW1v+/fdfYmJiEgy7lVp169Zl9OjRFtOyZs1K69atOX36NN27d+fVV18FYlu2GjZsSEREBGFhYdy+fRtXV9cU1+7g4ECDBg1Ys2YNEBuUOnfuDMSe9oz70G7UqBFZs2Z9bP1Hjhxh165dQOxp8GnTplGpUiUgtktGjx49OHHiBKGhocyaNYthw4bx0UcfceDAATZv3gzEBu3U9K8tV66cRT9gsOz+UKVKlSS7PxQqVIjGjRtz8eJFJkyYQK5cuYDYVs+4lsG40/uPc+XKFYuWsuHDhxth8OHDh3z++efs2rWLqKgoJk2alOQwWpMmTUrWcFYNGjQgZ86cSb52LVu2ZNasWZjNZnbs2GF0DYmKiuKvv/4CYv+fmjVr9sRtQezrMXnyZCD2vfHpp59iY2PDqVOnjB8Q9vb21K1bF4Dly5cbo0J4eHgwZ84c40dFYGAgHTt2JCwsjICAADZu3EiLFi3o3bs3N2/e5OzZs0BsS3b8sxvz5s0z/v7iiy+MMz49e/akffv2XLt2ja1bt9K7d2/y5ctn8f/Ws2dPWrdubTyeMmUKV65coVixYhatdsn1v//9j3bt2gGxIadz584EBgYSHR3N6tWr6du3LwULFsTLy4u///6bBw8esHPnTuM9Ef9HRGLdmBKzY8cOo+U+rhEAoFWrVkYw3rhxI3369LHomnDgwAHOnz8PxP6f//zzz0Y/7sDAQN5//30ePHiQYHspfb2TEv8iV8A4xuLs27ePnj17JvrcxLpkxEnsWIl7j0LsD+zPP//c+IyeO3eu0bo8c+ZMWrdunaIf2o9ja2vLrFmzKFu2LABvvfUWXbp0wWw2c+7cOfbv35+ss0jy7KkFWDK1a9eu4evrC8RezBT/gqBWrVoZf2/ZssWilQX+/zQ4QLt27Sz6Qvbs2ZNVq1bx119/0aFDhwTLv/766xb9typWrMjChQvZuXMnc+bMSbfwCyTa2uft7c3gwYOZN28er776Kg8ePODw4cMsWLDAokUh7ssrNbU/+vrFiT/MUnJaCeMv36hRIyP8QmxLdPzxY3fs2GFxejKtsmTJYvTTDQgI4M6dOxYXwD2uy8Vbb73FyJEjWbBgAbly5eLOnTvs3r3bortNYuHgUdu2bTP2qWLFihYXgmXNmtXilOuhQ4eMIBNf8eLF020s1/z58xstnWFhYezZsweIvTAwrjWuevXqSXYNeVSTJk2M1swbN25w8OBBwLL7w+uvv26caYj/fujcubPFdooWLUr79u2Nx4928UnMjRs3OHfuHAB2dnYWYTZHjhzUrl0biG3tjPvxExdGAEaPHs1nn33G0qVLje4Aw4cPp3Pnzim+yMrFxcWiu1WOHDl48803jcfHjx83/o5/fMX9WInfJcDW1jbZAfjR7g9xKleuTKFChYDYlvdHh0iL3yXp1VdftbiIsWjRoon+CErN652UuNbQOKn5wfGoxI6VgIAA48dYtmzZ6NOnj8Vn9Icffkj+/PmB2GPiSXWnRN26dS3ebxUqVDAaLIAE3cIk81ALsGRqa9euNT40bW1t+eyzzyzmm0wmzGYzYWFhbN682aJPW/z+h3EffnFcXV0trkJ+0vJg+aWaHMk99ZXYtiC2ZXHFihX4+fkZF6E8Ki54pab2ChUqULRoUQIDAzlz5gznz58ne/bsxpd40aJFKVeu3BPrj9/nOLHtxJ9279497ty5k+C1T4u4fsBxX8j//PMPAEWKFHliyDt+/DirV6/mn3/+SdAXGEhWWH/S/hcsWBBHR0fCwsIwm81cunSJnDlzWiyT1HsgtVq1asW+ffuA2BbHevXqpbj7Q5x8+fJRqVIlI/hu3boVLy8vi+4P8YNUSt4PyemCEH+M4cjIyMe2psW1djZo0MD4MfPgwQP++usvo/U7R44c1KlThw4dOlCsWLEnbj++AgUKYGtrazEt/sWN8Vs869ati7OzM/fu3cPPz4979+5x+vRprl+/DiT/R8iVK1eM/0uIHSFh06ZNxuP79+8bf69YscLi/zZuW0CiYT+x/U/N652UR/t4X7161WKbHh4extCCENtdJO4sQFISO1biv+cKFSqUYFQgW1tbSpUqZVzQFn/5x0nO8Z/Y61q0aFH27t0LJGwFl8xDAVgyLbPZbJyih9jT6Y+7ucHKlSuTvKgjpS0PqWmpeDTwxnW/eJLEhnCLu0glPDwck8lExYoVqVy5MuXLl2fEiBEWX2yPSkntrVq1YsKECUBsK3D8C1SSG5Lit6wn5tHXJf4oAukhfj/fhQsXGq2cj+v/C7FdZMaNG4fZbCZbtmzUrl2bihUrki9fPr788stkb/9J+/+oxPY/vYfxq1OnDi4uLty5c4ddu3Zx9+5do4+ys7Oz0YqXXE2aNDEC8LZt22jbtq0RflxcXCxavFL6fniS+CHExsbmsT+e4tZtMpn45ptvaNOmDRs3bsTX19e40PTu3busWbOGjRs3Mn36dIuL+p4ksRt0xD/e4u+7vb09TZo0Yfny5URGRrJ9+3aLaxWS2/q7du1ai9cg7uLVxBw9epSzZ88a/anjv9bJPfOSmtc7Ka6urhQoUMDoknLgwAGLazAKFSpk0X0nfjeYpCR2rCTnGIxfa2LHYGKvT3JuyJLYTTvij2CR3p93kn4UgCXT+ueff5LVBzPOiRMnCAgIoEyZMkDs2LJxv/QDAwMtWmouXrzI77//TvHixSlTpgxly5a1GKYrsZsoTJs2DWdnZ0qUKEGlSpXIli2bxWm2+C0xQKKnuhMT/8Myzrhx44wuHfH7lELiH8qpqR1iv4SnTJlCVFSUMQA9xH7xJbePaPwWmfgXFCY2LUeOHE+8cjylXn75ZaMfcPxT0I8LwHfv3mXSpEmYzWbs7OxYtmyZMfRa3Onf5HrS/gcHBxvDQNnY2FCgQIEEyyT2HkiLrFmz0rRpU5YsWcL9+/cZPXq0MXZ2w4YNE5yafpIGDRowevRoIiMjuXXrlsUFUA0bNrQIIPnz5zcuugoICEjQChz/NSpcuPATtx3/vW1nZ8fGjRstjrvo6OgErbJxihYtysCBA8mSJQtXrlzh8OHD/Pbbbxw+fJjIyEhmzZrFpEmTnlhDnODgYO7fv2/Rzzb+mYNHW3RbtWpl9A/ftGmTEe6cnJyoU6fOE7dnNptTfMvtlStXGmfK8uTJk2idcc6cOZNgWlpe78Q0adLEGBEjbnzfR8+AxElOSE/sWIl/DAYFBREWFmYRlKOjoy32Na7bSPz9ePTzOyYmxjhmHiex1zD+ax3//0AyF/UBlkwr7i5UAO3btzeGL3r0X/wru+Nf1Rw/AC1btsyiRXbZsmUsWrSI4cOHGx/O8Zf39fW1aIk4efIks2fPZvz48fTr18/41Z8jRw5jmUeDU/w+ko+TWAvB6dOnjb/jf1n4+vpa3C0r7gsjNbVD7EUpceOXXrhwgRMnTgCxFyHF/yJ8nPijRGzevJnDhw8bj8PCwiyGNqpTp066t4jY2dkleve4xwXgCxcuGK+Dra2txZ3d4i4qguR9Icff/0OHDll0NYiMjOSnn36yqCmxHwApfU3if3En1UoVvw9q3A0GIGXdH+LkyJGDmjVrGo/j/x8/evOL+K/HnDlzuHHjhvH4woULLF261Hgcd+EcYBGy4u9Tvnz5jB8NDx484PfffzfmRURE0Lp1a1q1akX//v2NMPL111/TqFEjGjRoYHwm5MuXjyZNmvDWW28Zz0/pbbfjxhaOExoaanEB5KOjHJQtW9b4Qb5//37jdHhyf4Ts27fPaLl2cXHBz88v0c/A+DeR2bBhg9F3PX5/fF9fX+P4htjRFOJ3pYiTmtf7cdq1a2d8ht2+fZv+/fsnGB7v4cOHzJ07N8GoJYlJ7FgpXbq0EYLv37/P5MmTLVp8FyxYYHR/cHJyomrVqoDlHR3v3r1r8V7dsWNHss7ixf2fxDlz5ozR/QEs/w8kc1ELsGRK9+7ds7hA5nF3w2rcuLHRNWLTpk3069eP7Nmz0759e9atW0dUVBT79+/n3XffpWrVqly6dMniA+qdd94BYr+8ypcvb9xUoVOnTtSuXZts2bJZhJpmzZoZwTf+xRh79+5l1KhRlClThh07dhgXH6VG7ty5jS++QYMG0ahRI27evMnOnTstlov7oktN7XFatWqV4GKklISkKlWqUKlSJQ4dOkR0dDTdu3fn9ddfx8XFBV9fX6NPobOzc4rHXU2uypUrW3SPeVL/3/jz7t+/T6dOnahevTr+/v4Wp5iTcxFcwYIFadq0qREyBw0axLp168ifPz8HDhwwhsays7OzuCAwLeK3bl2/fp2hQ4cCWNxxq1SpUnh6elqEnsKFC6fqVtMQG3Tj+tHGKVCgQILQ99Zbb/H7779z69YtLl26xLvvvkutWrWIiopix44dxpkNT09Pi/Acf5/WrFlDaGgopUqV4s033+S9994zRkr54Ycf2LVrF4ULF2bfvn1GsImKijL6Y5YsWdL4/xg7diy+vr4UKlTIGBM2Tkq6P8SZMWMGR48epWDBguzdu9c4S2Vvb5/ozShatWqVYMiw5B5f8S9+q1OnTpKn+mvXro29vT0PHjzg7t27/PHHH7zxxhtUqVKF4sWLc+7cOWJiYujatSv16tXDbDazffv2RE/fAyl+vR/Hzc2NwYMH8/nnnxMdHc2xY8do06YNNWrUIH/+/Ny6dQtfX98EZ8xS0i3IZDLx8ccfM2LECCB2JJLjx49Trlw5zp49a3TfAejWrZux7sKFCxuvm9lspl+/frRp04aQkJBkD4FoNpvp3bs3derUIVu2bGzbts343ChdurTFMGySuagFWDKljRs3Gh8iefLkeewXVb169YzTYnEXw0Hsl+CXX35ptJYFBgayfPlyi/DbqVMni5ECRowYYbR+hIeHs3HjRlauXEloaCgQewVyv379LLYd/5T277//znfffceePXt4++23U73/cSNTQGzLxG+//cb27duJjo62GL4n/sUcKa09zquvvmpxms7R0TFZp2fj2NjYMGrUKF566SUg9otx27ZtrFy50gi/OXLkYOzYsel+sVecR0d7eFL/3/z581v8qAoMDGTp0qUcPXqULFmyGKe479y5k6zToF9++aXRt9FsNrNnzx5+++03I/za29szfPjwRG8lnBrFihWzaElev349GzduTNAa/GggS03rb5zXXnstQShJbAST3Llz8+OPP+Lm5gbE3nBk7dq1bNy40Qi/JUuWZMyYMRYt2fGD9M2bN1m+fLlxBf3bb79tsa29e/eyZMkSox+yk5MTP/zwg/E58MEHH9CwYUMg9vT3rl27+PXXX9m0aZNRQ9GiRenRo0eKXoOGDRvi5uaGr68vy5cvN8KvjY0NX3zxRaJDgsUfGxZiQ1dygvedO3csbqzyuEYABwcHi5b3lStXGnUNHz7c+H+7f/8+GzZsYOPGjcTExBivEVi2rKb09X6SOnXqMGXKFOM98eDBA7Zv386vv/7Kxo0bLcKvs7Mz3bp1o3///slad5zWrVvz0UcfGfvh7+/P8uXLLcLv+++/z7vvvms8zpo1q9EAArFny0aNGsW8efPImzevxdnFpHh5eWFjY8PWrVtZu3at0d3JxcUlVbd3l2dHAVgypfgtH/Xq1XvsKWJnZ2eLWxrHffhDbOvL3LlzjS8uW1tbcuTIQfXq1RkzZkyCMSg9PDxYsGABnTt3plixYtjb22Nvb0+JEiXo2rUr8+bNswge2bNnZ9asWTRt2pScOXOSLVs2ypUrx4gRIxINm8n19ttv8/333+Pp6YmDgwPZs2enXLlyDB8+3GK98btZpLT2OLa2thbBrEGDBsm+zWmc3LlzM3fuXL788ksqV66Mi4sLWbNmpVChQrz77rssXbr0qbaExPUDjvOkAAzw7bff0qNHD4oWLUrWrFlxcXGhVq1azJo1yzg1bzabjdEOHr04KD4HBwcmTZrEiBEjqFGjBm5ubtjZ2ZEvXz5atWrFr7/++tgAk1J2dnaMHj0aT09P7OzsyJEjB15eXglarOO39ppMpmT3606Mvb099erVs5iW1O2EK1WqxJIlS/Dx8aF06dLGe/ill16ib9++/PLLLwm62NSrV49u3brh7u5OlixZyJs3r9HCaGNjw4gRIxg+fDhVq1a1eH+9+eabLFq0yGLEEltbW0aOHMmPP/6It7c3+fPnJ0uWLDg6OvLSSy/RvXt35s+fn+LRSDw8PFi0aBEtWrQwjvfKlSszefLkJO/o5uzsbNFSmtz/g40bNxottC4uLsZp+6TED6yHDx82wmqZMmWYN28edevWJUeOHGTPnp3q1aszZ84ciyAed2MhSPnrnRxeXl78/vvvDBgwgGrVqpErVy5sbW1xdHSkcOHCNGnShGHDhrFhwwZ8fHxSfHEpQK9evZg1axbNmjUjf/782NnZ4erqyuuvv87UqVMTDdW9e/emX79+FClShKxZs5I/f346dOjA/Pnzk3W9QqVKlZg9ezZVq1YlW7ZsuLi4GLcQj39zF8l8TGbdpkTEql28eJH27dsbX7YzZsxIVoC0Nr/88osx2H6JEiUs+rJmVt9++60xkkqVKlWYMWNGBldkfQ4ePEjXrl2B2B8hq1evNi64fNquXLnCxo0byZkzJy4uLlSqVMki9H/zzTfGRXb9+vVLcEt0SdywYcNYt24dAD4+PhY3bZHnh/oAi1ihy5cvs2zZMqKjo9m0aZMRfkuUKKHw+4hNmzYxevRoi1u6Pq2uHOnht99+49q1a5w8edKiu09auuRIypw8eZKtW7cSHh5ucWOVmjVrPrPwC7FnMOJfhFqoUCFq1KiBjY0NZ86cMW4IYTKZqFWr1jOrSyQzyLQB+OrVq7zzzjuMGTPGon9fUFAQ48aN49ChQ9ja2tKgQQN69+5t0S8yPDycSZMmsW3bNsLDw6lUqRKffvqpxTBYItbMZDJZXM0OsafVBw4cmEEVZV7//vuvRfiF2DveZVYnTpywGD8bYu8sWL9+/QyqyPpERERY3E4YYvvN9u3b95nWkT9/ftq0aWN0CwsKCkr0zMV7772n70exOpkyAF+5coXevXsbF+/EuXfvHt27d8fNzY1hw4Zx69YtJk6cSEhIiMVYjl999RXHjx+nT58+ODo6MnPmTLp3786yZcsSXAEvYo3y5MlDoUKFuHbtGtmyZaNMmTJ07tz5sbcOtmYuLi6Eh4fj4eHBO++8k6a+tE9b6dKlyZkzJxEREeTJk4cGDRrQpUsXDcj/DHl4eJAvXz7+++8/nJ2dKVeuHF27dk3xnefSw6BBg6hQoQKbN2/m9OnTxgVnLi4ulClThtatWyfo2y1iDTJVH+CYmBjWr1/P+PHjgdirYKdPn258Kc+dO5fZs2ezbt06Y1zBPXv20LdvX2bNmkXFihU5evQonTt3ZsKECca4lbdu3aJly5Z89NFHfPzxxxmxayIiIiKSSWSqUSBOnz7NqFGjeOONNyzGs4zj6+tLpUqVLG4M4O3tjaOjozHmqq+vL9mzZ7e43aKrqyuVK1dO07isIiIiIvJiyFQBOF++fKxcuZJPP/000WGYAgMDE9w609bWFg8PD+P2r4GBgRQoUCDBrRoLFSqU6C1iRURERMS6ZKo+wC4uLo8ddy80NDTRu8M4ODgYg08nZ5mUCggIMJ6b3IG/RUREROTZioyMxGQyPfE21JkqAD9J/IHoHxU3MH1ylkmNuK7SSd06UkRERESeD89VAHZycjJuYxlfWFiYcVchJycn/vvvv0SXiT9UWkqUKVOGY8eOYTabKVmyZKrWISLytMTExLB06VLWrFnD9evXyZcvH23atKFt27bGMsePH+fnn38mICCA7NmzU7duXXx8fHBwcHjsujdu3MiSJUu4dOkSefPmNdarUSVEJDM6c+ZMsj6fnqsAXKRIEYKCgiymRUdHExISYty6tEiRIvj5+RETE2PR4hsUFJTmcQ5NJtMTvyxERJ61sWPHsnjxYtq2bUvdunUJDg5m2rRp3Lhxg/79+3P69Gn69+9PtWrVGD16NNevX2fy5MlcunTJuLtdYlatWsWoUaP48MMP8fb25vjx40yZMoWoqCg6d+78DPdQRCR5kvvj/LkKwN7e3syfP59bt24Zt3P08/MjPDzcGPXB29ubOXPm4OvrazEM2qFDh+jUqVOG1S4i8jTcvn2bZcuW0bp1a7788ktjet68eRkwYABt2rTh119/xcXFhR9//NHiOoZvvvmGwMDAJBsH5s6dS/369enTpw8A1apV4+LFiyxdulQBWESea89VAH7rrbdYunQpPXv2xMfHhzt37jBx4kRq1KhBhQoVAKhcuTJVqlTh66+/pk+fPri4uPDzzz/j7OzMW2+9lcF7ICKSvi5cuEB0dDSvvfaaxXQvLy9iYmLYu3cvn3zyCe+9955F+I37+3HXNYwfPx57e3uLaXZ2droWQkSee89VAHZ1dWX69OmMGzeOwYMH4+joSP369enXr5/FcqNHj+ann35iwoQJxMTEUKFCBUaNGqW7wInICyduXPTLly9bTA8ODgbg0qVLuLu7G9dJREREcPToUaZMmUKFChUoXbp0kuuOu3OZ2Wzm7t27bN++nfXr1/P+++8/hT0REXl2MtWd4DKzY8eOAfDKK69kcCUiIpa6dOnCuXPn+Prrr6latSrBwcF89913nDlzhiZNmjBkyBAgNsjWqlWLBw8e4OLiwpQpUyhbtuwT1x93h00AT09PJk2a9NghK0VEMkpy81qmuhGGiIik3A8//EClSpUYOHAgderU4ZNPPqFNmza4uLhY3FQoOjqasWPHMm7cOIoUKULXrl05derUE9efP39+ZsyYwdChQ7lx4wadO3fm/v37T3OXRESequeqC4SIiCTk5ubG2LFjuXfvHtevX6dgwYLY2NgwatQoi5baLFmyGBcMV65cmRYtWrB48WKGDh362PXnyZOHPHnyUKVKFQoUKEDXrl35448/aN68+VPdLxGRp0UtwCIiz7nNmzdz+vRpnJ2dKV68OFmzZuXUqVPExMRQpkwZdu7cycGDBy2e4+TkRMGCBblx40ai6wwPD2fTpk0Jhp6M6zKR1PNERJ4HCsAiIs+52bNnM3fuXItpv/76K05OTnh5efHrr7/y/fffEx0dbcy/evUq586dS/LmPra2tgwfPpz58+dbTPfz8wPQTYFE5LmmLhAiIs+59u3bM2rUKEqUKEGFChXYvHkzmzZt4osvvsDJyYkuXbrQs2dPvvzyS958801u3brFrFmzyJEjBx988IGxnmPHjuHq6krBggWxt7enU6dOzJgxg1y5cuHl5cWpU6eYOXMm1apVM8ZZFxF5HmkUiGTSKBAikpktXryYpUuXcuPGDYoUKUKHDh1o0qSJMf/AgQNMnz6d06dPY2try6uvvkrv3r3Jly+fsYyXlxfNmzdn2LBhQOyoEb///jvLli3j0qVL5MyZkyZNmtC1a9cE4wOLiGQGyc1rCsDJpAAsIiIikrlpGDQRERERkUQoAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIikQo6HTMy3934hIculWyCIiKWBjMrHE7xTX7oZndCkSj3sOB9p7l87oMkTkOaEALCKSQtfuhhNyKyyjyxARkVRSFwgRERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVyZLRBYgAHDhwgO7duyc5v2vXrnTt2pVdu3Yxc+ZMzpw5Q86cOalfvz6ffPIJDg4Oj13/xx9/zJEjRxJMnz9/Pp6enmmuX0RERJ4fCsCSKZQtW5a5c+cmmD5t2jT+/fdfGjduzPbt2/nf//5HlSpVGDVqFJGRkcyePZtPPvmE2bNnkyVL4m9ns9nMmTNneP/992nQoIHFvGLFij2V/REREZHMSwFYMgUnJydeeeUVi2k7duxg//79fP/99xQpUoQvvviCYsWKMWnSJOzs7ACoVKkSrVu3Zu3atbRp0ybRdQcHBxMWFkbNmjUTbENERESsj/oAS6Z0//59Ro8eTa1atYxW2/Pnz+Pt7W2EXwA3NzeKFSvG7t27k1xXQEAAAKVLl366RYuIiMhzQS3AkiktWbKE69evM23aNGNazpw5uXz5ssVyUVFRXLlyhYcPHya5rlOnTuHg4MCECRPYuXMnEREReHl58emnn1K0aNGntQsiIiKSSakFWDKdyMhIFi9eTKNGjShUqJAxvWXLlmzfvp1ffvmFW7duceXKFb799ltCQ0OJiIhIcn2nTp0iPDwcZ2dnxowZw+DBgwkKCsLHx4fr168/i10SERGRTEQtwJLp/Pnnn9y8eZMOHTpYTO/atSvR0dFMnz6dyZMnkyVLFtq0aUPt2rU5d+5ckuvr0aMHH374IZUrVwZi+w2XL1+et99+m8WLF9OnT5+nuj8iIiKSuSgAS6bz559/Urx48QR9drNkyULv3r3p2rUrly5dIk+ePDg7O+Pj44OLi0uS60us72/BggUpVqwYp0+fTvf6RUREJHNTFwjJVKKiovD19aVhw4YJ5h04cABfX1/s7e0pXrw4zs7OREVFcebMGcqUKZPk+tatW8fRo0cTzLt//z45c+ZM710QERGRTE4BWDKVM2fOcP/+fSpUqJBg3p9//smIESOIiooypq1Zs4Z79+5Rp06dRNeXJUsWZs6cyYQJEyymnzx5kuDgYLy8vNK1fhEREcn8FIAlUzlz5gwAxYsXTzCvbdu2/PfffwwbNoz9+/ezcOFCfvzxRxo2bEiVKlWM5U6ePGnRJ9jHx4cjR44wZMgQ/Pz8WLVqFf369aN06dI0b9786e+UiIiIZCrqAyyZys2bNwFwdnZOMK9kyZL89NNPTJkyhf79+5M7d246d+5M586dLZYbOHAg+fPn5+effwagefPm2NvbM3/+fD777DOyZ89OnTp16NWrF7a2tk9/p0RERCRTMZnNZnNGF5FSK1euZPHixYSEhJAvXz7atWvH22+/jclkAiAoKIhx48Zx6NAhbG1tadCgAb1798bJySnV2zx27BiA7iQmIkzccpiQW2EZXYbE4+HqSJ9GFTO6DBHJYMnNa89dC/CqVasYOXIk77zzDrVr1+bQoUOMHj2ahw8f8sEHH3Dv3j26d++Om5sbw4YN49atW0ycOJGQkBAmTZqU0eWLiIiISAZ77gLwmjVrqFixIgMHDgSgWrVqXLhwgWXLlvHBBx/w22+/cefOHRYtWmRc4e/u7k7fvn05fPgwFStWzLjiRURERCTDPXcXwT148ABHR0eLaS4uLty5cwcAX19fKlWqZDG8lbe3N46OjuzZs+dZlioiIiIimdBzF4Dfffdd/Pz82LBhA6Ghofj6+rJ+/XqaNWsGQGBgIIULF7Z4jq2tLR4eHly4cCEjShYRERGRTOS56wLRuHFj/vnnH4YMGWJMe/XVVxkwYAAAoaGhCVqIARwcHAgLS9tFK2azmfDw8DStQ0SeXyaTiezZs2d0GfIYERERPIfXdotIOjGbzcagCI/z3AXgAQMGcPjwYfr06cPLL7/MmTNn+Pnnn/n8888ZM2YMMTExST7XxiZtDd6RkZH4+/unaR0i8vzKnj07np6eGV2GPMb58+eJiIjI6DJEJANlzZr1ics8VwH4yJEj7N27l8GDB9O6dWsAqlSpQoECBejXrx+7d+/Gyckp0VbasLAw3N3d07R9Ozs7SpYsmaZ1ZBbJ+XUkGUutWJmPjpvMr1ixYjp2RKxY3A21nuS5CsCXL18GSHCb3MqVKwNw9uxZihQpQlBQkMX86OhoQkJCqFu3bpq2bzKZcHBwSNM6MosYsxkbfZlnWvr/EUkddVERsW7Jbah4rgJw0aJFATh06BDFihUzph85cgSAggUL4u3tzfz587l16xaurq4A+Pn5ER4ejre39zOvObOyMZlY4neKa3fVpzmzcc/hQHvv0hldhoiIyAvruQrAZcuWpV69evz000/cvXuXcuXKce7cOX7++Wdeeukl6tSpQ5UqVVi6dCk9e/bEx8eHO3fuMHHiRGrUqJGg5djaXbsbrrtZiYiIiNV5rgIwwMiRI5k9ezYrVqxgxowZ5MuXjxYtWuDj40OWLFlwdXVl+vTpjBs3jsGDB+Po6Ej9+vXp169fRpcuIiIiIpnAcxeA7ezs6N69O927d09ymZIlSzJ16tRnWJWIiIiIPC+euxthiIiIiIikhQKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsSpa0PDk4OJirV69y69YtsmTJQs6cOSlevDg5cuRIr/pERERERNJVigPw8ePHWblyJX5+fly/fj3RZQoXLsxrr71GixYtKF68eJqLFBERERFJL8kOwIcPH2bixIkcP34cALPZnOSyFy5c4OLFiyxatIiKFSvSr18/PD09016tiIiIiEgaJSsAjxw5kjVr1hATEwNA0aJFeeWVVyhVqhR58uTB0dERgLt373L9+nVOnz7NyZMnOXfuHIcOHaJTp040a9aMoUOHPr09ERERERFJhmQF4FWrVuHu7s6bb75JgwYNKFKkSLJWfvPmTf744w9WrFjB+vXrFYBFREREJMMlKwD/+OOP1K5dGxublA0a4ebmxjvvvMM777yDn59fqgoUEREREUlPyQrAdevWTfOGvL2907wOEREREZG0StMwaAChoaFMmzaN3bt3c/PmTdzd3WnSpAmdOnXCzs4uPWoUEREREUk3aQ7A3377Ldu3bzceBwUFMWvWLCIiIujbt29aVy8iIiIikq7SFIAjIyPZsWMH9erVo0OHDuTMmZPQ0FBWr17N5s2bFYBFREREJNNJ1lVtI0eO5MaNGwmmP3jwgJiYGIoXL87LL79MwYIFKVu2LC+//DIPHjxI92JFRERERNIq2cOgbdy4kXbt2vHRRx8Ztzp2cnKiVKlSzJ49m0WLFuHs7Ex4eDhhYWHUrl37qRYuIiIiIpIayWoB/uabb3Bzc2PBggW0atWKuXPncv/+fWNe0aJFiYiI4Nq1a4SGhlK+fHkGDhz4VAsXEREREUmNZLUAN2vWjEaNGrFixQrmzJnD1KlTWbp0KV26dKFNmzYsXbqUy5cv899//+Hu7o67u/vTrltEREREJFWSfWeLLFmy0K5dO1atWsUnn3zCw4cP+fHHH3nrrbfYvHkzHh4elCtXTuFXRERERDK1lN3aDciWLRudO3dm9erVdOjQgevXrzNkyBDee+899uzZ8zRqFBERERFJN8kOwDdv3mT9+vUsWLCAzZs3YzKZ6N27N6tWraJNmzacP3+e/v3707VrV44ePfo0axYRERERSbVk9QE+cOAAAwYMICIiwpjm6urKjBkzKFq0KF9++SUdOnRg2rRpbN26lS5dulCrVi3GjRv31AoXEREREUmNZLUAT5w4kSxZslCzZk0aN25M7dq1yZIlC1OnTjWWKViwICNHjmThwoW8+uqr7N69+6kVLSIiIiKSWslqAQ4MDGTixIlUrFjRmHbv3j26dOmSYNnSpUszYcIEDh8+nF41ioiIiIikm2QF4Hz58jF8+HBq1KiBk5MTERERHD58mPz58yf5nPhhWUREREQks0hWAO7cuTNDhw5lyZIlmEwmzGYzdnZ2Fl0gRERERESeB8kKwE2aNKFYsWLs2LHDuNlFo0aNKFiw4NOuT0REREQkXSUrAAOUKVOGMmXKPM1aRERERESeumSNAjFgwAD279+f6o2cOHGCwYMHp/r5jzp27BjdunWjVq1aNGrUiKFDh/Lff/8Z84OCgujfvz916tShfv36jBo1itDQ0HTbvoiIiIg8v5LVArxr1y527dpFwYIFqV+/PnXq1OGll17Cxibx/BwVFcWRI0fYv38/u3bt4syZMwCMGDEizQX7+/vTvXt3qlWrxpgxY7h+/TqTJ08mKCiIOXPmcO/ePbp3746bmxvDhg3j1q1bTJw4kZCQECZNmpTm7YuIiIjI8y1ZAXjmzJn88MMPnD59mnnz5jFv3jzs7OwoVqwYefLkwdHREZPJRHh4OFeuXOHixYs8ePAAALPZTNmyZRkwYEC6FDxx4kTKlCnD2LFjjQDu6OjI2LFjuXTpElu2bOHOnTssWrSInDlzAuDu7k7fvn05fPiwRqcQERERsXLJCsAVKlRg4cKF/PnnnyxYsAB/f38ePnxIQEAAp06dsljWbDYDYDKZqFatGm3btqVOnTqYTKY0F3v79m3++ecfhg0bZtH6XK9ePerVqweAr68vlSpVMsIvgLe3N46OjuzZs0cBWERERMTKJfsiOBsbGxo2bEjDhg0JCQlh7969HDlyhOvXrxv9b3PlykXBggWpWLEiVatWJW/evOla7JkzZ4iJicHV1ZXBgwezc+dOzGYzdevWZeDAgTg7OxMYGEjDhg0tnmdra4uHhwcXLlxI0/bNZjPh4eFpWkdmYDKZyJ49e0aXIU8QERFh/KCUzEHHTuan40bEupnN5mQ1uiY7AMfn4eHBW2+9xVtvvZWap6farVu3APj222+pUaMGY8aM4eLFi0yZMoVLly4xa9YsQkNDcXR0TPBcBwcHwsLC0rT9yMhI/P3907SOzCB79ux4enpmdBnyBOfPnyciIiKjy5B4dOxkfjpuRCRr1qxPXCZVATijREZGAlC2bFm+/vprAKpVq4azszNfffUV+/btIyYmJsnnJ3XRXnLZ2dlRsmTJNK0jM0iP7ijy9BUrVkwtWZmMjp3MT8eNiHWLG3jhSZ6rAOzg4ADAa6+9ZjG9Ro0aAJw8eRInJ6dEuymEhYXh7u6epu2bTCajBpGnTafaRVJOx42IdUtuQ0XamkSfscKFCwPw8OFDi+lRUVEAZMuWjSJFihAUFGQxPzo6mpCQEIoWLfpM6hQRERGRzOu5CsDFihXDw8ODLVu2WJzi2rFjBwAVK1bE29ubgwcPGv2FAfz8/AgPD8fb2/uZ1ywiIiIimctzFYBNJhN9+vTh2LFjDBo0iH379rFkyRLGjRtHvXr1KFu2LG+99Rb29vb07NmT7du3s2rVKr7++mtq1KhBhQoVMnoXRERERCSDpaoP8PHjxylXrlx615IsDRo0wN7enpkzZ9K/f39y5MhB27Zt+eSTTwBwdXVl+vTpjBs3jsGDB+Po6Ej9+vXp169fhtQrIiIiIplLqgJwp06dKFasGG+88QbNmjUjT5486V3XY7322msJLoSLr2TJkkydOvUZViQiIiIiz4tUd4EIDAxkypQpNG/enF69erF582bj9sciIiIiIplVqlqAO3bsyJ9//klwcDBms5n9+/ezf/9+HBwcaNiwIW+88YZuOSwiIiIimVKqAnCvXr3o1asXAQEB/PHHH/z5558EBQURFhbG6tWrWb16NR4eHjRv3pzmzZuTL1++9K5bRERERCRV0jQKRJkyZejZsycrVqxg0aJFtGrVCrPZjNlsJiQkhJ9//pnWrVszevTox96hTURERETkWUnzneDu3bvHn3/+ydatW/nnn38wmUxGCIbYm1AsX76cHDly0K1btzQXLCIiIiKSFqkKwOHh4fz1119s2bKF/fv3G3diM5vN2NjYUL16dVq2bInJZGLSpEmEhISwadMmBWARERERyXCpCsANGzYkMjISwGjp9fDwoEWLFgn6/Lq7u/Pxxx9z7dq1dChXRERERCRtUhWAHz58CEDWrFmpV68erVq1wsvLK9FlPTw8AHB2dk5liSIiIiIi6SdVAfill16iZcuWNGnSBCcnp8cumz17dqZMmUKBAgVSVaCIiIiISHpKVQCeP38+ENsXODIyEjs7OwAuXLhA7ty5cXR0NJZ1dHSkWrVq6VCqiIiIiEjapXoYtNWrV9O8eXOOHTtmTFu4cCFNmzZlzZo16VKciIiIiEh6S1UA3rNnDyNGjCA0NJQzZ84Y0wMDA4mIiGDEiBHs378/3YoUEREREUkvqQrAixYtAiB//vyUKFHCmP7+++9TqFAhzGYzCxYsSJ8KRURERETSUar6AJ89exaTycSQIUOoUqWKMb1OnTq4uLjQtWtXTp8+nW5FioiIiIikl1S1AIeGhgLg6uqaYF7ccGf37t1LQ1kiIiIiIk9HqgJw3rx5AVixYoXFdLPZzJIlSyyWERERERHJTFLVBaJOnTosWLCAZcuW4efnR6lSpYiKiuLUqVNcvnwZk8lE7dq107tWEREREZE0S1UA7ty5M3/99RdBQUFcvHiRixcvGvPMZjOFChXi448/TrciRURERJ6mgQMHcvLkSdauXWtM+/jjjzly5EiCZefPn4+np2ei63nw4AGvv/460dHRFtOzZ8/Orl270rdoSbVUBWAnJyfmzp3L5MmT+fPPP43+vk5OTjRo0ICePXs+8Q5xIiIiIpnBhg0b2L59O/nz5zemmc1mzpw5w/vvv0+DBg0sli9WrFiS6zp79izR0dEMHz6cggULGtNtbFJ96wV5ClIVgAFcXFz46quvGDRoELdv38ZsNuPq6orJZErP+kRERESemuvXrzNmzJgE1y4FBwcTFhZGzZo1eeWVV5K9vlOnTmFra0v9+vXJmjVrepcr6STNP0dMJhOurq7kypXLCL8xMTHs3bs3zcWJiIiIPE3Dhw+nevXqVK1a1WJ6QEAAAKVLl07R+gICAihatKjCbyaXqhZgs9nMnDlz2LlzJ3fv3iUmJsaYFxUVxe3bt4mKimLfvn3pVqiIiIhIelq1ahUnT55k2bJljB8/3mLeqVOncHBwYMKECezcuZOIiAi8vLz49NNPKVq0aJLrjGsB7tmzJ0eOHCFr1qzUr1+ffv364ejo+HR3SJItVQF46dKlTJ8+HZPJhNlstpgXN01dIURERCSzunz5Mj/99BNDhgwhZ86cCeafOnWK8PBwnJ2dGTNmDJcvX2bmzJn4+Pjw66+/kidPngTPies3bDabad26NR9//DEnTpxg5syZnD9/np9//ll9gTOJVAXg9evXA7FXNLq5uREcHIynpyfh4eGcP38ek8nE559/nq6FioiIiKQHs9nMt99+S40aNahfv36iy/To0YMPP/yQypUrA1CpUiXKly/P22+/zeLFi+nTp0+i6x07diyurq6UKFECgMqVK+Pm5sbXX3+Nr68vNWvWfHo7JsmWqp8hwcHBmEwmfvjhB0aNGoXZbKZbt24sW7aM9957D7PZTGBgYDqXKiIiIpJ2y5Yt4/Tp0wwYMICoqCiioqKMM9pRUVHExMRQunRpI/zGKViwIMWKFeP06dOJrtfGxgYvLy8j/MapVasWQJLPk2cvVQH4wYMHABQuXJjSpUvj4ODA8ePHAWjTpg0Ae/bsSacSRURERNLPn3/+ye3bt2nSpAne3t54e3uzfv16Ll++jLe3N9OnT2fdunUcPXo0wXPv37+faJcJiB1RYuXKlVy5csVielxuSup58uylqgtErly5uHbtGgEBAXh4eFCqVCn27NmDj48PwcHBAFy7di1dCxURERFJD4MGDSI8PNxi2syZM/H392fcuHHkyZOHLl26kDt3bmbPnm0sc/LkSYKDg+nYsWOi642OjmbkyJF06tSJnj17GtO3bNmCra0tlSpVejo7JCmWqgBcoUIFtmzZwtdff83ixYupVKkS8+bNo127dsavnly5cqVroSIiIiLpIbFRHFxcXLCzszPu8Obj48OwYcMYMmQIzZo148qVK0yfPp3SpUvTvHlzAB4+fEhAQADu7u7kzZuXfPny0aJFCxYsWIC9vT3ly5fn8OHDzJ07l3bt2lGkSJFnuZvyGKkKwF26dMHPz4/Q0FDy5MlD48aNmT9/PoGBgcYIEI/eNUVERETkedG8eXPs7e2ZP38+n332GdmzZ6dOnTr06tULW1tbAG7cuEGnTp3w8fGhW7duAHz55ZcUKFCADRs2MGfOHNzd3enWrRsffvhhRu6OPMJkfnQcs2QKCQlhw4YNdOnSBYi9jeC0adMIDw+nXr16fPbZZ9jb26drsRnp2LFjACm6G0xmN3HLYUJuhWV0GfIID1dH+jSqmNFlyGPo2Ml8dNyICCQ/r6WqBXjPnj2UL1/eCL8AzZo1o1mzZqlZnYiIiIjIM5OqUSCGDBlCkyZN2LlzZ3rXIyIiIiLyVKUqAN+/f5/IyMjH3gpQRERERCQzSlUAjrtryvbt29O1GBERERGRpy1VfYBLly7N7t27mTJlCitWrKB48eI4OTmRJcv/r85kMjFkyJB0K1REREREJD2kKgBPmDABk8kEwOXLl7l8+XKiyykAi4iIiEhmk6oADPCk0dPiArKIiIiISGaSqgC8Zs2a9K5DREREXmAxZjM2ahzLlKzx/yZVATh//vzpXYeIiIi8wGxMJpb4neLa3fCMLkXicc/hQHvv0hldxjOXqgB88ODBZC1XuXLl1KxeREREXkDX7obrLoqSKaQqAHfr1u2JfXxNJhP79u1LVVEiIiIiIk/LU7sITkREREQkM0pVAPbx8bF4bDabefjwIVeuXGH79u2ULVuWzp07p0uBIiIiIiLpKVUBuGvXrknO++OPPxg0aBD37t1LdVEiIiIiIk9Lqm6F/Dj16tUDYPHixem9ahERERGRNEv3APz3339jNps5e/Zseq9aRERERCTNUtUFonv37gmmxcTEEBoayrlz5wDIlStX2ioTEREREXkKUhWA//nnnySHQYsbHaJ58+apr0pERERE5ClJ12HQ7OzsyJMnD40bN6ZLly5pKiy5Bg4cyMmTJ1m7dq0xLSgoiHHjxnHo0CFsbW1p0KABvXv3xsnJ6ZnUJCIiIiKZV6oC8N9//53edaTKhg0b2L59u8Wtme/du0f37t1xc3Nj2LBh3Lp1i4kTJxISEsKkSZMysFoRERERyQxS3QKcmMjISOzs7NJzlUm6fv06Y8aMIW/evBbTf/vtN+7cucOiRYvImTMnAO7u7vTt25fDhw9TsWLFZ1KfiIiIiGROqR4FIiAggB49enDy5Elj2sSJE+nSpQunT59Ol+IeZ/jw4VSvXp2qVataTPf19aVSpUpG+AXw9vbG0dGRPXv2PPW6RERERCRzS1UAPnfuHN26dePAgQMWYTcwMJAjR47QtWtXAgMD06vGBFatWsXJkyf5/PPPE8wLDAykcOHCFtNsbW3x8PDgwoULT60mEREREXk+pKoLxJw5cwgLCyNr1qwWo0G89NJLHDx4kLCwMH755ReGDRuWXnUaLl++zE8//cSQIUMsWnnjhIaG4ujomGC6g4MDYWFhadq22WwmPDw8TevIDEwmE9mzZ8/oMuQJIiIiEr3YVDKOjp3MT8dN5qRjJ/N7UY4ds9mc5Ehl8aUqAB8+fBiTycTgwYNp2rSpMb1Hjx6ULFmSr776ikOHDqVm1Y9lNpv59ttvqVGjBvXr1090mZiYmCSfb2OTtvt+REZG4u/vn6Z1ZAbZs2fH09Mzo8uQJzh//jwREREZXYbEo2Mn89Nxkznp2Mn8XqRjJ2vWrE9cJlUB+L///gOgXLlyCeaVKVMGgBs3bqRm1Y+1bNkyTp8+zZIlS4iKigL+fzi2qKgobGxscHJySrSVNiwsDHd39zRt387OjpIlS6ZpHZlBcn4ZScYrVqzYC/Fr/EWiYyfz03GTOenYyfxelGPnzJkzyVouVQHYxcWFmzdv8vfff1OoUCGLeXv37gXA2dk5Nat+rD///JPbt2/TpEmTBPO8vb3x8fGhSJEiBAUFWcyLjo4mJCSEunXrpmn7JpMJBweHNK1DJLl0ulAk5XTciKTOi3LsJPfHVqoCsJeXF5s2bWLs2LH4+/tTpkwZoqKiOHHiBFu3bsVkMiUYnSE9DBo0KEHr7syZM/H392fcuHHkyZMHGxsb5s+fz61bt3B1dQXAz8+P8PBwvL29070mEREREXm+pCoAd+nShZ07dxIREcHq1ast5pnNZrJnz87HH3+cLgXGV7Ro0QTTXFxcsLOzM/oWvfXWWyxdupSePXvi4+PDnTt3mDhxIjVq1KBChQrpXpOIiIiIPF9SdVVYkSJFmDRpEoULF8ZsNlv8K1y4MJMmTUo0rD4Lrq6uTJ8+nZw5czJ48GCmTp1K/fr1GTVqVIbUIyIiIiKZS6rvBFe+fHl+++03AgICCAoKwmw2U6hQIcqUKfNMO7snNtRayZIlmTp16jOrQURERESeH2m6FXJ4eDjFixc3Rn64cOEC4eHhiY7DKyIiIiKSGaR6YNzVq1fTvHlzjh07ZkxbuHAhTZs2Zc2aNelSnIiIiIhIektVAN6zZw8jRowgNDTUYry1wMBAIiIiGDFiBPv370+3IkVERERE0kuqAvCiRYsAyJ8/PyVKlDCmv//++xQqVAiz2cyCBQvSp0IRERERkXSUqj7AZ8+exWQyMWTIEKpUqWJMr1OnDi4uLnTt2pXTp0+nW5EiIiIiIuklVS3AoaGhAMaNJuKLuwPcvXv30lCWiIiIiMjTkaoAnDdvXgBWrFhhMd1sNrNkyRKLZUREREREMpNUdYGoU6cOCxYsYNmyZfj5+VGqVCmioqI4deoUly9fxmQyUbt27fSuVUREREQkzVIVgDt37sxff/1FUFAQFy9e5OLFi8a8uBtiPI1bIYuIiIiIpFWqukA4OTkxd+5cWrdujZOTk3EbZEdHR1q3bs2cOXNwcnJK71pFRERERNIs1XeCc3Fx4auvvmLQoEHcvn0bs9mMq6vrM70NsoiIiIhISqX6TnBxTCYTrq6u5MqVC5PJREREBCtXruTDDz9Mj/pERERERNJVqluAH+Xv78+KFSvYsmULERER6bVaEREREZF0laYAHB4ezsaNG1m1ahUBAQHGdLPZrK4QIiIiIpIppSoA//vvv6xcuZKtW7carb1msxkAW1tbateuTdu2bdOvShERERGRdJLsABwWFsbGjRtZuXKlcZvjuNAbx2QysW7dOnLnzp2+VYqIiIiIpJNkBeBvv/2WP/74g/v371uEXgcHB+rVq0e+fPmYNWsWgMKviIiIiGRqyQrAa9euxWQyYTabyZIlC97e3jRt2pTatWtjb2+Pr6/v065TRERERCRdpGgYNJPJhLu7O+XKlcPT0xN7e/unVZeIiIiIyFORrBbgihUrcvjwYQAuX77MjBkzmDFjBp6enjRp0kR3fRMRERGR50ayAvDMmTO5ePEiq1atYsOGDdy8eROAEydOcOLECYtlo6OjsbW1Tf9KRURERETSQbK7QBQuXJg+ffqwfv16Ro8eTa1atYx+wfHH/W3SpAnjx4/n7NmzT61oEREREZHUSvE4wLa2ttSpU4c6depw48YN1qxZw9q1awkODgbgzp07/PrrryxevJh9+/ale8EiIiIiImmRoovgHpU7d246d+7MypUrmTZtGk2aNMHOzs5oFRYRERERyWzSdCvk+Ly8vPDy8uLzzz9nw4YNrFmzJr1WLSIiIiKSbtItAMdxcnKiXbt2tGvXLr1XLSIiIiKSZmnqAiEiIiIi8rxRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVbJkdAEpFRMTw4oVK/jtt9+4dOkSuXLl4vXXX6dbt244OTkBEBQUxLhx4zh06BC2trY0aNCA3r17G/NFRERExHo9dwF4/vz5TJs2jQ4dOlC1alUuXrzI9OnTOXv2LFOmTCE0NJTu3bvj5ubGsGHDuHXrFhMnTiQkJIRJkyZldPkiIiIiksGeqwAcExPDvHnzePPNN+nVqxcA1atXx8XFhUGDBuHv78++ffu4c+cOixYtImfOnAC4u7vTt29fDh8+TMWKFTNuB0REREQkwz1XfYDDwsJo1qwZjRs3tphetGhRAIKDg/H19aVSpUpG+AXw9vbG0dGRPXv2PMNqRURERCQzeq5agJ2dnRk4cGCC6X/99RcAxYsXJzAwkIYNG1rMt7W1xcPDgwsXLjyLMkVEREQkE3uuAnBijh8/zrx583jttdcoWbIkoaGhODo6JljOwcGBsLCwNG3LbDYTHh6epnVkBiaTiezZs2d0GfIEERERmM3mjC5D4tGxk/npuMmcdOxkfi/KsWM2mzGZTE9c7rkOwIcPH6Z///54eHgwdOhQILafcFJsbNLW4yMyMhJ/f/80rSMzyJ49O56enhldhjzB+fPniYiIyOgyJB4dO5mfjpvMScdO5vciHTtZs2Z94jLPbQDesmUL33zzDYULF2bSpElGn18nJ6dEW2nDwsJwd3dP0zbt7OwoWbJkmtaRGSTnl5FkvGLFir0Qv8ZfJDp2Mj8dN5mTjp3M70U5ds6cOZOs5Z7LALxgwQImTpxIlSpVGDNmjMX4vkWKFCEoKMhi+ejoaEJCQqhbt26atmsymXBwcEjTOkSSS6cLRVJOx41I6rwox05yf2w9V6NAAPz+++9MmDCBBg0aMGnSpAQ3t/D29ubgwYPcunXLmObn50d4eDje3t7PulwRERERyWSeqxbgGzduMG7cODw8PHjnnXc4efKkxfyCBQvy1ltvsXTpUnr27ImPjw937txh4sSJ1KhRgwoVKmRQ5SIiIiKSWTxXAXjPnj08ePCAkJAQunTpkmD+0KFDadGiBdOnT2fcuHEMHjwYR0dH6tevT79+/Z59wSIiIiKS6TxXAbhVq1a0atXqicuVLFmSqVOnPoOKREREROR589z1ARYRERERSQsFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKzKCx2A/fz8+PDDD6lZsyYtW7ZkwYIFmM3mjC5LRERERDLQCxuAjx07Rr9+/ShSpAijR4+mSZMmTJw4kXnz5mV0aSIiIiKSgbJkdAFPy4wZMyhTpgzDhw8HoEaNGkRFRTF37lzat29PtmzZMrhCEREREckIL2QL8MOHD/nnn3+oW7euxfT69esTFhbG4cOHM6YwEREREclwL2QAvnTpEpGRkRQuXNhieqFChQC4cOFCRpQlIiIiIpnAC9kFIjQ0FABHR0eL6Q4ODgCEhYWlaH0BAQE8fPgQgKNHj6ZDhRnPZDJRLVcM0TnVFSSzsbWJ4dixY7pgM5PSsZM56bjJ/HTsZE4v2rETGRmJyWR64nIvZACOiYl57Hwbm5Q3fMe9mMl5UZ8XjvZ2GV2CPMaL9F570ejYybx03GRuOnYyrxfl2DGZTNYbgJ2cnAAIDw+3mB7X8hs3P7nKlCmTPoWJiIiISIZ7IfsAFyxYEFtbW4KCgiymxz0uWrRoBlQlIiIiIpnBCxmA7e3tqVSpEtu3b7fo07Jt2zacnJwoV65cBlYnIiIiIhnphQzAAB9//DHHjx/niy++YM+ePUybNo0FCxbQqVMnjQEsIiIiYsVM5hflsr9EbN++nRkzZnDhwgXc3d15++23+eCDDzK6LBERERHJQC90ABYRERERedQL2wVCRERERCQxCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWKyeRgKUF11i73G970XEmikAy3MpJCQELy8v1q5dm+rn3Lt3jyFDhnDo0KGnVabIU9GiRQuGDRuW6LwZM2bg5eVlPD58+DB9+/a1WGbWrFksWLDgaZYoYlVS850kGUsBWKxWQEAAGzZsICYmJqNLEUk3rVu3Zu7cucbjVatWcf78eYtlpk+fTkRExLMuTeSFlTt3bubOnUutWrUyuhRJpiwZXYCIiKSfvHnzkjdv3owuQ8SqZM2alVdeeSWjy5AUUAuwZLj79+8zefJk2rRpw6uvvkrt2rXp0aMHAQEBxjLbtm3j3XffpWbNmrz//vucOnXKYh1r167Fy8uLkJAQi+lJnSo+cOAA3bt3B6B79+507do1/XdM5BlZvXo1VatWZdasWRZdIIYNG8a6deu4fPmycXo2bt7MmTMtukqcOXOGfv36Ubt2bWrXrs1nn31GcHCwMf/AgQN4eXmxf/9+evbsSc2aNWncuDETJ04kOjr62e6wSAr4+/vzySefULt2bV5//XV69OjBsWPHjPmHDh2ia9eu1KxZk3r16jF06FBu3bplzF+7di3Vq1fn+PHjdOrUiRo1atC8eXOLbkSJdYG4ePEi//vf/2jcuDG1atWiW7duHD58OMFzFi5cSNu2balZsyZr1qx5ui+GGBSAJcMNHTqUNWvW8NFHHzF58mT69+/PuXPnGDx4MGazmZ07d/L5559TsmRJxowZQ8OGDfn666/TtM2yZcvy+eefA/D555/zxRdfpMeuiDxzW7ZsYeTIkXTp0oUuXbpYzOvSpQs1a9bEzc3NOD0b1z2iVatWxt8XLlzg448/5r///mPYsGF8/fXXXLp0yZgW39dff02lSpUYP348jRs3Zv78+axateqZ7KtISoWGhtK7d29y5szJjz/+yHfffUdERAS9evUiNDSUgwcP8sknn5AtWza+//57Pv30U/755x+6devG/fv3jfXExMTwxRdf0KhRIyZMmEDFihWZMGECvr6+iW733LlzdOjQgcuXLzNw4EBGjBiByWSie/fu/PPPPxbLzpw5k44dO/Ltt99SvXr1p/p6yP9TFwjJUJGRkYSHhzNw4EAaNmwIQJUqVQgNDWX8+PHcvHmTWbNm8fLLLzN8+HAAXn31VQAmT56c6u06OTlRrFgxAIoVK0bx4sXTuCciz96uXbsYMmQIH330Ed26dUswv2DBgri6ulqcnnV1dQXA3d3dmDZz5kyyZcvG1KlTcXJyAqBq1aq0atWKBQsWWFxE17p1ayNoV61alR07drB7927atm37VPdVJDXOnz/P7du3ad++PRUqVACgaNGirFixgrCwMCZPnkyRIkX46aefsLW1BeCVV16hXbt2rFmzhnbt2gGxo6Z06dKF1q1bA1ChQgW2b9/Orl27jO+k+GbOnImdnR3Tp0/H0dERgFq1avHOO+8wYcIE5s+fbyzboEEDWrZs+TRfBkmEWoAlQ9nZ2TFp0iQaNmzItWvXOHDgAL///ju7d+8GYgOyv78/r732msXz4sKyiLXy9/fniy++wN3d3ejOk1p///03lStXJlu2bERFRREVFYWjoyOVKlVi3759Fss+2s/R3d1dF9RJplWiRAlcXV3p378/3333Hdu3b8fNzY0+ffrg4uLC8ePHqVWrFmaz2XjvFyhQgKJFiyZ475cvX974O2vWrOTMmTPJ9/4///zDa6+9ZoRfgCxZstCoUSP8/f0JDw83ppcuXTqd91qSQy3AkuF8fX0ZO3YsgYGBODo6UqpUKRwcHAC4du0aZrOZnDlzWjwnd+7cGVCpSOZx9uxZatWqxe7du1m2bBnt27dP9bpu377N1q1b2bp1a4J5cS3GcbJly2bx2GQyaSQVybQcHByYOXMms2fPZuvWraxYsQJ7e3veeOMNOnXqRExMDPPmzWPevHkJnmtvb2/x+NH3vo2NTZLjad+5cwc3N7cE093c3DCbzYSFhVnUKM+eArBkqODgYD777DNq167N+PHjKVCgACaTieXLl7N3715cXFywsbFJ0A/xzp07Fo9NJhNAgi/i+L+yRV4kNWrUYPz48Xz55ZdMnTqVOnXqkC9fvlSty9nZmWrVqvHBBx8kmBd3WljkeVW0aFGGDx9OdHQ0//77Lxs2bOC3337D3d0dk8nEe++9R+PGjRM879HAmxIuLi7cvHkzwfS4aS4uLty4cSPV65e0UxcIyVD+/v48ePCAjz76iIIFCxpBdu/evUDsKaPy5cuzbds2i1/aO3futFhP3Gmmq1evGtMCAwMTBOX49MUuz7NcuXIBMGDAAGxsbPj+++8TXc7GJuHH/KPTKleuzPnz5yldujSenp54enry0ksvsWjRIv766690r13kWfnjjz9o0KABN27cwNbWlvLly/PFF1/g7OzMzZs3KVu2LIGBgcb73tPTk+LFizNjxowEF6ulROXKldm1a5dFS290dDSbN2/G09OTrFmzpsfuSRooAEuGKlu2LLa2tkyaNAk/Pz927drFwIEDjT7A9+/fp2fPnpw7d46BAweyd+9eFi9ezIwZMyzW4+Xlhb29PePHj2fPnj1s2bKFAQMG4OLikuS2nZ2dAdizZ0+CYdVEnhe5c+emZ8+e7N69m02bNiWY7+zszH///ceePXuMFidnZ2eOHDnCwYMHMZvN+Pj4EBQURP/+/fnrr7/w9fXlf//7H1u2bKFUqVLPepdE0k3FihWJiYnhs88+46+//uLvv/9m5MiRhIaGUr9+fXr27Imfnx+DBw9m9+7d7Ny5kz59+vD3339TtmzZVG/Xx8eHBw8e0L17d/744w927NhB7969uXTpEj179kzHPZTUUgCWDFWoUCFGjhzJ1atXGTBgAN999x0QeztXk8nEoUOHqFSpEhMnTuTatWsMHDiQFStWMGTIEIv1ODs7M3r0aKKjo/nss8+YPn06Pj4+eHp6Jrnt4sWL07hxY5YtW8bgwYOf6n6KPE1t27bl5ZdfZuzYsQnOerRo0YL8+fMzYMAA1q1bB0CnTp3w9/enT58+XL16lVKlSjFr1ixMJhNDhw7l888/58aNG4wZM4Z69eplxC6JpIvcuXMzadIknJycGD58OP369SMgIIAff/wRLy8vvL29mTRpElevXuXzzz9nyJAh2NraMnXq1DTd2KJEiRLMmjULV1dXvv32W+M7a8aMGRrqLJMwmZPqwS0iIiIi8gJSC7CIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlYlS0YXICLyIvDx8eHQoUNA7M0nhg4dmsEVJXTmzBl+//139u/fz40bN3j48CGurq689NJLtGzZktq1a2d0iSIiz4RuhCEikkYXLlygbdu2xuNs2bKxadMmnJycMrAqS7/88gvTp08nKioqyWWaNm3KN998g42NTg6KyItNn3IiImm0evVqi8f3799nw4YNGVRNQsuWLWPy5MlERUWRN29eBg0axPLly1myZAn9+vXD0dERgI0bN/Lrr79mcLUiIk+fWoBFRNIgKiqKN954g5s3b+Lh4cHVq1eJjo6mdOnSmSJM3rhxgxYtWhAZGUnevHmZP38+bm5uFsvs2bOHvn37ApAnTx42bNiAyWTKiHJFRJ4J9QEWEUmD3bt3c/PmTQBatmzJ8ePH2b17N6dOneL48eOUK1cuwXNCQkKYPHkyfn5+REZGUqlSJT799FO+++47Dh48SOXKlfn555+N5QMDA5kxYwZ///034eHh5M+fn6ZNm9KhQwfs7e0fW9+6deuIjIwEoEuXLgnCL0DNmjXp168fHh4eeHp6GuF37dq1fPPNNwCMGzeOefPmceLECVxdXVmwYAFubm5ERkayZMkSNm3aRFBQEAAlSpSgdevWtGzZ0iJId+3alYMHDwJw4MABY/qBAwfo3r07ENuXulu3bhbLly5dmh9++IEJEybw999/YzKZePXVV+nduzceHh6P3X8RkcQoAIuIpEH87g+NGzemUKFC7N69G4AVK1YkCMCXL1+mY8eO3Lp1y5i2d+9eTpw4kWif4X///ZcePXoQFhZmTLtw4QLTp09n//79TJ06lSxZkv4ojwucAN7e3kku98EHHzxmL2Ho0KHcu3cPADc3N9zc3AgPD6dr166cPHnSYtljx45x7Ngx9uzZw6hRo7C1tX3sup/k1q1bdOrUidu3bxvTtm7dysGDB5k3bx758uVL0/pFxPqoD7CISCpdv36dvXv3AuDp6UmhQoWoXbu20ad269athIaGWjxn8uTJRvht2rQpixcvZtq0aeTKlYvg4GCLZc1mM99++y1hYWHkzJmT0aNH8/vvvzNw4EBsbGw4ePAgS5cufWyNV69eNf7OkyePxbwbN25w9erVBP8ePnyYYD2RkZGMGzeOX3/9lU8//RSA8ePHG+G3UaNGLFy4kDlz5lC9enUAtm3bxoIFCx7/IibD9evXyZEjB5MnT2bx4sU0bdoUgJs3bzJp0qQ0r19ErI8CsIhIKq1du5bo6GgAmjRpAsSOAFG3bl0AIiIi2LRpk7F8TEyM0TqcN29ehg4dSqlSpahatSojR45MsP7Tp09z9uxZAJo3b46npyfZsmWjTp06VK5cGYD169c/tsb4Izo8OgLEhx9+yBtvvJHg39GjRxOsp0GDBrz++uuULl2aSpUqERYWZmy7RIkSDB8+nLJly1K+fHnGjBljdLV4UkBPrq+//hpvb29KlSrF0KFDyZ8/PwC7du0y/g9ERJJLAVhEJBXMZjNr1qwxHjs5ObF371727t1rcUp+5cqVxt+3bt0yujJ4enpadF0oVaqU0XIc5+LFi8bfCxcutAipcX1oz549m2iLbZy8efMaf4eEhKR0Nw0lSpRIUNuDBw8A8PLysujmkD17dsqXLw/Ett7G77qQGiaTyaIrSZYsWfD09AQgPDw8zesXEeujPsAiIqnwzz//WHRZ+PbbbxNdLiAggH///ZeXX34ZOzs7Y3pyBuBJTt/Z6Oho7t69S+7cuROdX61aNaPVeffu3RQvXtyYF3+otmHDhrFu3bokt/No/+Qn1fak/YuOjjbWERekH7euqKioJF8/jVghIimlFmARkVR4dOzfx4lrBc6RIwfOzs4A+Pv7W3RJOHnypMWFbgCFChUy/u7RowcHDhww/i1cuJBNmzZx4MCBJMMvxPbNzZYtGwDz5s1LshX40W0/6tEL7QoUKEDWrFmB2FEcYmJijHkREREcO3YMiG2BzpkzJ4Cx/KPbu3LlymO3DbE/OOJER0cTEBAAxAbzuPWLiCSXArCISArdu3ePbdu2AeDi4oKvr69FOD1w4ACbNm0yWji3bNliBL7GjRsDsRenffPNN5w5cwY/Pz+++uqrBNspUaIEpUuXBmK7QGzevJng4GA2bNhAx44dadKkCQMHDnxsrblz56Z///4A3Llzh06dOrF8+XICAwMJDAxk06ZNdOvWje3bt6foNXB0dKR+/fpAbDeMIUOGcPLkSY4dO8b//vc/Y2i4du3aGc+JfxHe4sWLiYmJISAggHnz5j1xe99//z27du3izJkzfP/991y6dAmAOnXq6M51IpJi6gIhIpJCGzduNE7bN2vWzOLUfJzcuXNTu3Zttm3bRnh4OJs2baJt27Z07tyZ7du3c/PmTTZu3MjGjRsByJcvH9mzZyciIsI4pW8ymRgwYAB9+vTh7t27CUKyi4uLMWbu47Rt25bIyEgmTJjAzZs3+eGHHxJdztbWllatWhn9a59k4MCBnDp1irNnz7Jp0yaLC/4A6tWrZzG8WuPGjVm7di0AM2fOZNasWZjNZl555ZUn9k82m81GkI+TJ08eevXqlaxaRUTi089mEZEUit/9oVWrVkku17ZtW+PvuG4Q7u7uzJ49m7p16+Lo6IijoyP16tVj1qxZRheB+F0FqlSpwi+//ELDhg1xc3PDzs6OvHnz0qJFC3755RdKliyZrJrbt2/P8uXL6dSpE2XKlMHFxQU7Ozty585NtWrV6NWrF2vXrmXQoEE4ODgka505cuRgwYIF9O3bl5deegkHBweyZctGuXLlGDx4MD/88INFX2Fvb2+GDx9OiRIlyJo1K/nz58fHx4effvrpiduKe82yZ8+Ok5MTjRo1Yu7cuY/t/iEikhTdCllE5Bny8/Mja9asuLu7ky9fPqNvbUxMDK+99hoPHjygUaNGfPfddxlcacZL6s5xIiJppS4QIiLP0NKlS9m1axcArVu3pmPHjjx8+JB169YZ3SqS2wVBRERSRwFYROQZeuedd9izZw8xMTGsWrWKVatWWczPmzcvLVu2zJjiRESshPoAi4g8Q97e3kydOpXXXnsNNzc3bG1tyZo1KwULFqRt27b88ssv5MiRI6PLFBF5oakPsIiIiIhYFbUAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFX5P7mRA7rR7JtCAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbf768-64c2-48ec-80e3-3a961b0b12a6",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "fe16003e-4015-4d28-ae37-ca0c94952758",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          555            418  75.315315\n",
      "1           kitten          136            114  83.823529\n",
      "2           senior          178            101  56.741573\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "5b5b8766-b4bd-4dd2-92e2-6513741b03ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efa447-1e32-42df-8664-a3f1f0e0f812",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "5eecd6a6-4094-4747-8029-f795a87f8ff0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      163     76.53\n",
      "1          M    337      230     68.25\n",
      "2          X    319      240     75.24\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "115592e9-47eb-42fe-8cb5-26beeb7328ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOjUlEQVR4nO3dd3hU1d728XsSShollAAhdJAqHQxICb1IVdqjotLh0I+Pja7Cg4cSNJF2QDg0BUQ6ihQB6QjSS6ghgdCFQAqQkHn/4M0+GRMgTCbMhPl+rovrmlm7/XbiNvesWXttk9lsNgsAAABwEi72LgAAAAB4kQjAAAAAcCoEYAAAADgVAjAAAACcCgEYAAAAToUADAAAAKdCAAYAAIBTIQADAADAqRCAAQAA4FQy2bsAAC+32NhYNW/eXNHR0ZKk0qVLa9GiRXauChEREWrTpo3xfv/+/XasRrp27ZrWrl2r33//XVevXlVkZKSyZs2q/Pnzq1KlSmrXrp3KlStn1xqfpnr16sbr1atXy9fX147VAHgWAjCAdLVx40Yj/EpSSEiIjh8/rvLly9uxKjiS1atXa/LkyRb/nUhSfHy8zp07p3PnzmnFihXq0qWL/vnPf8pkMtmpUgAvCwIwgHS1atWqZG0rVqwgAEOStHDhQn399dfG+xw5cui1115Tnjx5dPPmTe3atUtRUVEym8364Ycf5O3tre7du9uvYAAvBQIwgHQTGhqqw4cPS5KyZ8+uu3fvSpI2bNigoUOHytPT057lwc6OHj2q4OBg432LFi306aefWvx3ERUVpY8//lj79u2TJM2ZM0edOnWSl5fXC68XwMuDAAwg3STt/e3YsaP27Nmj48ePKyYmRuvXr9dbb731xG1PnTqlBQsW6M8//9SdO3eUK1culShRQl26dFHt2rWTrR8VFaVFixZpy5YtunTpkjJnzixfX181bdpUHTt2lIeHh7HumDFjtHbtWklSr1691KdPH2PZ/v371bdvX0lSgQIFtGbNGmNZ4jjP3Llza9asWRozZoxOnjyp7Nmz6+OPP1ajRo308OFDLVq0SBs3blR4eLgePHggT09PFStWTG+99ZbeeOMNq2vv3r27jhw5IkkaMmSI3n33XYv9/PDDD5o8ebIkqU6dOhY9q8/y8OFDzZ07V2vWrNFff/0lPz8/tWnTRl26dFGmTI//VAwfPly//vqrJKlTp076+OOPLfaxdetW/e///q8kqUSJElqyZMlTjzljxgw9evRIklS+fHmNGTNGrq6uFut4eXnp888/1/Dhw1WkSBGVKFFC8fHxFuskJCRo5cqVWrlypc6fPy9XV1cVLVpUb7zxht58802j/kRJf4+//vqrVq5cqaVLl+rixYvKli2bGjRooD59+ihnzpwW2z169EiLFy/WqlWrdOnSJeXKlUutW7dWt27dnnqeN2/e1Jw5c7R9+3bdvHlT2bNnV8WKFfX++++rQoUKFuvOnDlTs2bNkiR9+umnunv3rr7//nvFxsaqXLlyxjIAaUMABpAu4uPjtW7dOuN969atlT9/fh0/flzS42EQTwrAa9eu1ZdffmmEI+nxTVLXrl3Trl27NGDAAH3wwQfGsqtXr6pfv34KDw832u7fv6+QkBCFhIRo8+bNmjFjhkUITov79+9rwIABioiIkCTdunVLr7zyihISEjR8+HBt2bLFYv179+7pyJEjOnLkiC5dumQRuJ+n9jZt2hgBeMOGDckC8MaNG43XrVq1eq5zGjJkiNHLKknnz5/X119/rcOHD2vChAkymUxq27atEYA3b96s//3f/5WLy38nE3qe40dGRuqPP/4w3r/zzjvJwm+ivHnz6t///neKy+Lj4/XJJ59o27ZtFu3Hjx/X8ePHtW3bNk2ZMkVZsmRJcfuvvvpKy5YtM94/ePBAP/74o44dO6a5c+ca4dlsNuvTTz+1+N1evXpVs2bNMn4nKTl79qz69++vW7duGW23bt3Sli1btG3bNg0bNkzt2rVLcdvly5fr9OnTxvv8+fM/8TgAng/ToAFIF9u3b9dff/0lSapSpYr8/PzUtGlTubu7S3rcw3vy5Mlk250/f17jxo0zwm+pUqXUsWNH+fv7G+t8++23CgkJMd4PHz7cCJBeXl5q1aqV2rZta3yVfuLECU2fPt1m5xYdHa2IiAjVrVtX7du312uvvaZChQppx44dRkDy9PRU27Zt1aVLF73yyivGtt9//73MZrNVtTdt2tQI8SdOnNClS5eM/Vy9elVHjx6V9Hi4Sb169Z7rnPbt26eyZcuqY8eOKlOmjNG+ZcsWoye/Ro0aKliwoKTHIe7AgQPGeg8ePND27dslSa6urmrRosVTjxcSEqKEhATjfeXKlZ+r3kT/+c9/jPCbKVMmNW3aVO3bt1f27NklSXv37n1ir+mtW7e0bNkyvfLKK8l+TydPnrSYGWPVqlUW4bd06dLGz2rv3r0p7j8xnCeG3wIFCqhDhw56/fXXJT3uuf7qq6909uzZFLc/ffq08uTJo06dOqlq1apq1qxZan8sAJ6BHmAA6SLp8IfWrVtLehwKGzdubAwrWL58uYYPH26x3Q8//KC4uDhJUkBAgL766iujF27s2LFauXKlPD09tW/fPpUuXVqHDx82xhl7enpq4cKF8vPzM47bs2dPubq66vjx40pISLDosUyLBg0aaOLEiRZtWbJkUbt27XTmzBn17dtXtWrVkvS4R7dJkyaKjY1VdHS07ty5I29v7+eu3cPDQ40bN9bq1aslPe4FTrwhbNOmTUawbtq06RN7PJ+kSZMmGjdunFxcXJSQkKCRI0cavb3Lly9Xu3btZDKZ1Lp1a82YMcM4fo0aNSRJO3fuVExMjCQZN7E9TeKHo0S5cuWyeL9y5UqNHTs2xW0Th63ExcVZTKk3ZcoU42f+/vvv6+2331ZMTIyWLl2qHj16yM3NLdm+6tSpo8DAQLm4uOj+/ftq3769bty4Ienxh7HED17Lly83tmnQoIG++uorubq6JvtZJbV161ZdvHhRklS4cGEtXLjQ+AAzf/58BQUFKT4+XosXL9aIESNSPNfg4GCVKlUqxWUArEcPMACbu379unbv3i1Jcnd3V+PGjY1lbdu2NV5v2LDBCE2Jkva6derUyWL8Zv/+/bVy5Upt3bpVXbt2TbZ+vXr1jAApPe5VXLhwoX7//XfNmTPHZuFXUoq9cf7+/hoxYoTmzZunWrVq6cGDBzp06JAWLFhg0ev74MEDq2v/+88v0aZNm4zXzzv8QZK6detmHMPFxUXvvfeesSwkJMT4UNKqVStjvd9++80Yj5t0+EPiB56nyZo1q8X7v4/rTY1Tp07p3r17kqSCBQsa4VeS/Pz8VLVqVUmPe+yPHTuW4j66dOlinI+bm5vF7CSJ/23GxcVZfOOQ+MFESv6zSirpkJKWLVtaDMFJOgfzk3qQixcvTvgF0gk9wABsbs2aNcYQBldXV+PGqEQmk0lms1nR0dH69ddf1b59e2PZ9evXjdcFChSw2M7b21ve3t4WbU9bX5LF1/mpkTSoPk1Kx5IeD0VYvny59uzZo5CQEItxzIkSv/q3pvZKlSqpaNGiCg0N1dmzZ3XhwgW5u7sbAa9o0aLJbqxKjcKFC1u8L1q0qPH60aNHioyMVJ48eZQ/f375+/tr165dioyM1N69e1WtWjXt2LFDkpQtW7ZUDb/w8fGxeH/t2jUVKVLEeF+qVCm9//77xvv169fr2rVrFttcvXrVeH358mWLh1H8XWhoaIrL/z6uNmlITfzdRUZGWvwek9YpWf6snlTfjBkzjJ7zv7ty5Yru37+frIf6Sf+NAUg7AjAAmzKbzcZX9NLjGQ6S9oT93YoVKywCcFIphcened71peSBN7Gn81lSmsLt8OHDGjhwoGJiYmQymVS5cmVVrVpVFStW1NixY42v1lPyPLW3bdtW33zzjaTHvcBJQ5s1vb/S4/NOGsD+Xk/SG9TatGmjXbt2GcePjY1VbGyspMdDKf7eu5uSEiVKyMPDw+hl3b9/v0WwLF++vEVv7NGjR5MF4KQ1ZsqUSTly5Hji8Z7Uw/z3oSKp+Zbg7/t60r6TjnH29PRMcQhGopiYmGTLmSYQSD8EYAA2deDAAV2+fDnV6584cUIhISEqXbq0pMc9g4k3hYWGhlr0roWFhemnn35S8eLFVbp0aZUpU8aiJzFxvGVS06dPV7Zs2VSiRAlVqVJFbm5uFiHn/v37FuvfuXMnVXVnzpw5WVtgYKAR6L788ks1b97cWJZSSLKmdkl64403NHXqVMXHx2vDhg1GUHJxcVHLli1TVf/fnTlzxhgyID3+WSfKmjWrcVOZJNWvX185c+bUnTt3tHXrVmN+Zyl1wx+kx8MN6tevr19++UXS47HfrVu3fuLY5ZR65pP+/Hx9fS3G6UqPA/KTZpZ4Hjlz5lSWLFn08OFDSY9/Nkkfy3zhwoUUt8ubN6/x+oMPPrCYLi0149FT+m8MgG0wBhiATa1cudJ43aVLF+3fvz/FfzVr1jTWSxpcqlWrZrxeunSpRY/s0qVLtWjRIn355Zf67rvvkq2/e/dunTt3znh/6tQpfffdd/r66681ZMgQI8AkDXPnz5+3qH/z5s2pOs+UHsd75swZ43XSOWR3796t27dvG+8TewatqV16fMNY3bp1JT0OzidOnJAk1axZM9nQgtSaM2eOEdLNZrPmzZtnLKtQoYJFkMycObMRtKOjo43ZHwoXLqxXX3011cfs1q2b0VscGhqqTz/91BjTmygqKkqBgYE6dOhQsu3LlStn9H6HhYUZwzCkx3PvNmzYUG+++aY++uijp/a+P0umTJkszivpmO74+HjNnj07xe2S/n5Xr16tqKgo4/3SpUtVv359vf/++08cGsEjn4H0Qw8wAJu5d++exVRRSW9++7tmzZoZQyPWr1+vIUOGyN3dXV26dNHatWsVHx+vffv26X/+539Uo0YNXb582fjaXZI6d+4s6fHNYhUrVtSRI0f04MEDdevWTfXr15ebm5vFjVktW7Y0gm/SG4t27dql8ePHq3Tp0tq2bZt27txp9fnnyZPHmBt42LBhatq0qW7duqXff//dYr3Em+CsqT1R27Ztk803bO3wB0nas2eP3n33XVWvXl3Hjh2zuGmsU6dOydZv27atvv/++zQdv3jx4ho8eLAmTJggSfr999/Vpk0b1apVS3ny5NG1a9e0Z88eRUdHW2yX2OPt5uamN998UwsXLpQkffjhh6pXr558fHy0bds2RUdHKzo6WtmyZbPojbVGly5djGnfNm7cqCtXrqh8+fI6ePCgxVy9STVu3FjTp0/XtWvXFB4ero4dO6pu3bqKiYnRpk2bFB8fr+PHj6e61xyA7dADDMBmfvnlFyPc5c2bV5UqVXriug0bNjS+4k28GU6SSpYsqc8++8zocQwNDdWPP/5oEX67detmcUPT2LFjjflpY2Ji9Msvv2jFihVGj1vx4sU1ZMgQi2Mnri9JP/30k/7v//5PO3fuVMeOHa0+/8SZKSTp7t27WrZsmbZs2aJHjx5ZPLo36UMvnrf2RLVq1bIIdZ6engoICLCq7ldeeUVVq1bV2bNntXjxYovw26ZNGzVq1CjZNiVKlLC42c7a4RedOnXS+PHjjZ7ce/fuacOGDfr++++1efNmi/CbJ08effzxx3rnnXeMtr59+xo9rY8ePdKWLVu0ZMkS4wa0fPnyady4cc9d1981aNDA4sEtx44d05IlS3T69GlVrVrVYg7hRG5ubvrXv/5lBPYbN25o+fLlWr9+vdHb3qJFC7355ptprg/A86EHGIDNJJ37t2HDhk/9CjdbtmyqXbu28RCDFStWGE/Eatu2rUqVKmXxKGRPT0/jQQ1/D3q+vr5asGCBFi5cqC1bthi9sH5+fmrUqJG6du1qPIBDejw12+zZsxUUFKTdu3fr/v37KlmypLp06aIGDRroxx9/tOr8O3bsKG9vb82fP1+hoaEym80qUaKEOnfurAcPHhjz2m7evNk4h+etPZGrq6vKly+vrVu3Snrc2/i0m6yeJkuWLPr22281d+5crVu3Tjdv3pSfn586der01MdVv/rqq0ZYrl69utVPKmvSpImqVq2qVatWaffu3Tp//ryioqLk4eGhvHnz6tVXX1WtWrUUEBCQ7LHGbm5umjp1qhEsz58/r7i4OBUoUEB169bVu+++q9y5c1tV1999+umnKlOmjJYsWaKwsDDlzp1bb7zxhrp3767evXunuE2FChW0ZMkSzZs3T7t379aNGzfk7u6uIkWK6M0331SLFi1sOj0fgNQxmVM75w8AwGGEhYWpS5cuxtjgmTNnWow5TW937txRx44djbHNY8aMSdMQDAB4kegBBoAM4sqVK1q6dKkePXqk9evXG+G3RIkSLyT8xsbGavr06XJ1ddVvv/1mhF9vb++njvcGAEfjsAH42rVr6ty5syZNmmQx1i88PFyBgYE6ePCgXF1d1bhxYw0cONBifF1MTIyCg4P122+/KSYmRlWqVNE///nPJ05WDgAZgclk0oIFCyzaMmfOrI8++uiFHD9r1qxaunSpxZRuJpNJ//znP60efgEA9uCQAfjq1asaOHCgxZQx0uObI/r27avcuXNrzJgxun37toKCghQREaHg4GBjveHDh+vYsWMaNGiQPD09NWvWLPXt21dLly5Ndic1AGQUefPmVaFChXT9+nW5ubmpdOnS6t69+1OfgGZLLi4uevXVV3Xy5EllzpxZxYoV07vvvquGDRu+kOMDgK04VABOSEjQunXr9PXXX6e4fNmyZYqMjNSiRYuMOTZ9fHw0ePBgHTp0SJUrV9aRI0e0fft2ffPNN3r99dclSVWqVFGbNm30448/qkePHi/obADAtlxdXbVixQq71jBr1iy7Hh8AbMGhbj09c+aMxo8frzfeeEOff/55suW7d+9WlSpVLCaY9/f3l6enpzF35+7du+Xu7i5/f39jHW9vb1WtWjVN83sCAADg5eBQATh//vxasWLFE8eThYaGqnDhwhZtrq6u8vX1NR4jGhoaqoIFCyZ7/GWhQoVSfNQoAAAAnItDDYHIkSOHcuTI8cTlUVFRxoTiSXl4eBiTpadmnecVEhJibMuz2QEAABxTXFycTCaTqlSp8tT1HCoAP0tCQsITlyVOJJ6adayROF1y4rRDAAAAyJgyVAD28vJSTExMsvbo6Gj5+PgY6/z1118prpN0qrTnUbp0aR09elRms1klS5a0ah8AAABIX2fPnn3qU0gTZagAXKRIEYWHh1u0PXr0SBEREWrQoIGxzp49e5SQkGDR4xseHp7meYBNJpPxvHoAAAA4ltSEX8nBboJ7Fn9/f/3555/G04ckac+ePYqJiTFmffD391d0dLR2795trHP79m0dPHjQYmYIAAAAOKcMFYA7dOigrFmzqn///tqyZYtWrlypkSNHqnbt2qpUqZIkqWrVqqpWrZpGjhyplStXasuWLfrHP/6hbNmyqUOHDnY+AwAAANhbhhoC4e3trRkzZigwMFAjRoyQp6enGjVqpCFDhlisN3HiRE2ZMkXffPONEhISVKlSJY0fP56nwAEAAEAmc+L0Bniqo0ePSpJeffVVO1cCAACAlKQ2r2WoIRAAAABAWhGAAQAA4FQIwAAAAHAqBGAAAAA4FQIwAAAAnAoBGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMAAAABwKgRgAAAAOBUCMAAAAJwKARgAAABOhQAMAAAAp0IABgAAgFMhAAMAAMCpEIABAADgVAjAAAAAcCoEYAAAADgVAjAAAACcCgEYAAAAToUADAAAAKdCAAYAAIBTIQADAADAqRCAAQAA4FQIwAAAAHAqBGAAAAA4FQIwAAAAnAoBGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMAAAABwKgRgAAAAOBUCMAAAAJwKARgAAABOhQAMAAAAp0IABgAAgFMhAAMAAMCpEIABAADgVAjAAAAAcCoEYAAAADgVAjAAAACcCgEYAAAATiWTvQsAJGn//v3q27fvE5f37t1bvXv31vXr1xUUFKTdu3crPj5e5cuX16BBg1SmTJmn7r9ly5a6fv16svZNmzYpZ86caS0fAABkIARgOIQyZcpo7ty5ydqnT5+u48ePq1mzZoqOjlavXr2UJUsWffbZZ8qaNatmz56t/v37a8mSJcqTJ0+K+75z546uX7+uwYMHq3LlyhbLvLy80uN0AACAAyMAwyF4eXnp1VdftWjbtm2b9u3bp6+++kpFihTR7NmzFRkZqWXLlhlht2zZsuratav279+v5s2bp7jvkJAQSVKDBg3k5+eXvicCAAAcXoYMwCtWrNAPP/ygiIgI5c+fX506dVLHjh1lMpkkSeHh4QoMDNTBgwfl6uqqxo0ba+DAgfT2ZSD379/XxIkTVadOHTVu3FiStHnzZjVq1MiipzdPnjz65Zdfnrqv06dPy9PTUwULFkzXmgEAQMaQ4QLwypUrNW7cOHXu3Fn169fXwYMHNXHiRD18+FDvvvuu7t27p759+yp37twaM2aMbt++raCgIEVERCg4ONje5SOVFi9erBs3bmj69OmSpPj4eJ0/f14tWrTQ9OnTtXLlSt25c0eVK1fWxx9/rBIlSjxxX6dPn1b27Nn18ccfa9++fUpISFCdOnX04YcfPnHYBADg5Zbae0969Oihw4cPJ1s+f/58lStXLsVtExIStHz5ci1btkyXL19Wrly5VK9ePfXp04fOOAeR4QLw6tWrVblyZX300UeSpJo1a+rixYtaunSp3n33XS1btkyRkZFatGiRcXOTj4+PBg8erEOHDiUbAwrHExcXpx9++EFNmzZVoUKFJEl3797Vo0eP9P3336tgwYIaOXKkHj58qBkzZqh3795avHix8ubNm+L+QkJCdP36dbVv315vv/22Lly4oJkzZ6p3795atGiR3N3dX+TpAQAcQGruPTGbzTp79qzeeecd49vIRMWKFXvivufPn6/p06era9euqlGjhsLCwjRjxgydO3dOU6dONb6xhv1kuAD84MGDZL12OXLkUGRkpCRp9+7dqlKlisWd/f7+/vL09NTOnTsJwBnA5s2bdevWLXXt2tVoi4uLM14HBwfLw8NDklSuXDm1b99eS5cuVf/+/VPc34gRI+Tq6qry5ctLkqpUqaLixYurZ8+eWrdunTp06JCOZwMAcESpufckPDxc0dHRev3115Ot+yQJCQmaN2+e3nzzTQ0YMECS9NprrylHjhwaNmyYTp48+cSeY7w4GW4e4P/5n//Rnj179PPPPysqKkq7d+/WunXr1LJlS0lSaGioChcubLGNq6urfH19dfHiRXuUjOe0efNmFS9eXK+88orR5unpKUmqVq2aEX4lKX/+/CpWrJhxo1tKKlasaITfRJUrV5aXl5dOnz5t4+oBABlRSveeJP5tSfr36Fmio6PVsmVLNWvWzKK9aNGikqRLly7ZpmCkSYbrAW7WrJkOHDigUaNGGW21atXShx9+KEmKiooywlJSHh4eio6OTtOxzWazYmJi0rQPPF18fLx2796tt99+2+Jn7eLiopw5cyo2NjbZ7+Dhw4dydXVN8XcTFRWlbdu2qWzZsipevLjRnpCQoLi4OHl5efE7BQBo4cKFunHjhgIDA42/C8ePH5e7u7smT56sXbt2KTY2VlWqVNHAgQOTdbYlcnV1Nb6RTPr3ZePGjZIkX19f/u6kI7PZnKohJhkuAH/44Yc6dOiQBg0apPLly+vs2bP697//rU8++USTJk1SQkLCE7d1cUlbh3dcXJxOnjyZpn3g6cLCwnT//n1lz5492c+6bNmy2rdvn/744w/jJoKrV68qLCxMNWrUSPF3ExcXp8DAQFWpUkU9evQw2g8dOqQHDx4od+7c/E4BwMnFx8dr8eLFqlatmu7du2f8XTh06JBiY2P18OFD9erVS7du3dK6devUr18/jRgxItUPUrpw4YIWLlyoihUr6sGDB/zdSWdZsmR55joZKgAfPnxYu3bt0ogRI9SuXTtJj78SL1iwoIYMGaIdO3Y8sUcvOjpaPj4+aTp+5syZVbJkyTTtA08XGhoqSapXr16ysd6DBw9Wz549NWPGDH3wwQeKi4vTrFmz5OPjox49ehhDI44fP66cOXMa05517dpVc+bMUdGiReXv76/z589r/vz5qlOnjtq3b/9Czw8A4Hg2btyou3fvqm/fvhZ/54cOHaqoqCiL+4eaNWumrl276tChQ+rXr98z93306FF9++238vX11bhx45QjR470OAX8f2fPnk3VehkqAF+5ckWSVKlSJYv2qlWrSpLOnTtnDFpP6tGjR4qIiFCDBg3SdHyTyWQx/hS2FxUVJUnKly+fsmbNarGsZMmSmjNnjoKDgzVu3Di5uLjotdde0z//+U+LsNyvXz+1atVKY8aMkST17dtXPj4+Wrp0qVatWqUcOXLorbfeUu/eveXm5vbCzg0A4Jh27Nih4sWLq2LFihbtf38vPf5bVKxYMYWGhj4zE2zYsEGff/65ChcurODgYKbefAFSO8NGhgrAiQPIDx48aDH9SOL8fH5+fvL399f8+fN1+/ZteXt7S5L27NmjmJgY+fv7v/Ca8Xzef/99vf/++09cXrx4cU2ZMuWp+9i/f7/FexcXF3Xo0IHZHgAAySTee/L3vz3x8fFav369ChcunCwI379//5nDHxYsWKCgoCBVq1ZNkyZNYv5fB5OhAnCZMmXUsGFDTZkyRXfv3lWFChV0/vx5/fvf/1bZsmUVEBCgatWqacmSJerfv7969eqlyMhIBQUFqXbt2sl6jgEAgHM7e/as7t+/nywjZMqUSbNmzVKePHn03XffGe2nTp3SpUuXntpZ89NPP+mbb75RkyZN9MUXXyhz5szpVj+sYzKbzWZ7F/E84uLi9N133+nnn3/WjRs3lD9/fgUEBKhXr17GVxFnz55VYGCgDh8+LE9PT9WvX19DhgxJcXaI1Dp69KgkpXoeQAAA4PjWrl2rMWPGaP369cmGKCQua9mypVq2bKmrV69qxowZypMnj+bNmydXV1c9fPhQISEh8vHxUb58+XTz5k21bdtWuXPn1hdffCFXV1eLffr5+RnfUMP2UpvXMlwAthcCMAAAL5958+YpODhYO3fuTHbvifT4Brn58+frwoULcnd3V0BAgAYMGGDczBYREaE2bdqoV69e6tOnj1atWqUvv/zyiccbPXq0WrdunW7n4+wIwDZGAAYAAHBsqc1rGe5JcAAAAEBaEIABAADgVAjAAAAAcCoEYCeVwNBvh8bvBwCA9JOh5gGG7biYTFq857Su303+2GjYl092D3Xxf8XeZQAA8NIiADux63djFHE72t5lAAAAvFAMgQAAAIBTIQADAIB0x70NjssZfzcMgQCAl8DRo0f17bff6vjx4/Lw8FCtWrU0ePBg5cqVS5J08OBBTZ06VWfOnJGXl5caNGigfv36PfMR8Zs2bdL8+fMVGhqqbNmyqWbNmhowYIBy5879Ik4LLxHuPXFMznrfCQEYADK4kydPqm/fvqpZs6YmTZqkGzdu6Ntvv1V4eLjmzJmjc+fOqX///qpcubLGjx+v69evKzg4WJcvX9aUKVOeuN9ff/1Vw4cP15tvvql//OMfunnzpmbMmKF+/fppwYIFKT42Fnga7j2BoyAAA0AGFxQUpNKlS2vy5MlycXk8ss3T01OTJ0/W5cuXtX79eplMJk2aNEkeHh6SpEePHmn8+PG6cuWKChQokOJ+586dq9dff13Dhg0z2ooWLaoPPvhA27dvV+PGjdP/5AAgHTAGGAAysDt37ujAgQPq0KGDEX4lqWHDhlq3bp0KFiyoBw8eKFOmTHJzczOW58iRQ5IUGRmZ4n4TEhL02muvqX379hbtRYsWlSRdunTJxmcCAC8OARgAMrCzZ88qISFB3t7eGjFihOrVq6e6detq1KhRunfvniSpTZs2kqQpU6bozp07OnfunGbNmqWSJUuqVKlSKe7XxcVFQ4cOVUBAgEX71q1bJUklSpRIt3MCgPTGEAgAyMBu374tSfriiy9Uu3ZtTZo0SWFhYZo6daouX76s2bNnq2TJkho4cKAmTJigH374QZJUoEABzZo1S66urqk+1qVLl/T111/rlVde0euvv54u5wMALwIBGAAysLi4OElSmTJlNHLkSElSzZo1lS1bNg0fPlx79+7VqVOn9O2336pjx45q2LCh7ty5o9mzZ+sf//iHZs2alaoZHUJDQ9W/f3+5urpqwoQJFsMtACCj4f9gAJCBJd7UVrduXYv22rVrS5JOnTql2bNnq0WLFvrkk09Uo0YNNWnSRNOnT9fNmze1YMGCZx5j//796t69uyRp5syZ8vPzs/FZAMCLRQAGgAyscOHCkqSHDx9atMfHxxvt9+/fV6VKlSyW58qVS0WKFNH58+efuv/169drwIAB8vHx0dy5c42b4AAgIyMAA0AGVqxYMfn6+mrDhg0yJ3ma07Zt2yQ97hnOkSOHDh48aLHdnTt3FBYWpoIFCz5x3zt27NDo0aNVsWJFzZ49Wz4+PulzEgDwgjEGGAAyMJPJpEGDBumzzz7TsGHD1K5dO124cEHTpk1Tw4YNVbZsWfXu3VsTJ06Up6enGjdurDt37ug///mPXFxc9M477xj7Onr0qLy9veXn56cHDx5o7Nix8vDwUPfu3XXhwgWL4/r4+Chfvnwv+nQBwCYIwACQwTVu3FhZs2bVrFmzNHToUGXPnl1vvfWW+vXrJ0nq3LmzsmXLpoULF2rNmjXKmTOnKleurIkTJ1r0AHfr1k2tWrXSmDFjdOTIEd28eVOSNGDAgGTH7NWrl/r06fNiThAAbIwADAAvgbp16ya7ES6pli1bqmXLlk/dx/79+43XNWrUsHgPAC8TxgADAADAqRCAAQAA4FQIwAAAAHAqaRoDfOnSJV27dk23b99WpkyZlDNnThUvXlzZs2e3VX0AAACATT13AD527JhWrFihPXv26MaNGymuU7hwYdWtW1etW7dW8eLF01wkAAAAYCupDsCHDh1SUFCQjh07JkkWE67/3cWLFxUWFqZFixapcuXKGjJkiMqVK5f2agEAAIA0SlUAHjdunFavXq2EhARJUtGiRfXqq6+qVKlSyps3rzw9PSVJd+/e1Y0bN3TmzBmdOnVK58+f18GDB9WtWze1bNlSo0ePTr8zAQAAAFIhVQF45cqV8vHx0ZtvvqnGjRurSJEiqdr5rVu3tGnTJi1fvlzr1q0jAAPI8BLMZrmYTPYuAyngdwMgtVIVgCdMmKD69evLxeX5Jo3InTu3OnfurM6dO2vPnj1WFQgAjsTFZNLiPad1/W6MvUtBEj7ZPdTF/xV7lwEgg0hVAG7QoEGaD+Tv75/mfQCAI7h+N0YRt6PtXQYAwEppfhRyVFSUpk+frh07dujWrVvy8fFR8+bN1a1bN2XOnNkWNQIAAAA2k+YA/MUXX2jLli3G+/DwcM2ePVuxsbEaPHhwWncPAAAA2FSaAnBcXJy2bdumhg0bqmvXrsqZM6eioqK0atUq/frrrwRgAAAAOJxU3dU2btw43bx5M1n7gwcPlJCQoOLFi6t8+fLy8/NTmTJlVL58eT148MDmxQIAAABplepp0H755Rd16tRJH3zwgfGoYy8vL5UqVUrfffedFi1apGzZsikmJkbR0dGqX79+uhYOAAAAWCNVPcCff/65cufOrQULFqht27aaO3eu7t+/bywrWrSoYmNjdf36dUVFRalixYr66KOP0rVwAAAAwBqp6gFu2bKlmjZtquXLl2vOnDmaNm2alixZop49e6p9+/ZasmSJrly5or/++ks+Pj7y8fFJ77oBAAAAq6T6yRaZMmVSp06dtHLlSvXr108PHz7UhAkT1KFDB/3666/y9fVVhQoVCL8AAABwaM/3aDdJbm5u6t69u1atWqWuXbvqxo0bGjVqlN5++23t3LkzPWoEAAAAbCbVAfjWrVtat26dFixYoF9//VUmk0kDBw7UypUr1b59e124cEFDhw5V7969deTIkfSsGQAAALBaqsYA79+/Xx9++KFiY2ONNm9vb82cOVNFixbVZ599pq5du2r69OnauHGjevbsqTp16igwMDDdCgcAAACskaoe4KCgIGXKlEmvv/66mjVrpvr16ytTpkyaNm2asY6fn5/GjRunhQsXqlatWtqxY0e6FQ0AAABYK1U9wKGhoQoKClLlypWNtnv37qlnz57J1n3llVf0zTff6NChQ7aqEQAAALCZVAXg/Pnz68svv1Tt2rXl5eWl2NhYHTp0SAUKFHjiNknDMgAAAOAoUhWAu3fvrtGjR2vx4sUymUwym83KnDmzxRAIAAAAICNIVQBu3ry5ihUrpm3bthkPu2jatKn8/PzSuz4AAADAplIVgCWpdOnSKl26dHrWAgAAAKS7VM0C8eGHH2rfvn1WH+TEiRMaMWKE1dv/3dGjR9WnTx/VqVNHTZs21ejRo/XXX38Zy8PDwzV06FAFBASoUaNGGj9+vKKiomx2fAAAAGRcqeoB3r59u7Zv3y4/Pz81atRIAQEBKlu2rFxcUs7P8fHxOnz4sPbt26ft27fr7NmzkqSxY8emueCTJ0+qb9++qlmzpiZNmqQbN27o22+/VXh4uObMmaN79+6pb9++yp07t8aMGaPbt28rKChIERERCg4OTvPxAQAAkLGlKgDPmjVL//rXv3TmzBnNmzdP8+bNU+bMmVWsWDHlzZtXnp6eMplMiomJ0dWrVxUWFqYHDx5Iksxms8qUKaMPP/zQJgUHBQWpdOnSmjx5shHAPT09NXnyZF2+fFkbNmxQZGSkFi1apJw5c0qSfHx8NHjwYB06dIjZKQAAAJxcqgJwpUqVtHDhQm3evFkLFizQyZMn9fDhQ4WEhOj06dMW65rNZkmSyWRSzZo19dZbbykgIEAmkynNxd65c0cHDhzQmDFjLHqfGzZsqIYNG0qSdu/erSpVqhjhV5L8/f3l6empnTt3EoABAACcXKpvgnNxcVGTJk3UpEkTRUREaNeuXTp8+LBu3LhhjL/NlSuX/Pz8VLlyZdWoUUP58uWzabFnz55VQkKCvL29NWLECP3+++8ym81q0KCBPvroI2XLlk2hoaFq0qSJxXaurq7y9fXVxYsX03R8s9msmJiYNO3DEZhMJrm7u9u7DDxDbGys8YESjoFrx/Fx3Tgmrh3H97JcO2azOVWdrqkOwEn5+vqqQ4cO6tChgzWbW+327duSpC+++EK1a9fWpEmTFBYWpqlTp+ry5cuaPXu2oqKi5OnpmWxbDw8PRUdHp+n4cXFxOnnyZJr24Qjc3d1Vrlw5e5eBZ7hw4YJiY2PtXQaS4NpxfFw3jolrx/G9TNdOlixZnrmOVQHYXuLi4iRJZcqU0ciRIyVJNWvWVLZs2TR8+HDt3btXCQkJT9z+STftpVbmzJlVsmTJNO3DEdhiOArSX7FixV6KT+MvE64dx8d145i4dhzfy3LtJE688CwZKgB7eHhIkurWrWvRXrt2bUnSqVOn5OXlleIwhejoaPn4+KTp+CaTyagBSG98XQg8P64bwDovy7WT2g9baesSfcEKFy4sSXr48KFFe3x8vCTJzc1NRYoUUXh4uMXyR48eKSIiQkWLFn0hdQIAAMBxZagAXKxYMfn6+mrDhg0W3fTbtm2TJFWuXFn+/v76888/jfHCkrRnzx7FxMTI39//hdcMAAAAx5KhArDJZNKgQYN09OhRDRs2THv37tXixYsVGBiohg0bqkyZMurQoYOyZs2q/v37a8uWLVq5cqVGjhyp2rVrq1KlSvY+BQAAANiZVWOAjx07pgoVKti6llRp3LixsmbNqlmzZmno0KHKnj273nrrLfXr10+S5O3trRkzZigwMFAjRoyQp6enGjVqpCFDhtilXgAAADgWqwJwt27dVKxYMb3xxhtq2bKl8ubNa+u6nqpu3brJboRLqmTJkpo2bdoLrAgAAAAZhdVDIEJDQzV16lS1atVKAwYM0K+//mo8/hgAAABwVFb1AL///vvavHmzLl26JLPZrH379mnfvn3y8PBQkyZN9MYbb/DIYQAAADgkqwLwgAEDNGDAAIWEhGjTpk3avHmzwsPDFR0drVWrVmnVqlXy9fVVq1at1KpVK+XPn9/WdQMAAABWSdMsEKVLl1b//v21fPlyLVq0SG3btpXZbJbZbFZERIT+/e9/q127dpo4ceJTn9AGAAAAvChpfhLcvXv3tHnzZm3cuFEHDhyQyWQyQrD0+CEUP/74o7Jnz64+ffqkuWAAAAAgLawKwDExMdq6das2bNigffv2GU9iM5vNcnFx0WuvvaY2bdrIZDIpODhYERERWr9+PQEYAAAAdmdVAG7SpIni4uIkyejp9fX1VevWrZON+fXx8VGPHj10/fp1G5QLAAAApI1VAfjhw4eSpCxZsqhhw4Zq27atqlevnuK6vr6+kqRs2bJZWSIAAABgO1YF4LJly6pNmzZq3ry5vLy8nrquu7u7pk6dqoIFC1pVIAAAAGBLVgXg+fPnS3o8FjguLk6ZM2eWJF28eFF58uSRp6ensa6np6dq1qxpg1IBAACAtLN6GrRVq1apVatWOnr0qNG2cOFCtWjRQqtXr7ZJcQAAAICtWRWAd+7cqbFjxyoqKkpnz5412kNDQxUbG6uxY8dq3759NisSAAAAsBWrAvCiRYskSQUKFFCJEiWM9nfeeUeFChWS2WzWggULbFMhAAAAYENWjQE+d+6cTCaTRo0apWrVqhntAQEBypEjh3r37q0zZ87YrEgAAADAVqzqAY6KipIkeXt7J1uWON3ZvXv30lAWAAAAkD6sCsD58uWTJC1fvtyi3Ww2a/HixRbrAAAAAI7EqiEQAQEBWrBggZYuXao9e/aoVKlSio+P1+nTp3XlyhWZTCbVr1/f1rUCAAAAaWZVAO7evbu2bt2q8PBwhYWFKSwszFhmNptVqFAh9ejRw2ZFAgAAALZi1RAILy8vzZ07V+3atZOXl5fMZrPMZrM8PT3Vrl07zZkz55lPiAMAAADswaoeYEnKkSOHhg8frmHDhunOnTsym83y9vaWyWSyZX0AAACATVn9JLhEJpNJ3t7eypUrlxF+ExIStGvXrjQXBwAAANiaVT3AZrNZc+bM0e+//667d+8qISHBWBYfH687d+4oPj5ee/futVmhAAAAgC1YFYCXLFmiGTNmyGQyyWw2WyxLbGMoBAAAAByRVUMg1q1bJ0lyd3dXoUKFZDKZVL58eRUrVswIv5988olNCwUAAABswaoAfOnSJZlMJv3rX//S+PHjZTab1adPHy1dulRvv/22zGazQkNDbVwqAAAAkHZWBeAHDx5IkgoXLqxXXnlFHh4eOnbsmCSpffv2kqSdO3faqEQAAADAdqwKwLly5ZIkhYSEyGQyqVSpUkbgvXTpkiTp+vXrNioRAAAAsB2rAnClSpVkNps1cuRIhYeHq0qVKjpx4oQ6deqkYcOGSfpvSAYAAAAciVUBuGfPnsqePbvi4uKUN29eNWvWTCaTSaGhoYqNjZXJZFLjxo1tXSsAAACQZlYF4GLFimnBggXq1auX3NzcVLJkSY0ePVr58uVT9uzZ1bZtW/Xp08fWtQIAAABpZtU8wDt37lTFihXVs2dPo61ly5Zq2bKlzQoDAAAA0oNVPcCjRo1S8+bN9fvvv9u6HgAAACBdWRWA79+/r7i4OBUtWtTG5QAAAADpy6oA3KhRI0nSli1bbFoMAAAAkN6sGgP8yiuvaMeOHZo6daqWL1+u4sWLy8vLS5ky/Xd3JpNJo0aNslmhAAAAgC1YFYC/+eYbmUwmSdKVK1d05cqVFNcjAAMAAMDRWBWAJclsNj91eWJABgAAAByJVQF49erVtq4DAAAAeCGsCsAFChSwdR0AAADAC2FVAP7zzz9TtV7VqlWt2T0AAACQbqwKwH369HnmGF+TyaS9e/daVRQAAACQXtLtJjgAAADAEVkVgHv16mXx3mw26+HDh7p69aq2bNmiMmXKqHv37jYpEAAAALAlqwJw7969n7hs06ZNGjZsmO7du2d1UQAAAEB6sepRyE/TsGFDSdIPP/xg610DAAAAaWbzAPzHH3/IbDbr3Llztt41AAAAkGZWDYHo27dvsraEhARFRUXp/PnzkqRcuXKlrTIAAAAgHVgVgA8cOPDEadASZ4do1aqV9VUBAAAA6cSm06BlzpxZefPmVbNmzdSzZ880FZZaH330kU6dOqU1a9YYbeHh4QoMDNTBgwfl6uqqxo0ba+DAgfLy8nohNQEAAMBxWRWA//jjD1vXYZWff/5ZW7ZssXg0871799S3b1/lzp1bY8aM0e3btxUUFKSIiAgFBwfbsVoAAAA4Aqt7gFMSFxenzJkz23KXT3Tjxg1NmjRJ+fLls2hftmyZIiMjtWjRIuXMmVOS5OPjo8GDB+vQoUOqXLnyC6kPAAAAjsnqWSBCQkL0j3/8Q6dOnTLagoKC1LNnT505c8YmxT3Nl19+qddee001atSwaN+9e7eqVKlihF9J8vf3l6enp3bu3JnudQEAAMCxWRWAz58/rz59+mj//v0WYTc0NFSHDx9W7969FRoaaqsak1m5cqVOnTqlTz75JNmy0NBQFS5c2KLN1dVVvr6+unjxYrrVBAAAgIzBqiEQc+bMUXR0tLJkyWIxG0TZsmX1559/Kjo6Wv/5z380ZswYW9VpuHLliqZMmaJRo0ZZ9PImioqKkqenZ7J2Dw8PRUdHp+nYZrNZMTExadqHIzCZTHJ3d7d3GXiG2NjYFG82hf1w7Tg+rhvHxLXj+F6Wa8dsNj9xprKkrArAhw4dkslk0ogRI9SiRQuj/R//+IdKliyp4cOH6+DBg9bs+qnMZrO++OIL1a5dW40aNUpxnYSEhCdu7+KStud+xMXF6eTJk2nahyNwd3dXuXLl7F0GnuHChQuKjY21dxlIgmvH8XHdOCauHcf3Ml07WbJkeeY6VgXgv/76S5JUoUKFZMtKly4tSbp586Y1u36qpUuX6syZM1q8eLHi4+Ml/Xc6tvj4eLm4uMjLyyvFXtro6Gj5+Pik6fiZM2dWyZIl07QPR5CaT0awv2LFir0Un8ZfJlw7jo/rxjFx7Ti+l+XaOXv2bKrWsyoA58iRQ7du3dIff/yhQoUKWSzbtWuXJClbtmzW7PqpNm/erDt37qh58+bJlvn7+6tXr14qUqSIwsPDLZY9evRIERERatCgQZqObzKZ5OHhkaZ9AKnF14XA8+O6Aazzslw7qf2wZVUArl69utavX6/Jkyfr5MmTKl26tOLj43XixAlt3LhRJpMp2ewMtjBs2LBkvbuzZs3SyZMnFRgYqLx588rFxUXz58/X7du35e3tLUnas2ePYmJi5O/vb/OaAAAAkLFYFYB79uyp33//XbGxsVq1apXFMrPZLHd3d/Xo0cMmBSZVtGjRZG05cuRQ5syZjbFFHTp00JIlS9S/f3/16tVLkZGRCgoKUu3atVWpUiWb1wQAAICMxaq7wooUKaLg4GAVLlxYZrPZ4l/hwoUVHBycYlh9Eby9vTVjxgzlzJlTI0aM0LRp09SoUSONHz/eLvUAAADAsVj9JLiKFStq2bJlCgkJUXh4uMxmswoVKqTSpUu/0MHuKU21VrJkSU2bNu2F1QAAAICMI02PQo6JiVHx4sWNmR8uXryomJiYFOfhBQAAAByB1RPjrlq1Sq1atdLRo0eNtoULF6pFixZavXq1TYoDAAAAbM2qALxz506NHTtWUVFRFvOthYaGKjY2VmPHjtW+fftsViQAAABgK1YF4EWLFkmSChQooBIlShjt77zzjgoVKiSz2awFCxbYpkIAAADAhqwaA3zu3DmZTCaNGjVK1apVM9oDAgKUI0cO9e7dW2fOnLFZkQAAAICtWNUDHBUVJUnGgyaSSnwC3L1799JQFgAAAJA+rArA+fLlkyQtX77cot1sNmvx4sUW6wAAAACOxKohEAEBAVqwYIGWLl2qPXv2qFSpUoqPj9fp06d15coVmUwm1a9f39a1AgAAAGlmVQDu3r27tm7dqvDwcIWFhSksLMxYlvhAjPR4FDIAAACQVlYNgfDy8tLcuXPVrl07eXl5GY9B9vT0VLt27TRnzhx5eXnZulYAAAAgzax+ElyOHDk0fPhwDRs2THfu3JHZbJa3t/cLfQwyAAAA8LysfhJcIpPJJG9vb+XKlUsmk0mxsbFasWKF3nvvPVvUBwAAANiU1T3Af3fy5EktX75cGzZsUGxsrK12CwAAANhUmgJwTEyMfvnlF61cuVIhISFGu9lsZigEAAAAHJJVAfj48eNasWKFNm7caPT2ms1mSZKrq6vq16+vt956y3ZVAgAAADaS6gAcHR2tX375RStWrDAec5wYehOZTCatXbtWefLksW2VAAAAgI2kKgB/8cUX2rRpk+7fv28Rej08PNSwYUPlz59fs2fPliTCLwAAABxaqgLwmjVrZDKZZDablSlTJvn7+6tFixaqX7++smbNqt27d6d3nQAAAIBNPNc0aCaTST4+PqpQoYLKlSunrFmzplddAAAAQLpIVQ9w5cqVdejQIUnSlStXNHPmTM2cOVPlypVT8+bNeeobAAAAMoxUBeBZs2YpLCxMK1eu1M8//6xbt25Jkk6cOKETJ05YrPvo0SO5urravlIAAADABlI9BKJw4cIaNGiQ1q1bp4kTJ6pOnTrGuOCk8/42b95cX3/9tc6dO5duRQMAAADWeu55gF1dXRUQEKCAgADdvHlTq1ev1po1a3Tp0iVJUmRkpL7//nv98MMP2rt3r80LBgAAANLiuW6C+7s8efKoe/fuWrFihaZPn67mzZsrc+bMRq8wAAAA4GjS9CjkpKpXr67q1avrk08+0c8//6zVq1fbatcAAACAzdgsACfy8vJSp06d1KlTJ1vvGgAAAEizNA2BAAAAADIaAjAAAACcCgEYAAAAToUADAAAAKdCAAYAAIBTIQADAADAqRCAAQAA4FQIwAAAAHAqBGAAAAA4FQIwAAAAnAoBGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMAAAABwKgRgAAAAOBUCMAAAAJwKARgAAABOhQAMAAAAp0IABgAAgFMhAAMAAMCpEIABAADgVDLZu4DnlZCQoOXLl2vZsmW6fPmycuXKpXr16qlPnz7y8vKSJIWHhyswMFAHDx6Uq6urGjdurIEDBxrLAQAA4LwyXACeP3++pk+frq5du6pGjRoKCwvTjBkzdO7cOU2dOlVRUVHq27evcufOrTFjxuj27dsKCgpSRESEgoOD7V0+AAAA7CxDBeCEhATNmzdPb775pgYMGCBJeu2115QjRw4NGzZMJ0+e1N69exUZGalFixYpZ86ckiQfHx8NHjxYhw4dUuXKle13AgAAALC7DDUGODo6Wi1btlSzZs0s2osWLSpJunTpknbv3q0qVaoY4VeS/P395enpqZ07d77AagEAAOCIMlQPcLZs2fTRRx8la9+6daskqXjx4goNDVWTJk0slru6usrX11cXL158EWUCAADAgWWoAJySY8eOad68eapbt65KliypqKgoeXp6JlvPw8ND0dHRaTqW2WxWTExMmvbhCEwmk9zd3e1dBp4hNjZWZrPZ3mUgCa4dx8d145i4dhzfy3LtmM1mmUymZ66XoQPwoUOHNHToUPn6+mr06NGSHo8TfhIXl7SN+IiLi9PJkyfTtA9H4O7urnLlytm7DDzDhQsXFBsba+8ykATXjuPjunFMXDuO72W6drJkyfLMdTJsAN6wYYM+//xzFS5cWMHBwcaYXy8vrxR7aaOjo+Xj45OmY2bOnFklS5ZM0z4cQWo+GcH+ihUr9lJ8Gn+ZcO04Pq4bx8S14/helmvn7NmzqVovQwbgBQsWKCgoSNWqVdOkSZMs5vctUqSIwsPDLdZ/9OiRIiIi1KBBgzQd12QyycPDI037AFKLrwuB58d1A1jnZbl2UvthK0PNAiFJP/30k7755hs1btxYwcHByR5u4e/vrz///FO3b9822vbs2aOYmBj5+/u/6HIBAADgYDJUD/DNmzcVGBgoX19fde7cWadOnbJY7ufnpw4dOmjJkiXq37+/evXqpcjISAUFBal27dqqVKmSnSoHAACAo8hQAXjnzp168OCBIiIi1LNnz2TLR48erdatW2vGjBkKDAzUiBEj5OnpqUaNGmnIkCEvvmAAAAA4nAwVgNu2bau2bds+c72SJUtq2rRpL6AiAAAAZDQZbgwwAAAAkBYEYAAAADgVAjAAAACcCgEYAAAAToUADAAAAKdCAAYAAIBTIQADAADAqRCAAQAA4FQIwAAAAHAqBGAAAAA4FQIwAAAAnAoBGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMAAAABwKgRgAAAAOBUCMAAAAJwKARgAAABOhQAMAAAAp0IABgAAgFMhAAMAAMCpEIABAADgVAjAAAAAcCoEYAAAADgVAjAAAACcCgEYAAAAToUADAAAAKdCAAYAAIBTIQADAADAqRCAAQAA4FQIwAAAAHAqBGAAAAA4FQIwAAAAnAoBGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMAAAABwKgRgAAAAOBUCMAAAAJwKARgAAABOhQAMAAAAp0IABgAAgFMhAAMAAMCpEIABAADgVF7qALxnzx699957ev3119WmTRstWLBAZrPZ3mUBAADAjl7aAHz06FENGTJERYoU0cSJE9W8eXMFBQVp3rx59i4NAAAAdpTJ3gWkl5kzZ6p06dL68ssvJUm1a9dWfHy85s6dqy5dusjNzc3OFQIAAMAeXsoe4IcPH+rAgQNq0KCBRXujRo0UHR2tQ4cO2acwAAAA2N1LGYAvX76suLg4FS5c2KK9UKFCkqSLFy/aoywAAAA4gJdyCERUVJQkydPT06Ldw8NDkhQdHf1c+wsJCdHDhw8lSUeOHLFBhfZnMplUM1eCHuVkKIijcXVJ0NGjR7lh00Fx7TgmrhvHx7XjmF62aycuLk4mk+mZ672UATghIeGpy11cnr/jO/GHmZofakbhmTWzvUvAU7xM/629bLh2HBfXjWPj2nFcL8u1YzKZnDcAe3l5SZJiYmIs2hN7fhOXp1bp0qVtUxgAAADs7qUcA+zn5ydXV1eFh4dbtCe+L1q0qB2qAgAAgCN4KQNw1qxZVaVKFW3ZssViTMtvv/0mLy8vVahQwY7VAQAAwJ5eygAsST169NCxY8f06aefaufOnZo+fboWLFigbt26MQcwAACAEzOZX5bb/lKwZcsWzZw5UxcvXpSPj486duyod999195lAQAAwI5e6gAMAAAA/N1LOwQCAAAASAkBGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMAAAABwKgRgZEhjxoxR9erVn/hv06ZN9i4RcCi9e/dW9erV1b179yeu89lnn6l69eoaM2bMiysMcHA3b95Uo0aN1KVLFz18+DDZ8sWLF6tGjRrasWOHHaqDtTLZuwDAWrlz59akSZNSXFa4cOEXXA3g+FxcXHT06FFdu3ZN+fLls1gWGxur7du326kywHHlyZNHw4cP18cff6xp06ZpyJAhxrITJ07om2++0TvvvKM6derYr0g8NwIwMqwsWbLo1VdftXcZQIZRpkwZnTt3Tps2bdI777xjsez333+Xu7u7smfPbqfqAMfVsGFDtW7dWosWLVKdOnVUvXp13bt3T5999plKlSqlAQMG2LtEPCeGQACAk3Bzc1OdOnW0efPmZMs2btyoRo0aydXV1Q6VAY7vo48+kq+vr0aPHq2oqCiNGzdOkZGRGj9+vDJloj8xoyEAI0OLj49P9s9sNtu7LMBhNWnSxBgGkSgqKkq7du1Ss2bN7FgZ4Ng8PDz05Zdf6ubNm+rTp482bdqkESNGqGDBgvYuDVYgACPDunLlivz9/ZP9mzdvnr1LAxxWnTp15O7ubnGj6NatW+Xt7a3KlSvbrzAgA6hYsaK6dOmikJAQBQQEqHHjxvYuCVaizx4ZVp48eRQYGJis3cfHxw7VABmDm5ub6tatq82bNxvjgDds2KCmTZvKZDLZuTrAsd2/f187d+6UyWTSH3/8oUuXLsnPz8/eZcEK9AAjw8qcObPKlSuX7F+ePHnsXRrg0JIOg7hz54727t2rpk2b2rsswOH961//0qVLlzRx4kQ9evRIo0aN0qNHj+xdFqxAAAYAJ1O7dm15eHho8+bN2rJliwoWLKiyZcvauyzAoa1fv15r1qxRv379FBAQoCFDhujIkSOaPXu2vUuDFRgCAQBOJkuWLAoICNDmzZuVNWtWbn4DnuHSpUsaP368atSooa5du0qSOnTooO3bt2vOnDmqVauWKlasaOcq8TzoAQYAJ9SkSRMdOXJEBw4cIAADTxEXF6dhw4YpU6ZM+vzzz+Xi8t/oNHLkSGXLlk0jR45UdHS0HavE8yIAA4AT8vf3V7Zs2VSiRAkVLVrU3uUADis4OFgnTpzQsGHDkt1knfiUuMuXL2vChAl2qhDWMJmZNBUAAABOhB5gAAAAOBUCMAAAAJwKARgAAABOhQAMAAAAp0IABgAAgFMhAAMAAMCpEIABAADgVHgUMgA4gB07dmjt2rU6fvy4/vrrL0lSvnz5VLlyZXXu3FmlS5e2a33Xrl3TG2+8IUlq1aqVxowZY9d6ACAtCMAAYEcxMTEaO3asNmzYkGxZWFiYwsLCtHbtWn388cfq0KGDHSoEgJcPARgA7OiLL77Qpk2bJEkVK1bUe++9pxIlSuju3btau3atfvzxRyUkJGjChAkqU6aMKlSoYOeKASDjIwADgJ1s2bLFCL+1a9dWYGCgMmX67/+Wy5cvL3d3d82fP18JCQn6/vvv9X//93/2KhcAXhoEYACwk+XLlxuvP/zwQ4vwm+i9995TtmzZVLZsWZUrV85ov379umbOnKmdO3cqMjJSefPmVYMGDdSzZ09ly5bNWG/MmDFau3atcuTIoVWrVmnatGnavHmz7t27p5IlS6pv376qXbu2xTGPHTum6dOn68iRI8qUKZMCAgLUpUuXJ57HsWPHNGvWLB0+fFhxcXEqUqSI2rRpo06dOsnF5b/3WlevXl2S9M4770iSVqxYIZPJpEGDBumtt956zp8eAFjPZDabzfYuAgCcUZ06dXT//n35+vpq9erVqd7u8uXL6t69u27dupVsWbFixTR37lx5eXlJ+m8A9vT0VMGCBXX69GmL9V1dXbV06VIVKVJEkvTnn3+qf//+iouLs1gvb968unHjhiTLm+C2bdumTz75RPHx8clqad68ucaOHWu8TwzA2bJl071794z2xYsXq2TJkqk+fwBIK6ZBAwA7uHPnju7fvy9JypMnj8WyR48e6dq1ayn+k6QJEybo1q1bypo1q8aMGaPly5dr7NixcnNz04ULFzRjxoxkx4uOjta9e/cUFBSkZcuW6bXXXjOO9fPPPxvrTZo0yQi/7733npYuXaoJEyakGHDv37+vsWPHKj4+Xn5+fvr222+1bNky9ezZU5K0fv16bdmyJdl29+7dU6dOnfTTTz/pq6++IvwCeOEYAgEAdpB0aMCjR48slkVERKh9+/Ypbvfbb79p9+7dkqR69eqpRo0akqQqVaqoYcOG+vnnn/Xzzz/rww8/lMlksth2yJAhxnCH/v37a+/evZJk9CTfuHHD6CGuXLmyBg0aJEkqXry4IiMjNW7cOIv97dmzR7dv35Ykde7cWcWKFZMktW/fXr/++qvCw8O1du1aNWjQwGK7rFmzatCgQXJzczN6ngHgRSIAA4AdZM+eXe7u7oqNjdWVK1dSvV14eLgSEhIkSRs3btTGjRuTrXP37l1dvnxZfn5+Fu3Fixc3Xnt7exuvE3t3r169arT9fbaJV199NdlxwsLCjNeTJ0/W5MmTk61z6tSpZG0FCxaUm5tbsnYAeFEYAgEAdlKzZk1J0l9//aXjx48b7YUKFdL+/fuNfwUKFDCWubq6pmrfiT2zSWXNmtV4nbQHOlHSHuPEkP209VNTS0p1JI5PBgB7oQcYAOykbdu22rZtmyQpMDBQ06ZNswipkhQXF6eHDx8a75P26rZv317Dhw833p87d06enp7Knz+/VfUULFjQeJ00kEvS4cOHk61fqFAh4/XYsWPVvHlz4/2xY8dUqFAh5ciRI9l2Kc12AQAvEj3AAGAn9erVU9OmTSU9Dpg9evTQb7/9pkuXLun06dNavHixOnXqZDHbg5eXl+rWrStJWrt2rX766SeFhYVp+/bt6t69u1q1aqWuXbvKmgl+vL29VbVqVaOeKVOm6OzZs9q0aZOmTp2abP2aNWsqd+7ckqRp06Zp+/btunTpkhYuXKgPPvhAjRo10pQpU567DgBIb3wMBwA7GjVqlLJmzao1a9bo1KlT+vjjj1Ncz8vLS3369JEkDRo0SEeOHFFkZKTGjx9vsV7WrFk1cODAZDfApdZHH32knj17Kjo6WosWLdKiRYskSYULF9bDhw8VExNjrOvm5qahQ4dq1KhRioiI0NChQy325evrq3fffdeqOgAgPRGAAcCO3NzcNHr0aLVt21Zr1qzR4cOHdePGDcXHxyt37twqW7asatWqpWbNmsnd3V3S47l+58+fr9mzZ2vfvn26deuWcubMqYoVK6p79+4qU6aM1fWUKlVKc+bMUXBwsA4cOKAsWbKoXr16GjBggDp16pRs/ebNmytv3rxasGCBjh49qpiYGPn4+KhOnTrq1q1bsineAMAR8CAMAAAAOBXGAAMAAMCpEIABAADgVAjAAAAAcCoEYAAAADgVAjAAAACcCgEYAAAAToUADAAAAKdCAAYAAIBTIQADAADAqRCAAQAA4FQIwAAAAHAqBGAAAAA4FQIwAAAAnMr/A6l4+IbPH1NmAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae066af0-659b-43f0-924c-2c50be0e6c40",
   "metadata": {},
   "source": [
    "# RANDOM SEED 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "cc76296a-4029-447e-b12c-41520160251a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[1]))\n",
    "np.random.seed(int(random_seeds[1]))\n",
    "tf.random.set_seed(int(random_seeds[1]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_13.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "093977a2-1813-4a02-9573-f757b3d2a567",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "0c89dbfa-4c9b-4e46-a0d9-f659db30532f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a62fdf-86b5-45e7-9249-73a55985936a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "ee1b36c7-4a91-4071-a713-c904da913c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "106A    14\n",
      "097B    14\n",
      "001A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "005A    10\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "022A     9\n",
      "051B     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "033A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "027A     7\n",
      "117A     7\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "053A     6\n",
      "008A     6\n",
      "023A     6\n",
      "037A     6\n",
      "007A     6\n",
      "034A     5\n",
      "025C     5\n",
      "070A     5\n",
      "023B     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "062A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "052A     4\n",
      "009A     4\n",
      "060A     3\n",
      "012A     3\n",
      "006A     3\n",
      "064A     3\n",
      "058A     3\n",
      "054A     2\n",
      "061A     2\n",
      "087A     2\n",
      "069A     2\n",
      "032A     2\n",
      "011A     2\n",
      "018A     2\n",
      "025B     2\n",
      "093A     2\n",
      "088A     1\n",
      "091A     1\n",
      "100A     1\n",
      "090A     1\n",
      "019B     1\n",
      "115A     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "073A     1\n",
      "026C     1\n",
      "076A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "014B    10\n",
      "015A     9\n",
      "099A     7\n",
      "050A     7\n",
      "021A     5\n",
      "003A     4\n",
      "056A     3\n",
      "014A     3\n",
      "113A     3\n",
      "038A     2\n",
      "102A     2\n",
      "096A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    233\n",
      "M    226\n",
      "F    210\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    115\n",
      "M    111\n",
      "F     42\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 040A, 046A, 109A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 059A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 103A, 074A, 002B, 101A, 038A, 099A, 014...\n",
      "kitten                 [014B, 111A, 047A, 042A, 050A, 110A]\n",
      "senior                       [097A, 113A, 056A, 051A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "669\n",
      "Length of y_train_val:\n",
      "669\n",
      "Length of groups_train_val:\n",
      "669\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age group distribution: Counter({0: 856, 2: 715, 1: 490})\n",
      "Epoch 1/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 1.1795 - accuracy: 0.4789\n",
      "Epoch 2/1500\n",
      "33/33 [==============================] - 0s 921us/step - loss: 0.9956 - accuracy: 0.5662\n",
      "Epoch 3/1500\n",
      "33/33 [==============================] - 0s 868us/step - loss: 0.9461 - accuracy: 0.5847\n",
      "Epoch 4/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.9158 - accuracy: 0.5987\n",
      "Epoch 5/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.8631 - accuracy: 0.6312\n",
      "Epoch 6/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.8280 - accuracy: 0.6443\n",
      "Epoch 7/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.8147 - accuracy: 0.6434\n",
      "Epoch 8/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.7493 - accuracy: 0.6836\n",
      "Epoch 9/1500\n",
      "33/33 [==============================] - 0s 757us/step - loss: 0.7580 - accuracy: 0.6696\n",
      "Epoch 10/1500\n",
      "33/33 [==============================] - 0s 757us/step - loss: 0.7237 - accuracy: 0.6822\n",
      "Epoch 11/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.7275 - accuracy: 0.6943\n",
      "Epoch 12/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.7159 - accuracy: 0.6958\n",
      "Epoch 13/1500\n",
      "33/33 [==============================] - 0s 763us/step - loss: 0.7187 - accuracy: 0.6943\n",
      "Epoch 14/1500\n",
      "33/33 [==============================] - 0s 795us/step - loss: 0.6987 - accuracy: 0.7021\n",
      "Epoch 15/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.6642 - accuracy: 0.7142\n",
      "Epoch 16/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.6656 - accuracy: 0.7147\n",
      "Epoch 17/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.6725 - accuracy: 0.7103\n",
      "Epoch 18/1500\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.6428 - accuracy: 0.7254\n",
      "Epoch 19/1500\n",
      "33/33 [==============================] - 0s 743us/step - loss: 0.6524 - accuracy: 0.7200\n",
      "Epoch 20/1500\n",
      "33/33 [==============================] - 0s 758us/step - loss: 0.6372 - accuracy: 0.7288\n",
      "Epoch 21/1500\n",
      "33/33 [==============================] - 0s 797us/step - loss: 0.6282 - accuracy: 0.7341\n",
      "Epoch 22/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.6248 - accuracy: 0.7346\n",
      "Epoch 23/1500\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.6049 - accuracy: 0.7327\n",
      "Epoch 24/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.5914 - accuracy: 0.7365\n",
      "Epoch 25/1500\n",
      "33/33 [==============================] - 0s 737us/step - loss: 0.5963 - accuracy: 0.7516\n",
      "Epoch 26/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.5633 - accuracy: 0.7545\n",
      "Epoch 27/1500\n",
      "33/33 [==============================] - 0s 723us/step - loss: 0.5684 - accuracy: 0.7603\n",
      "Epoch 28/1500\n",
      "33/33 [==============================] - 0s 746us/step - loss: 0.5661 - accuracy: 0.7545\n",
      "Epoch 29/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.5774 - accuracy: 0.7627\n",
      "Epoch 30/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.5632 - accuracy: 0.7593\n",
      "Epoch 31/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.5390 - accuracy: 0.7686\n",
      "Epoch 32/1500\n",
      "33/33 [==============================] - 0s 733us/step - loss: 0.5697 - accuracy: 0.7608\n",
      "Epoch 33/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.5526 - accuracy: 0.7666\n",
      "Epoch 34/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.5455 - accuracy: 0.7598\n",
      "Epoch 35/1500\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.5524 - accuracy: 0.7642\n",
      "Epoch 36/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.5443 - accuracy: 0.7608\n",
      "Epoch 37/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.5421 - accuracy: 0.7613\n",
      "Epoch 38/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.5177 - accuracy: 0.7821\n",
      "Epoch 39/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.5329 - accuracy: 0.7671\n",
      "Epoch 40/1500\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.5131 - accuracy: 0.7870\n",
      "Epoch 41/1500\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.5215 - accuracy: 0.7787\n",
      "Epoch 42/1500\n",
      "33/33 [==============================] - 0s 753us/step - loss: 0.5129 - accuracy: 0.7841\n",
      "Epoch 43/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.5187 - accuracy: 0.7846\n",
      "Epoch 44/1500\n",
      "33/33 [==============================] - 0s 744us/step - loss: 0.5081 - accuracy: 0.7812\n",
      "Epoch 45/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.4958 - accuracy: 0.7865\n",
      "Epoch 46/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.5175 - accuracy: 0.7826\n",
      "Epoch 47/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.4982 - accuracy: 0.7851\n",
      "Epoch 48/1500\n",
      "33/33 [==============================] - 0s 776us/step - loss: 0.4973 - accuracy: 0.7802\n",
      "Epoch 49/1500\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.4974 - accuracy: 0.7899\n",
      "Epoch 50/1500\n",
      "33/33 [==============================] - 0s 850us/step - loss: 0.4944 - accuracy: 0.7914\n",
      "Epoch 51/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.4765 - accuracy: 0.7962\n",
      "Epoch 52/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.4791 - accuracy: 0.7943\n",
      "Epoch 53/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.4902 - accuracy: 0.7933\n",
      "Epoch 54/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.4649 - accuracy: 0.8117\n",
      "Epoch 55/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.4656 - accuracy: 0.8045\n",
      "Epoch 56/1500\n",
      "33/33 [==============================] - 0s 742us/step - loss: 0.4968 - accuracy: 0.7836\n",
      "Epoch 57/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.4781 - accuracy: 0.7938\n",
      "Epoch 58/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.4566 - accuracy: 0.8088\n",
      "Epoch 59/1500\n",
      "33/33 [==============================] - 0s 793us/step - loss: 0.4553 - accuracy: 0.8054\n",
      "Epoch 60/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.4719 - accuracy: 0.7986\n",
      "Epoch 61/1500\n",
      "33/33 [==============================] - 0s 757us/step - loss: 0.4640 - accuracy: 0.7991\n",
      "Epoch 62/1500\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.4793 - accuracy: 0.7938\n",
      "Epoch 63/1500\n",
      "33/33 [==============================] - 0s 735us/step - loss: 0.4692 - accuracy: 0.7986\n",
      "Epoch 64/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.4560 - accuracy: 0.8083\n",
      "Epoch 65/1500\n",
      "33/33 [==============================] - 0s 744us/step - loss: 0.4682 - accuracy: 0.8025\n",
      "Epoch 66/1500\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.4564 - accuracy: 0.8113\n",
      "Epoch 67/1500\n",
      "33/33 [==============================] - 0s 745us/step - loss: 0.4593 - accuracy: 0.8001\n",
      "Epoch 68/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.4335 - accuracy: 0.8180\n",
      "Epoch 69/1500\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.4510 - accuracy: 0.8049\n",
      "Epoch 70/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.4365 - accuracy: 0.8103\n",
      "Epoch 71/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.4399 - accuracy: 0.8132\n",
      "Epoch 72/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.4460 - accuracy: 0.8156\n",
      "Epoch 73/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.4238 - accuracy: 0.8248\n",
      "Epoch 74/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.4283 - accuracy: 0.8190\n",
      "Epoch 75/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.4340 - accuracy: 0.8234\n",
      "Epoch 76/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.4271 - accuracy: 0.8147\n",
      "Epoch 77/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.4155 - accuracy: 0.8287\n",
      "Epoch 78/1500\n",
      "33/33 [==============================] - 0s 754us/step - loss: 0.4235 - accuracy: 0.8171\n",
      "Epoch 79/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.4420 - accuracy: 0.8074\n",
      "Epoch 80/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.4139 - accuracy: 0.8258\n",
      "Epoch 81/1500\n",
      "33/33 [==============================] - 0s 758us/step - loss: 0.4350 - accuracy: 0.8263\n",
      "Epoch 82/1500\n",
      "33/33 [==============================] - 0s 745us/step - loss: 0.4167 - accuracy: 0.8214\n",
      "Epoch 83/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.4260 - accuracy: 0.8219\n",
      "Epoch 84/1500\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.4173 - accuracy: 0.8210\n",
      "Epoch 85/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.4175 - accuracy: 0.8244\n",
      "Epoch 86/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.4179 - accuracy: 0.8214\n",
      "Epoch 87/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.4051 - accuracy: 0.8341\n",
      "Epoch 88/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.4058 - accuracy: 0.8316\n",
      "Epoch 89/1500\n",
      "33/33 [==============================] - 0s 744us/step - loss: 0.4075 - accuracy: 0.8224\n",
      "Epoch 90/1500\n",
      "33/33 [==============================] - 0s 804us/step - loss: 0.3982 - accuracy: 0.8365\n",
      "Epoch 91/1500\n",
      "33/33 [==============================] - 0s 741us/step - loss: 0.3976 - accuracy: 0.8321\n",
      "Epoch 92/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.3969 - accuracy: 0.8336\n",
      "Epoch 93/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.4040 - accuracy: 0.8316\n",
      "Epoch 94/1500\n",
      "33/33 [==============================] - 0s 747us/step - loss: 0.4001 - accuracy: 0.8311\n",
      "Epoch 95/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.3984 - accuracy: 0.8345\n",
      "Epoch 96/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.3773 - accuracy: 0.8423\n",
      "Epoch 97/1500\n",
      "33/33 [==============================] - 0s 760us/step - loss: 0.3900 - accuracy: 0.8375\n",
      "Epoch 98/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.3961 - accuracy: 0.8258\n",
      "Epoch 99/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.3991 - accuracy: 0.8375\n",
      "Epoch 100/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.3991 - accuracy: 0.8355\n",
      "Epoch 101/1500\n",
      "33/33 [==============================] - 0s 731us/step - loss: 0.3840 - accuracy: 0.8467\n",
      "Epoch 102/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.3872 - accuracy: 0.8345\n",
      "Epoch 103/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.3804 - accuracy: 0.8515\n",
      "Epoch 104/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.3868 - accuracy: 0.8447\n",
      "Epoch 105/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.3832 - accuracy: 0.8438\n",
      "Epoch 106/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.3752 - accuracy: 0.8443\n",
      "Epoch 107/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.3764 - accuracy: 0.8375\n",
      "Epoch 108/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.3677 - accuracy: 0.8404\n",
      "Epoch 109/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.3706 - accuracy: 0.8423\n",
      "Epoch 110/1500\n",
      "33/33 [==============================] - 0s 787us/step - loss: 0.3526 - accuracy: 0.8452\n",
      "Epoch 111/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.3772 - accuracy: 0.8418\n",
      "Epoch 112/1500\n",
      "33/33 [==============================] - 0s 778us/step - loss: 0.3693 - accuracy: 0.8404\n",
      "Epoch 113/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.3742 - accuracy: 0.8389\n",
      "Epoch 114/1500\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.3646 - accuracy: 0.8476\n",
      "Epoch 115/1500\n",
      "33/33 [==============================] - 0s 811us/step - loss: 0.3645 - accuracy: 0.8457\n",
      "Epoch 116/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.3587 - accuracy: 0.8540\n",
      "Epoch 117/1500\n",
      "33/33 [==============================] - 0s 834us/step - loss: 0.3882 - accuracy: 0.8423\n",
      "Epoch 118/1500\n",
      "33/33 [==============================] - 0s 934us/step - loss: 0.3566 - accuracy: 0.8588\n",
      "Epoch 119/1500\n",
      "33/33 [==============================] - 0s 760us/step - loss: 0.3539 - accuracy: 0.8578\n",
      "Epoch 120/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.3623 - accuracy: 0.8496\n",
      "Epoch 121/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.3521 - accuracy: 0.8472\n",
      "Epoch 122/1500\n",
      "33/33 [==============================] - 0s 757us/step - loss: 0.3563 - accuracy: 0.8476\n",
      "Epoch 123/1500\n",
      "33/33 [==============================] - 0s 802us/step - loss: 0.3585 - accuracy: 0.8520\n",
      "Epoch 124/1500\n",
      "33/33 [==============================] - 0s 824us/step - loss: 0.3476 - accuracy: 0.8564\n",
      "Epoch 125/1500\n",
      "33/33 [==============================] - 0s 854us/step - loss: 0.3428 - accuracy: 0.8574\n",
      "Epoch 126/1500\n",
      "33/33 [==============================] - 0s 800us/step - loss: 0.3419 - accuracy: 0.8632\n",
      "Epoch 127/1500\n",
      "33/33 [==============================] - 0s 776us/step - loss: 0.3723 - accuracy: 0.8389\n",
      "Epoch 128/1500\n",
      "33/33 [==============================] - 0s 787us/step - loss: 0.3380 - accuracy: 0.8612\n",
      "Epoch 129/1500\n",
      "33/33 [==============================] - 0s 790us/step - loss: 0.3353 - accuracy: 0.8583\n",
      "Epoch 130/1500\n",
      "33/33 [==============================] - 0s 760us/step - loss: 0.3370 - accuracy: 0.8656\n",
      "Epoch 131/1500\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.3374 - accuracy: 0.8612\n",
      "Epoch 132/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.3248 - accuracy: 0.8729\n",
      "Epoch 133/1500\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.3362 - accuracy: 0.8574\n",
      "Epoch 134/1500\n",
      "33/33 [==============================] - 0s 760us/step - loss: 0.3430 - accuracy: 0.8564\n",
      "Epoch 135/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.3485 - accuracy: 0.8525\n",
      "Epoch 136/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.3253 - accuracy: 0.8685\n",
      "Epoch 137/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.3340 - accuracy: 0.8675\n",
      "Epoch 138/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.3236 - accuracy: 0.8724\n",
      "Epoch 139/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.3380 - accuracy: 0.8656\n",
      "Epoch 140/1500\n",
      "33/33 [==============================] - 0s 763us/step - loss: 0.3277 - accuracy: 0.8714\n",
      "Epoch 141/1500\n",
      "33/33 [==============================] - 0s 816us/step - loss: 0.3164 - accuracy: 0.8724\n",
      "Epoch 142/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.3412 - accuracy: 0.8564\n",
      "Epoch 143/1500\n",
      "33/33 [==============================] - 0s 793us/step - loss: 0.3103 - accuracy: 0.8758\n",
      "Epoch 144/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.3239 - accuracy: 0.8675\n",
      "Epoch 145/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.3263 - accuracy: 0.8709\n",
      "Epoch 146/1500\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.3210 - accuracy: 0.8637\n",
      "Epoch 147/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.3165 - accuracy: 0.8690\n",
      "Epoch 148/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.3207 - accuracy: 0.8714\n",
      "Epoch 149/1500\n",
      "33/33 [==============================] - 0s 745us/step - loss: 0.3298 - accuracy: 0.8637\n",
      "Epoch 150/1500\n",
      "33/33 [==============================] - 0s 795us/step - loss: 0.3099 - accuracy: 0.8787\n",
      "Epoch 151/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.3261 - accuracy: 0.8675\n",
      "Epoch 152/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.3286 - accuracy: 0.8700\n",
      "Epoch 153/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.3225 - accuracy: 0.8700\n",
      "Epoch 154/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.3269 - accuracy: 0.8627\n",
      "Epoch 155/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.3064 - accuracy: 0.8802\n",
      "Epoch 156/1500\n",
      "33/33 [==============================] - 0s 797us/step - loss: 0.3033 - accuracy: 0.8811\n",
      "Epoch 157/1500\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.2979 - accuracy: 0.8748\n",
      "Epoch 158/1500\n",
      "33/33 [==============================] - 0s 738us/step - loss: 0.3031 - accuracy: 0.8802\n",
      "Epoch 159/1500\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.3182 - accuracy: 0.8714\n",
      "Epoch 160/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.3118 - accuracy: 0.8734\n",
      "Epoch 161/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.3071 - accuracy: 0.8768\n",
      "Epoch 162/1500\n",
      "33/33 [==============================] - 0s 809us/step - loss: 0.3088 - accuracy: 0.8685\n",
      "Epoch 163/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.3085 - accuracy: 0.8748\n",
      "Epoch 164/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.3073 - accuracy: 0.8768\n",
      "Epoch 165/1500\n",
      "33/33 [==============================] - 0s 763us/step - loss: 0.3194 - accuracy: 0.8680\n",
      "Epoch 166/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.2856 - accuracy: 0.8826\n",
      "Epoch 167/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.3143 - accuracy: 0.8656\n",
      "Epoch 168/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.3027 - accuracy: 0.8787\n",
      "Epoch 169/1500\n",
      "33/33 [==============================] - 0s 790us/step - loss: 0.2913 - accuracy: 0.8899\n",
      "Epoch 170/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.3126 - accuracy: 0.8758\n",
      "Epoch 171/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.3168 - accuracy: 0.8734\n",
      "Epoch 172/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.2981 - accuracy: 0.8729\n",
      "Epoch 173/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.2923 - accuracy: 0.8797\n",
      "Epoch 174/1500\n",
      "33/33 [==============================] - 0s 742us/step - loss: 0.2904 - accuracy: 0.8845\n",
      "Epoch 175/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.2815 - accuracy: 0.8884\n",
      "Epoch 176/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.2867 - accuracy: 0.8816\n",
      "Epoch 177/1500\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.3000 - accuracy: 0.8831\n",
      "Epoch 178/1500\n",
      "33/33 [==============================] - 0s 778us/step - loss: 0.2969 - accuracy: 0.8802\n",
      "Epoch 179/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.2964 - accuracy: 0.8850\n",
      "Epoch 180/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.2874 - accuracy: 0.8889\n",
      "Epoch 181/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.2911 - accuracy: 0.8826\n",
      "Epoch 182/1500\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2815 - accuracy: 0.8865\n",
      "Epoch 183/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2774 - accuracy: 0.8937\n",
      "Epoch 184/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.2823 - accuracy: 0.8894\n",
      "Epoch 185/1500\n",
      "33/33 [==============================] - 0s 828us/step - loss: 0.2732 - accuracy: 0.8908\n",
      "Epoch 186/1500\n",
      "33/33 [==============================] - 0s 847us/step - loss: 0.2689 - accuracy: 0.8869\n",
      "Epoch 187/1500\n",
      "33/33 [==============================] - 0s 809us/step - loss: 0.2991 - accuracy: 0.8831\n",
      "Epoch 188/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.2742 - accuracy: 0.8976\n",
      "Epoch 189/1500\n",
      "33/33 [==============================] - 0s 793us/step - loss: 0.2773 - accuracy: 0.8894\n",
      "Epoch 190/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.2933 - accuracy: 0.8899\n",
      "Epoch 191/1500\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.2800 - accuracy: 0.8894\n",
      "Epoch 192/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.2711 - accuracy: 0.8942\n",
      "Epoch 193/1500\n",
      "33/33 [==============================] - 0s 817us/step - loss: 0.2668 - accuracy: 0.8991\n",
      "Epoch 194/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.2932 - accuracy: 0.8748\n",
      "Epoch 195/1500\n",
      "33/33 [==============================] - 0s 828us/step - loss: 0.2888 - accuracy: 0.8879\n",
      "Epoch 196/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.2822 - accuracy: 0.8826\n",
      "Epoch 197/1500\n",
      "33/33 [==============================] - 0s 804us/step - loss: 0.2658 - accuracy: 0.9000\n",
      "Epoch 198/1500\n",
      "33/33 [==============================] - 0s 812us/step - loss: 0.2915 - accuracy: 0.8777\n",
      "Epoch 199/1500\n",
      "33/33 [==============================] - 0s 746us/step - loss: 0.2722 - accuracy: 0.8933\n",
      "Epoch 200/1500\n",
      "33/33 [==============================] - 0s 800us/step - loss: 0.2741 - accuracy: 0.8894\n",
      "Epoch 201/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.2693 - accuracy: 0.8952\n",
      "Epoch 202/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.2589 - accuracy: 0.9005\n",
      "Epoch 203/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.2821 - accuracy: 0.8884\n",
      "Epoch 204/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.2670 - accuracy: 0.8933\n",
      "Epoch 205/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.2887 - accuracy: 0.8869\n",
      "Epoch 206/1500\n",
      "33/33 [==============================] - 0s 795us/step - loss: 0.2823 - accuracy: 0.8782\n",
      "Epoch 207/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.2658 - accuracy: 0.8923\n",
      "Epoch 208/1500\n",
      "33/33 [==============================] - 0s 778us/step - loss: 0.2653 - accuracy: 0.9005\n",
      "Epoch 209/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.2583 - accuracy: 0.8986\n",
      "Epoch 210/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.2623 - accuracy: 0.8942\n",
      "Epoch 211/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.2543 - accuracy: 0.9015\n",
      "Epoch 212/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.2602 - accuracy: 0.8991\n",
      "Epoch 213/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.2671 - accuracy: 0.8860\n",
      "Epoch 214/1500\n",
      "33/33 [==============================] - 0s 809us/step - loss: 0.2609 - accuracy: 0.8918\n",
      "Epoch 215/1500\n",
      "33/33 [==============================] - 0s 877us/step - loss: 0.2645 - accuracy: 0.8840\n",
      "Epoch 216/1500\n",
      "33/33 [==============================] - 0s 852us/step - loss: 0.2572 - accuracy: 0.9005\n",
      "Epoch 217/1500\n",
      "33/33 [==============================] - 0s 938us/step - loss: 0.2556 - accuracy: 0.9034\n",
      "Epoch 218/1500\n",
      "33/33 [==============================] - 0s 915us/step - loss: 0.2644 - accuracy: 0.8899\n",
      "Epoch 219/1500\n",
      "33/33 [==============================] - 0s 919us/step - loss: 0.2602 - accuracy: 0.9015\n",
      "Epoch 220/1500\n",
      "33/33 [==============================] - 0s 912us/step - loss: 0.2587 - accuracy: 0.9005\n",
      "Epoch 221/1500\n",
      "33/33 [==============================] - 0s 854us/step - loss: 0.2519 - accuracy: 0.8942\n",
      "Epoch 222/1500\n",
      "33/33 [==============================] - 0s 824us/step - loss: 0.2482 - accuracy: 0.8996\n",
      "Epoch 223/1500\n",
      "33/33 [==============================] - 0s 808us/step - loss: 0.2647 - accuracy: 0.8913\n",
      "Epoch 224/1500\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.2735 - accuracy: 0.8894\n",
      "Epoch 225/1500\n",
      "33/33 [==============================] - 0s 778us/step - loss: 0.2511 - accuracy: 0.8962\n",
      "Epoch 226/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.2562 - accuracy: 0.8957\n",
      "Epoch 227/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.2552 - accuracy: 0.8981\n",
      "Epoch 228/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.2619 - accuracy: 0.8957\n",
      "Epoch 229/1500\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.2376 - accuracy: 0.9039\n",
      "Epoch 230/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2524 - accuracy: 0.8976\n",
      "Epoch 231/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2568 - accuracy: 0.8971\n",
      "Epoch 232/1500\n",
      "33/33 [==============================] - 0s 978us/step - loss: 0.2430 - accuracy: 0.9044\n",
      "Epoch 233/1500\n",
      "33/33 [==============================] - 0s 914us/step - loss: 0.2615 - accuracy: 0.8942\n",
      "Epoch 234/1500\n",
      "33/33 [==============================] - 0s 951us/step - loss: 0.2551 - accuracy: 0.8928\n",
      "Epoch 235/1500\n",
      "33/33 [==============================] - 0s 959us/step - loss: 0.2500 - accuracy: 0.9005\n",
      "Epoch 236/1500\n",
      "33/33 [==============================] - 0s 986us/step - loss: 0.2477 - accuracy: 0.8996\n",
      "Epoch 237/1500\n",
      "33/33 [==============================] - 0s 894us/step - loss: 0.2510 - accuracy: 0.8933\n",
      "Epoch 238/1500\n",
      "33/33 [==============================] - 0s 918us/step - loss: 0.2574 - accuracy: 0.9068\n",
      "Epoch 239/1500\n",
      "33/33 [==============================] - 0s 982us/step - loss: 0.2355 - accuracy: 0.9093\n",
      "Epoch 240/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2478 - accuracy: 0.9005\n",
      "Epoch 241/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2269 - accuracy: 0.9078\n",
      "Epoch 242/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2651 - accuracy: 0.8962\n",
      "Epoch 243/1500\n",
      "33/33 [==============================] - 0s 971us/step - loss: 0.2479 - accuracy: 0.8962\n",
      "Epoch 244/1500\n",
      "33/33 [==============================] - 0s 945us/step - loss: 0.2573 - accuracy: 0.8933\n",
      "Epoch 245/1500\n",
      "33/33 [==============================] - 0s 936us/step - loss: 0.2350 - accuracy: 0.9083\n",
      "Epoch 246/1500\n",
      "33/33 [==============================] - 0s 917us/step - loss: 0.2331 - accuracy: 0.9102\n",
      "Epoch 247/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2391 - accuracy: 0.9034\n",
      "Epoch 248/1500\n",
      "33/33 [==============================] - 0s 948us/step - loss: 0.2479 - accuracy: 0.9044\n",
      "Epoch 249/1500\n",
      "33/33 [==============================] - 0s 904us/step - loss: 0.2338 - accuracy: 0.9098\n",
      "Epoch 250/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.8976\n",
      "Epoch 251/1500\n",
      "33/33 [==============================] - 0s 941us/step - loss: 0.2519 - accuracy: 0.8952\n",
      "Epoch 252/1500\n",
      "33/33 [==============================] - 0s 929us/step - loss: 0.2385 - accuracy: 0.9044\n",
      "Epoch 253/1500\n",
      "33/33 [==============================] - 0s 913us/step - loss: 0.2326 - accuracy: 0.9088\n",
      "Epoch 254/1500\n",
      "33/33 [==============================] - 0s 921us/step - loss: 0.2307 - accuracy: 0.9010\n",
      "Epoch 255/1500\n",
      "33/33 [==============================] - 0s 889us/step - loss: 0.2405 - accuracy: 0.9025\n",
      "Epoch 256/1500\n",
      "33/33 [==============================] - 0s 937us/step - loss: 0.2492 - accuracy: 0.9039\n",
      "Epoch 257/1500\n",
      "33/33 [==============================] - 0s 933us/step - loss: 0.2194 - accuracy: 0.9175\n",
      "Epoch 258/1500\n",
      "33/33 [==============================] - 0s 938us/step - loss: 0.2561 - accuracy: 0.8976\n",
      "Epoch 259/1500\n",
      "33/33 [==============================] - 0s 952us/step - loss: 0.2258 - accuracy: 0.9054\n",
      "Epoch 260/1500\n",
      "33/33 [==============================] - 0s 992us/step - loss: 0.2330 - accuracy: 0.9117\n",
      "Epoch 261/1500\n",
      "33/33 [==============================] - 0s 891us/step - loss: 0.2343 - accuracy: 0.9102\n",
      "Epoch 262/1500\n",
      "33/33 [==============================] - 0s 835us/step - loss: 0.2253 - accuracy: 0.9068\n",
      "Epoch 263/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.2285 - accuracy: 0.9059\n",
      "Epoch 264/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.2350 - accuracy: 0.9015\n",
      "Epoch 265/1500\n",
      "33/33 [==============================] - 0s 863us/step - loss: 0.2345 - accuracy: 0.9078\n",
      "Epoch 266/1500\n",
      "33/33 [==============================] - 0s 895us/step - loss: 0.2323 - accuracy: 0.9093\n",
      "Epoch 267/1500\n",
      "33/33 [==============================] - 0s 834us/step - loss: 0.2223 - accuracy: 0.9131\n",
      "Epoch 268/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.2222 - accuracy: 0.9180\n",
      "Epoch 269/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.2216 - accuracy: 0.9088\n",
      "Epoch 270/1500\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.2270 - accuracy: 0.9136\n",
      "Epoch 271/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.2300 - accuracy: 0.9064\n",
      "Epoch 272/1500\n",
      "33/33 [==============================] - 0s 813us/step - loss: 0.2153 - accuracy: 0.9141\n",
      "Epoch 273/1500\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.2119 - accuracy: 0.9238\n",
      "Epoch 274/1500\n",
      "33/33 [==============================] - 0s 813us/step - loss: 0.2378 - accuracy: 0.9078\n",
      "Epoch 275/1500\n",
      "33/33 [==============================] - 0s 800us/step - loss: 0.2156 - accuracy: 0.9141\n",
      "Epoch 276/1500\n",
      "33/33 [==============================] - 0s 814us/step - loss: 0.2316 - accuracy: 0.9117\n",
      "Epoch 277/1500\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.2232 - accuracy: 0.9131\n",
      "Epoch 278/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2465 - accuracy: 0.9025\n",
      "Epoch 279/1500\n",
      "33/33 [==============================] - 0s 797us/step - loss: 0.2223 - accuracy: 0.9088\n",
      "Epoch 280/1500\n",
      "33/33 [==============================] - 0s 811us/step - loss: 0.2314 - accuracy: 0.9034\n",
      "Epoch 281/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.2164 - accuracy: 0.9146\n",
      "Epoch 282/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.2180 - accuracy: 0.9151\n",
      "Epoch 283/1500\n",
      "33/33 [==============================] - 0s 794us/step - loss: 0.2080 - accuracy: 0.9204\n",
      "Epoch 284/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.2227 - accuracy: 0.9141\n",
      "Epoch 285/1500\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.2191 - accuracy: 0.9127\n",
      "Epoch 286/1500\n",
      "33/33 [==============================] - 0s 845us/step - loss: 0.2227 - accuracy: 0.9161\n",
      "Epoch 287/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.2019 - accuracy: 0.9185\n",
      "Epoch 288/1500\n",
      "33/33 [==============================] - 0s 829us/step - loss: 0.2358 - accuracy: 0.9025\n",
      "Epoch 289/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.2296 - accuracy: 0.8996\n",
      "Epoch 290/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.2193 - accuracy: 0.9146\n",
      "Epoch 291/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.2296 - accuracy: 0.9034\n",
      "Epoch 292/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.2147 - accuracy: 0.9151\n",
      "Epoch 293/1500\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.2235 - accuracy: 0.9195\n",
      "Epoch 294/1500\n",
      "33/33 [==============================] - 0s 861us/step - loss: 0.2245 - accuracy: 0.9054\n",
      "Epoch 295/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.2129 - accuracy: 0.9161\n",
      "Epoch 296/1500\n",
      "33/33 [==============================] - 0s 787us/step - loss: 0.2265 - accuracy: 0.9131\n",
      "Epoch 297/1500\n",
      "33/33 [==============================] - 0s 792us/step - loss: 0.2119 - accuracy: 0.9151\n",
      "Epoch 298/1500\n",
      "33/33 [==============================] - 0s 803us/step - loss: 0.2199 - accuracy: 0.9122\n",
      "Epoch 299/1500\n",
      "33/33 [==============================] - 0s 795us/step - loss: 0.2133 - accuracy: 0.9209\n",
      "Epoch 300/1500\n",
      "33/33 [==============================] - 0s 827us/step - loss: 0.2125 - accuracy: 0.9161\n",
      "Epoch 301/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.2238 - accuracy: 0.9117\n",
      "Epoch 302/1500\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.2112 - accuracy: 0.9141\n",
      "Epoch 303/1500\n",
      "33/33 [==============================] - 0s 800us/step - loss: 0.2124 - accuracy: 0.9141\n",
      "Epoch 304/1500\n",
      "33/33 [==============================] - 0s 815us/step - loss: 0.2154 - accuracy: 0.9136\n",
      "Epoch 305/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.2044 - accuracy: 0.9146\n",
      "Epoch 306/1500\n",
      "33/33 [==============================] - 0s 790us/step - loss: 0.2002 - accuracy: 0.9233\n",
      "Epoch 307/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.2082 - accuracy: 0.9127\n",
      "Epoch 308/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.2055 - accuracy: 0.9185\n",
      "Epoch 309/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.1947 - accuracy: 0.9258\n",
      "Epoch 310/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.2176 - accuracy: 0.9156\n",
      "Epoch 311/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.1884 - accuracy: 0.9287\n",
      "Epoch 312/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.2010 - accuracy: 0.9190\n",
      "Epoch 313/1500\n",
      "33/33 [==============================] - 0s 788us/step - loss: 0.2204 - accuracy: 0.9117\n",
      "Epoch 314/1500\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.2111 - accuracy: 0.9199\n",
      "Epoch 315/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.1965 - accuracy: 0.9262\n",
      "Epoch 316/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.2105 - accuracy: 0.9224\n",
      "Epoch 317/1500\n",
      "33/33 [==============================] - 0s 778us/step - loss: 0.2019 - accuracy: 0.9209\n",
      "Epoch 318/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.2131 - accuracy: 0.9161\n",
      "Epoch 319/1500\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.2070 - accuracy: 0.9199\n",
      "Epoch 320/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.2022 - accuracy: 0.9233\n",
      "Epoch 321/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.2082 - accuracy: 0.9185\n",
      "Epoch 322/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.2006 - accuracy: 0.9195\n",
      "Epoch 323/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.2150 - accuracy: 0.9185\n",
      "Epoch 324/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.2011 - accuracy: 0.9190\n",
      "Epoch 325/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.2043 - accuracy: 0.9204\n",
      "Epoch 326/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.2067 - accuracy: 0.9209\n",
      "Epoch 327/1500\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.2131 - accuracy: 0.9175\n",
      "Epoch 328/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.2045 - accuracy: 0.9175\n",
      "Epoch 329/1500\n",
      "33/33 [==============================] - 0s 753us/step - loss: 0.1978 - accuracy: 0.9209\n",
      "Epoch 330/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.1859 - accuracy: 0.9287\n",
      "Epoch 331/1500\n",
      "33/33 [==============================] - 0s 787us/step - loss: 0.1855 - accuracy: 0.9282\n",
      "Epoch 332/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.1916 - accuracy: 0.9248\n",
      "Epoch 333/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.1935 - accuracy: 0.9253\n",
      "Epoch 334/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.1873 - accuracy: 0.9282\n",
      "Epoch 335/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.2096 - accuracy: 0.9185\n",
      "Epoch 336/1500\n",
      "33/33 [==============================] - 0s 783us/step - loss: 0.1930 - accuracy: 0.9272\n",
      "Epoch 337/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.1972 - accuracy: 0.9272\n",
      "Epoch 338/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.1878 - accuracy: 0.9233\n",
      "Epoch 339/1500\n",
      "33/33 [==============================] - 0s 798us/step - loss: 0.1890 - accuracy: 0.9258\n",
      "Epoch 340/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.1995 - accuracy: 0.9238\n",
      "Epoch 341/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.1934 - accuracy: 0.9195\n",
      "Epoch 342/1500\n",
      "33/33 [==============================] - 0s 797us/step - loss: 0.2085 - accuracy: 0.9224\n",
      "Epoch 343/1500\n",
      "33/33 [==============================] - 0s 857us/step - loss: 0.1804 - accuracy: 0.9311\n",
      "Epoch 344/1500\n",
      "33/33 [==============================] - 0s 753us/step - loss: 0.1840 - accuracy: 0.9345\n",
      "Epoch 345/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.2008 - accuracy: 0.9199\n",
      "Epoch 346/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.1913 - accuracy: 0.9258\n",
      "Epoch 347/1500\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.1857 - accuracy: 0.9326\n",
      "Epoch 348/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.2076 - accuracy: 0.9165\n",
      "Epoch 349/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.1921 - accuracy: 0.9219\n",
      "Epoch 350/1500\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1932 - accuracy: 0.9258\n",
      "Epoch 351/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9195\n",
      "Epoch 352/1500\n",
      "33/33 [==============================] - 0s 808us/step - loss: 0.1824 - accuracy: 0.9267\n",
      "Epoch 353/1500\n",
      "33/33 [==============================] - 0s 797us/step - loss: 0.1843 - accuracy: 0.9326\n",
      "Epoch 354/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.2035 - accuracy: 0.9204\n",
      "Epoch 355/1500\n",
      "33/33 [==============================] - 0s 778us/step - loss: 0.1898 - accuracy: 0.9243\n",
      "Epoch 356/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.1826 - accuracy: 0.9326\n",
      "Epoch 357/1500\n",
      "33/33 [==============================] - 0s 805us/step - loss: 0.1928 - accuracy: 0.9233\n",
      "Epoch 358/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.1804 - accuracy: 0.9364\n",
      "Epoch 359/1500\n",
      "33/33 [==============================] - 0s 793us/step - loss: 0.1809 - accuracy: 0.9321\n",
      "Epoch 360/1500\n",
      "33/33 [==============================] - 0s 817us/step - loss: 0.1739 - accuracy: 0.9301\n",
      "Epoch 361/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.1768 - accuracy: 0.9335\n",
      "Epoch 362/1500\n",
      "33/33 [==============================] - 0s 818us/step - loss: 0.2005 - accuracy: 0.9224\n",
      "Epoch 363/1500\n",
      "33/33 [==============================] - 0s 793us/step - loss: 0.2008 - accuracy: 0.9190\n",
      "Epoch 364/1500\n",
      "33/33 [==============================] - 0s 750us/step - loss: 0.2005 - accuracy: 0.9277\n",
      "Epoch 365/1500\n",
      "33/33 [==============================] - 0s 754us/step - loss: 0.1762 - accuracy: 0.9301\n",
      "Epoch 366/1500\n",
      "33/33 [==============================] - 0s 800us/step - loss: 0.1866 - accuracy: 0.9267\n",
      "Epoch 367/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.1907 - accuracy: 0.9185\n",
      "Epoch 368/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.1868 - accuracy: 0.9233\n",
      "Epoch 369/1500\n",
      "33/33 [==============================] - 0s 753us/step - loss: 0.1943 - accuracy: 0.9180\n",
      "Epoch 370/1500\n",
      "33/33 [==============================] - 0s 807us/step - loss: 0.1953 - accuracy: 0.9233\n",
      "Epoch 371/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.1814 - accuracy: 0.9253\n",
      "Epoch 372/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.1949 - accuracy: 0.9272\n",
      "Epoch 373/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.1869 - accuracy: 0.9277\n",
      "Epoch 374/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.1658 - accuracy: 0.9379\n",
      "Epoch 375/1500\n",
      "33/33 [==============================] - 0s 733us/step - loss: 0.1892 - accuracy: 0.9287\n",
      "Epoch 376/1500\n",
      "33/33 [==============================] - 0s 797us/step - loss: 0.1746 - accuracy: 0.9321\n",
      "Epoch 377/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.1735 - accuracy: 0.9437\n",
      "Epoch 378/1500\n",
      "33/33 [==============================] - 0s 790us/step - loss: 0.1883 - accuracy: 0.9219\n",
      "Epoch 379/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.1896 - accuracy: 0.9311\n",
      "Epoch 380/1500\n",
      "33/33 [==============================] - 0s 831us/step - loss: 0.1714 - accuracy: 0.9369\n",
      "Epoch 381/1500\n",
      "33/33 [==============================] - 0s 822us/step - loss: 0.1820 - accuracy: 0.9296\n",
      "Epoch 382/1500\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.1701 - accuracy: 0.9374\n",
      "Epoch 383/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.1716 - accuracy: 0.9340\n",
      "Epoch 384/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.1864 - accuracy: 0.9258\n",
      "Epoch 385/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.1682 - accuracy: 0.9418\n",
      "Epoch 386/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.1793 - accuracy: 0.9311\n",
      "Epoch 387/1500\n",
      "33/33 [==============================] - 0s 778us/step - loss: 0.1764 - accuracy: 0.9379\n",
      "Epoch 388/1500\n",
      "33/33 [==============================] - 0s 758us/step - loss: 0.1732 - accuracy: 0.9364\n",
      "Epoch 389/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.1840 - accuracy: 0.9296\n",
      "Epoch 390/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.1765 - accuracy: 0.9292\n",
      "Epoch 391/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.1870 - accuracy: 0.9287\n",
      "Epoch 392/1500\n",
      "33/33 [==============================] - 0s 754us/step - loss: 0.1791 - accuracy: 0.9277\n",
      "Epoch 393/1500\n",
      "33/33 [==============================] - 0s 840us/step - loss: 0.1836 - accuracy: 0.9292\n",
      "Epoch 394/1500\n",
      "33/33 [==============================] - 0s 842us/step - loss: 0.1815 - accuracy: 0.9311\n",
      "Epoch 395/1500\n",
      "33/33 [==============================] - 0s 841us/step - loss: 0.1785 - accuracy: 0.9262\n",
      "Epoch 396/1500\n",
      "33/33 [==============================] - 0s 836us/step - loss: 0.1828 - accuracy: 0.9233\n",
      "Epoch 397/1500\n",
      "33/33 [==============================] - 0s 790us/step - loss: 0.1793 - accuracy: 0.9316\n",
      "Epoch 398/1500\n",
      "33/33 [==============================] - 0s 816us/step - loss: 0.1730 - accuracy: 0.9311\n",
      "Epoch 399/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.1668 - accuracy: 0.9398\n",
      "Epoch 400/1500\n",
      "33/33 [==============================] - 0s 806us/step - loss: 0.1715 - accuracy: 0.9311\n",
      "Epoch 401/1500\n",
      "33/33 [==============================] - 0s 879us/step - loss: 0.1614 - accuracy: 0.9403\n",
      "Epoch 402/1500\n",
      "33/33 [==============================] - 0s 803us/step - loss: 0.1808 - accuracy: 0.9292\n",
      "Epoch 403/1500\n",
      "33/33 [==============================] - 0s 822us/step - loss: 0.1759 - accuracy: 0.9311\n",
      "Epoch 404/1500\n",
      "33/33 [==============================] - 0s 886us/step - loss: 0.1723 - accuracy: 0.9306\n",
      "Epoch 405/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.1504 - accuracy: 0.9447\n",
      "Epoch 406/1500\n",
      "33/33 [==============================] - 0s 800us/step - loss: 0.1694 - accuracy: 0.9330\n",
      "Epoch 407/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.1683 - accuracy: 0.9350\n",
      "Epoch 408/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.1671 - accuracy: 0.9379\n",
      "Epoch 409/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.1704 - accuracy: 0.9311\n",
      "Epoch 410/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.1627 - accuracy: 0.9369\n",
      "Epoch 411/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.1558 - accuracy: 0.9335\n",
      "Epoch 412/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.1673 - accuracy: 0.9267\n",
      "Epoch 413/1500\n",
      "33/33 [==============================] - 0s 5ms/step - loss: 0.1748 - accuracy: 0.9306\n",
      "Epoch 414/1500\n",
      "33/33 [==============================] - 0s 804us/step - loss: 0.1644 - accuracy: 0.9437\n",
      "Epoch 415/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.1830 - accuracy: 0.9301\n",
      "Epoch 416/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.1627 - accuracy: 0.9418\n",
      "Epoch 417/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.1784 - accuracy: 0.9296\n",
      "Epoch 418/1500\n",
      "33/33 [==============================] - 0s 801us/step - loss: 0.1750 - accuracy: 0.9316\n",
      "Epoch 419/1500\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.1671 - accuracy: 0.9379\n",
      "Epoch 420/1500\n",
      "33/33 [==============================] - 0s 776us/step - loss: 0.1849 - accuracy: 0.9258\n",
      "Epoch 421/1500\n",
      "33/33 [==============================] - 0s 754us/step - loss: 0.1568 - accuracy: 0.9369\n",
      "Epoch 422/1500\n",
      "33/33 [==============================] - 0s 827us/step - loss: 0.1653 - accuracy: 0.9398\n",
      "Epoch 423/1500\n",
      "33/33 [==============================] - 0s 749us/step - loss: 0.1696 - accuracy: 0.9326\n",
      "Epoch 424/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.1605 - accuracy: 0.9413\n",
      "Epoch 425/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.1683 - accuracy: 0.9345\n",
      "Epoch 426/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.1873 - accuracy: 0.9316\n",
      "Epoch 427/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.1552 - accuracy: 0.9369\n",
      "Epoch 428/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.1586 - accuracy: 0.9427\n",
      "Epoch 429/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.1691 - accuracy: 0.9389\n",
      "Epoch 430/1500\n",
      "33/33 [==============================] - 0s 816us/step - loss: 0.1494 - accuracy: 0.9437\n",
      "Epoch 431/1500\n",
      "33/33 [==============================] - 0s 839us/step - loss: 0.1741 - accuracy: 0.9403\n",
      "Epoch 432/1500\n",
      "33/33 [==============================] - 0s 872us/step - loss: 0.1632 - accuracy: 0.9355\n",
      "Epoch 433/1500\n",
      "33/33 [==============================] - 0s 925us/step - loss: 0.1657 - accuracy: 0.9374\n",
      "Epoch 434/1500\n",
      "33/33 [==============================] - 0s 903us/step - loss: 0.1640 - accuracy: 0.9350\n",
      "Epoch 435/1500\n",
      "33/33 [==============================] - 0s 927us/step - loss: 0.1505 - accuracy: 0.9432\n",
      "Epoch 436/1500\n",
      "33/33 [==============================] - 0s 849us/step - loss: 0.1666 - accuracy: 0.9326\n",
      "Epoch 437/1500\n",
      "33/33 [==============================] - 0s 797us/step - loss: 0.1642 - accuracy: 0.9369\n",
      "Epoch 438/1500\n",
      "33/33 [==============================] - 0s 808us/step - loss: 0.1706 - accuracy: 0.9335\n",
      "Epoch 439/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.1599 - accuracy: 0.9408\n",
      "Epoch 440/1500\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.1514 - accuracy: 0.9408\n",
      "Epoch 441/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.1505 - accuracy: 0.9427\n",
      "Epoch 442/1500\n",
      "33/33 [==============================] - 0s 788us/step - loss: 0.1528 - accuracy: 0.9408\n",
      "Epoch 443/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.1670 - accuracy: 0.9335\n",
      "Epoch 444/1500\n",
      "33/33 [==============================] - 0s 804us/step - loss: 0.1565 - accuracy: 0.9408\n",
      "Epoch 445/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.1542 - accuracy: 0.9457\n",
      "Epoch 446/1500\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.1593 - accuracy: 0.9408\n",
      "Epoch 447/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.1740 - accuracy: 0.9316\n",
      "Epoch 448/1500\n",
      "33/33 [==============================] - 0s 778us/step - loss: 0.1652 - accuracy: 0.9413\n",
      "Epoch 449/1500\n",
      "33/33 [==============================] - 0s 775us/step - loss: 0.1684 - accuracy: 0.9374\n",
      "Epoch 450/1500\n",
      "33/33 [==============================] - 0s 788us/step - loss: 0.1453 - accuracy: 0.9476\n",
      "Epoch 451/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.1576 - accuracy: 0.9423\n",
      "Epoch 452/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.1551 - accuracy: 0.9413\n",
      "Epoch 453/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.1683 - accuracy: 0.9340\n",
      "Epoch 454/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.1515 - accuracy: 0.9437\n",
      "Epoch 455/1500\n",
      "33/33 [==============================] - 0s 901us/step - loss: 0.1740 - accuracy: 0.9330\n",
      "Epoch 456/1500\n",
      "33/33 [==============================] - 0s 838us/step - loss: 0.1675 - accuracy: 0.9360\n",
      "Epoch 457/1500\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.1513 - accuracy: 0.9389\n",
      "Epoch 458/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.1839 - accuracy: 0.9224\n",
      "Epoch 459/1500\n",
      "33/33 [==============================] - 0s 758us/step - loss: 0.1474 - accuracy: 0.9442\n",
      "Epoch 460/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.1707 - accuracy: 0.9316\n",
      "Epoch 461/1500\n",
      "33/33 [==============================] - 0s 753us/step - loss: 0.1584 - accuracy: 0.9403\n",
      "Epoch 462/1500\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.1533 - accuracy: 0.9418\n",
      "Epoch 463/1500\n",
      "33/33 [==============================] - 0s 790us/step - loss: 0.1539 - accuracy: 0.9432\n",
      "Epoch 464/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.1617 - accuracy: 0.9384\n",
      "Epoch 465/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.1584 - accuracy: 0.9393\n",
      "Epoch 466/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.1593 - accuracy: 0.9330\n",
      "Epoch 467/1500\n",
      "33/33 [==============================] - 0s 801us/step - loss: 0.1606 - accuracy: 0.9427\n",
      "Epoch 468/1500\n",
      "33/33 [==============================] - 0s 3ms/step - loss: 0.1728 - accuracy: 0.9326\n",
      "Epoch 469/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9437\n",
      "Epoch 470/1500\n",
      "33/33 [==============================] - 0s 844us/step - loss: 0.1793 - accuracy: 0.9335\n",
      "Epoch 471/1500\n",
      "33/33 [==============================] - 0s 811us/step - loss: 0.1649 - accuracy: 0.9350\n",
      "Epoch 472/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.1600 - accuracy: 0.9350\n",
      "Epoch 473/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.1479 - accuracy: 0.9408\n",
      "Epoch 474/1500\n",
      "33/33 [==============================] - 0s 772us/step - loss: 0.1507 - accuracy: 0.9495\n",
      "Epoch 475/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.1499 - accuracy: 0.9442\n",
      "Epoch 476/1500\n",
      "33/33 [==============================] - 0s 813us/step - loss: 0.1693 - accuracy: 0.9335\n",
      "Epoch 477/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.1453 - accuracy: 0.9447\n",
      "Epoch 478/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.1617 - accuracy: 0.9432\n",
      "Epoch 479/1500\n",
      "33/33 [==============================] - 0s 804us/step - loss: 0.1536 - accuracy: 0.9355\n",
      "Epoch 480/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.1409 - accuracy: 0.9457\n",
      "Epoch 481/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.1565 - accuracy: 0.9369\n",
      "Epoch 482/1500\n",
      "33/33 [==============================] - 0s 790us/step - loss: 0.1593 - accuracy: 0.9418\n",
      "Epoch 483/1500\n",
      "33/33 [==============================] - 0s 784us/step - loss: 0.1624 - accuracy: 0.9340\n",
      "Epoch 484/1500\n",
      "33/33 [==============================] - 0s 756us/step - loss: 0.1544 - accuracy: 0.9413\n",
      "Epoch 485/1500\n",
      "33/33 [==============================] - 0s 792us/step - loss: 0.1466 - accuracy: 0.9442\n",
      "Epoch 486/1500\n",
      "33/33 [==============================] - 0s 799us/step - loss: 0.1468 - accuracy: 0.9447\n",
      "Epoch 487/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.1435 - accuracy: 0.9461\n",
      "Epoch 488/1500\n",
      "33/33 [==============================] - 0s 760us/step - loss: 0.1430 - accuracy: 0.9432\n",
      "Epoch 489/1500\n",
      "33/33 [==============================] - 0s 787us/step - loss: 0.1443 - accuracy: 0.9481\n",
      "Epoch 490/1500\n",
      "33/33 [==============================] - 0s 741us/step - loss: 0.1469 - accuracy: 0.9466\n",
      "Epoch 491/1500\n",
      "33/33 [==============================] - 0s 739us/step - loss: 0.1431 - accuracy: 0.9461\n",
      "Epoch 492/1500\n",
      "33/33 [==============================] - 0s 769us/step - loss: 0.1493 - accuracy: 0.9427\n",
      "Epoch 493/1500\n",
      "33/33 [==============================] - 0s 752us/step - loss: 0.1619 - accuracy: 0.9384\n",
      "Epoch 494/1500\n",
      "33/33 [==============================] - 0s 777us/step - loss: 0.1442 - accuracy: 0.9457\n",
      "Epoch 495/1500\n",
      "33/33 [==============================] - 0s 798us/step - loss: 0.1389 - accuracy: 0.9491\n",
      "Epoch 496/1500\n",
      "33/33 [==============================] - 0s 789us/step - loss: 0.1695 - accuracy: 0.9345\n",
      "Epoch 497/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.1520 - accuracy: 0.9403\n",
      "Epoch 498/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.1595 - accuracy: 0.9389\n",
      "Epoch 499/1500\n",
      "33/33 [==============================] - 0s 757us/step - loss: 0.1558 - accuracy: 0.9345\n",
      "Epoch 500/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.1515 - accuracy: 0.9432\n",
      "Epoch 501/1500\n",
      "33/33 [==============================] - 0s 780us/step - loss: 0.1391 - accuracy: 0.9476\n",
      "Epoch 502/1500\n",
      "33/33 [==============================] - 0s 743us/step - loss: 0.1404 - accuracy: 0.9476\n",
      "Epoch 503/1500\n",
      "33/33 [==============================] - 0s 774us/step - loss: 0.1424 - accuracy: 0.9461\n",
      "Epoch 504/1500\n",
      "33/33 [==============================] - 0s 798us/step - loss: 0.1468 - accuracy: 0.9495\n",
      "Epoch 505/1500\n",
      "33/33 [==============================] - 0s 751us/step - loss: 0.1355 - accuracy: 0.9471\n",
      "Epoch 506/1500\n",
      "33/33 [==============================] - 0s 776us/step - loss: 0.1501 - accuracy: 0.9413\n",
      "Epoch 507/1500\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.1639 - accuracy: 0.9384\n",
      "Epoch 508/1500\n",
      "33/33 [==============================] - 0s 766us/step - loss: 0.1480 - accuracy: 0.9471\n",
      "Epoch 509/1500\n",
      "33/33 [==============================] - 0s 793us/step - loss: 0.1461 - accuracy: 0.9457\n",
      "Epoch 510/1500\n",
      "33/33 [==============================] - 0s 800us/step - loss: 0.1371 - accuracy: 0.9476\n",
      "Epoch 511/1500\n",
      "33/33 [==============================] - 0s 759us/step - loss: 0.1523 - accuracy: 0.9374\n",
      "Epoch 512/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.1507 - accuracy: 0.9418\n",
      "Epoch 513/1500\n",
      "33/33 [==============================] - 0s 747us/step - loss: 0.1607 - accuracy: 0.9369\n",
      "Epoch 514/1500\n",
      "33/33 [==============================] - 0s 823us/step - loss: 0.1268 - accuracy: 0.9520\n",
      "Epoch 515/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.1497 - accuracy: 0.9471\n",
      "Epoch 516/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.1533 - accuracy: 0.9437\n",
      "Epoch 517/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.1602 - accuracy: 0.9398\n",
      "Epoch 518/1500\n",
      "33/33 [==============================] - 0s 773us/step - loss: 0.1359 - accuracy: 0.9476\n",
      "Epoch 519/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.1351 - accuracy: 0.9495\n",
      "Epoch 520/1500\n",
      "33/33 [==============================] - 0s 4ms/step - loss: 0.1353 - accuracy: 0.9427\n",
      "Epoch 521/1500\n",
      "33/33 [==============================] - 0s 810us/step - loss: 0.1571 - accuracy: 0.9398\n",
      "Epoch 522/1500\n",
      "33/33 [==============================] - 0s 785us/step - loss: 0.1637 - accuracy: 0.9384\n",
      "Epoch 523/1500\n",
      "33/33 [==============================] - 0s 914us/step - loss: 0.1390 - accuracy: 0.9486\n",
      "Epoch 524/1500\n",
      "33/33 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9437\n",
      "Epoch 525/1500\n",
      "33/33 [==============================] - 0s 846us/step - loss: 0.1630 - accuracy: 0.9393\n",
      "Epoch 526/1500\n",
      "33/33 [==============================] - 0s 829us/step - loss: 0.1606 - accuracy: 0.9330\n",
      "Epoch 527/1500\n",
      "33/33 [==============================] - 0s 822us/step - loss: 0.1357 - accuracy: 0.9486\n",
      "Epoch 528/1500\n",
      "33/33 [==============================] - 0s 863us/step - loss: 0.1375 - accuracy: 0.9491\n",
      "Epoch 529/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.1398 - accuracy: 0.9442\n",
      "Epoch 530/1500\n",
      "33/33 [==============================] - 0s 815us/step - loss: 0.1313 - accuracy: 0.9476\n",
      "Epoch 531/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.1466 - accuracy: 0.9447\n",
      "Epoch 532/1500\n",
      "33/33 [==============================] - 0s 764us/step - loss: 0.1333 - accuracy: 0.9505\n",
      "Epoch 533/1500\n",
      "33/33 [==============================] - 0s 738us/step - loss: 0.1441 - accuracy: 0.9457\n",
      "Epoch 534/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.1540 - accuracy: 0.9457\n",
      "Epoch 535/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.1232 - accuracy: 0.9568\n",
      "Epoch 536/1500\n",
      "33/33 [==============================] - 0s 791us/step - loss: 0.1601 - accuracy: 0.9379\n",
      "Epoch 537/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.1241 - accuracy: 0.9592\n",
      "Epoch 538/1500\n",
      "33/33 [==============================] - 0s 761us/step - loss: 0.1429 - accuracy: 0.9515\n",
      "Epoch 539/1500\n",
      "33/33 [==============================] - 0s 771us/step - loss: 0.1282 - accuracy: 0.9500\n",
      "Epoch 540/1500\n",
      "33/33 [==============================] - 0s 811us/step - loss: 0.1404 - accuracy: 0.9452\n",
      "Epoch 541/1500\n",
      "33/33 [==============================] - 0s 765us/step - loss: 0.1407 - accuracy: 0.9491\n",
      "Epoch 542/1500\n",
      "33/33 [==============================] - 0s 762us/step - loss: 0.1392 - accuracy: 0.9500\n",
      "Epoch 543/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.1360 - accuracy: 0.9447\n",
      "Epoch 544/1500\n",
      "33/33 [==============================] - 0s 814us/step - loss: 0.1372 - accuracy: 0.9447\n",
      "Epoch 545/1500\n",
      "33/33 [==============================] - 0s 755us/step - loss: 0.1490 - accuracy: 0.9437\n",
      "Epoch 546/1500\n",
      "33/33 [==============================] - 0s 741us/step - loss: 0.1428 - accuracy: 0.9505\n",
      "Epoch 547/1500\n",
      "33/33 [==============================] - 0s 779us/step - loss: 0.1330 - accuracy: 0.9500\n",
      "Epoch 548/1500\n",
      "33/33 [==============================] - 0s 753us/step - loss: 0.1291 - accuracy: 0.9515\n",
      "Epoch 549/1500\n",
      "33/33 [==============================] - 0s 782us/step - loss: 0.1407 - accuracy: 0.9461\n",
      "Epoch 550/1500\n",
      "33/33 [==============================] - 0s 770us/step - loss: 0.1505 - accuracy: 0.9418\n",
      "Epoch 551/1500\n",
      "33/33 [==============================] - 0s 734us/step - loss: 0.1267 - accuracy: 0.9505\n",
      "Epoch 552/1500\n",
      "33/33 [==============================] - 0s 760us/step - loss: 0.1431 - accuracy: 0.9481\n",
      "Epoch 553/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.1402 - accuracy: 0.9461\n",
      "Epoch 554/1500\n",
      "33/33 [==============================] - 0s 746us/step - loss: 0.1508 - accuracy: 0.9452\n",
      "Epoch 555/1500\n",
      "33/33 [==============================] - 0s 717us/step - loss: 0.1365 - accuracy: 0.9505\n",
      "Epoch 556/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.1366 - accuracy: 0.9457\n",
      "Epoch 557/1500\n",
      "33/33 [==============================] - 0s 748us/step - loss: 0.1365 - accuracy: 0.9471\n",
      "Epoch 558/1500\n",
      "33/33 [==============================] - 0s 796us/step - loss: 0.1518 - accuracy: 0.9393\n",
      "Epoch 559/1500\n",
      "33/33 [==============================] - 0s 735us/step - loss: 0.1537 - accuracy: 0.9389\n",
      "Epoch 560/1500\n",
      "33/33 [==============================] - 0s 786us/step - loss: 0.1294 - accuracy: 0.9500\n",
      "Epoch 561/1500\n",
      "33/33 [==============================] - 0s 768us/step - loss: 0.1296 - accuracy: 0.9486\n",
      "Epoch 562/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.1348 - accuracy: 0.9544\n",
      "Epoch 563/1500\n",
      "33/33 [==============================] - 0s 781us/step - loss: 0.1421 - accuracy: 0.9452\n",
      "Epoch 564/1500\n",
      "33/33 [==============================] - 0s 767us/step - loss: 0.1422 - accuracy: 0.9437\n",
      "Epoch 565/1500\n",
      " 1/33 [..............................] - ETA: 0s - loss: 0.2086 - accuracy: 0.8906Restoring model weights from the end of the best epoch: 535.\n",
      "33/33 [==============================] - 0s 857us/step - loss: 0.1284 - accuracy: 0.9500\n",
      "Epoch 565: early stopping\n",
      "9/9 [==============================] - 0s 745us/step - loss: 0.9985 - accuracy: 0.6903\n",
      "9/9 [==============================] - 0s 540us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (18/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "Final Test Results - Loss: 0.99848473072052, Accuracy: 0.6902984976768494, Precision: 0.6975470531207167, Recall: 0.653379810828441, F1 Score: 0.6424491845316549\n",
      "Confusion Matrix:\n",
      " [[123   1  36]\n",
      " [ 34  39   0]\n",
      " [ 12   0  23]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "053A     6\n",
      "108A     6\n",
      "023A     6\n",
      "007A     6\n",
      "025C     5\n",
      "044A     5\n",
      "023B     5\n",
      "021A     5\n",
      "070A     5\n",
      "034A     5\n",
      "003A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "104A     4\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "113A     3\n",
      "056A     3\n",
      "018A     2\n",
      "038A     2\n",
      "025B     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "102A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "073A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "063A    11\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "095A     8\n",
      "037A     6\n",
      "008A     6\n",
      "109A     6\n",
      "075A     5\n",
      "105A     4\n",
      "064A     3\n",
      "060A     3\n",
      "093A     2\n",
      "087A     2\n",
      "011A     2\n",
      "061A     2\n",
      "076A     1\n",
      "043A     1\n",
      "091A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    238\n",
      "F    229\n",
      "X    221\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    127\n",
      "M     99\n",
      "F     23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 015A, 103A, 071A, 097B, 028A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 050A, 049...\n",
      "senior    [097A, 057A, 106A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 001A, 019A, 067A, 022A, 029A, 095A, 072...\n",
      "kitten                             [046A, 109A, 043A, 045A]\n",
      "senior                             [093A, 051B, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '044A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '043A'\n",
      " '045A' '046A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "No common groups found between train and test sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'040A'}\n",
      "Moved to Test Set:\n",
      "{'040A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '041A' '042A'\n",
      " '044A' '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '040A'\n",
      " '043A' '045A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "Length of X_train_val:\n",
      "741\n",
      "Length of y_train_val:\n",
      "741\n",
      "Length of groups_train_val:\n",
      "741\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     155\n",
      "kitten     79\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten    145\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     155\n",
      "kitten     26\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 866, 2: 815, 1: 725})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 1s 1ms/step - loss: 1.1530 - accuracy: 0.4942\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.8824 - accuracy: 0.6276\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.8077 - accuracy: 0.6741\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.7754 - accuracy: 0.6737\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.7260 - accuracy: 0.7007\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.6889 - accuracy: 0.7116\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.6862 - accuracy: 0.7103\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.6552 - accuracy: 0.7373\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.6163 - accuracy: 0.7461\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.6055 - accuracy: 0.7556\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.6126 - accuracy: 0.7535\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.5813 - accuracy: 0.7594\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.5781 - accuracy: 0.7556\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.5849 - accuracy: 0.7652\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.5626 - accuracy: 0.7594\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.5477 - accuracy: 0.7689\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.5691 - accuracy: 0.7681\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.5432 - accuracy: 0.7764\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.5273 - accuracy: 0.7793\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.4928 - accuracy: 0.8042\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.5261 - accuracy: 0.7909\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.5020 - accuracy: 0.7922\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.5039 - accuracy: 0.7922\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.4894 - accuracy: 0.7984\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.5214 - accuracy: 0.7860\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.4849 - accuracy: 0.8001\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.4872 - accuracy: 0.8038\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.4748 - accuracy: 0.8063\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.4699 - accuracy: 0.8142\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.4672 - accuracy: 0.8113\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.4532 - accuracy: 0.8159\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.4673 - accuracy: 0.8038\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.4724 - accuracy: 0.7988\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.4646 - accuracy: 0.8076\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.4627 - accuracy: 0.8105\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.4529 - accuracy: 0.8109\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 853us/step - loss: 0.4403 - accuracy: 0.8225\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.4447 - accuracy: 0.8146\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.4412 - accuracy: 0.8146\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.4470 - accuracy: 0.8200\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.4215 - accuracy: 0.8263\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 724us/step - loss: 0.4230 - accuracy: 0.8271\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.4259 - accuracy: 0.8283\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.4340 - accuracy: 0.8234\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.4138 - accuracy: 0.8321\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.4136 - accuracy: 0.8259\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.4082 - accuracy: 0.8308\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.4117 - accuracy: 0.8367\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.4241 - accuracy: 0.8292\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.3962 - accuracy: 0.8354\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.4087 - accuracy: 0.8317\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.4045 - accuracy: 0.8337\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.4027 - accuracy: 0.8325\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.4000 - accuracy: 0.8325\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.3976 - accuracy: 0.8333\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.3968 - accuracy: 0.8367\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.4055 - accuracy: 0.8288\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.3828 - accuracy: 0.8367\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.3849 - accuracy: 0.8375\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.3849 - accuracy: 0.8437\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.3724 - accuracy: 0.8437\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.3991 - accuracy: 0.8437\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.3777 - accuracy: 0.8470\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.3811 - accuracy: 0.8396\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.3828 - accuracy: 0.8446\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.3753 - accuracy: 0.8433\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3637 - accuracy: 0.8529\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.3720 - accuracy: 0.8425\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.3696 - accuracy: 0.8516\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.3640 - accuracy: 0.8533\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.3571 - accuracy: 0.8537\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.3649 - accuracy: 0.8437\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.3515 - accuracy: 0.8549\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.3623 - accuracy: 0.8441\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.3672 - accuracy: 0.8504\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.3601 - accuracy: 0.8570\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.3377 - accuracy: 0.8649\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.3519 - accuracy: 0.8529\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.3434 - accuracy: 0.8583\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.3399 - accuracy: 0.8653\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.3457 - accuracy: 0.8554\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.3351 - accuracy: 0.8591\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.3471 - accuracy: 0.8537\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.3511 - accuracy: 0.8504\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.3539 - accuracy: 0.8475\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.3307 - accuracy: 0.8579\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.3403 - accuracy: 0.8529\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.3357 - accuracy: 0.8520\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.3260 - accuracy: 0.8608\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.3211 - accuracy: 0.8687\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.3199 - accuracy: 0.8695\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.3242 - accuracy: 0.8720\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.3350 - accuracy: 0.8554\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.3231 - accuracy: 0.8687\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.3251 - accuracy: 0.8649\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.3316 - accuracy: 0.8637\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.3348 - accuracy: 0.8595\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.3117 - accuracy: 0.8666\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.3110 - accuracy: 0.8728\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.3155 - accuracy: 0.8645\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.3259 - accuracy: 0.8703\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.3123 - accuracy: 0.8662\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.3309 - accuracy: 0.8608\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.3028 - accuracy: 0.8766\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.3195 - accuracy: 0.8724\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.3186 - accuracy: 0.8666\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.3223 - accuracy: 0.8761\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.3142 - accuracy: 0.8745\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.3135 - accuracy: 0.8687\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2983 - accuracy: 0.8720\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.3162 - accuracy: 0.8761\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.3064 - accuracy: 0.8753\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.3038 - accuracy: 0.8770\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.3061 - accuracy: 0.8682\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.3125 - accuracy: 0.8682\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.3010 - accuracy: 0.8741\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.3032 - accuracy: 0.8770\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.3042 - accuracy: 0.8828\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2870 - accuracy: 0.8869\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.3034 - accuracy: 0.8786\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 929us/step - loss: 0.2928 - accuracy: 0.8799\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.3015 - accuracy: 0.8811\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2877 - accuracy: 0.8849\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2839 - accuracy: 0.8845\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.2987 - accuracy: 0.8761\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2866 - accuracy: 0.8761\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2799 - accuracy: 0.8886\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2855 - accuracy: 0.8815\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.2937 - accuracy: 0.8807\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.2880 - accuracy: 0.8832\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2771 - accuracy: 0.8857\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2820 - accuracy: 0.8807\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2963 - accuracy: 0.8828\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2813 - accuracy: 0.8815\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2896 - accuracy: 0.8869\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2877 - accuracy: 0.8857\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2720 - accuracy: 0.8865\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2838 - accuracy: 0.8803\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2797 - accuracy: 0.8911\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2626 - accuracy: 0.8973\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2800 - accuracy: 0.8811\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2798 - accuracy: 0.8824\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2581 - accuracy: 0.8953\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.2759 - accuracy: 0.8903\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2759 - accuracy: 0.8911\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2529 - accuracy: 0.8978\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2740 - accuracy: 0.8890\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.2705 - accuracy: 0.8890\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.2604 - accuracy: 0.8924\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.2610 - accuracy: 0.8957\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2622 - accuracy: 0.8857\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2734 - accuracy: 0.8791\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2665 - accuracy: 0.8899\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.2573 - accuracy: 0.9015\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.2604 - accuracy: 0.8948\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2747 - accuracy: 0.8915\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.2777 - accuracy: 0.8836\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2623 - accuracy: 0.8919\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2627 - accuracy: 0.8940\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.2589 - accuracy: 0.8957\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.2511 - accuracy: 0.8986\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2531 - accuracy: 0.8961\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 743us/step - loss: 0.2521 - accuracy: 0.8965\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2444 - accuracy: 0.9057\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2487 - accuracy: 0.8932\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2495 - accuracy: 0.8944\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2496 - accuracy: 0.9032\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.2432 - accuracy: 0.9048\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2608 - accuracy: 0.8998\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2585 - accuracy: 0.9002\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2522 - accuracy: 0.9048\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2759 - accuracy: 0.8861\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2444 - accuracy: 0.9019\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2616 - accuracy: 0.8907\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2451 - accuracy: 0.8973\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2575 - accuracy: 0.8973\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.2327 - accuracy: 0.9061\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.2459 - accuracy: 0.8990\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2436 - accuracy: 0.9032\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2561 - accuracy: 0.8940\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2518 - accuracy: 0.9015\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2493 - accuracy: 0.9007\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2379 - accuracy: 0.9052\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.2373 - accuracy: 0.9048\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2476 - accuracy: 0.9032\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.2339 - accuracy: 0.9057\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2487 - accuracy: 0.9011\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2390 - accuracy: 0.9073\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.2423 - accuracy: 0.9069\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2436 - accuracy: 0.9019\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.2098 - accuracy: 0.9177\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2408 - accuracy: 0.9019\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2156 - accuracy: 0.9127\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2349 - accuracy: 0.9032\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2447 - accuracy: 0.8978\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.2411 - accuracy: 0.9036\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2542 - accuracy: 0.8928\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2321 - accuracy: 0.9123\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2375 - accuracy: 0.9011\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2162 - accuracy: 0.9206\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2334 - accuracy: 0.9098\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2231 - accuracy: 0.9140\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2236 - accuracy: 0.9169\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.2236 - accuracy: 0.9140\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2111 - accuracy: 0.9169\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2176 - accuracy: 0.9098\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.2248 - accuracy: 0.9065\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2337 - accuracy: 0.9065\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2162 - accuracy: 0.9131\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.2094 - accuracy: 0.9223\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2231 - accuracy: 0.9127\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.2285 - accuracy: 0.9023\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2085 - accuracy: 0.9185\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2275 - accuracy: 0.9077\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2384 - accuracy: 0.9073\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2186 - accuracy: 0.9173\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2203 - accuracy: 0.9160\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2237 - accuracy: 0.9140\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2219 - accuracy: 0.9144\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2207 - accuracy: 0.9123\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2266 - accuracy: 0.9086\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.2208 - accuracy: 0.9131\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.2177 - accuracy: 0.9160\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2233 - accuracy: 0.9111\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.2167 - accuracy: 0.9156\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2028 - accuracy: 0.9214\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2084 - accuracy: 0.9156\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2092 - accuracy: 0.9165\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2188 - accuracy: 0.9102\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2148 - accuracy: 0.9131\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.2047 - accuracy: 0.9185\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2036 - accuracy: 0.9219\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2079 - accuracy: 0.9173\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2060 - accuracy: 0.9185\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.2118 - accuracy: 0.9135\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2009 - accuracy: 0.9248\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.2066 - accuracy: 0.9198\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2090 - accuracy: 0.9214\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.2070 - accuracy: 0.9210\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.2153 - accuracy: 0.9173\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2025 - accuracy: 0.9190\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1935 - accuracy: 0.9248\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.1931 - accuracy: 0.9206\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2174 - accuracy: 0.9098\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2014 - accuracy: 0.9219\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2046 - accuracy: 0.9177\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2004 - accuracy: 0.9256\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1820 - accuracy: 0.9289\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2078 - accuracy: 0.9169\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1959 - accuracy: 0.9202\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.1955 - accuracy: 0.9268\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.2009 - accuracy: 0.9202\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.1952 - accuracy: 0.9289\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2003 - accuracy: 0.9194\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2024 - accuracy: 0.9219\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1914 - accuracy: 0.9244\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.1945 - accuracy: 0.9214\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.1876 - accuracy: 0.9318\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.1942 - accuracy: 0.9327\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.1830 - accuracy: 0.9289\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.1977 - accuracy: 0.9252\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1889 - accuracy: 0.9281\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.2026 - accuracy: 0.9239\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1896 - accuracy: 0.9323\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.1974 - accuracy: 0.9206\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.1806 - accuracy: 0.9256\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1914 - accuracy: 0.9223\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.1904 - accuracy: 0.9264\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1958 - accuracy: 0.9231\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2028 - accuracy: 0.9194\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1984 - accuracy: 0.9248\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1932 - accuracy: 0.9277\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.1834 - accuracy: 0.9244\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2154 - accuracy: 0.9135\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.1942 - accuracy: 0.9252\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.1925 - accuracy: 0.9289\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.1914 - accuracy: 0.9248\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1768 - accuracy: 0.9335\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.1795 - accuracy: 0.9323\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.1720 - accuracy: 0.9335\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1888 - accuracy: 0.9223\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1884 - accuracy: 0.9323\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1784 - accuracy: 0.9327\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.1882 - accuracy: 0.9281\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.1950 - accuracy: 0.9177\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1856 - accuracy: 0.9310\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.1917 - accuracy: 0.9273\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.1805 - accuracy: 0.9314\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.1665 - accuracy: 0.9318\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1906 - accuracy: 0.9310\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.1838 - accuracy: 0.9264\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.1856 - accuracy: 0.9268\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1919 - accuracy: 0.9264\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.1803 - accuracy: 0.9302\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.1672 - accuracy: 0.9352\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.1913 - accuracy: 0.9231\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.1830 - accuracy: 0.9231\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1838 - accuracy: 0.9306\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.1803 - accuracy: 0.9277\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.1775 - accuracy: 0.9318\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.1776 - accuracy: 0.9273\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.1654 - accuracy: 0.9389\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.1759 - accuracy: 0.9339\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.1890 - accuracy: 0.9223\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.1720 - accuracy: 0.9293\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.1827 - accuracy: 0.9298\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.1698 - accuracy: 0.9310\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.1791 - accuracy: 0.9293\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.1850 - accuracy: 0.9252\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1781 - accuracy: 0.9273\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.1677 - accuracy: 0.9364\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1714 - accuracy: 0.9302\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.1732 - accuracy: 0.9331\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.1676 - accuracy: 0.9343\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1843 - accuracy: 0.9260\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.1770 - accuracy: 0.9327\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1821 - accuracy: 0.9256\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1803 - accuracy: 0.9339\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.1767 - accuracy: 0.9339\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.1751 - accuracy: 0.9339\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1755 - accuracy: 0.9281\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.1833 - accuracy: 0.9252\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1713 - accuracy: 0.9343\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.1718 - accuracy: 0.9327\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1794 - accuracy: 0.9306\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 750us/step - loss: 0.1771 - accuracy: 0.9264\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.1681 - accuracy: 0.9318\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.1720 - accuracy: 0.9352\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.1663 - accuracy: 0.9306\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.1703 - accuracy: 0.9331\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.1644 - accuracy: 0.9372\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1819 - accuracy: 0.9244\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.1597 - accuracy: 0.9385\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.1735 - accuracy: 0.9335\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1622 - accuracy: 0.9364\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.1512 - accuracy: 0.9360\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.1718 - accuracy: 0.9327\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1641 - accuracy: 0.9385\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1736 - accuracy: 0.9323\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1558 - accuracy: 0.9431\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1702 - accuracy: 0.9281\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1625 - accuracy: 0.9356\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1635 - accuracy: 0.9406\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1526 - accuracy: 0.9418\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1621 - accuracy: 0.9347\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.1641 - accuracy: 0.9339\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1638 - accuracy: 0.9414\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.1541 - accuracy: 0.9406\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1773 - accuracy: 0.9306\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1704 - accuracy: 0.9347\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1586 - accuracy: 0.9431\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.1426 - accuracy: 0.9443\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1754 - accuracy: 0.9281\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.1436 - accuracy: 0.9439\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.1557 - accuracy: 0.9347\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.1649 - accuracy: 0.9314\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.1609 - accuracy: 0.9406\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.1666 - accuracy: 0.9293\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.1512 - accuracy: 0.9410\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.1468 - accuracy: 0.9510\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.1597 - accuracy: 0.9410\n",
      "Epoch 362/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1541 - accuracy: 0.9435\n",
      "Epoch 363/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1439 - accuracy: 0.9418\n",
      "Epoch 364/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.1563 - accuracy: 0.9414\n",
      "Epoch 365/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.1726 - accuracy: 0.9360\n",
      "Epoch 366/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.1479 - accuracy: 0.9456\n",
      "Epoch 367/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1522 - accuracy: 0.9410\n",
      "Epoch 368/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.1561 - accuracy: 0.9443\n",
      "Epoch 369/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.1520 - accuracy: 0.9431\n",
      "Epoch 370/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.1599 - accuracy: 0.9389\n",
      "Epoch 371/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.1488 - accuracy: 0.9443\n",
      "Epoch 372/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.1639 - accuracy: 0.9306\n",
      "Epoch 373/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1517 - accuracy: 0.9426\n",
      "Epoch 374/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.1429 - accuracy: 0.9456\n",
      "Epoch 375/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1546 - accuracy: 0.9356\n",
      "Epoch 376/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.1675 - accuracy: 0.9360\n",
      "Epoch 377/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.1616 - accuracy: 0.9356\n",
      "Epoch 378/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1584 - accuracy: 0.9356\n",
      "Epoch 379/1500\n",
      "38/38 [==============================] - 0s 736us/step - loss: 0.1416 - accuracy: 0.9451\n",
      "Epoch 380/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.1681 - accuracy: 0.9323\n",
      "Epoch 381/1500\n",
      "38/38 [==============================] - 0s 759us/step - loss: 0.1580 - accuracy: 0.9314\n",
      "Epoch 382/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.0955 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 352.\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.1617 - accuracy: 0.9339\n",
      "Epoch 382: early stopping\n",
      "7/7 [==============================] - 0s 771us/step - loss: 0.6696 - accuracy: 0.7398\n",
      "7/7 [==============================] - 0s 606us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (21/29)\n",
      "Before appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "After appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "Final Test Results - Loss: 0.6696128249168396, Accuracy: 0.7397959232330322, Precision: 0.6110679807063323, Recall: 0.7792941825199889, F1 Score: 0.6460605814849357\n",
      "Confusion Matrix:\n",
      " [[110  14  31]\n",
      " [  1  25   0]\n",
      " [  5   0  10]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "042A    14\n",
      "097B    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "095A     8\n",
      "117A     7\n",
      "050A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "008A     6\n",
      "109A     6\n",
      "053A     6\n",
      "037A     6\n",
      "108A     6\n",
      "023A     6\n",
      "023B     5\n",
      "075A     5\n",
      "021A     5\n",
      "105A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "003A     4\n",
      "056A     3\n",
      "113A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "012A     3\n",
      "058A     3\n",
      "061A     2\n",
      "011A     2\n",
      "102A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "069A     2\n",
      "041A     1\n",
      "019B     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "096A     1\n",
      "076A     1\n",
      "088A     1\n",
      "092A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "073A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "000B    19\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "025A    11\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "010A     8\n",
      "013B     8\n",
      "007A     6\n",
      "070A     5\n",
      "044A     5\n",
      "034A     5\n",
      "025C     5\n",
      "104A     4\n",
      "009A     4\n",
      "006A     3\n",
      "018A     2\n",
      "054A     2\n",
      "025B     2\n",
      "032A     2\n",
      "049A     1\n",
      "026C     1\n",
      "066A     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    322\n",
      "M    241\n",
      "F    159\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "F    93\n",
      "X    26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 001A, 103A, 097B, 019A, 074...\n",
      "kitten    [014B, 111A, 040A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 057A, 055A, 113A, 051B, 117A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 071A, 028A, 020A, 034A, 005A, 002A, 009...\n",
      "kitten                                   [044A, 049A, 115A]\n",
      "senior           [106A, 104A, 059A, 116A, 054A, 016A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 13, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 3, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "722\n",
      "Length of y_train_val:\n",
      "722\n",
      "Length of groups_train_val:\n",
      "722\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 874, 1: 820, 2: 605})\n",
      "Epoch 1/1500\n",
      "36/36 [==============================] - 0s 942us/step - loss: 1.1469 - accuracy: 0.5072\n",
      "Epoch 2/1500\n",
      "36/36 [==============================] - 0s 928us/step - loss: 0.8871 - accuracy: 0.6259\n",
      "Epoch 3/1500\n",
      "36/36 [==============================] - 0s 853us/step - loss: 0.8276 - accuracy: 0.6507\n",
      "Epoch 4/1500\n",
      "36/36 [==============================] - 0s 804us/step - loss: 0.7577 - accuracy: 0.6873\n",
      "Epoch 5/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.7400 - accuracy: 0.6907\n",
      "Epoch 6/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 0.6881 - accuracy: 0.7107\n",
      "Epoch 7/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.7015 - accuracy: 0.6929\n",
      "Epoch 8/1500\n",
      "36/36 [==============================] - 0s 775us/step - loss: 0.6662 - accuracy: 0.7199\n",
      "Epoch 9/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.6493 - accuracy: 0.7338\n",
      "Epoch 10/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.6510 - accuracy: 0.7268\n",
      "Epoch 11/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.6356 - accuracy: 0.7321\n",
      "Epoch 12/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.6261 - accuracy: 0.7329\n",
      "Epoch 13/1500\n",
      "36/36 [==============================] - 0s 810us/step - loss: 0.6074 - accuracy: 0.7390\n",
      "Epoch 14/1500\n",
      "36/36 [==============================] - 0s 793us/step - loss: 0.5932 - accuracy: 0.7429\n",
      "Epoch 15/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.6010 - accuracy: 0.7525\n",
      "Epoch 16/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.5728 - accuracy: 0.7538\n",
      "Epoch 17/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 0.5453 - accuracy: 0.7699\n",
      "Epoch 18/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.5528 - accuracy: 0.7682\n",
      "Epoch 19/1500\n",
      "36/36 [==============================] - 0s 814us/step - loss: 0.5440 - accuracy: 0.7629\n",
      "Epoch 20/1500\n",
      "36/36 [==============================] - 0s 802us/step - loss: 0.5340 - accuracy: 0.7812\n",
      "Epoch 21/1500\n",
      "36/36 [==============================] - 0s 803us/step - loss: 0.5299 - accuracy: 0.7716\n",
      "Epoch 22/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.5171 - accuracy: 0.7777\n",
      "Epoch 23/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.5144 - accuracy: 0.7669\n",
      "Epoch 24/1500\n",
      "36/36 [==============================] - 0s 792us/step - loss: 0.4931 - accuracy: 0.7899\n",
      "Epoch 25/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.4965 - accuracy: 0.7916\n",
      "Epoch 26/1500\n",
      "36/36 [==============================] - 0s 768us/step - loss: 0.4993 - accuracy: 0.7986\n",
      "Epoch 27/1500\n",
      "36/36 [==============================] - 0s 768us/step - loss: 0.4939 - accuracy: 0.7864\n",
      "Epoch 28/1500\n",
      "36/36 [==============================] - 0s 760us/step - loss: 0.4969 - accuracy: 0.7908\n",
      "Epoch 29/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 0.4824 - accuracy: 0.7934\n",
      "Epoch 30/1500\n",
      "36/36 [==============================] - 0s 792us/step - loss: 0.4642 - accuracy: 0.8051\n",
      "Epoch 31/1500\n",
      "36/36 [==============================] - 0s 759us/step - loss: 0.4697 - accuracy: 0.7921\n",
      "Epoch 32/1500\n",
      "36/36 [==============================] - 0s 766us/step - loss: 0.4758 - accuracy: 0.7999\n",
      "Epoch 33/1500\n",
      "36/36 [==============================] - 0s 755us/step - loss: 0.4789 - accuracy: 0.7973\n",
      "Epoch 34/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.4660 - accuracy: 0.8047\n",
      "Epoch 35/1500\n",
      "36/36 [==============================] - 0s 803us/step - loss: 0.4612 - accuracy: 0.8025\n",
      "Epoch 36/1500\n",
      "36/36 [==============================] - 0s 783us/step - loss: 0.4512 - accuracy: 0.8095\n",
      "Epoch 37/1500\n",
      "36/36 [==============================] - 0s 779us/step - loss: 0.4642 - accuracy: 0.8008\n",
      "Epoch 38/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.4431 - accuracy: 0.8073\n",
      "Epoch 39/1500\n",
      "36/36 [==============================] - 0s 783us/step - loss: 0.4416 - accuracy: 0.8121\n",
      "Epoch 40/1500\n",
      "36/36 [==============================] - 0s 783us/step - loss: 0.4325 - accuracy: 0.8182\n",
      "Epoch 41/1500\n",
      "36/36 [==============================] - 0s 767us/step - loss: 0.4321 - accuracy: 0.8169\n",
      "Epoch 42/1500\n",
      "36/36 [==============================] - 0s 804us/step - loss: 0.4313 - accuracy: 0.8182\n",
      "Epoch 43/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.4396 - accuracy: 0.8217\n",
      "Epoch 44/1500\n",
      "36/36 [==============================] - 0s 769us/step - loss: 0.4531 - accuracy: 0.8047\n",
      "Epoch 45/1500\n",
      "36/36 [==============================] - 0s 810us/step - loss: 0.4215 - accuracy: 0.8330\n",
      "Epoch 46/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.4288 - accuracy: 0.8243\n",
      "Epoch 47/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.4237 - accuracy: 0.8186\n",
      "Epoch 48/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 0.4168 - accuracy: 0.8260\n",
      "Epoch 49/1500\n",
      "36/36 [==============================] - 0s 792us/step - loss: 0.4140 - accuracy: 0.8325\n",
      "Epoch 50/1500\n",
      "36/36 [==============================] - 0s 761us/step - loss: 0.4159 - accuracy: 0.8256\n",
      "Epoch 51/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.4037 - accuracy: 0.8291\n",
      "Epoch 52/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.4138 - accuracy: 0.8286\n",
      "Epoch 53/1500\n",
      "36/36 [==============================] - 0s 779us/step - loss: 0.4067 - accuracy: 0.8264\n",
      "Epoch 54/1500\n",
      "36/36 [==============================] - 0s 755us/step - loss: 0.4053 - accuracy: 0.8278\n",
      "Epoch 55/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.3882 - accuracy: 0.8334\n",
      "Epoch 56/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.3949 - accuracy: 0.8295\n",
      "Epoch 57/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.4071 - accuracy: 0.8356\n",
      "Epoch 58/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.3874 - accuracy: 0.8334\n",
      "Epoch 59/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 0.3870 - accuracy: 0.8382\n",
      "Epoch 60/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.3775 - accuracy: 0.8351\n",
      "Epoch 61/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.3734 - accuracy: 0.8386\n",
      "Epoch 62/1500\n",
      "36/36 [==============================] - 0s 783us/step - loss: 0.3913 - accuracy: 0.8321\n",
      "Epoch 63/1500\n",
      "36/36 [==============================] - 0s 773us/step - loss: 0.3940 - accuracy: 0.8238\n",
      "Epoch 64/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.3898 - accuracy: 0.8304\n",
      "Epoch 65/1500\n",
      "36/36 [==============================] - 0s 764us/step - loss: 0.3956 - accuracy: 0.8343\n",
      "Epoch 66/1500\n",
      "36/36 [==============================] - 0s 754us/step - loss: 0.3779 - accuracy: 0.8465\n",
      "Epoch 67/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 0.3715 - accuracy: 0.8443\n",
      "Epoch 68/1500\n",
      "36/36 [==============================] - 0s 756us/step - loss: 0.3846 - accuracy: 0.8347\n",
      "Epoch 69/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.3731 - accuracy: 0.8378\n",
      "Epoch 70/1500\n",
      "36/36 [==============================] - 0s 810us/step - loss: 0.3600 - accuracy: 0.8434\n",
      "Epoch 71/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.3729 - accuracy: 0.8360\n",
      "Epoch 72/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.3676 - accuracy: 0.8421\n",
      "Epoch 73/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.3637 - accuracy: 0.8469\n",
      "Epoch 74/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.3771 - accuracy: 0.8412\n",
      "Epoch 75/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.3637 - accuracy: 0.8556\n",
      "Epoch 76/1500\n",
      "36/36 [==============================] - 0s 801us/step - loss: 0.3547 - accuracy: 0.8482\n",
      "Epoch 77/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.3586 - accuracy: 0.8504\n",
      "Epoch 78/1500\n",
      "36/36 [==============================] - 0s 783us/step - loss: 0.3487 - accuracy: 0.8417\n",
      "Epoch 79/1500\n",
      "36/36 [==============================] - 0s 801us/step - loss: 0.3668 - accuracy: 0.8438\n",
      "Epoch 80/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.3619 - accuracy: 0.8504\n",
      "Epoch 81/1500\n",
      "36/36 [==============================] - 0s 817us/step - loss: 0.3489 - accuracy: 0.8582\n",
      "Epoch 82/1500\n",
      "36/36 [==============================] - 0s 763us/step - loss: 0.3503 - accuracy: 0.8560\n",
      "Epoch 83/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.3650 - accuracy: 0.8460\n",
      "Epoch 84/1500\n",
      "36/36 [==============================] - 0s 806us/step - loss: 0.3322 - accuracy: 0.8665\n",
      "Epoch 85/1500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.3446 - accuracy: 0.8512\n",
      "Epoch 86/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3396 - accuracy: 0.8608\n",
      "Epoch 87/1500\n",
      "36/36 [==============================] - 0s 793us/step - loss: 0.3381 - accuracy: 0.8591\n",
      "Epoch 88/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.3406 - accuracy: 0.8643\n",
      "Epoch 89/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.3415 - accuracy: 0.8547\n",
      "Epoch 90/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.3517 - accuracy: 0.8599\n",
      "Epoch 91/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.3395 - accuracy: 0.8621\n",
      "Epoch 92/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.3392 - accuracy: 0.8573\n",
      "Epoch 93/1500\n",
      "36/36 [==============================] - 0s 757us/step - loss: 0.3366 - accuracy: 0.8599\n",
      "Epoch 94/1500\n",
      "36/36 [==============================] - 0s 792us/step - loss: 0.3185 - accuracy: 0.8639\n",
      "Epoch 95/1500\n",
      "36/36 [==============================] - 0s 781us/step - loss: 0.3367 - accuracy: 0.8604\n",
      "Epoch 96/1500\n",
      "36/36 [==============================] - 0s 779us/step - loss: 0.3322 - accuracy: 0.8630\n",
      "Epoch 97/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.3171 - accuracy: 0.8647\n",
      "Epoch 98/1500\n",
      "36/36 [==============================] - 0s 800us/step - loss: 0.3365 - accuracy: 0.8617\n",
      "Epoch 99/1500\n",
      "36/36 [==============================] - 0s 775us/step - loss: 0.3307 - accuracy: 0.8573\n",
      "Epoch 100/1500\n",
      "36/36 [==============================] - 0s 783us/step - loss: 0.3437 - accuracy: 0.8569\n",
      "Epoch 101/1500\n",
      "36/36 [==============================] - 0s 773us/step - loss: 0.3417 - accuracy: 0.8512\n",
      "Epoch 102/1500\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.3229 - accuracy: 0.8656\n",
      "Epoch 103/1500\n",
      "36/36 [==============================] - 0s 803us/step - loss: 0.3127 - accuracy: 0.8717\n",
      "Epoch 104/1500\n",
      "36/36 [==============================] - 0s 786us/step - loss: 0.3134 - accuracy: 0.8699\n",
      "Epoch 105/1500\n",
      "36/36 [==============================] - 0s 769us/step - loss: 0.3276 - accuracy: 0.8552\n",
      "Epoch 106/1500\n",
      "36/36 [==============================] - 0s 762us/step - loss: 0.3133 - accuracy: 0.8678\n",
      "Epoch 107/1500\n",
      "36/36 [==============================] - 0s 799us/step - loss: 0.3018 - accuracy: 0.8747\n",
      "Epoch 108/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.3164 - accuracy: 0.8704\n",
      "Epoch 109/1500\n",
      "36/36 [==============================] - 0s 786us/step - loss: 0.3123 - accuracy: 0.8691\n",
      "Epoch 110/1500\n",
      "36/36 [==============================] - 0s 772us/step - loss: 0.3060 - accuracy: 0.8747\n",
      "Epoch 111/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.3072 - accuracy: 0.8721\n",
      "Epoch 112/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.3013 - accuracy: 0.8743\n",
      "Epoch 113/1500\n",
      "36/36 [==============================] - 0s 801us/step - loss: 0.2954 - accuracy: 0.8817\n",
      "Epoch 114/1500\n",
      "36/36 [==============================] - 0s 762us/step - loss: 0.3118 - accuracy: 0.8686\n",
      "Epoch 115/1500\n",
      "36/36 [==============================] - 0s 859us/step - loss: 0.3160 - accuracy: 0.8682\n",
      "Epoch 116/1500\n",
      "36/36 [==============================] - 0s 907us/step - loss: 0.2978 - accuracy: 0.8782\n",
      "Epoch 117/1500\n",
      "36/36 [==============================] - 0s 810us/step - loss: 0.3142 - accuracy: 0.8678\n",
      "Epoch 118/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.3088 - accuracy: 0.8726\n",
      "Epoch 119/1500\n",
      "36/36 [==============================] - 0s 803us/step - loss: 0.3031 - accuracy: 0.8786\n",
      "Epoch 120/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.2946 - accuracy: 0.8743\n",
      "Epoch 121/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.2908 - accuracy: 0.8760\n",
      "Epoch 122/1500\n",
      "36/36 [==============================] - 0s 781us/step - loss: 0.2948 - accuracy: 0.8804\n",
      "Epoch 123/1500\n",
      "36/36 [==============================] - 0s 802us/step - loss: 0.2880 - accuracy: 0.8773\n",
      "Epoch 124/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.3138 - accuracy: 0.8656\n",
      "Epoch 125/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 0.2968 - accuracy: 0.8826\n",
      "Epoch 126/1500\n",
      "36/36 [==============================] - 0s 799us/step - loss: 0.2929 - accuracy: 0.8795\n",
      "Epoch 127/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.2854 - accuracy: 0.8878\n",
      "Epoch 128/1500\n",
      "36/36 [==============================] - 0s 835us/step - loss: 0.2817 - accuracy: 0.8878\n",
      "Epoch 129/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.2955 - accuracy: 0.8799\n",
      "Epoch 130/1500\n",
      "36/36 [==============================] - 0s 793us/step - loss: 0.2837 - accuracy: 0.8839\n",
      "Epoch 131/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.2971 - accuracy: 0.8765\n",
      "Epoch 132/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.2846 - accuracy: 0.8782\n",
      "Epoch 133/1500\n",
      "36/36 [==============================] - 0s 775us/step - loss: 0.2907 - accuracy: 0.8826\n",
      "Epoch 134/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.2831 - accuracy: 0.8834\n",
      "Epoch 135/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.2810 - accuracy: 0.8826\n",
      "Epoch 136/1500\n",
      "36/36 [==============================] - 0s 802us/step - loss: 0.2759 - accuracy: 0.8808\n",
      "Epoch 137/1500\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.2752 - accuracy: 0.8873\n",
      "Epoch 138/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.2923 - accuracy: 0.8847\n",
      "Epoch 139/1500\n",
      "36/36 [==============================] - 0s 753us/step - loss: 0.2848 - accuracy: 0.8834\n",
      "Epoch 140/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.2833 - accuracy: 0.8852\n",
      "Epoch 141/1500\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.2715 - accuracy: 0.8856\n",
      "Epoch 142/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.2693 - accuracy: 0.8873\n",
      "Epoch 143/1500\n",
      "36/36 [==============================] - 0s 766us/step - loss: 0.2820 - accuracy: 0.8886\n",
      "Epoch 144/1500\n",
      "36/36 [==============================] - 0s 790us/step - loss: 0.2911 - accuracy: 0.8747\n",
      "Epoch 145/1500\n",
      "36/36 [==============================] - 0s 768us/step - loss: 0.2772 - accuracy: 0.8900\n",
      "Epoch 146/1500\n",
      "36/36 [==============================] - 0s 814us/step - loss: 0.2722 - accuracy: 0.8852\n",
      "Epoch 147/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.2718 - accuracy: 0.8886\n",
      "Epoch 148/1500\n",
      "36/36 [==============================] - 0s 806us/step - loss: 0.2681 - accuracy: 0.8930\n",
      "Epoch 149/1500\n",
      "36/36 [==============================] - 0s 775us/step - loss: 0.2702 - accuracy: 0.8886\n",
      "Epoch 150/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.2782 - accuracy: 0.8886\n",
      "Epoch 151/1500\n",
      "36/36 [==============================] - 0s 807us/step - loss: 0.2570 - accuracy: 0.8913\n",
      "Epoch 152/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.2663 - accuracy: 0.8917\n",
      "Epoch 153/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.2642 - accuracy: 0.8965\n",
      "Epoch 154/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.2585 - accuracy: 0.8930\n",
      "Epoch 155/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.2719 - accuracy: 0.8886\n",
      "Epoch 156/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.2542 - accuracy: 0.8943\n",
      "Epoch 157/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.2615 - accuracy: 0.8947\n",
      "Epoch 158/1500\n",
      "36/36 [==============================] - 0s 790us/step - loss: 0.2673 - accuracy: 0.8917\n",
      "Epoch 159/1500\n",
      "36/36 [==============================] - 0s 793us/step - loss: 0.2741 - accuracy: 0.8921\n",
      "Epoch 160/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.2596 - accuracy: 0.9034\n",
      "Epoch 161/1500\n",
      "36/36 [==============================] - 0s 792us/step - loss: 0.2658 - accuracy: 0.8882\n",
      "Epoch 162/1500\n",
      "36/36 [==============================] - 0s 783us/step - loss: 0.2584 - accuracy: 0.8926\n",
      "Epoch 163/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.2457 - accuracy: 0.9021\n",
      "Epoch 164/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.2713 - accuracy: 0.8930\n",
      "Epoch 165/1500\n",
      "36/36 [==============================] - 0s 802us/step - loss: 0.2573 - accuracy: 0.8952\n",
      "Epoch 166/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.2560 - accuracy: 0.8973\n",
      "Epoch 167/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.2584 - accuracy: 0.8930\n",
      "Epoch 168/1500\n",
      "36/36 [==============================] - 0s 781us/step - loss: 0.2499 - accuracy: 0.9013\n",
      "Epoch 169/1500\n",
      "36/36 [==============================] - 0s 783us/step - loss: 0.2540 - accuracy: 0.8978\n",
      "Epoch 170/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.2483 - accuracy: 0.8934\n",
      "Epoch 171/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.2507 - accuracy: 0.9030\n",
      "Epoch 172/1500\n",
      "36/36 [==============================] - 0s 761us/step - loss: 0.2603 - accuracy: 0.8956\n",
      "Epoch 173/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.2553 - accuracy: 0.8965\n",
      "Epoch 174/1500\n",
      "36/36 [==============================] - 0s 775us/step - loss: 0.2448 - accuracy: 0.8939\n",
      "Epoch 175/1500\n",
      "36/36 [==============================] - 0s 806us/step - loss: 0.2532 - accuracy: 0.8956\n",
      "Epoch 176/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.2474 - accuracy: 0.8921\n",
      "Epoch 177/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.2519 - accuracy: 0.8965\n",
      "Epoch 178/1500\n",
      "36/36 [==============================] - 0s 781us/step - loss: 0.2226 - accuracy: 0.9113\n",
      "Epoch 179/1500\n",
      "36/36 [==============================] - 0s 804us/step - loss: 0.2459 - accuracy: 0.8973\n",
      "Epoch 180/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.2370 - accuracy: 0.9013\n",
      "Epoch 181/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.2488 - accuracy: 0.8947\n",
      "Epoch 182/1500\n",
      "36/36 [==============================] - 0s 793us/step - loss: 0.2351 - accuracy: 0.9074\n",
      "Epoch 183/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.2491 - accuracy: 0.8956\n",
      "Epoch 184/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 0.2326 - accuracy: 0.8995\n",
      "Epoch 185/1500\n",
      "36/36 [==============================] - 0s 763us/step - loss: 0.2411 - accuracy: 0.9095\n",
      "Epoch 186/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.2457 - accuracy: 0.9043\n",
      "Epoch 187/1500\n",
      "36/36 [==============================] - 0s 803us/step - loss: 0.2399 - accuracy: 0.8991\n",
      "Epoch 188/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.2388 - accuracy: 0.9056\n",
      "Epoch 189/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.2648 - accuracy: 0.8908\n",
      "Epoch 190/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.2309 - accuracy: 0.9052\n",
      "Epoch 191/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 0.2156 - accuracy: 0.9143\n",
      "Epoch 192/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.2310 - accuracy: 0.9017\n",
      "Epoch 193/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.2243 - accuracy: 0.9108\n",
      "Epoch 194/1500\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.2430 - accuracy: 0.9082\n",
      "Epoch 195/1500\n",
      "36/36 [==============================] - 0s 781us/step - loss: 0.2314 - accuracy: 0.9047\n",
      "Epoch 196/1500\n",
      "36/36 [==============================] - 0s 779us/step - loss: 0.2400 - accuracy: 0.9047\n",
      "Epoch 197/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.2318 - accuracy: 0.9039\n",
      "Epoch 198/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 0.2309 - accuracy: 0.9108\n",
      "Epoch 199/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.2369 - accuracy: 0.8995\n",
      "Epoch 200/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.2271 - accuracy: 0.9143\n",
      "Epoch 201/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.2196 - accuracy: 0.9152\n",
      "Epoch 202/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.2284 - accuracy: 0.9126\n",
      "Epoch 203/1500\n",
      "36/36 [==============================] - 0s 801us/step - loss: 0.2238 - accuracy: 0.9091\n",
      "Epoch 204/1500\n",
      "36/36 [==============================] - 0s 827us/step - loss: 0.2248 - accuracy: 0.9087\n",
      "Epoch 205/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.2161 - accuracy: 0.9117\n",
      "Epoch 206/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 0.2208 - accuracy: 0.9104\n",
      "Epoch 207/1500\n",
      "36/36 [==============================] - 0s 779us/step - loss: 0.2162 - accuracy: 0.9174\n",
      "Epoch 208/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.2251 - accuracy: 0.9130\n",
      "Epoch 209/1500\n",
      "36/36 [==============================] - 0s 799us/step - loss: 0.2285 - accuracy: 0.9065\n",
      "Epoch 210/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.2158 - accuracy: 0.9156\n",
      "Epoch 211/1500\n",
      "36/36 [==============================] - 0s 804us/step - loss: 0.2140 - accuracy: 0.9161\n",
      "Epoch 212/1500\n",
      "36/36 [==============================] - 0s 797us/step - loss: 0.2281 - accuracy: 0.9113\n",
      "Epoch 213/1500\n",
      "36/36 [==============================] - 0s 797us/step - loss: 0.2185 - accuracy: 0.9126\n",
      "Epoch 214/1500\n",
      "36/36 [==============================] - 0s 864us/step - loss: 0.2226 - accuracy: 0.9095\n",
      "Epoch 215/1500\n",
      "36/36 [==============================] - 0s 802us/step - loss: 0.2244 - accuracy: 0.9126\n",
      "Epoch 216/1500\n",
      "36/36 [==============================] - 0s 835us/step - loss: 0.2119 - accuracy: 0.9169\n",
      "Epoch 217/1500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.2008 - accuracy: 0.9200\n",
      "Epoch 218/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9195\n",
      "Epoch 219/1500\n",
      "36/36 [==============================] - 0s 886us/step - loss: 0.2192 - accuracy: 0.9152\n",
      "Epoch 220/1500\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.2106 - accuracy: 0.9161\n",
      "Epoch 221/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.2209 - accuracy: 0.9113\n",
      "Epoch 222/1500\n",
      "36/36 [==============================] - 0s 801us/step - loss: 0.2242 - accuracy: 0.9095\n",
      "Epoch 223/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.2073 - accuracy: 0.9247\n",
      "Epoch 224/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.2063 - accuracy: 0.9221\n",
      "Epoch 225/1500\n",
      "36/36 [==============================] - 0s 817us/step - loss: 0.2096 - accuracy: 0.9169\n",
      "Epoch 226/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.2088 - accuracy: 0.9169\n",
      "Epoch 227/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 0.2018 - accuracy: 0.9182\n",
      "Epoch 228/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.2255 - accuracy: 0.9130\n",
      "Epoch 229/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.2158 - accuracy: 0.9139\n",
      "Epoch 230/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.2132 - accuracy: 0.9130\n",
      "Epoch 231/1500\n",
      "36/36 [==============================] - 0s 793us/step - loss: 0.2210 - accuracy: 0.9113\n",
      "Epoch 232/1500\n",
      "36/36 [==============================] - 0s 827us/step - loss: 0.1890 - accuracy: 0.9304\n",
      "Epoch 233/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.2128 - accuracy: 0.9165\n",
      "Epoch 234/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.2202 - accuracy: 0.9091\n",
      "Epoch 235/1500\n",
      "36/36 [==============================] - 0s 792us/step - loss: 0.1996 - accuracy: 0.9182\n",
      "Epoch 236/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.2089 - accuracy: 0.9161\n",
      "Epoch 237/1500\n",
      "36/36 [==============================] - 0s 766us/step - loss: 0.2145 - accuracy: 0.9187\n",
      "Epoch 238/1500\n",
      "36/36 [==============================] - 0s 902us/step - loss: 0.2153 - accuracy: 0.9191\n",
      "Epoch 239/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.2054 - accuracy: 0.9261\n",
      "Epoch 240/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.2037 - accuracy: 0.9182\n",
      "Epoch 241/1500\n",
      "36/36 [==============================] - 0s 783us/step - loss: 0.2164 - accuracy: 0.9152\n",
      "Epoch 242/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.2167 - accuracy: 0.9121\n",
      "Epoch 243/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.1973 - accuracy: 0.9226\n",
      "Epoch 244/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.2029 - accuracy: 0.9217\n",
      "Epoch 245/1500\n",
      "36/36 [==============================] - 0s 801us/step - loss: 0.2005 - accuracy: 0.9204\n",
      "Epoch 246/1500\n",
      "36/36 [==============================] - 0s 799us/step - loss: 0.2083 - accuracy: 0.9126\n",
      "Epoch 247/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.1949 - accuracy: 0.9234\n",
      "Epoch 248/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.1920 - accuracy: 0.9269\n",
      "Epoch 249/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 0.2013 - accuracy: 0.9174\n",
      "Epoch 250/1500\n",
      "36/36 [==============================] - 0s 797us/step - loss: 0.1921 - accuracy: 0.9243\n",
      "Epoch 251/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1936 - accuracy: 0.9308\n",
      "Epoch 252/1500\n",
      "36/36 [==============================] - 0s 800us/step - loss: 0.1962 - accuracy: 0.9261\n",
      "Epoch 253/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.2048 - accuracy: 0.9169\n",
      "Epoch 254/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.1940 - accuracy: 0.9278\n",
      "Epoch 255/1500\n",
      "36/36 [==============================] - 0s 772us/step - loss: 0.1895 - accuracy: 0.9247\n",
      "Epoch 256/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.1855 - accuracy: 0.9274\n",
      "Epoch 257/1500\n",
      "36/36 [==============================] - 0s 762us/step - loss: 0.2100 - accuracy: 0.9182\n",
      "Epoch 258/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.2159 - accuracy: 0.9147\n",
      "Epoch 259/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.2003 - accuracy: 0.9221\n",
      "Epoch 260/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 0.2009 - accuracy: 0.9178\n",
      "Epoch 261/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 0.1994 - accuracy: 0.9217\n",
      "Epoch 262/1500\n",
      "36/36 [==============================] - 0s 779us/step - loss: 0.1951 - accuracy: 0.9252\n",
      "Epoch 263/1500\n",
      "36/36 [==============================] - 0s 767us/step - loss: 0.2067 - accuracy: 0.9221\n",
      "Epoch 264/1500\n",
      "36/36 [==============================] - 0s 814us/step - loss: 0.2060 - accuracy: 0.9200\n",
      "Epoch 265/1500\n",
      "36/36 [==============================] - 0s 790us/step - loss: 0.1985 - accuracy: 0.9252\n",
      "Epoch 266/1500\n",
      "36/36 [==============================] - 0s 810us/step - loss: 0.1892 - accuracy: 0.9247\n",
      "Epoch 267/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.2002 - accuracy: 0.9195\n",
      "Epoch 268/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.1820 - accuracy: 0.9352\n",
      "Epoch 269/1500\n",
      "36/36 [==============================] - 0s 807us/step - loss: 0.1784 - accuracy: 0.9326\n",
      "Epoch 270/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.1884 - accuracy: 0.9247\n",
      "Epoch 271/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.1857 - accuracy: 0.9274\n",
      "Epoch 272/1500\n",
      "36/36 [==============================] - 0s 804us/step - loss: 0.1924 - accuracy: 0.9261\n",
      "Epoch 273/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.1832 - accuracy: 0.9317\n",
      "Epoch 274/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 0.1846 - accuracy: 0.9330\n",
      "Epoch 275/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 0.2112 - accuracy: 0.9169\n",
      "Epoch 276/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.1915 - accuracy: 0.9243\n",
      "Epoch 277/1500\n",
      "36/36 [==============================] - 0s 786us/step - loss: 0.1933 - accuracy: 0.9265\n",
      "Epoch 278/1500\n",
      "36/36 [==============================] - 0s 801us/step - loss: 0.1856 - accuracy: 0.9252\n",
      "Epoch 279/1500\n",
      "36/36 [==============================] - 0s 786us/step - loss: 0.1885 - accuracy: 0.9195\n",
      "Epoch 280/1500\n",
      "36/36 [==============================] - 0s 804us/step - loss: 0.1936 - accuracy: 0.9217\n",
      "Epoch 281/1500\n",
      "36/36 [==============================] - 0s 808us/step - loss: 0.1864 - accuracy: 0.9295\n",
      "Epoch 282/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.1930 - accuracy: 0.9230\n",
      "Epoch 283/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 0.1997 - accuracy: 0.9195\n",
      "Epoch 284/1500\n",
      "36/36 [==============================] - 0s 828us/step - loss: 0.1772 - accuracy: 0.9313\n",
      "Epoch 285/1500\n",
      "36/36 [==============================] - 0s 775us/step - loss: 0.1763 - accuracy: 0.9343\n",
      "Epoch 286/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.1842 - accuracy: 0.9282\n",
      "Epoch 287/1500\n",
      "36/36 [==============================] - 0s 800us/step - loss: 0.1768 - accuracy: 0.9265\n",
      "Epoch 288/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.1869 - accuracy: 0.9256\n",
      "Epoch 289/1500\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 290/1500\n",
      "36/36 [==============================] - 0s 792us/step - loss: 0.1820 - accuracy: 0.9348\n",
      "Epoch 291/1500\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.1712 - accuracy: 0.9291\n",
      "Epoch 292/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 0.1833 - accuracy: 0.9230\n",
      "Epoch 293/1500\n",
      "36/36 [==============================] - 0s 790us/step - loss: 0.1771 - accuracy: 0.9287\n",
      "Epoch 294/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 0.1806 - accuracy: 0.9291\n",
      "Epoch 295/1500\n",
      "36/36 [==============================] - 0s 766us/step - loss: 0.1865 - accuracy: 0.9304\n",
      "Epoch 296/1500\n",
      "36/36 [==============================] - 0s 790us/step - loss: 0.1919 - accuracy: 0.9239\n",
      "Epoch 297/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.1795 - accuracy: 0.9274\n",
      "Epoch 298/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.1900 - accuracy: 0.9265\n",
      "Epoch 299/1500\n",
      "36/36 [==============================] - 0s 815us/step - loss: 0.1741 - accuracy: 0.9282\n",
      "Epoch 300/1500\n",
      "36/36 [==============================] - 0s 973us/step - loss: 0.1837 - accuracy: 0.9334\n",
      "Epoch 301/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.1749 - accuracy: 0.9300\n",
      "Epoch 302/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.1650 - accuracy: 0.9378\n",
      "Epoch 303/1500\n",
      "36/36 [==============================] - 0s 797us/step - loss: 0.1783 - accuracy: 0.9287\n",
      "Epoch 304/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.1820 - accuracy: 0.9282\n",
      "Epoch 305/1500\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.1740 - accuracy: 0.9295\n",
      "Epoch 306/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1691 - accuracy: 0.9334\n",
      "Epoch 307/1500\n",
      "36/36 [==============================] - 0s 819us/step - loss: 0.1823 - accuracy: 0.9308\n",
      "Epoch 308/1500\n",
      "36/36 [==============================] - 0s 816us/step - loss: 0.1817 - accuracy: 0.9291\n",
      "Epoch 309/1500\n",
      "36/36 [==============================] - 0s 763us/step - loss: 0.1686 - accuracy: 0.9334\n",
      "Epoch 310/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.1556 - accuracy: 0.9343\n",
      "Epoch 311/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.1903 - accuracy: 0.9247\n",
      "Epoch 312/1500\n",
      "36/36 [==============================] - 0s 802us/step - loss: 0.1593 - accuracy: 0.9430\n",
      "Epoch 313/1500\n",
      "36/36 [==============================] - 0s 790us/step - loss: 0.1734 - accuracy: 0.9352\n",
      "Epoch 314/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 0.1683 - accuracy: 0.9317\n",
      "Epoch 315/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.1569 - accuracy: 0.9365\n",
      "Epoch 316/1500\n",
      "36/36 [==============================] - 0s 781us/step - loss: 0.1841 - accuracy: 0.9304\n",
      "Epoch 317/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.1677 - accuracy: 0.9378\n",
      "Epoch 318/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.1681 - accuracy: 0.9348\n",
      "Epoch 319/1500\n",
      "36/36 [==============================] - 0s 761us/step - loss: 0.1620 - accuracy: 0.9348\n",
      "Epoch 320/1500\n",
      "36/36 [==============================] - 0s 828us/step - loss: 0.1538 - accuracy: 0.9400\n",
      "Epoch 321/1500\n",
      "36/36 [==============================] - 0s 797us/step - loss: 0.1864 - accuracy: 0.9221\n",
      "Epoch 322/1500\n",
      "36/36 [==============================] - 0s 751us/step - loss: 0.1719 - accuracy: 0.9308\n",
      "Epoch 323/1500\n",
      "36/36 [==============================] - 0s 795us/step - loss: 0.1774 - accuracy: 0.9300\n",
      "Epoch 324/1500\n",
      "36/36 [==============================] - 0s 824us/step - loss: 0.1642 - accuracy: 0.9348\n",
      "Epoch 325/1500\n",
      "36/36 [==============================] - 0s 814us/step - loss: 0.1559 - accuracy: 0.9387\n",
      "Epoch 326/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 0.1850 - accuracy: 0.9295\n",
      "Epoch 327/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.1780 - accuracy: 0.9308\n",
      "Epoch 328/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.1661 - accuracy: 0.9348\n",
      "Epoch 329/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.1755 - accuracy: 0.9343\n",
      "Epoch 330/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.1542 - accuracy: 0.9421\n",
      "Epoch 331/1500\n",
      "36/36 [==============================] - 0s 790us/step - loss: 0.1588 - accuracy: 0.9404\n",
      "Epoch 332/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.1617 - accuracy: 0.9361\n",
      "Epoch 333/1500\n",
      "36/36 [==============================] - 0s 818us/step - loss: 0.1469 - accuracy: 0.9448\n",
      "Epoch 334/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.1626 - accuracy: 0.9408\n",
      "Epoch 335/1500\n",
      "36/36 [==============================] - 0s 803us/step - loss: 0.1902 - accuracy: 0.9239\n",
      "Epoch 336/1500\n",
      "36/36 [==============================] - 0s 818us/step - loss: 0.1580 - accuracy: 0.9378\n",
      "Epoch 337/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.1669 - accuracy: 0.9369\n",
      "Epoch 338/1500\n",
      "36/36 [==============================] - 0s 807us/step - loss: 0.1540 - accuracy: 0.9408\n",
      "Epoch 339/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.1430 - accuracy: 0.9417\n",
      "Epoch 340/1500\n",
      "36/36 [==============================] - 0s 822us/step - loss: 0.1570 - accuracy: 0.9395\n",
      "Epoch 341/1500\n",
      "36/36 [==============================] - 0s 775us/step - loss: 0.1581 - accuracy: 0.9404\n",
      "Epoch 342/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.1434 - accuracy: 0.9465\n",
      "Epoch 343/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.1758 - accuracy: 0.9300\n",
      "Epoch 344/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.1492 - accuracy: 0.9482\n",
      "Epoch 345/1500\n",
      "36/36 [==============================] - 0s 786us/step - loss: 0.1619 - accuracy: 0.9361\n",
      "Epoch 346/1500\n",
      "36/36 [==============================] - 0s 822us/step - loss: 0.1581 - accuracy: 0.9374\n",
      "Epoch 347/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 0.1666 - accuracy: 0.9295\n",
      "Epoch 348/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.1552 - accuracy: 0.9421\n",
      "Epoch 349/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.1596 - accuracy: 0.9421\n",
      "Epoch 350/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.1547 - accuracy: 0.9426\n",
      "Epoch 351/1500\n",
      "36/36 [==============================] - 0s 799us/step - loss: 0.1597 - accuracy: 0.9348\n",
      "Epoch 352/1500\n",
      "36/36 [==============================] - 0s 760us/step - loss: 0.1560 - accuracy: 0.9395\n",
      "Epoch 353/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.1524 - accuracy: 0.9413\n",
      "Epoch 354/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.1601 - accuracy: 0.9365\n",
      "Epoch 355/1500\n",
      "36/36 [==============================] - 0s 770us/step - loss: 0.1619 - accuracy: 0.9391\n",
      "Epoch 356/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.1480 - accuracy: 0.9448\n",
      "Epoch 357/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.1513 - accuracy: 0.9413\n",
      "Epoch 358/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.1493 - accuracy: 0.9382\n",
      "Epoch 359/1500\n",
      "36/36 [==============================] - 0s 797us/step - loss: 0.1595 - accuracy: 0.9365\n",
      "Epoch 360/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.1637 - accuracy: 0.9369\n",
      "Epoch 361/1500\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.1433 - accuracy: 0.9487\n",
      "Epoch 362/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.1481 - accuracy: 0.9387\n",
      "Epoch 363/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.1368 - accuracy: 0.9474\n",
      "Epoch 364/1500\n",
      "36/36 [==============================] - 0s 811us/step - loss: 0.1462 - accuracy: 0.9439\n",
      "Epoch 365/1500\n",
      "36/36 [==============================] - 0s 790us/step - loss: 0.1569 - accuracy: 0.9426\n",
      "Epoch 366/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 0.1538 - accuracy: 0.9400\n",
      "Epoch 367/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.1508 - accuracy: 0.9352\n",
      "Epoch 368/1500\n",
      "36/36 [==============================] - 0s 4ms/step - loss: 0.1497 - accuracy: 0.9413\n",
      "Epoch 369/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.1677 - accuracy: 0.9395\n",
      "Epoch 370/1500\n",
      "36/36 [==============================] - 0s 823us/step - loss: 0.1442 - accuracy: 0.9456\n",
      "Epoch 371/1500\n",
      "36/36 [==============================] - 0s 797us/step - loss: 0.1521 - accuracy: 0.9448\n",
      "Epoch 372/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.1492 - accuracy: 0.9382\n",
      "Epoch 373/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.1645 - accuracy: 0.9382\n",
      "Epoch 374/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.1451 - accuracy: 0.9469\n",
      "Epoch 375/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.1531 - accuracy: 0.9413\n",
      "Epoch 376/1500\n",
      "36/36 [==============================] - 0s 764us/step - loss: 0.1541 - accuracy: 0.9400\n",
      "Epoch 377/1500\n",
      "36/36 [==============================] - 0s 764us/step - loss: 0.1587 - accuracy: 0.9426\n",
      "Epoch 378/1500\n",
      "36/36 [==============================] - 0s 807us/step - loss: 0.1450 - accuracy: 0.9426\n",
      "Epoch 379/1500\n",
      "36/36 [==============================] - 0s 767us/step - loss: 0.1432 - accuracy: 0.9408\n",
      "Epoch 380/1500\n",
      "36/36 [==============================] - 0s 815us/step - loss: 0.1445 - accuracy: 0.9461\n",
      "Epoch 381/1500\n",
      "36/36 [==============================] - 0s 775us/step - loss: 0.1506 - accuracy: 0.9382\n",
      "Epoch 382/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.1413 - accuracy: 0.9491\n",
      "Epoch 383/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 0.1564 - accuracy: 0.9435\n",
      "Epoch 384/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.1367 - accuracy: 0.9482\n",
      "Epoch 385/1500\n",
      "36/36 [==============================] - 0s 815us/step - loss: 0.1367 - accuracy: 0.9452\n",
      "Epoch 386/1500\n",
      "36/36 [==============================] - 0s 802us/step - loss: 0.1461 - accuracy: 0.9448\n",
      "Epoch 387/1500\n",
      "36/36 [==============================] - 0s 814us/step - loss: 0.1493 - accuracy: 0.9430\n",
      "Epoch 388/1500\n",
      "36/36 [==============================] - 0s 786us/step - loss: 0.1434 - accuracy: 0.9413\n",
      "Epoch 389/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.1597 - accuracy: 0.9395\n",
      "Epoch 390/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.1444 - accuracy: 0.9461\n",
      "Epoch 391/1500\n",
      "36/36 [==============================] - 0s 767us/step - loss: 0.1414 - accuracy: 0.9482\n",
      "Epoch 392/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.1643 - accuracy: 0.9382\n",
      "Epoch 393/1500\n",
      " 1/36 [..............................] - ETA: 0s - loss: 0.1376 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 363.\n",
      "36/36 [==============================] - 0s 835us/step - loss: 0.1507 - accuracy: 0.9421\n",
      "Epoch 393: early stopping\n",
      "7/7 [==============================] - 0s 768us/step - loss: 0.9235 - accuracy: 0.7581\n",
      "7/7 [==============================] - 0s 579us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (25/30)\n",
      "Before appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "After appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "Final Test Results - Loss: 0.9235323071479797, Accuracy: 0.7581395506858826, Precision: 0.7559175001346277, Recall: 0.7742701123670103, F1 Score: 0.7619874338624338\n",
      "Confusion Matrix:\n",
      " [[118   1  32]\n",
      " [  1   6   0]\n",
      " [ 18   0  39]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "001A    14\n",
      "042A    14\n",
      "059A    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "025A    11\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "005A    10\n",
      "016A    10\n",
      "014B    10\n",
      "071A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "015A     9\n",
      "045A     9\n",
      "022A     9\n",
      "065A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "050A     7\n",
      "099A     7\n",
      "109A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "070A     5\n",
      "021A     5\n",
      "075A     5\n",
      "009A     4\n",
      "104A     4\n",
      "105A     4\n",
      "003A     4\n",
      "113A     3\n",
      "056A     3\n",
      "064A     3\n",
      "060A     3\n",
      "014A     3\n",
      "006A     3\n",
      "025B     2\n",
      "102A     2\n",
      "011A     2\n",
      "061A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "018A     2\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "043A     1\n",
      "115A     1\n",
      "076A     1\n",
      "066A     1\n",
      "026C     1\n",
      "096A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "057A    27\n",
      "055A    20\n",
      "097B    14\n",
      "039A    12\n",
      "040A    10\n",
      "094A     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "023A     6\n",
      "053A     6\n",
      "108A     6\n",
      "023B     5\n",
      "026A     4\n",
      "062A     4\n",
      "052A     4\n",
      "035A     4\n",
      "058A     3\n",
      "012A     3\n",
      "069A     2\n",
      "048A     1\n",
      "088A     1\n",
      "004A     1\n",
      "073A     1\n",
      "041A     1\n",
      "092A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    306\n",
      "X    268\n",
      "F    158\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    94\n",
      "X    80\n",
      "M    31\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 106A, 104A, 059A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 097B, 062A, 039A, 023A, 027A, 069A, 026...\n",
      "kitten                                   [040A, 041A, 048A]\n",
      "senior                 [057A, 055A, 117A, 058A, 094A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 13, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 3, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '020A'\n",
      " '021A' '022A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '012A' '019B' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'022A'}\n",
      "Moved to Test Set:\n",
      "{'022A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A'\n",
      " '009A' '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A'\n",
      " '020A' '021A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '012A' '019B' '022A' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "Length of X_train_val:\n",
      "762\n",
      "Length of y_train_val:\n",
      "762\n",
      "Length of groups_train_val:\n",
      "762\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     466\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     122\n",
      "senior     71\n",
      "kitten     12\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     496\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     92\n",
      "senior    71\n",
      "kitten    12\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 992, 1: 795, 2: 535})\n",
      "Epoch 1/1500\n",
      "37/37 [==============================] - 0s 972us/step - loss: 1.2367 - accuracy: 0.4634\n",
      "Epoch 2/1500\n",
      "37/37 [==============================] - 0s 930us/step - loss: 0.9503 - accuracy: 0.5870\n",
      "Epoch 3/1500\n",
      "37/37 [==============================] - 0s 853us/step - loss: 0.8616 - accuracy: 0.6193\n",
      "Epoch 4/1500\n",
      "37/37 [==============================] - 0s 778us/step - loss: 0.7967 - accuracy: 0.6537\n",
      "Epoch 5/1500\n",
      "37/37 [==============================] - 0s 792us/step - loss: 0.7507 - accuracy: 0.6753\n",
      "Epoch 6/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.7237 - accuracy: 0.6878\n",
      "Epoch 7/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.6839 - accuracy: 0.7127\n",
      "Epoch 8/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.6899 - accuracy: 0.6912\n",
      "Epoch 9/1500\n",
      "37/37 [==============================] - 0s 745us/step - loss: 0.6867 - accuracy: 0.7011\n",
      "Epoch 10/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.6644 - accuracy: 0.7158\n",
      "Epoch 11/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.6225 - accuracy: 0.7472\n",
      "Epoch 12/1500\n",
      "37/37 [==============================] - 0s 769us/step - loss: 0.6217 - accuracy: 0.7300\n",
      "Epoch 13/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.6032 - accuracy: 0.7433\n",
      "Epoch 14/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.5865 - accuracy: 0.7489\n",
      "Epoch 15/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.5753 - accuracy: 0.7519\n",
      "Epoch 16/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.5456 - accuracy: 0.7657\n",
      "Epoch 17/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.5624 - accuracy: 0.7653\n",
      "Epoch 18/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.5610 - accuracy: 0.7661\n",
      "Epoch 19/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.5183 - accuracy: 0.7838\n",
      "Epoch 20/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.5596 - accuracy: 0.7649\n",
      "Epoch 21/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.5295 - accuracy: 0.7868\n",
      "Epoch 22/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.5249 - accuracy: 0.7812\n",
      "Epoch 23/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.5072 - accuracy: 0.7855\n",
      "Epoch 24/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.5018 - accuracy: 0.7855\n",
      "Epoch 25/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.5099 - accuracy: 0.7873\n",
      "Epoch 26/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.5127 - accuracy: 0.7821\n",
      "Epoch 27/1500\n",
      "37/37 [==============================] - 0s 741us/step - loss: 0.5099 - accuracy: 0.7847\n",
      "Epoch 28/1500\n",
      "37/37 [==============================] - 0s 778us/step - loss: 0.4954 - accuracy: 0.7937\n",
      "Epoch 29/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.4811 - accuracy: 0.7954\n",
      "Epoch 30/1500\n",
      "37/37 [==============================] - 0s 933us/step - loss: 0.4747 - accuracy: 0.8028\n",
      "Epoch 31/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.4692 - accuracy: 0.7963\n",
      "Epoch 32/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.4719 - accuracy: 0.8084\n",
      "Epoch 33/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.4611 - accuracy: 0.8058\n",
      "Epoch 34/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.4623 - accuracy: 0.8023\n",
      "Epoch 35/1500\n",
      "37/37 [==============================] - 0s 691us/step - loss: 0.4816 - accuracy: 0.7972\n",
      "Epoch 36/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.4495 - accuracy: 0.8165\n",
      "Epoch 37/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.4354 - accuracy: 0.8174\n",
      "Epoch 38/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.4513 - accuracy: 0.8118\n",
      "Epoch 39/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.4425 - accuracy: 0.8161\n",
      "Epoch 40/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.4357 - accuracy: 0.8208\n",
      "Epoch 41/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.4339 - accuracy: 0.8127\n",
      "Epoch 42/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.4248 - accuracy: 0.8252\n",
      "Epoch 43/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.4423 - accuracy: 0.8066\n",
      "Epoch 44/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.4297 - accuracy: 0.8239\n",
      "Epoch 45/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.4268 - accuracy: 0.8122\n",
      "Epoch 46/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.4214 - accuracy: 0.8230\n",
      "Epoch 47/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.4236 - accuracy: 0.8208\n",
      "Epoch 48/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.4157 - accuracy: 0.8204\n",
      "Epoch 49/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.4123 - accuracy: 0.8282\n",
      "Epoch 50/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4434 - accuracy: 0.8165\n",
      "Epoch 51/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.4024 - accuracy: 0.8355\n",
      "Epoch 52/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.4019 - accuracy: 0.8282\n",
      "Epoch 53/1500\n",
      "37/37 [==============================] - 0s 741us/step - loss: 0.3855 - accuracy: 0.8355\n",
      "Epoch 54/1500\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.4136 - accuracy: 0.8256\n",
      "Epoch 55/1500\n",
      "37/37 [==============================] - 0s 766us/step - loss: 0.3989 - accuracy: 0.8325\n",
      "Epoch 56/1500\n",
      "37/37 [==============================] - 0s 752us/step - loss: 0.4073 - accuracy: 0.8230\n",
      "Epoch 57/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.3822 - accuracy: 0.8458\n",
      "Epoch 58/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.3880 - accuracy: 0.8359\n",
      "Epoch 59/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.3879 - accuracy: 0.8411\n",
      "Epoch 60/1500\n",
      "37/37 [==============================] - 0s 752us/step - loss: 0.3916 - accuracy: 0.8338\n",
      "Epoch 61/1500\n",
      "37/37 [==============================] - 0s 756us/step - loss: 0.3855 - accuracy: 0.8299\n",
      "Epoch 62/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.4015 - accuracy: 0.8269\n",
      "Epoch 63/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.3941 - accuracy: 0.8372\n",
      "Epoch 64/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.3732 - accuracy: 0.8407\n",
      "Epoch 65/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.3601 - accuracy: 0.8501\n",
      "Epoch 66/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.3807 - accuracy: 0.8381\n",
      "Epoch 67/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.3829 - accuracy: 0.8381\n",
      "Epoch 68/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.3764 - accuracy: 0.8471\n",
      "Epoch 69/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.3794 - accuracy: 0.8415\n",
      "Epoch 70/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.3656 - accuracy: 0.8467\n",
      "Epoch 71/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.3809 - accuracy: 0.8415\n",
      "Epoch 72/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.3654 - accuracy: 0.8549\n",
      "Epoch 73/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.3513 - accuracy: 0.8566\n",
      "Epoch 74/1500\n",
      "37/37 [==============================] - 0s 766us/step - loss: 0.3761 - accuracy: 0.8407\n",
      "Epoch 75/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.3528 - accuracy: 0.8506\n",
      "Epoch 76/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.3754 - accuracy: 0.8493\n",
      "Epoch 77/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.3784 - accuracy: 0.8471\n",
      "Epoch 78/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.3463 - accuracy: 0.8630\n",
      "Epoch 79/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.3573 - accuracy: 0.8523\n",
      "Epoch 80/1500\n",
      "37/37 [==============================] - 0s 756us/step - loss: 0.3632 - accuracy: 0.8329\n",
      "Epoch 81/1500\n",
      "37/37 [==============================] - 0s 807us/step - loss: 0.3426 - accuracy: 0.8639\n",
      "Epoch 82/1500\n",
      "37/37 [==============================] - 0s 752us/step - loss: 0.3578 - accuracy: 0.8514\n",
      "Epoch 83/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.3574 - accuracy: 0.8493\n",
      "Epoch 84/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.3433 - accuracy: 0.8450\n",
      "Epoch 85/1500\n",
      "37/37 [==============================] - 0s 769us/step - loss: 0.3443 - accuracy: 0.8596\n",
      "Epoch 86/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.3601 - accuracy: 0.8519\n",
      "Epoch 87/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.3386 - accuracy: 0.8626\n",
      "Epoch 88/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.3430 - accuracy: 0.8583\n",
      "Epoch 89/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.3438 - accuracy: 0.8484\n",
      "Epoch 90/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.3318 - accuracy: 0.8630\n",
      "Epoch 91/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.3448 - accuracy: 0.8562\n",
      "Epoch 92/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.3460 - accuracy: 0.8570\n",
      "Epoch 93/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.3330 - accuracy: 0.8570\n",
      "Epoch 94/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.3408 - accuracy: 0.8622\n",
      "Epoch 95/1500\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.3296 - accuracy: 0.8648\n",
      "Epoch 96/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.3188 - accuracy: 0.8635\n",
      "Epoch 97/1500\n",
      "37/37 [==============================] - 0s 750us/step - loss: 0.3457 - accuracy: 0.8557\n",
      "Epoch 98/1500\n",
      "37/37 [==============================] - 0s 750us/step - loss: 0.3251 - accuracy: 0.8691\n",
      "Epoch 99/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.3192 - accuracy: 0.8639\n",
      "Epoch 100/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.3227 - accuracy: 0.8674\n",
      "Epoch 101/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.3247 - accuracy: 0.8600\n",
      "Epoch 102/1500\n",
      "37/37 [==============================] - 0s 766us/step - loss: 0.3258 - accuracy: 0.8665\n",
      "Epoch 103/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.3297 - accuracy: 0.8575\n",
      "Epoch 104/1500\n",
      "37/37 [==============================] - 0s 729us/step - loss: 0.3214 - accuracy: 0.8678\n",
      "Epoch 105/1500\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.3132 - accuracy: 0.8648\n",
      "Epoch 106/1500\n",
      "37/37 [==============================] - 0s 788us/step - loss: 0.3203 - accuracy: 0.8721\n",
      "Epoch 107/1500\n",
      "37/37 [==============================] - 0s 777us/step - loss: 0.3394 - accuracy: 0.8587\n",
      "Epoch 108/1500\n",
      "37/37 [==============================] - 0s 735us/step - loss: 0.3246 - accuracy: 0.8656\n",
      "Epoch 109/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.3185 - accuracy: 0.8674\n",
      "Epoch 110/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.3155 - accuracy: 0.8669\n",
      "Epoch 111/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.3084 - accuracy: 0.8786\n",
      "Epoch 112/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.3224 - accuracy: 0.8557\n",
      "Epoch 113/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.2959 - accuracy: 0.8833\n",
      "Epoch 114/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.3060 - accuracy: 0.8717\n",
      "Epoch 115/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.3123 - accuracy: 0.8708\n",
      "Epoch 116/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.3190 - accuracy: 0.8708\n",
      "Epoch 117/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.3120 - accuracy: 0.8734\n",
      "Epoch 118/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.3130 - accuracy: 0.8613\n",
      "Epoch 119/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.3060 - accuracy: 0.8678\n",
      "Epoch 120/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.3173 - accuracy: 0.8721\n",
      "Epoch 121/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.2966 - accuracy: 0.8764\n",
      "Epoch 122/1500\n",
      "37/37 [==============================] - 0s 756us/step - loss: 0.3090 - accuracy: 0.8712\n",
      "Epoch 123/1500\n",
      "37/37 [==============================] - 0s 777us/step - loss: 0.3103 - accuracy: 0.8699\n",
      "Epoch 124/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.2978 - accuracy: 0.8738\n",
      "Epoch 125/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.2963 - accuracy: 0.8755\n",
      "Epoch 126/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.3060 - accuracy: 0.8678\n",
      "Epoch 127/1500\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.2848 - accuracy: 0.8816\n",
      "Epoch 128/1500\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.3208 - accuracy: 0.8656\n",
      "Epoch 129/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.3000 - accuracy: 0.8669\n",
      "Epoch 130/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.3128 - accuracy: 0.8669\n",
      "Epoch 131/1500\n",
      "37/37 [==============================] - 0s 777us/step - loss: 0.2977 - accuracy: 0.8764\n",
      "Epoch 132/1500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.3005 - accuracy: 0.8764\n",
      "Epoch 133/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2914 - accuracy: 0.8842\n",
      "Epoch 134/1500\n",
      "37/37 [==============================] - 0s 777us/step - loss: 0.2904 - accuracy: 0.8816\n",
      "Epoch 135/1500\n",
      "37/37 [==============================] - 0s 826us/step - loss: 0.3048 - accuracy: 0.8755\n",
      "Epoch 136/1500\n",
      "37/37 [==============================] - 0s 750us/step - loss: 0.2802 - accuracy: 0.8910\n",
      "Epoch 137/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.2755 - accuracy: 0.8880\n",
      "Epoch 138/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.2802 - accuracy: 0.8846\n",
      "Epoch 139/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.2844 - accuracy: 0.8824\n",
      "Epoch 140/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.2861 - accuracy: 0.8798\n",
      "Epoch 141/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.2857 - accuracy: 0.8893\n",
      "Epoch 142/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.2814 - accuracy: 0.8863\n",
      "Epoch 143/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.2852 - accuracy: 0.8816\n",
      "Epoch 144/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.2692 - accuracy: 0.8833\n",
      "Epoch 145/1500\n",
      "37/37 [==============================] - 0s 778us/step - loss: 0.2786 - accuracy: 0.8880\n",
      "Epoch 146/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.2830 - accuracy: 0.8816\n",
      "Epoch 147/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2916 - accuracy: 0.8764\n",
      "Epoch 148/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2722 - accuracy: 0.8889\n",
      "Epoch 149/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.2699 - accuracy: 0.8842\n",
      "Epoch 150/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.2724 - accuracy: 0.8889\n",
      "Epoch 151/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.2571 - accuracy: 0.8910\n",
      "Epoch 152/1500\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.2861 - accuracy: 0.8837\n",
      "Epoch 153/1500\n",
      "37/37 [==============================] - 0s 923us/step - loss: 0.2745 - accuracy: 0.8807\n",
      "Epoch 154/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.2693 - accuracy: 0.8867\n",
      "Epoch 155/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.2734 - accuracy: 0.8936\n",
      "Epoch 156/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.2707 - accuracy: 0.8928\n",
      "Epoch 157/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.2685 - accuracy: 0.8910\n",
      "Epoch 158/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.2797 - accuracy: 0.8898\n",
      "Epoch 159/1500\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.2675 - accuracy: 0.8915\n",
      "Epoch 160/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.2722 - accuracy: 0.8898\n",
      "Epoch 161/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.2826 - accuracy: 0.8742\n",
      "Epoch 162/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.2643 - accuracy: 0.8902\n",
      "Epoch 163/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.2606 - accuracy: 0.8997\n",
      "Epoch 164/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.2583 - accuracy: 0.8893\n",
      "Epoch 165/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.2633 - accuracy: 0.8893\n",
      "Epoch 166/1500\n",
      "37/37 [==============================] - 0s 766us/step - loss: 0.2701 - accuracy: 0.8910\n",
      "Epoch 167/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.2705 - accuracy: 0.8885\n",
      "Epoch 168/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.2605 - accuracy: 0.8928\n",
      "Epoch 169/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.2506 - accuracy: 0.8936\n",
      "Epoch 170/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.2557 - accuracy: 0.8936\n",
      "Epoch 171/1500\n",
      "37/37 [==============================] - 0s 752us/step - loss: 0.2455 - accuracy: 0.8988\n",
      "Epoch 172/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.2538 - accuracy: 0.8962\n",
      "Epoch 173/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.2527 - accuracy: 0.8971\n",
      "Epoch 174/1500\n",
      "37/37 [==============================] - 0s 766us/step - loss: 0.2527 - accuracy: 0.8945\n",
      "Epoch 175/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.2670 - accuracy: 0.8859\n",
      "Epoch 176/1500\n",
      "37/37 [==============================] - 0s 769us/step - loss: 0.2620 - accuracy: 0.8910\n",
      "Epoch 177/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.2554 - accuracy: 0.9005\n",
      "Epoch 178/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2640 - accuracy: 0.8910\n",
      "Epoch 179/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.2503 - accuracy: 0.9022\n",
      "Epoch 180/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.2349 - accuracy: 0.9035\n",
      "Epoch 181/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.2652 - accuracy: 0.8949\n",
      "Epoch 182/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.2507 - accuracy: 0.8923\n",
      "Epoch 183/1500\n",
      "37/37 [==============================] - 0s 756us/step - loss: 0.2573 - accuracy: 0.8975\n",
      "Epoch 184/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.2577 - accuracy: 0.8966\n",
      "Epoch 185/1500\n",
      "37/37 [==============================] - 0s 739us/step - loss: 0.2526 - accuracy: 0.8936\n",
      "Epoch 186/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.2554 - accuracy: 0.8988\n",
      "Epoch 187/1500\n",
      "37/37 [==============================] - 0s 739us/step - loss: 0.2476 - accuracy: 0.8984\n",
      "Epoch 188/1500\n",
      "37/37 [==============================] - 0s 777us/step - loss: 0.2500 - accuracy: 0.9022\n",
      "Epoch 189/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.2341 - accuracy: 0.9070\n",
      "Epoch 190/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.2514 - accuracy: 0.8902\n",
      "Epoch 191/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.2423 - accuracy: 0.8997\n",
      "Epoch 192/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.2497 - accuracy: 0.8979\n",
      "Epoch 193/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.2292 - accuracy: 0.9087\n",
      "Epoch 194/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.2611 - accuracy: 0.8906\n",
      "Epoch 195/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.2372 - accuracy: 0.8988\n",
      "Epoch 196/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.2327 - accuracy: 0.9100\n",
      "Epoch 197/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.2307 - accuracy: 0.9109\n",
      "Epoch 198/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.2289 - accuracy: 0.9048\n",
      "Epoch 199/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.2457 - accuracy: 0.8988\n",
      "Epoch 200/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.2349 - accuracy: 0.9117\n",
      "Epoch 201/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.2364 - accuracy: 0.9035\n",
      "Epoch 202/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.2329 - accuracy: 0.9070\n",
      "Epoch 203/1500\n",
      "37/37 [==============================] - 0s 833us/step - loss: 0.2420 - accuracy: 0.9014\n",
      "Epoch 204/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.2286 - accuracy: 0.9074\n",
      "Epoch 205/1500\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.2240 - accuracy: 0.9208\n",
      "Epoch 206/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.2418 - accuracy: 0.9001\n",
      "Epoch 207/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.2410 - accuracy: 0.9070\n",
      "Epoch 208/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.2313 - accuracy: 0.9165\n",
      "Epoch 209/1500\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.2271 - accuracy: 0.9130\n",
      "Epoch 210/1500\n",
      "37/37 [==============================] - 0s 788us/step - loss: 0.2071 - accuracy: 0.9216\n",
      "Epoch 211/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.2352 - accuracy: 0.9053\n",
      "Epoch 212/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.2286 - accuracy: 0.9096\n",
      "Epoch 213/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.2300 - accuracy: 0.9087\n",
      "Epoch 214/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.2125 - accuracy: 0.9121\n",
      "Epoch 215/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.2167 - accuracy: 0.9117\n",
      "Epoch 216/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.2252 - accuracy: 0.9083\n",
      "Epoch 217/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.2177 - accuracy: 0.9126\n",
      "Epoch 218/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.2249 - accuracy: 0.9044\n",
      "Epoch 219/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.2366 - accuracy: 0.9022\n",
      "Epoch 220/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.2437 - accuracy: 0.9053\n",
      "Epoch 221/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.2065 - accuracy: 0.9199\n",
      "Epoch 222/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.2239 - accuracy: 0.9143\n",
      "Epoch 223/1500\n",
      "37/37 [==============================] - 0s 766us/step - loss: 0.2200 - accuracy: 0.9190\n",
      "Epoch 224/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.2148 - accuracy: 0.9121\n",
      "Epoch 225/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2176 - accuracy: 0.9091\n",
      "Epoch 226/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.2254 - accuracy: 0.9113\n",
      "Epoch 227/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.2161 - accuracy: 0.9109\n",
      "Epoch 228/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2106 - accuracy: 0.9147\n",
      "Epoch 229/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.2120 - accuracy: 0.9160\n",
      "Epoch 230/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2213 - accuracy: 0.9147\n",
      "Epoch 231/1500\n",
      "37/37 [==============================] - 0s 752us/step - loss: 0.2267 - accuracy: 0.9078\n",
      "Epoch 232/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.2143 - accuracy: 0.9130\n",
      "Epoch 233/1500\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.2136 - accuracy: 0.9147\n",
      "Epoch 234/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.2435 - accuracy: 0.9018\n",
      "Epoch 235/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.2111 - accuracy: 0.9160\n",
      "Epoch 236/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.2138 - accuracy: 0.9130\n",
      "Epoch 237/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.2305 - accuracy: 0.9018\n",
      "Epoch 238/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.2047 - accuracy: 0.9190\n",
      "Epoch 239/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.2138 - accuracy: 0.9165\n",
      "Epoch 240/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.2012 - accuracy: 0.9199\n",
      "Epoch 241/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.2263 - accuracy: 0.9044\n",
      "Epoch 242/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.2147 - accuracy: 0.9165\n",
      "Epoch 243/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.2112 - accuracy: 0.9242\n",
      "Epoch 244/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.2148 - accuracy: 0.9126\n",
      "Epoch 245/1500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.2234 - accuracy: 0.9087\n",
      "Epoch 246/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2210 - accuracy: 0.9065\n",
      "Epoch 247/1500\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.2021 - accuracy: 0.9233\n",
      "Epoch 248/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.2033 - accuracy: 0.9216\n",
      "Epoch 249/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.2133 - accuracy: 0.9220\n",
      "Epoch 250/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.1930 - accuracy: 0.9242\n",
      "Epoch 251/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.2322 - accuracy: 0.9074\n",
      "Epoch 252/1500\n",
      "37/37 [==============================] - 0s 739us/step - loss: 0.2148 - accuracy: 0.9121\n",
      "Epoch 253/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.1930 - accuracy: 0.9225\n",
      "Epoch 254/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.2074 - accuracy: 0.9096\n",
      "Epoch 255/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.2081 - accuracy: 0.9190\n",
      "Epoch 256/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.1965 - accuracy: 0.9281\n",
      "Epoch 257/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.1987 - accuracy: 0.9242\n",
      "Epoch 258/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.2128 - accuracy: 0.9152\n",
      "Epoch 259/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.1998 - accuracy: 0.9276\n",
      "Epoch 260/1500\n",
      "37/37 [==============================] - 0s 792us/step - loss: 0.2110 - accuracy: 0.9156\n",
      "Epoch 261/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.1981 - accuracy: 0.9216\n",
      "Epoch 262/1500\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.2063 - accuracy: 0.9169\n",
      "Epoch 263/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.1917 - accuracy: 0.9238\n",
      "Epoch 264/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.2055 - accuracy: 0.9130\n",
      "Epoch 265/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.2145 - accuracy: 0.9156\n",
      "Epoch 266/1500\n",
      "37/37 [==============================] - 0s 805us/step - loss: 0.2046 - accuracy: 0.9186\n",
      "Epoch 267/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.1991 - accuracy: 0.9233\n",
      "Epoch 268/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.1988 - accuracy: 0.9195\n",
      "Epoch 269/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.1988 - accuracy: 0.9220\n",
      "Epoch 270/1500\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.1898 - accuracy: 0.9264\n",
      "Epoch 271/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.1874 - accuracy: 0.9302\n",
      "Epoch 272/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.1979 - accuracy: 0.9233\n",
      "Epoch 273/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.2054 - accuracy: 0.9177\n",
      "Epoch 274/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.1943 - accuracy: 0.9195\n",
      "Epoch 275/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.1970 - accuracy: 0.9233\n",
      "Epoch 276/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.1937 - accuracy: 0.9246\n",
      "Epoch 277/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.2034 - accuracy: 0.9233\n",
      "Epoch 278/1500\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.1770 - accuracy: 0.9272\n",
      "Epoch 279/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.1960 - accuracy: 0.9307\n",
      "Epoch 280/1500\n",
      "37/37 [==============================] - 0s 746us/step - loss: 0.1890 - accuracy: 0.9233\n",
      "Epoch 281/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.1976 - accuracy: 0.9233\n",
      "Epoch 282/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.1909 - accuracy: 0.9233\n",
      "Epoch 283/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.1942 - accuracy: 0.9246\n",
      "Epoch 284/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.1810 - accuracy: 0.9302\n",
      "Epoch 285/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.1934 - accuracy: 0.9264\n",
      "Epoch 286/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.1885 - accuracy: 0.9220\n",
      "Epoch 287/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.1966 - accuracy: 0.9216\n",
      "Epoch 288/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.2080 - accuracy: 0.9147\n",
      "Epoch 289/1500\n",
      "37/37 [==============================] - 0s 777us/step - loss: 0.1904 - accuracy: 0.9238\n",
      "Epoch 290/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.1953 - accuracy: 0.9242\n",
      "Epoch 291/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.2017 - accuracy: 0.9190\n",
      "Epoch 292/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.1785 - accuracy: 0.9289\n",
      "Epoch 293/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.1787 - accuracy: 0.9281\n",
      "Epoch 294/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.1987 - accuracy: 0.9190\n",
      "Epoch 295/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.1842 - accuracy: 0.9264\n",
      "Epoch 296/1500\n",
      "37/37 [==============================] - 0s 739us/step - loss: 0.1849 - accuracy: 0.9268\n",
      "Epoch 297/1500\n",
      "37/37 [==============================] - 0s 769us/step - loss: 0.2009 - accuracy: 0.9156\n",
      "Epoch 298/1500\n",
      "37/37 [==============================] - 0s 736us/step - loss: 0.1784 - accuracy: 0.9272\n",
      "Epoch 299/1500\n",
      "37/37 [==============================] - 0s 792us/step - loss: 0.1780 - accuracy: 0.9272\n",
      "Epoch 300/1500\n",
      "37/37 [==============================] - 0s 751us/step - loss: 0.1895 - accuracy: 0.9199\n",
      "Epoch 301/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.1867 - accuracy: 0.9298\n",
      "Epoch 302/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.1891 - accuracy: 0.9281\n",
      "Epoch 303/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.1773 - accuracy: 0.9302\n",
      "Epoch 304/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.1889 - accuracy: 0.9255\n",
      "Epoch 305/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.1745 - accuracy: 0.9298\n",
      "Epoch 306/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.1740 - accuracy: 0.9328\n",
      "Epoch 307/1500\n",
      "37/37 [==============================] - 0s 742us/step - loss: 0.1783 - accuracy: 0.9281\n",
      "Epoch 308/1500\n",
      "37/37 [==============================] - 0s 729us/step - loss: 0.1900 - accuracy: 0.9294\n",
      "Epoch 309/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.1867 - accuracy: 0.9285\n",
      "Epoch 310/1500\n",
      "37/37 [==============================] - 0s 744us/step - loss: 0.1990 - accuracy: 0.9195\n",
      "Epoch 311/1500\n",
      "37/37 [==============================] - 0s 791us/step - loss: 0.1819 - accuracy: 0.9264\n",
      "Epoch 312/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.1770 - accuracy: 0.9307\n",
      "Epoch 313/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.1881 - accuracy: 0.9242\n",
      "Epoch 314/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.1712 - accuracy: 0.9302\n",
      "Epoch 315/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.1952 - accuracy: 0.9298\n",
      "Epoch 316/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.1734 - accuracy: 0.9332\n",
      "Epoch 317/1500\n",
      "37/37 [==============================] - 0s 753us/step - loss: 0.1782 - accuracy: 0.9354\n",
      "Epoch 318/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.1939 - accuracy: 0.9233\n",
      "Epoch 319/1500\n",
      "37/37 [==============================] - 0s 769us/step - loss: 0.1793 - accuracy: 0.9289\n",
      "Epoch 320/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.1703 - accuracy: 0.9337\n",
      "Epoch 321/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.1802 - accuracy: 0.9307\n",
      "Epoch 322/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.1808 - accuracy: 0.9259\n",
      "Epoch 323/1500\n",
      "37/37 [==============================] - 0s 746us/step - loss: 0.1929 - accuracy: 0.9255\n",
      "Epoch 324/1500\n",
      "37/37 [==============================] - 0s 835us/step - loss: 0.1691 - accuracy: 0.9350\n",
      "Epoch 325/1500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1894 - accuracy: 0.9294\n",
      "Epoch 326/1500\n",
      "37/37 [==============================] - 0s 903us/step - loss: 0.1944 - accuracy: 0.9242\n",
      "Epoch 327/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.1649 - accuracy: 0.9324\n",
      "Epoch 328/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.1766 - accuracy: 0.9324\n",
      "Epoch 329/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.1789 - accuracy: 0.9294\n",
      "Epoch 330/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.1687 - accuracy: 0.9298\n",
      "Epoch 331/1500\n",
      "37/37 [==============================] - 0s 777us/step - loss: 0.1734 - accuracy: 0.9298\n",
      "Epoch 332/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.1726 - accuracy: 0.9272\n",
      "Epoch 333/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.1659 - accuracy: 0.9371\n",
      "Epoch 334/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.1626 - accuracy: 0.9401\n",
      "Epoch 335/1500\n",
      "37/37 [==============================] - 0s 746us/step - loss: 0.1730 - accuracy: 0.9371\n",
      "Epoch 336/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.1668 - accuracy: 0.9358\n",
      "Epoch 337/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.1913 - accuracy: 0.9233\n",
      "Epoch 338/1500\n",
      "37/37 [==============================] - 0s 740us/step - loss: 0.1638 - accuracy: 0.9358\n",
      "Epoch 339/1500\n",
      "37/37 [==============================] - 0s 727us/step - loss: 0.1655 - accuracy: 0.9320\n",
      "Epoch 340/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.1588 - accuracy: 0.9332\n",
      "Epoch 341/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.1744 - accuracy: 0.9294\n",
      "Epoch 342/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.1690 - accuracy: 0.9363\n",
      "Epoch 343/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.1657 - accuracy: 0.9384\n",
      "Epoch 344/1500\n",
      "37/37 [==============================] - 0s 708us/step - loss: 0.1785 - accuracy: 0.9298\n",
      "Epoch 345/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.1659 - accuracy: 0.9324\n",
      "Epoch 346/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.1775 - accuracy: 0.9289\n",
      "Epoch 347/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.1568 - accuracy: 0.9363\n",
      "Epoch 348/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.1707 - accuracy: 0.9307\n",
      "Epoch 349/1500\n",
      "37/37 [==============================] - 0s 748us/step - loss: 0.1664 - accuracy: 0.9320\n",
      "Epoch 350/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.1786 - accuracy: 0.9233\n",
      "Epoch 351/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.1692 - accuracy: 0.9345\n",
      "Epoch 352/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.1773 - accuracy: 0.9307\n",
      "Epoch 353/1500\n",
      "37/37 [==============================] - 0s 753us/step - loss: 0.1718 - accuracy: 0.9337\n",
      "Epoch 354/1500\n",
      "37/37 [==============================] - 0s 883us/step - loss: 0.1786 - accuracy: 0.9255\n",
      "Epoch 355/1500\n",
      "37/37 [==============================] - 0s 836us/step - loss: 0.1762 - accuracy: 0.9354\n",
      "Epoch 356/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.1569 - accuracy: 0.9380\n",
      "Epoch 357/1500\n",
      "37/37 [==============================] - 0s 752us/step - loss: 0.1704 - accuracy: 0.9341\n",
      "Epoch 358/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.1648 - accuracy: 0.9328\n",
      "Epoch 359/1500\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.1739 - accuracy: 0.9320\n",
      "Epoch 360/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.1693 - accuracy: 0.9337\n",
      "Epoch 361/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.1662 - accuracy: 0.9358\n",
      "Epoch 362/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.1750 - accuracy: 0.9363\n",
      "Epoch 363/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.1612 - accuracy: 0.9388\n",
      "Epoch 364/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.1557 - accuracy: 0.9345\n",
      "Epoch 365/1500\n",
      "37/37 [==============================] - 0s 731us/step - loss: 0.1657 - accuracy: 0.9332\n",
      "Epoch 366/1500\n",
      "37/37 [==============================] - 0s 714us/step - loss: 0.1583 - accuracy: 0.9419\n",
      "Epoch 367/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.1670 - accuracy: 0.9388\n",
      "Epoch 368/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.1598 - accuracy: 0.9354\n",
      "Epoch 369/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.1764 - accuracy: 0.9320\n",
      "Epoch 370/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.1628 - accuracy: 0.9341\n",
      "Epoch 371/1500\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.1628 - accuracy: 0.9328\n",
      "Epoch 372/1500\n",
      "37/37 [==============================] - 0s 724us/step - loss: 0.1780 - accuracy: 0.9307\n",
      "Epoch 373/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.1595 - accuracy: 0.9358\n",
      "Epoch 374/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.1529 - accuracy: 0.9427\n",
      "Epoch 375/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.1497 - accuracy: 0.9423\n",
      "Epoch 376/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.1784 - accuracy: 0.9276\n",
      "Epoch 377/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.1616 - accuracy: 0.9406\n",
      "Epoch 378/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.1423 - accuracy: 0.9479\n",
      "Epoch 379/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.1620 - accuracy: 0.9328\n",
      "Epoch 380/1500\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.1746 - accuracy: 0.9264\n",
      "Epoch 381/1500\n",
      "37/37 [==============================] - 0s 745us/step - loss: 0.1614 - accuracy: 0.9367\n",
      "Epoch 382/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.1616 - accuracy: 0.9397\n",
      "Epoch 383/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.1711 - accuracy: 0.9337\n",
      "Epoch 384/1500\n",
      "37/37 [==============================] - 0s 724us/step - loss: 0.1591 - accuracy: 0.9388\n",
      "Epoch 385/1500\n",
      "37/37 [==============================] - 0s 746us/step - loss: 0.1481 - accuracy: 0.9397\n",
      "Epoch 386/1500\n",
      "37/37 [==============================] - 0s 743us/step - loss: 0.1642 - accuracy: 0.9354\n",
      "Epoch 387/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.1565 - accuracy: 0.9397\n",
      "Epoch 388/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.1615 - accuracy: 0.9350\n",
      "Epoch 389/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.1523 - accuracy: 0.9384\n",
      "Epoch 390/1500\n",
      "37/37 [==============================] - 0s 6ms/step - loss: 0.1495 - accuracy: 0.9432\n",
      "Epoch 391/1500\n",
      "37/37 [==============================] - 0s 832us/step - loss: 0.1516 - accuracy: 0.9397\n",
      "Epoch 392/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.1572 - accuracy: 0.9414\n",
      "Epoch 393/1500\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.1587 - accuracy: 0.9406\n",
      "Epoch 394/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.1531 - accuracy: 0.9410\n",
      "Epoch 395/1500\n",
      "37/37 [==============================] - 0s 734us/step - loss: 0.1582 - accuracy: 0.9423\n",
      "Epoch 396/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.1446 - accuracy: 0.9427\n",
      "Epoch 397/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.1738 - accuracy: 0.9328\n",
      "Epoch 398/1500\n",
      "37/37 [==============================] - 0s 739us/step - loss: 0.1585 - accuracy: 0.9332\n",
      "Epoch 399/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.1555 - accuracy: 0.9432\n",
      "Epoch 400/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.1568 - accuracy: 0.9444\n",
      "Epoch 401/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.1586 - accuracy: 0.9350\n",
      "Epoch 402/1500\n",
      "37/37 [==============================] - 0s 722us/step - loss: 0.1512 - accuracy: 0.9449\n",
      "Epoch 403/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.1475 - accuracy: 0.9406\n",
      "Epoch 404/1500\n",
      "37/37 [==============================] - 0s 813us/step - loss: 0.1373 - accuracy: 0.9496\n",
      "Epoch 405/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.1644 - accuracy: 0.9289\n",
      "Epoch 406/1500\n",
      "37/37 [==============================] - 0s 777us/step - loss: 0.1548 - accuracy: 0.9376\n",
      "Epoch 407/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.1614 - accuracy: 0.9384\n",
      "Epoch 408/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.1491 - accuracy: 0.9406\n",
      "Epoch 409/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.1448 - accuracy: 0.9419\n",
      "Epoch 410/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.1567 - accuracy: 0.9423\n",
      "Epoch 411/1500\n",
      "37/37 [==============================] - 0s 756us/step - loss: 0.1627 - accuracy: 0.9332\n",
      "Epoch 412/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.1470 - accuracy: 0.9432\n",
      "Epoch 413/1500\n",
      "37/37 [==============================] - 0s 736us/step - loss: 0.1478 - accuracy: 0.9449\n",
      "Epoch 414/1500\n",
      "37/37 [==============================] - 0s 723us/step - loss: 0.1504 - accuracy: 0.9432\n",
      "Epoch 415/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.1551 - accuracy: 0.9410\n",
      "Epoch 416/1500\n",
      "37/37 [==============================] - 0s 763us/step - loss: 0.1541 - accuracy: 0.9376\n",
      "Epoch 417/1500\n",
      "37/37 [==============================] - 0s 755us/step - loss: 0.1510 - accuracy: 0.9397\n",
      "Epoch 418/1500\n",
      "37/37 [==============================] - 0s 752us/step - loss: 0.1500 - accuracy: 0.9427\n",
      "Epoch 419/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.1525 - accuracy: 0.9401\n",
      "Epoch 420/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.1503 - accuracy: 0.9427\n",
      "Epoch 421/1500\n",
      "37/37 [==============================] - 0s 718us/step - loss: 0.1476 - accuracy: 0.9414\n",
      "Epoch 422/1500\n",
      "37/37 [==============================] - 0s 749us/step - loss: 0.1596 - accuracy: 0.9393\n",
      "Epoch 423/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.1578 - accuracy: 0.9388\n",
      "Epoch 424/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.1461 - accuracy: 0.9419\n",
      "Epoch 425/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.1590 - accuracy: 0.9427\n",
      "Epoch 426/1500\n",
      "37/37 [==============================] - 0s 751us/step - loss: 0.1463 - accuracy: 0.9444\n",
      "Epoch 427/1500\n",
      "37/37 [==============================] - 0s 731us/step - loss: 0.1509 - accuracy: 0.9440\n",
      "Epoch 428/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.1429 - accuracy: 0.9453\n",
      "Epoch 429/1500\n",
      "37/37 [==============================] - 0s 750us/step - loss: 0.1478 - accuracy: 0.9414\n",
      "Epoch 430/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.1490 - accuracy: 0.9406\n",
      "Epoch 431/1500\n",
      "37/37 [==============================] - 0s 736us/step - loss: 0.1401 - accuracy: 0.9470\n",
      "Epoch 432/1500\n",
      "37/37 [==============================] - 0s 735us/step - loss: 0.1433 - accuracy: 0.9401\n",
      "Epoch 433/1500\n",
      "37/37 [==============================] - 0s 722us/step - loss: 0.1634 - accuracy: 0.9397\n",
      "Epoch 434/1500\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.2490 - accuracy: 0.8906Restoring model weights from the end of the best epoch: 404.\n",
      "37/37 [==============================] - 0s 837us/step - loss: 0.1398 - accuracy: 0.9500\n",
      "Epoch 434: early stopping\n",
      "6/6 [==============================] - 0s 851us/step - loss: 1.1388 - accuracy: 0.6629\n",
      "6/6 [==============================] - 0s 661us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "After appending - Cat IDs: 854, Predictions: 854, Actuals: 854, Gender: 854\n",
      "Final Test Results - Loss: 1.138780951499939, Accuracy: 0.6628571152687073, Precision: 0.6815448603683899, Recall: 0.7127815200381029, F1 Score: 0.6589057031214985\n",
      "Confusion Matrix:\n",
      " [[80  4  8]\n",
      " [ 1 11  0]\n",
      " [44  2 25]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6773507257501308\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.9326027035713196\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7127727717161179\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6865193485825166\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7299314064383858\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[1]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2845ad-17c1-494a-8bd0-971aefca9f01",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "3910db95-6772-4098-bef1-fb5857901e5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 854, Predictions: 854, Actuals: 854, Gender: 854\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "d543c7c2-c11b-4511-ba5a-c52969a61a62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "6b6f2173-89a8-40ba-afa6-410005c2dcb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (83/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "aa8d9ba5-9d96-4aee-b3e2-ba87553d1899",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "0b4ff1f7-d22d-4409-98d4-49e9a82bcb58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, senior, adult, kitten, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[adult, kitten, kitten, adult, kitten, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[kitten, adult, adult, kitten, adult, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, adult, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, senior, adult, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, adult, senior, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[kitten, adult, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, senior, adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [senior, senior, adult, adult, adult, adult, a...         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "56    051A  [senior, adult, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A            [kitten, adult, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "80    074A  [adult, senior, adult, kitten, adult, adult, a...         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [adult, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "103   109A    [kitten, adult, kitten, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "101   106A  [adult, senior, senior, senior, senior, adult,...        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, senior, adult, senior, a...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "18    016A  [senior, adult, adult, senior, senior, senior,...        senior           senior                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, adult, a...         adult            adult                   True\n",
       "16    014B  [adult, kitten, kitten, adult, kitten, kitten,...        kitten           kitten                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, senior, adult, adult, s...         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "12    011A                                   [senior, senior]        senior           senior                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, adult, senior]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "44    038A                                     [adult, adult]         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                     [senior, adult, senior, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "39    033A  [kitten, adult, adult, kitten, adult, adult, k...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "102   108A       [adult, adult, kitten, kitten, adult, adult]         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "52    047A  [kitten, adult, adult, adult, adult, adult, ki...         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "42    036A  [adult, senior, senior, senior, adult, adult, ...        senior            adult                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "48    042A  [adult, adult, kitten, adult, adult, adult, ad...         adult           kitten                  False\n",
       "30    025C            [senior, senior, senior, senior, adult]        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "90    095A  [senior, senior, adult, senior, senior, senior...        senior            adult                  False\n",
       "61    055A  [senior, adult, senior, adult, senior, adult, ...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "69    063A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "70    064A                            [kitten, adult, kitten]        kitten            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "76    070A             [adult, senior, adult, senior, senior]        senior            adult                  False\n",
       "74    068A  [adult, adult, senior, senior, senior, adult, ...        senior            adult                  False\n",
       "109   117A  [adult, adult, adult, adult, adult, adult, adult]         adult           senior                  False"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "5ec47984-bc63-4f3f-a7a4-d3979dbd4c52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     59\n",
      "kitten    11\n",
      "senior    13\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "952d49d2-180d-4f36-85b0-6e2b96610559",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             59  80.821918\n",
      "1           kitten           15             11  73.333333\n",
      "2           senior           22             13  59.090909\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "6a94de06-f9df-49f6-99e7-abc9024f3dc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885dd133-e156-4b01-8e84-4d94546e0eca",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "dd0127df-e0fb-4862-9ac4-2113bd346e78",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          558            431  77.240143\n",
      "1           kitten          118             81  68.644068\n",
      "2           senior          178             97  54.494382\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "4a0df220-5e5c-4c46-8474-ac4ecf291071",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABfBElEQVR4nO3dd3QUZf/+8fcmpIcSAgFC70UEQg1NIHSkKdVH9BGkSREUsdAFsVGkF0EQAw9FpQtIV1qkhE6ItEAgdAFJIaTs74/8Mt8sCRBSSMJer3M4Z3dmduYzmx322nvuucdkNpvNiIiIiIhYCZuMLkBERERE5HlSABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVcmW0QWIWKOwsDBWr17Nnj17uHDhAnfv3sXBwYF8+fJRrVo1Xn/9dUqVKpXRZaaZkJAQ2rZtazw/ePCg8bhNmzZcvXoVgDlz5lC9evVkrzciIoIWLVoQFhYGQNmyZVmyZEkaVS0p9aS/d0ZYv349Y8aMMZ4PGTKEN954I+MKegbR0dFs2bKFLVu2cO7cOW7fvo3ZbCZXrlyUKVOGxo0b06JFC7Jl09e5yLPQESPynPn7+/PZZ59x+/Zti+lRUVGEhoZy7tw5fv75Zzp16sSHH36oL7Yn2LJlixF+AQIDAzl58iQvvfRSBlYlmc3atWstnq9atSpLBOCgoCBGjRrFqVOnEs27fv06169fZ9euXSxZsoTvvvuO/PnzZ0CVIlmTvllFnqNjx44xcOBAIiMjAbC1taVmzZoUK1aMiIgIDhw4wJUrVzCbzaxYsYJ//vmHr7/+OoOrzrzWrFmTaNqqVasUgMVw6dIl/P39LaadP3+eI0eOUKVKlYwpKhkuX75M9+7duX//PgA2NjZUq1aNkiVLEhkZybFjxzh37hwAZ86c4f3332fJkiXY2dllZNkiWYYCsMhzEhkZyYgRI4zwW7BgQSZNmmTR1SEmJob58+czb948ALZu3cqqVat47bXXMqTmzCwoKIijR48CkCNHDv79918ANm/ezAcffICLi0tGlieZRMLW34Sfk1WrVmXaABwdHc3HH39shN/8+fMzadIkypYta7Hczz//zDfffAPEhfrffvuN9u3bP+9yRbIkBWCR5+T3338nJCQEiGvNmTBhQqJ+vra2tvTp04cLFy6wdetWABYuXEj79u35888/GTJkCACenp6sWbMGk8lk8fpOnTpx4cIFAKZMmUK9evWAuPC9bNkyNm7cSHBwMPb29pQuXZrXX3+d5s2bW6zn4MGD9O3bF4CmTZvSqlUrJk+ezLVr18iXLx8zZ86kYMGC3Lp1ix9++IF9+/Zx48YNYmJiyJUrFxUqVKB79+5UqlQpHd7F/5Ow9bdTp074+flx8uRJwsPD2bRpEx06dHjsa0+fPo2vry/+/v7cvXuX3LlzU7JkSbp27UqdOnUSLR8aGsqSJUvYsWMHly9fxs7ODk9PT5o1a0anTp1wdnY2lh0zZgzr168HoFevXvTp08eYl/C9LVCgAOvWrTPmxfd9dnd3Z968eYwZM4aAgABy5MjBxx9/TOPGjXn48CFLlixhy5YtBAcHExkZiYuLC8WLF6dDhw68+uqrKa69R48eHDt2DIDBgwfTrVs3i/UsXbqUSZMmAVCvXj2mTJny2Pf3UQ8fPmThwoWsW7eOf/75h0KFCtG2bVu6du1qdPEZPnw4v//+OwCdO3fm448/tljHzp07+eijjwAoWbIky5cvf+p2o6Ojjb8FxP1tPvzwQyDux+VHH31E9uzZk3xtWFgYCxYsYMuWLdy6dQtPT086duxIly5d8Pb2JiYmJtHfEOI+WwsWLMDf35+wsDA8PDyoXbs23bt3J1++fMl6v7Zu3crff/8NxP1fMXnyZMqUKZNouU6dOnHu3Dnu3btHiRIlKFmypDEvuccxwNWrV1mxYgW7du3i2rVrZMuWjVKlStGqVSvatm2bqBtWwn76a9euxdPT0+I9Turzv27dOj7//HMAunXrxhtvvMHMmTPZu3cvkZGRlC9fnl69elGjRo1kvUciqaUALPKc/Pnnn8bjGjVqJPmFFu/NN980AnBISAhnz56lbt26uLu7c/v2bUJCQjh69KhFC1ZAQIARfvPmzUvt2rWBuC/yAQMGcPz4cWPZyMhI/P398ff3x8/Pj9GjRycK0xB3avXjjz8mKioKiOun7OnpyZ07d+jduzeXLl2yWP727dvs2rWLvXv3Mm3aNGrVqvWM71LyREdH89tvvxnP27RpQ/78+Tl58iQQ17r3uAC8fv16xo0bR0xMjDEtvj/l3r17GTBgAO+8844x79q1a7z33nsEBwcb0x48eEBgYCCBgYFs27aNOXPmWITg1Hjw4AEDBgwwfizdvn2bMmXKEBsby/Dhw9mxY4fF8vfv3+fYsWMcO3aMy5cvWwTuZ6m9bdu2RgDevHlzogC8ZcsW43Hr1q2faZ8GDx7M/v37jefnz59nypQpHD16lG+//RaTyUS7du2MALxt2zY++ugjbGz+b6CilGx/z5493Lp1CwAvLy9eeeUVKlWqxLFjx4iMjOS3336ja9euiV4XGhpKr169OHPmjDEtKCiIiRMncvbs2cdub9OmTYwePdris3XlyhV++eUXtmzZwvTp06lQocJT6064r97e3k/8v+LTTz996voedxwD7N27l2HDhhEaGmrxmiNHjnDkyBE2bdrE5MmTcXV1fep2kiskJIRu3bpx584dY5q/vz/9+/dn5MiRtGnTJs22JfI4GgZN5DlJ+GX6tFOv5cuXt+jLFxAQQLZs2Sy++Ddt2mTxmg0bNhiPX331VWxtbQGYNGmSEX6dnJxo06YNr776Kg4ODkBcIFy1alWSdQQFBWEymWjTpg1NmjShZcuWmEwmfvzxRyP8FixYkK5du/L666+TJ08eIK4rx7Jly564j6mxa9cu/vnnHyAu2BQqVIhmzZrh5OQExLXCBQQEJHrd+fPnGT9+vBFQSpcuTadOnfD29jaWmTFjBoGBgcbz4cOHGwHS1dWV1q1b065dO6OLxalTp5g9e3aa7VtYWBghISHUr1+f1157jVq1alG4cGF2795thF8XFxfatWtH165dLcLR//73P8xmc4pqb9asmRHiT506xeXLl431XLt2zfgM5ciRg1deeeWZ9mn//v2UL1+eTp06Ua5cOWP6jh07jJb8GjVqGC2St2/f5tChQ8ZykZGR7Nq1C4g7S9KyZctkbTfhWYL4Y6ddu3bGtNWrVyf5umnTplkcr3Xq1OH111/H09OT1atXWwTceBcvXrT4YfXSSy9Z7O+9e/f47LPPjC5QT3L69GnjceXKlZ+6/NM87jgOCQnhs88+M8Jvvnz5eO211/Dx8TFaff39/Rk5cmSqa0ho+/bt3Llzhzp16vDaa6/h4eEBQGxsLF9//bUxKoxIelILsMhzkrC1w93d/YnLZsuWjRw5chgjRdy9exeAtm3bsmjRIiCuleijjz4iW7ZsxMTEsHnzZuP18UNQ3bp1y2gptbOzY8GCBZQuXRqAjh078u677xIbG8vixYt5/fXXk6zl/fffT9RKVrhwYZo3b86lS5eYOnUquXPnBqBly5b06tULiGv5Si8Jg018a5GLiwtNmjQxTkmvXLmS4cOHW7xu6dKlRitYw4YN+frrr40v+i+++ILVq1fj4uLC/v37KVu2LEePHjX6Gbu4uLB48WIKFSpkbLdnz57Y2tpy8uRJYmNjLVosU6NRo0ZMmDDBYpq9vT3t27fnzJkz9O3b12jhf/DgAU2bNiUiIoKwsDDu3r2Lm5vbM9fu7OxMkyZNjD6zmzdvpkePHkDcKfn4YN2sWTPs7e2faX+aNm3K+PHjsbGxITY2lpEjRxqtvStXrqR9+/ZGQJszZ46x/fjT4Xv27CE8PByAWrVqGT+0nuTWrVvs2bMHiPvh17RpU6OWSZMmER4eztmzZzl27JhFd52IiAiLswsJu4OEhYXRq1cvo3tCQsuWLTPCbYsWLRg3bhwmk4nY2FiGDBnCrl27uHLlCtu3b39qgE84Qkz8sRUvOjra4gdbQkl1yYiX1HG8cOFCYxSVChUqMGvWLKOl9/Dhw/Tt25eYmBh27drFwYMHn2mIwqf56KOPjHru3LlDt27duH79OpGRkaxatYp+/fql2bZEkqIWYJHnJDo62nicsJXucRIuE/+4aNGieHl5AXEtSvv27QPiWtjivzSrVKlCkSJFADh06JDRIlWlShUj/AK8/PLLFCtWDIi7Uj7+lPujmjdvnmhax44dGT9+PL6+vuTOnZt79+6xe/dui+CQnJaulLhx44ax305OTjRp0sSYl7B1b/PmzUZoipdwPNrOnTtb9G3s378/q1evZufOnbz11luJln/llVeMAAlx7+fixYv5888/WbBgQZqFX0j6Pff29mbEiBEsWrSI2rVrExkZyZEjR/D19bX4rMS/7ymp/dH3L158dxx49u4PAN27dze2YWNjw9tvv23MCwwMNH6UtG7d2lhu+/btxjGTsEtAck+Pr1+/3vjs+/j4GK3bzs7ORhgGEp39CAgIMN7D7NmzW4RGFxcXi9oTStjFo0OHDkaXIhsbG4u+2X/99ddTa48/OwMk2dqcEkl9phK+rwMGDLDo5uDl5UWzZs2M5zt37kyTOiCuAaBz587Gczc3Nzp16mQ8j//hJpKe1AIs8pzkzJmTmzdvAhj9Eh/n4cOH3Lt3z3ieK1cu43G7du04fPgwENcNon79+hbdHxLegODatWvG4wMHDjyxBefChQsWF7MAODo64ubmluTyJ06cYM2aNRw6dChRX2CIO52ZHtatW2eEAltbW+PCqHgmkwmz2UxYWBi///67xQgaN27cMB4XKFDA4nVubm6J9vVJywMWp/OTIzk/fB63LYj7e65cuRI/Pz8CAwOTDEfx73tKaq9cuTLFihUjKCiIs2fPcuHCBZycnDhx4gQAxYoVo2LFisnah4Tif5DFi//hBXEB7969e+TJk4f8+fPj7e3N3r17uXfvHn/99RfVqlVj9+7dQFwgTW73i4SjP5w6dcqiRTHh8bdlyxaGDBlihL/4YxTiuvc8egFY8eLFk9xewmMt/ixIUuL76T9Jvnz5OH/+PBDXPz0hGxsb/vvf/xrPz549a7R0P05Sx/Hdu3ct+v0m9XkoV64cGzduBLDoR/4kyTnuCxcunOgHY8L39dEx0kXSgwKwyHNSpkwZ48s1Yf/GpBw7dswi3CT8cmrSpAkTJkwgLCyMP//8k/v37/PHH38AiVu3En4ZOTg4PPFClvhWuIQeN5TY0qVLmTx5MmazGUdHRxo0aECVKlXInz8/n3322RP3LTXMZrNFsAkNDbVoeXvUk4aQe9aWtZS0xD0aeJN6j5OS1Pt+9OhRBg4cSHh4OCaTiSpVqlC1alUqVarEF198YRHcHvUstbdr146pU6cCca3ACS/uS0nrL8Ttt6Oj42Prie+vDnE/4Pbu3WtsPyIigoiICCCu+0LC1tHH8ff3t/hRduHChccGzwcPHrBhwwajRTLh3+xZfsQlXDZXrlwW+5RQcm5s89JLLxkB+NG76NnY2DBw4EDj+bp1654agJP6PCWnjoTvRVIXyULi9yg5n/GHDx8mmpbwmofHbUskLSkAizwn9evXN76oDh8+zPHjx3n55ZeTXNbX19d4nD9/fouuC46OjjRr1oxVq1YRERHBrFmzjFP9TZo0MS4Eg7jRIOJ5eXkxY8YMi+3ExMQ89osaSHJQ/X///Zfp06djNpuxs7NjxYoVRstx/Jd2ejl06NAz9S0+deoUgYGBxvipHh4eRktWUFCQRUvkpUuX+PXXXylRogRly5alXLlyxsU5EHeR06Nmz55N9uzZKVmyJF5eXjg6Olq0bD148MBi+fi+3E+T1Ps+efJk4+88btw4WrRoYcxL2L0mXkpqh7gLKGfOnEl0dDSbN282wpONjQ2tWrVKVv2POnPmDFWrVjWeJwynDg4O5MiRw3jeoEEDcuXKxd27d9m5c6cxbi8kv/tDUjdIeZLVq1cbATjhMRMSEkJ0dLRFWHzcKBAeHh7GZ3Py5MkW/Yqfdpw9qmXLlkZf3uPHj3Po0CGqVauW5LLJCelJfZ5cXV1xdXU1WoEDAwMTDUGW8GLQwoULG4/j+3JD4s94wjNXjxM/hF/CHzMJPxMJ/wYi6UV9gEWek9atWxsX75jNZj7++ONEtziNiopi8uTJFi0677zzTqLThQn7av7666/G44TdHwCqVatmtKYcOnTI4gvt77//pn79+nTp0oXhw4cn+iKDpFtiLl68aLTg2NraWoyjmrArRnp0gUh41X7Xrl05ePBgkv9q1qxpLLdy5UrjccIQsWLFCovWqhUrVrBkyRLGjRvHDz/8kGj5ffv2GXfegrgr9X/44QemTJnC4MGDjfckYZh79AfBtm3bkrWfjxuSLl7CLjH79u2zuMAy/n1PSe0Qd9FV/fr1gbi/dfxntGbNmhah+lksWLDACOlms9m4kBOgYsWKFuHQzs7OCNphYWHG6A9FihR57A/GhEJDQy3e58WLFyf5GVm/fr3xPv/9999GN4/y5csbwSw0NNRiNJN///2XH3/8McntJgz4S5cutfj8f/rppzRr1oy+ffta9Lt9nBo1alisb9iwYcYQdQlt376dmTNnPnV9j2tRTdidZObMmRa3FT9y5IhFP3AfHx/jccJjPuFn/Pr16xbDLT7O/fv3LT4DoaGhFsdp/HUOIulJLcAiz4mjoyPjx4+nf//+REdHc/PmTd555x2qV69OyZIlCQ8Px8/Pz6LP3yuvvJLkeLYVK1akZMmSnDt3zviiLVq0aKLh1QoUKECjRo3Yvn07UVFR9OjRAx8fH1xcXNi6dSsPHz7k3LlzlChRwuIU9ZMkvAL/wYMHdO/enVq1ahEQEGDxJZ3WF8Hdv3/fYgzchBe/Pap58+ZG14hNmzYxePBgnJyc6Nq1K+vXryc6Opr9+/fzxhtvUKNGDa5cuWKcdgfo0qULEHexWMJxY7t3706DBg1wdHS0CDKtWrUygm/C1vq9e/fy1VdfUbZsWf7444+nnqp+kjx58hgXKg4bNoxmzZpx+/Zti/Gl4f/e95TUHq9du3aJxhtOafcHAD8/P7p160b16tU5ceKEETYBi4uhEm7/f//7X4q2v2nTJuPHXKFChR7bTzt//vxUqVLF6E+/cuVKKlasiLOzM23atOGXX34B4m4oc/DgQfLmzcvevXsT9cmN98Ybb7BhwwZiYmLYsmULFy9exMvLiwsXLhifxbt37zJ06NCn7oPJZOLzzz+nW7du3Lt3j9u3b/Puu+/i5eVFmTJliIyMTLLv/bPe/fDtt99m27ZtREZGcuLECbp06ULt2rX5999/+eOPP4yuKg0bNrQIpWXKlOHAgQMATJw4kRs3bmA2m1m2bJnRXeVpvv/+ew4fPkyRIkXYt2+f8dl2cnKy+IEvkl7UAizyHFWrVo0ZM2YYw6DFxsayf/9+li5dypo1ayy+XNu3b88333zz2NabR78kHnd6eNiwYZQoUQKIC0cbN27kl19+MU7HlypVik8++STZ+1CgQAGL8BkUFMTy5cs5duwY2bJlM4L0vXv3LE5fp9bGjRuNcJc3b94njo/q4+NjnPaNvxgO4vb1s88+M1ocg4KC+Pnnny3Cb/fu3S0uFvziiy+M8WnDw8PZuHEjq1atMk4dlyhRgsGDB1tsO355iGuh//LLL9mzZ4/Fle7PKn5kCohrifzll1/YsWMHMTExFn27E16s9Ky1x6tdu7bFaWgXFxcaNmyYorrLlClD1apVOXv2LMuWLbMIv23btqVx48aJXlOyZEmLi+2epftFwj7iT/qRBJYjI2zZssV4XwYMGGAcMwC7d+9m1apVXL9+3SKIJzwzU6ZMGYYOHWrRqrx8+XIj/JpMJj7++GOLu7U9SYECBVi8eLFx4wyz2Yy/vz/Lli1j1apVFuHX1taWVq1aPfN41KVKlWLs2LFGcL527RqrVq1i27ZtRot9tWrVGDNmjMXr3nzzTWM///nnH6ZMmcLUqVP5999/k/VDpVixYhQsWJADBw7w66+/Wtwhc/jw4Sk+0yDyLBSARZ6z6tWrs2bNGoYOHYq3tzfu7u5ky5bNuKVtx44dWbx4MSNGjEiy7168Vq1aGfNtbW0f+8WTK1cufvrpJ/r160fZsmVxdnbG2dmZUqVK8d577zF//nyLU+rJMXbsWPr160exYsWwt7cnZ86c1KtXj/nz59OoUSMg7gt7+/btz7TeJ0nYr9PHx+eJF8pkz57d4pbGCYe6ateuHQsXLqRp06a4u7tja2tLjhw5qFWrFhMnTqR///4W6/L09MTX15cePXpQvHhxHBwccHBwoGTJkvTu3ZtFixaRM2dOY3knJyfmz59Py5YtyZUrF46OjlSsWJEvvvgiybCZXJ06deLrr7+mQoUKODs74+TkRMWKFRk3bpzFehOe/n/W2uPZ2try0ksvGc+bNGmS7DMEj7K3t2fGjBn06tULT09P7O3tKVGiBJ9++ukTb7CQsLtD9erVyZ8//1O3debMGYtuRU8LwE2aNDF+DEVERBg3l3F1dWXBggV07doVDw8P7O3tKVOmDF9++SVvvvmm8fpH35OOHTvyww8/0KRJE/LkyYOdnR358uXjlVdeYd68eXTs2PGp+5BQgQIFWLhwIV999RWNGzemQIEC2Nvb4+DgQP78+albty6DBw9m3bp1jB079rEjtjxJ48aNWbp0KW+99RbFixfH0dERFxcXKleuzPDhw5k5c2aii2fr1avHd999R6VKlYwRJpo1a8bixYuTNUpI7ty5WbhwIa+++io5cuTA0dGRatWqMXv2bIu+7SLpyWRO7rg8IiJiFS5dukTXrl2NvsFz58597EVY6eHu3bt06tTJ6Ns8ZsyYVHXBeFY//PADOXLkIGfOnJQpU8biYsn169cbLaL169fnu+++e251ZWXr1q3j888/B+L6S3///fcZXJFYO/UBFhERrl69yooVK4iJiWHTpk1G+C1ZsuRzCb8RERHMnj0bW1tb41a5EDc+89NactPa2rVrjREdsmfPTuPGjXFxceHatWvGRXkQ1xIqIllTpg3A169fp0uXLkycONGiP15wcDCTJ0/m8OHD2Nra0qRJEwYOHGhxiiY8PJzp06ezfft2wsPD8fLy4sMPP7T4FS8iIv/HZDJZDL8HcSMyJOeirbTg4ODAihUrLIZ0M5lMfPjhhynufpFSffv2ZdSoUZjNZu7fv28x+ki8SpUqJXtYNhHJfDJlAL527RoDBw60uEsNxF0F3rdvX9zd3RkzZgx37txh2rRphISEMH36dGO54cOHc+LECd5//31cXFyYN28effv2ZcWKFYmudhYRkbgLCwsXLsyNGzdwdHSkbNmy9OjR44l3D0xLNjY2vPzyywQEBGBnZ0fx4sXp1q2bxfBbz0vLli0pUKAAK1as4OTJk9y6dYvo6GicnZ0pXrw4Pj4+dO7cGXt7++dem4ikjUzVBzg2NpbffvuNKVOmAHFXkc+ZM8f4D3jhwoX88MMPrF+/3rhoZ8+ePQwaNIj58+dTpUoVjh07Ro8ePZg6dSp169YF4M6dO7Rt25Z33nmHd999NyN2TUREREQyiUw1CsSZM2f46quvePXVV43O8gnt27cPLy8viyvWvb29cXFxMcbX3LdvH05OTnh7exvLuLm5UbVq1VSNwSkiIiIiL4ZMFYDz58/PqlWrHtvnKygoiCJFilhMs7W1xdPT07jVZ1BQEAULFkx028nChQsneTtQEREREbEumaoPcM6cOZMckzJeaGhokne6cXZ2Nm7hmJxlnlVgYKDx2ieNyyoiIiIiGScqKgqTyfTUW2pnqgD8NAnvrf6o+DvyJGeZlIjvKh0/NJCIiIiIZE1ZKgC7uroSHh6eaHpYWJhx60RXV1f++eefJJd59G42yVW2bFmOHz+O2WymVKlSKVqHiIiIiKSvs2fPPvFOofGyVAAuWrSoxX3uAWJiYggJCTFuv1q0aFH8/PyIjY21aPENDg5O9TjAJpMJZ2fnVK1DRERERNJHcsIvZLKL4J7G29sbf39/4w5BAH5+foSHhxujPnh7exMWFsa+ffuMZe7cucPhw4ctRoYQEREREeuUpQJwx44dcXBwoH///uzYsYPVq1czcuRI6tSpQ+XKlYG4e4xXq1aNkSNHsnr1anbs2EG/fv3Inj07HTt2zOA9EBEREZGMlqW6QLi5uTFnzhwmT57MiBEjcHFxoXHjxgwePNhiuQkTJvDdd98xdepUYmNjqVy5Ml999ZXuAiciIiIimetOcJnZ8ePHAXj55ZczuBIRERERSUpy81qW6gIhIiIiIpJaCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKpky+gCRAAOHjxI3759Hzu/d+/efP/994+dX61aNebOnfvY+Vu3buWnn34iKCiI7NmzU7NmTQYMGIC7u3uq6hYREZGsRwFYMoVy5cqxcOHCRNNnz57NyZMnad68ObVr1040f/v27fj6+tKhQ4fHrvv3339n+PDhvP766/Tr149bt24xZ84c3nvvPXx9fXFwcEjTfREREZHMTQFYMgVXV1defvlli2l//PEH+/fv5+uvv6Zo0aKJXnPt2jVWr15Np06daNas2WPXvXDhQurWrcuwYcOMacWKFeOdd95h165dNGnSJO12RERERDI9BWDJlB48eMCECROoV6/eYwPqlClTcHBwoH///o9dT2xsLLVq1cLLy8tierFixQC4fPlymtUsIiIiWYMCsGRKy5Yt4+bNm8yePTvJ+cePH2fr1q2MHj0aV1fXx67HxsaGDz74INH0nTt3AlCyZMk0qVdERESyDo0CIZlOVFQUS5cupVmzZhQuXDjJZX766Sc8PT1p2bLlM6//8uXLTJkyhTJlylC3bt3UlisiIiJZTJYMwKtWraJz587Uq1ePjh07smLFCsxmszE/ODiYDz74gIYNG9K4cWO++uorQkNDM7BieRbbtm3j9u3bvPXWW0nOv379On/88QdvvPEG2bI920mMoKAg+vTpg62tLd9++y02NlnyEBAREZFUyHJdIFavXs348ePp0qULDRo04PDhw0yYMIGHDx/SrVs37t+/T9++fXF3d2fMmDHcuXOHadOmERISwvTp0zO6fEmGbdu2UaJECcqUKZPk/B07dmAymZ544VtSDh48yMcff4yTkxNz586lUKFCaVGuiIiIZDFZLgCvXbuWKlWqMHToUABq1qzJxYsXWbFiBd26deOXX37h3r17LFmyhFy5cgHg4eHBoEGDOHLkCFWqVMm44uWpoqOj2bdvH//9738fu8yuXbvw8vJ6pjF8N23axJgxYyhWrBjTpk3Dw8MjLcoVERGRLCjLnf+NjIzExcXFYlrOnDm5d+8eAPv27cPLy8sIvwDe3t64uLiwZ8+e51mqpMDZs2d58OABlStXTnK+2Wzm5MmTj52flN27dzN69GgqVarE/PnzFX5FRESsXJYLwG+88QZ+fn5s2LCB0NBQ9u3bx2+//UarVq2AuD6eRYoUsXiNra0tnp6eXLx4MSNKlmdw9uxZAEqUKJHk/GvXrhEaGkrx4sUfu47jx48bw5tFRkbyxRdf4OzsTI8ePbhw4QLHjx83/l2/fj3td0JEREQytSzXBaJ58+YcOnSIUaNGGdNq167NkCFDAAgNDU3UQgzg7OxMWFhYqrZtNpsJDw9P1Trkya5duwbE/WhJ6r2+cuUKAA4ODo/9W3Tv3p0WLVowbNgwDh06xK1btwAYMGBAomXfeecdevTokVbli4iISAYym82YTKanLmcyJxw+IQt4//33OXLkCD179uSll17i7NmzfP/991SpUoWJEydSu3Zt3n77bfr162fxunfffRdnZ+cUXwh3/PhxHj58mBa7ICIiIiLpxN7ePtHdZR+VpVqAjx49yt69exkxYgTt27cHoFq1ahQsWJDBgweze/duXF1dk2wZDAsLS3XfTzs7O0qVKpWqdYiIiIhI+ojvSvk0WSoAX716FSDRBVBVq1YF4Ny5cxQtWpTg4GCL+TExMYSEhNCoUaNUbd9kMuHs7JyqdYiIiIhI+khO9wfIYhfBFStWDIDDhw9bTD969CgAhQoVwtvbG39/f+7cuWPM9/PzIzw8HG9v7+dWq4iIiIhkTlmqBbhcuXL4+Pjw3Xff8e+//1KxYkXOnz/P999/T/ny5WnYsCHVqlVj+fLl9O/fn169enHv3j2mTZtGnTp1nmnoLBERERF5MWW5i+CioqL44Ycf2LBhAzdv3iR//vw0bNiQXr16Gd0Tzp49y+TJkzl69CguLi40aNCAwYMHJzk6RHIdP34c4KmdqkVEREQkYyQ3r2W5AJxRFIBFREREMrfk5rUs1QdYRERERCS1FICtVKwa/jM1/X1ERETST5a6CE7Sjo3JxDK/v7nxr+5sl9l45HCmq3eZjC5DRETkhaUAbMVu/BtOyJ3U3R5aREREJKtRFwgRERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFXRRXAiIi+A48ePM2PGDE6ePImzszO1a9dm0KBB5M6dG4DDhw8zc+ZMzpw5g6urK40aNeK999576h0yg4KCmDp1Kv7+/tja2lK1alUGDx5MoUKFnsduiYikC7UAi4hkcQEBAfTt2xdnZ2cmTpzIwIED8fPz46OPPgLg3Llz9O/fH3t7e7766it69erFxo0bGTFixBPXe+3aNd59913u3bvH+PHjGTZsGOfPn2fAgAE8ePDgeeyaiEi6UAuwiEgWN23aNMqWLcukSZOwsYlr13BxcWHSpElcuXKFTZs2YTKZmDhxIs7OzgDExMTw1VdfcfXqVQoUKJDker///ntcXV2ZNWsWjo6OAHh6evLhhx8SEBCAl5fX89lBEZE0pgAsIpKF3b17l0OHDjFmzBgj/AL4+Pjg4+MDQGRkJNmyZTNCLEDOnDkBuHfvXpIB2Gw2s337drp162bxugoVKrBp06b02h0RkedCXSBERLKws2fPEhsbi5ubGyNGjOCVV16hfv36jBo1ivv37wPQtm1bAL777jvu3r3LuXPnmDdvHqVKlaJ06dJJrjckJITQ0FAKFCjAN998g4+PD3Xq1OHDDz/k+vXrz23/RETSgwKwiEgWdufOHQDGjh2Lg4MDEydOZNCgQezatYvBgwdjNpspVaoUAwcOZPny5TRp0oQuXboQHh7OlClTsLW1feJ6p0+fzo0bN/jyyy8ZMWIEgYGB9O3bl4iIiOe2jyIiaU1dIEREsrCoqCgAypUrx8iRIwGoWbMm2bNnZ/jw4fz111+cPn2aGTNm0KlTJ3x8fLh79y7z58+nX79+zJs3D3d390TrjY6OBiB37txMmDDB6F5RuHBhunfvzsaNG3n99def016KiKQtBWARkSws/qK2+vXrW0yvU6cOAKdPn2b+/Pm0bNmSTz75xJhfrVo12rdvj6+vL4MHD37seuvWrWvRt/jll1/G1dWVwMDAtN4VEZHnRl0gRESysCJFigDw8OFDi+nxLbgPHz7kwYMHVK5c2WJ+7ty5KVq0KOfPn09yvYUKFcJkMiVaL8SNIOHg4JAW5YuIZAgFYBGRLKx48eJ4enqyefNmzGazMf2PP/4A4lqGc+bMyeHDhy1ed/fuXS5dukTBggWTXK+zszNeXl7s2LHDIgTv37+fiIgIDYEmIlmaukCIiGRhJpOJ999/n88++4xhw4bRvn17Lly4wKxZs/Dx8aF8+fL07t2bCRMm4OLiQpMmTbh79y4//vgjNjY2vPnmm8a6jh8/jpubm3GXtwEDBtCnTx8GDRpEt27d+Oeff5g+fToVK1bklVdeyahdFhFJNZM5YZOBPNbx48eBuP5vL4ppm48Qcicso8uQR3i6ufB+syoZXYZkMbt27WLevHmcPXuWHDly0LJlS9577z3s7e0B2LBhA4sXL+bChQvkypWLKlWqMGDAAIsW4OrVq9O6dWvGjBljTDt69CizZs3ixIkTODo60rBhQwYPHkz27Nmf9y6KiDxVcvOaAnAyKQDL86IALCIikjLJzWvqAywiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSqpGgbt8uXLXL9+nTt37pAtWzZy5cpFiRIlyJEjR1rVJyIiIiKSpp45AJ84cYJVq1bh5+fHzZs3k1ymSJEi1K9fnzZt2lCiRIlUFykiIiIiklaSHYCPHDnCtGnTOHHiBABPGj3t4sWLXLp0iSVLllClShUGDx5MhQoVUl+tiIiIiEgqJSsAjx8/nrVr1xIbGwtAsWLFePnllyldujR58+bFxcUFgH///ZebN29y5swZTp8+zfnz5zl8+DDdu3enVatWjB49Ov32RETkOYg1m7ExmTK6DEmC/jYiklzJCsCrV6/Gw8OD119/nSZNmlC0aNFkrfz27dts3bqVlStX8ttvvykAi0iWZ2Mysczvb278G57RpUgCHjmc6epdJqPLEJEsIlkB+Ntvv6VBgwbY2DzboBHu7u506dKFLl264Ofnl6ICRUQymxv/husuiiIiWViyAnCjRo1SvSFvb+9Ur0NEREREJLVSNQwaQGhoKLNnz2b37t3cvn0bDw8PWrRoQffu3bGzs0uLGkVERERE0kyqA/DYsWPZsWOH8Tw4OJj58+cTERHBoEGDUrt6EREREZE0laoAHBUVxR9//IGPjw9vvfUWuXLlIjQ0lDVr1vD7778rAIuIiIhIppOsq9rGjx/PrVu3Ek2PjIwkNjaWEiVK8NJLL1GoUCHKlSvHSy+9RGRkZJoXKyIiIiKSWskeBm3jxo107tyZd955x7jVsaurK6VLl+aHH35gyZIlZM+enfDwcMLCwmjQoEG6Fi4iIiIikhLJagH+/PPPcXd3x9fXl3bt2rFw4UIePHhgzCtWrBgRERHcuHGD0NBQKlWqxNChQ9O1cBERERGRlEhWC3CrVq1o1qwZK1euZMGCBcyaNYvly5fTs2dPXnvtNZYvX87Vq1f5559/8PDwwMPDI73rFhERERFJkWTf2SJbtmx07tyZ1atX89577/Hw4UO+/fZbOnbsyO+//46npycVK1ZU+BURERGRTO3Zbu0GODo60qNHD9asWcNbb73FzZs3GTVqFP/5z3/Ys2dPetQoIiIiIpJmkh2Ab9++zW+//Yavry+///47JpOJgQMHsnr1al577TUuXLjABx98QO/evTl27Fh61iwiIiIikmLJ6gN88OBBhgwZQkREhDHNzc2NuXPnUqxYMT777DPeeustZs+ezZYtW+jZsyf16tVj8uTJ6Va4iIiIiEhKJKsFeNq0aWTLlo26devSvHlzGjRoQLZs2Zg1a5axTKFChRg/fjyLFy+mdu3a7N69O92KFhERERFJqWS1AAcFBTFt2jSqVKliTLt//z49e/ZMtGyZMmWYOnUqR44cSasaRURERETSTLICcP78+Rk3bhx16tTB1dWViIgIjhw5QoECBR77moRhWUREREQks0hWAO7RowejR49m2bJlmEwmzGYzdnZ2Fl0gRERERESygmQF4BYtWlC8eHH++OMP42YXzZo1o1ChQuldn4iIiIhImkpWAAYoW7YsZcuWTc9aRERERETSXbJGgRgyZAj79+9P8UZOnTrFiBEjUvz6Rx0/fpw+ffpQr149mjVrxujRo/nnn3+M+cHBwXzwwQc0bNiQxo0b89VXXxEaGppm2xcRERGRrCtZLcC7du1i165dFCpUiMaNG9OwYUPKly+PjU3S+Tk6OpqjR4+yf/9+du3axdmzZwH44osvUl1wQEAAffv2pWbNmkycOJGbN28yY8YMgoODWbBgAffv36dv3764u7szZswY7ty5w7Rp0wgJCWH69Omp3r6IiIiIZG3JCsDz5s3jm2++4cyZMyxatIhFixZhZ2dH8eLFyZs3Ly4uLphMJsLDw7l27RqXLl0iMjISALPZTLly5RgyZEiaFDxt2jTKli3LpEmTjADu4uLCpEmTuHLlCps3b+bevXssWbKEXLlyAeDh4cGgQYM4cuSIRqcQERERACIjI3nllVeIiYmxmO7k5MSuXbsSLT9p0iSWLl3KwYMH03S98vwlKwBXrlyZxYsXs23bNnx9fQkICODhw4cEBgby999/WyxrNpsBMJlM1KxZkw4dOtCwYUNMJlOqi7179y6HDh1izJgxFq3PPj4++Pj4ALBv3z68vLyM8Avg7e2Ni4sLe/bsUQAWERERAM6dO0dMTAzjxo2zuLA/qTPc/v7+LFu2LM3XKxkj2RfB2djY0LRpU5o2bUpISAh79+7l6NGj3Lx50+h/mzt3bgoVKkSVKlWoUaMG+fLlS9Niz549S2xsLG5ubowYMYI///wTs9lMo0aNGDp0KNmzZycoKIimTZtavM7W1hZPT08uXryYqu2bzWbCw8NTtY7MwGQy4eTklNFlyFNEREQYPyglc9Cxk/npuJFnceLECWxtbalduzb29vYW8xJ+34eHhzNmzBjy5MnDzZs3n5oFkrteSXtmszlZja7JDsAJeXp60rFjRzp27JiSl6fYnTt3ABg7dix16tRh4sSJXLp0iZkzZ3LlyhXmz59PaGgoLi4uiV7r7OxMWFhYqrYfFRVFQEBAqtaRGTg5OVGhQoWMLkOe4sKFC0RERGR0GZKAjp3MT8eNPIv9+/eTL18+zp0798TllixZgpOTE15eXvz2229PzQLJXa+kj0d/dCQlRQE4o0RFRQFQrlw5Ro4cCUDNmjXJnj07w4cP56+//iI2Nvaxr0/tqQc7OztKlSqVqnVkBmnRHUXSX/HixdWSlcno2Mn8dNzIs7h9+zYuLi7MmzePEydOYGdnR8OGDenfvz/Ozs4AHDhwgP379/PDDz+wZcsWAMqXL5/q9Ur6iB944WmyVACO/9DUr1/fYnqdOnUAOH36NK6urkmeXggLC8PDwyNV2zeZTPrgynOjU+0iz07HjSSX2Wzm/PnzmM1mXnvtNXr37s2pU6eYN28ewcHBfP/994SHh/Ptt9/St29fypYty86dOwGemAWSs171BU4/yW2oyFIBuEiRIgA8fPjQYnp0dDQAjo6OFC1alODgYIv5MTExhISE0KhRo+dTqIiIiGRqZrOZSZMm4ebmRsmSJQGoWrUq7u7ujBw5kn379rF161by5cvHf/7znzRdb926ddNlnyT5stRPkOLFi+Pp6cnmzZstTnH98ccfAFSpUgVvb2/8/f2N/sIAfn5+hIeH4+3t/dxrFhERkczHxsaG6tWrGyE1Xr169YC4+w5s3ryZ4cOHExsbS3R0tJE9oqOjH9vl8mnrPXPmTFrviqRAlmoBNplMvP/++3z22WcMGzaM9u3bc+HCBWbNmoWPjw/lypUjX758LF++nP79+9OrVy/u3bvHtGnTqFOnDpUrV87oXRAREZFM4ObNm+zevZvatWuTP39+Y3r8fQxWrVpFZGQkXbp0SfRab29vWrduzZgxY555vQmHaZWMk6IAfOLECSpWrJjWtSRLkyZNcHBwYN68eXzwwQfkyJGDDh068N577wHg5ubGnDlzmDx5MiNGjMDFxYXGjRszePDgDKlXREREMp+YmBjGjx9P9+7d6d+/vzF98+bN2NraMmvWrESjR61atYpVq1bx008/PTbIPm29Xl5e6bI/8mxSFIC7d+9O8eLFefXVV2nVqhV58+ZN67qeqH79+okuhEuoVKlSzJo16zlWJCIiIllJ/vz5adOmDb6+vjg4OFCpUiWOHDnCwoUL6dy5M0WLFk30mvi7uCUcDjH+xmAeHh7ky5cvReuV5y/FXSCCgoKYOXMms2bNokaNGrRp04aGDRvi4OCQlvWJiIiIpIvPPvuMggULsmHDBhYsWICHhwd9+vTh7bffTvY6bt26Rffu3enVqxd9+vRJs/VK+jKZUzBg4owZM9i2bRuXL1+OW8n/H3LC2dmZpk2b8uqrr75wtxw+fvw4AC+//HIGV5J2pm0+Qsid1N0cRNKep5sL7zerktFlyBPo2Ml8dNyICCQ/r6WoBXjAgAEMGDCAwMBAtm7dyrZt2wgODiYsLIw1a9awZs0aPD09ad26Na1bt7boBC4iIiIikpFSNQxa2bJl6d+/PytXrmTJkiW0a9cOs9mM2WwmJCSE77//nvbt2zNhwoQn3qFNREREROR5SfUwaPfv32fbtm1s2bKFQ4cOYTKZjBAMcVdD/vzzz+TIkcPoGyMiIiIiklFSFIDDw8PZuXMnmzdvZv/+/cad2MxmMzY2NtSqVYu2bdtiMpmYPn06ISEhbNq0SQFYRERERDJcigJw06ZNiYqKAjBaej09PWnTpk2iPr8eHh68++673LhxIw3KFRERERFJnRQF4IcPHwJgb2+Pj48P7dq1o3r16kku6+npCUD27NlTWKKIiIiISNpJUQAuX748bdu2pUWLFri6uj5xWScnJ2bOnEnBggVTVKCIiIiISFpKUQD+6aefgLi+wFFRUdjZ2QFw8eJF8uTJg4uLi7Gsi4sLNWvWTINSRUREJKuKNZux+f/3DZDMxRr/NikeBWLNmjVMnTqViRMnUrVqVQAWL17M77//zkcffUTbtm3TrEgRERHJ2mxMJpb5/c2Nf8MzuhRJwCOHM129y2R0Gc9digLwnj17+OKLLzCZTJw9e9YIwEFBQURERPDFF1+QP39+tfyKiIiI4ca/4bqLomQKKboRxpIlSwAoUKAAJUuWNKa/+eabFC5cGLPZjK+vb9pUKCIiIiKShlLUAnzu3DlMJhOjRo2iWrVqxvSGDRuSM2dOevfuzZkzZ9KsSBERERGRtJKiFuDQ0FAA3NzcEs2LH+7s/v37qShLRERERCR9pCgA58uXD4CVK1daTDebzSxbtsxiGRERERGRzCRFXSAaNmyIr68vK1aswM/Pj9KlSxMdHc3ff//N1atXMZlMNGjQIK1rFRERERFJtRQF4B49erBz506Cg4O5dOkSly5dMuaZzWYKFy7Mu+++m2ZFioiIiIiklRR1gXB1dWXhwoW0b98eV1dXzGYzZrMZFxcX2rdvz4IFC556hzgRERERkYyQ4hth5MyZk+HDhzNs2DDu3r2L2WzGzc0Nk5XdSUREREREspYUtQAnZDKZcHNzI3fu3Eb4jY2NZe/evakuTkREREQkraWoBdhsNrNgwQL+/PNP/v33X2JjY4150dHR3L17l+joaP766680K1REREREJC2kKAAvX76cOXPmYDKZMJvNFvPip6krhIiIiIhkRinqAvHbb78B4OTkROHChTGZTLz00ksUL17cCL+ffPJJmhYqIiIiIpIWUhSAL1++jMlk4ptvvuGrr77CbDbTp08fVqxYwX/+8x/MZjNBQUFpXKqIiIiISOqlKABHRkYCUKRIEcqUKYOzszMnTpwA4LXXXgNgz549aVSiiIiIiEjaSVEAzp07NwCBgYGYTCZKly5tBN7Lly8DcOPGjTQqUUREREQk7aQoAFeuXBmz2czIkSMJDg7Gy8uLU6dO0blzZ4YNGwb8X0gWEREREclMUhSAe/bsSY4cOYiKiiJv3rw0b94ck8lEUFAQERERmEwmmjRpkta1ioiIiIikWooCcPHixfH19aVXr144OjpSqlQpRo8eTb58+ciRIwft2rWjT58+aV2riIiIiEiqpWgc4D179lCpUiV69uxpTGvVqhWtWrVKs8JERERERNJDilqAR40aRYsWLfjzzz/Tuh4RERERkXSVogD84MEDoqKiKFasWBqXIyIiIiKSvlIUgBs3bgzAjh070rQYEREREZH0lqI+wGXKlGH37t3MnDmTlStXUqJECVxdXcmW7f9WZzKZGDVqVJoVKiIiIiKSFlIUgKdOnYrJZALg6tWrXL16NcnlFIBFREREJLNJUQAGMJvNT5wfH5BFRERERDKTFAXgtWvXpnUdIiIiIiLPRYoCcIECBdK6DhERERGR5yJFAdjf3z9Zy1WtWjUlqxcRERERSTcpCsB9+vR5ah9fk8nEX3/9laKiRERERETSS7pdBCciIiIikhmlKAD36tXL4rnZbObhw4dcu3aNHTt2UK5cOXr06JEmBYqIiIiIpKUUBeDevXs/dt7WrVsZNmwY9+/fT3FRIiIiIiLpJUW3Qn4SHx8fAJYuXZrWqxYRERERSbU0D8AHDhzAbDZz7ty5tF61iIiIiEiqpagLRN++fRNNi42NJTQ0lPPnzwOQO3fu1FUmIiIiIpIOUhSADx069Nhh0OJHh2jdunXKqxIRERERSSdpOgyanZ0defPmpXnz5vTs2TNVhSXX0KFDOX36NOvWrTOmBQcHM3nyZA4fPoytrS1NmjRh4MCBuLq6PpeaRERERCTzSlEAPnDgQFrXkSIbNmxgx44dFrdmvn//Pn379sXd3Z0xY8Zw584dpk2bRkhICNOnT8/AakVEREQkM0hxC3BSoqKisLOzS8tVPtbNmzeZOHEi+fLls5j+yy+/cO/ePZYsWUKuXLkA8PDwYNCgQRw5coQqVao8l/pEREREJHNK8SgQgYGB9OvXj9OnTxvTpk2bRs+ePTlz5kyaFPck48aNo1atWtSoUcNi+r59+/Dy8jLCL4C3tzcuLi7s2bMn3esSERERkcwtRQH4/Pnz9OnTh4MHD1qE3aCgII4ePUrv3r0JCgpKqxoTWb16NadPn+aTTz5JNC8oKIgiRYpYTLO1tcXT05OLFy+mW00iIiIikjWkqAvEggULCAsLw97e3mI0iPLly+Pv709YWBg//vgjY8aMSas6DVevXuW7775j1KhRFq288UJDQ3FxcUk03dnZmbCwsFRt22w2Ex4enqp1ZAYmkwknJ6eMLkOeIiIiIsmLTSXj6NjJ/HTcZE46djK/F+XYMZvNjx2pLKEUBeAjR45gMpkYMWIELVu2NKb369ePUqVKMXz4cA4fPpySVT+R2Wxm7Nix1KlTh8aNGye5TGxs7GNfb2OTuvt+REVFERAQkKp1ZAZOTk5UqFAho8uQp7hw4QIREREZXYYkoGMn89Nxkznp2Mn8XqRjx97e/qnLpCgA//PPPwBUrFgx0byyZcsCcOvWrZSs+olWrFjBmTNnWLZsGdHR0cD/DccWHR2NjY0Nrq6uSbbShoWF4eHhkart29nZUapUqVStIzNIzi8jyXjFixd/IX6Nv0h07GR+Om4yJx07md+LcuycPXs2WculKADnzJmT27dvc+DAAQoXLmwxb+/evQBkz549Jat+om3btnH37l1atGiRaJ63tze9evWiaNGiBAcHW8yLiYkhJCSERo0apWr7JpMJZ2fnVK1DJLl0ulDk2em4EUmZF+XYSe6PrRQF4OrVq7Np0yYmTZpEQEAAZcuWJTo6mlOnTrFlyxZMJlOi0RnSwrBhwxK17s6bN4+AgAAmT55M3rx5sbGx4aeffuLOnTu4ubkB4OfnR3h4ON7e3mlek4iIiIhkLSkKwD179uTPP/8kIiKCNWvWWMwzm804OTnx7rvvpkmBCRUrVizRtJw5c2JnZ2f0LerYsSPLly+nf//+9OrVi3v37jFt2jTq1KlD5cqV07wmEREREclaUnRVWNGiRZk+fTpFihTBbDZb/CtSpAjTp09PMqw+D25ubsyZM4dcuXIxYsQIZs2aRePGjfnqq68ypB4RERERyVxSfCe4SpUq8csvvxAYGEhwcDBms5nChQtTtmzZ59rZPamh1kqVKsWsWbOeWw0iIiIiknWk6lbI4eHhlChRwhj54eLFi4SHhyc5Dq+IiIiISGaQ4oFx16xZQ+vWrTl+/LgxbfHixbRs2ZK1a9emSXEiIiIiImktRQF4z549fPHFF4SGhlqMtxYUFERERARffPEF+/fvT7MiRURERETSSooC8JIlSwAoUKAAJUuWNKa/+eabFC5cGLPZjK+vb9pUKCIiIiKShlLUB/jcuXOYTCZGjRpFtWrVjOkNGzYkZ86c9O7dmzNnzqRZkSIiIiIiaSVFLcChoaEAxo0mEoq/A9z9+/dTUZaIiIiISPpIUQDOly8fACtXrrSYbjabWbZsmcUyIiIiIiKZSYq6QDRs2BBfX19WrFiBn58fpUuXJjo6mr///purV69iMplo0KBBWtcqIiIiIpJqKQrAPXr0YOfOnQQHB3Pp0iUuXbpkzIu/IUZ63ApZRERERCS1UtQFwtXVlYULF9K+fXtcXV2N2yC7uLjQvn17FixYgKura1rXKiIiIiKSaim+E1zOnDkZPnw4w4YN4+7du5jNZtzc3J7rbZBFRERERJ5Viu8EF89kMuHm5kbu3LkxmUxERESwatUq3n777bSoT0REREQkTaW4BfhRAQEBrFy5ks2bNxMREZFWqxURERERSVOpCsDh4eFs3LiR1atXExgYaEw3m83qCiEiIiIimVKKAvDJkydZtWoVW7ZsMVp7zWYzALa2tjRo0IAOHTqkXZUiIiIiImkk2QE4LCyMjRs3smrVKuM2x/GhN57JZGL9+vXkyZMnbasUEREREUkjyQrAY8eOZevWrTx48MAi9Do7O+Pj40P+/PmZP38+gMKviIiIiGRqyQrA69atw2QyYTabyZYtG97e3rRs2ZIGDRrg4ODAvn370rtOEREREZE08UzDoJlMJjw8PKhYsSIVKlTAwcEhveoSEREREUkXyWoBrlKlCkeOHAHg6tWrzJ07l7lz51KhQgVatGihu76JiIiISJaRrAA8b948Ll26xOrVq9mwYQO3b98G4NSpU5w6dcpi2ZiYGGxtbdO+UhERERGRNJDsLhBFihTh/fff57fffmPChAnUq1fP6BeccNzfFi1aMGXKFM6dO5duRYuIiIiIpNQzjwNsa2tLw4YNadiwIbdu3WLt2rWsW7eOy5cvA3Dv3j3+97//sXTpUv766680L1hEREREJDWe6SK4R+XJk4cePXqwatUqZs+eTYsWLbCzszNahUVEREREMptU3Qo5oerVq1O9enU++eQTNmzYwNq1a9Nq1SIiIiIiaSbNAnA8V1dXOnfuTOfOndN61SIiIiIiqZaqLhAiIiIiIlmNArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq5Itowt4VrGxsaxcuZJffvmFK1eukDt3bl555RX69OmDq6srAMHBwUyePJnDhw9ja2tLkyZNGDhwoDFfRERERKxXlgvAP/30E7Nnz+att96iRo0aXLp0iTlz5nDu3DlmzpxJaGgoffv2xd3dnTFjxnDnzh2mTZtGSEgI06dPz+jyRURERCSDZakAHBsby6JFi3j99dcZMGAAALVq1SJnzpwMGzaMgIAA/vrrL+7du8eSJUvIlSsXAB4eHgwaNIgjR45QpUqVjNsBEREREclwWaoPcFhYGK1ataJ58+YW04sVKwbA5cuX2bdvH15eXkb4BfD29sbFxYU9e/Y8x2pFREREJDPKUi3A2bNnZ+jQoYmm79y5E4ASJUoQFBRE06ZNLebb2tri6enJxYsXn0eZIiIiIpKJZakAnJQTJ06waNEi6tevT6lSpQgNDcXFxSXRcs7OzoSFhaVqW2azmfDw8FStIzMwmUw4OTlldBnyFBEREZjN5owuQxLQsZP56bjJnHTsZH4vyrFjNpsxmUxPXS5LB+AjR47wwQcf4OnpyejRo4G4fsKPY2OTuh4fUVFRBAQEpGodmYGTkxMVKlTI6DLkKS5cuEBERERGlyEJ6NjJ/HTcZE46djK/F+nYsbe3f+oyWTYAb968mc8//5wiRYowffp0o8+vq6trkq20YWFheHh4pGqbdnZ2lCpVKlXryAyS88tIMl7x4sVfiF/jLxIdO5mfjpvMScdO5veiHDtnz55N1nJZMgD7+voybdo0qlWrxsSJEy3G9y1atCjBwcEWy8fExBASEkKjRo1StV2TyYSzs3Oq1iGSXDpdKPLsdNyIpMyLcuwk98dWlhoFAuDXX39l6tSpNGnShOnTpye6uYW3tzf+/v7cuXPHmObn50d4eDje3t7Pu1wRERERyWSyVAvwrVu3mDx5Mp6ennTp0oXTp09bzC9UqBAdO3Zk+fLl9O/fn169enHv3j2mTZtGnTp1qFy5cgZVLiIiIiKZRZYKwHv27CEyMpKQkBB69uyZaP7o0aNp06YNc+bMYfLkyYwYMQIXFxcaN27M4MGDn3/BIiIiIpLpZKkA3K5dO9q1a/fU5UqVKsWsWbOeQ0UiIiIiktVkuT7AIiIiIiKpoQAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVXmhA7Cfnx9vv/02devWpW3btvj6+mI2mzO6LBERERHJQC9sAD5+/DiDBw+maNGiTJgwgRYtWjBt2jQWLVqU0aWJiIiISAbKltEFpJe5c+dStmxZxo0bB0CdOnWIjo5m4cKFdO3aFUdHxwyuUEREREQywgvZAvzw4UMOHTpEo0aNLKY3btyYsLAwjhw5kjGFiYiIiEiGeyED8JUrV4iKiqJIkSIW0wsXLgzAxYsXM6IsEREREckEXsguEKGhoQC4uLhYTHd2dgYgLCzsmdYXGBjIw4cPATh27FgaVJjxTCYTNXPHEpNLXUEyG1ubWI4fP64LNjMpHTuZk46bzE/HTub0oh07UVFRmEympy73Qgbg2NjYJ863sXn2hu/4NzM5b2pW4eJgl9ElyBO8SJ+1F42OncxLx03mpmMn83pRjh2TyWS9AdjV1RWA8PBwi+nxLb/x85OrbNmyaVOYiIiIiGS4F7IPcKFChbC1tSU4ONhievzzYsWKZUBVIiIiIpIZvJAB2MHBAS8vL3bs2GHRp2X79u24urpSsWLFDKxORERERDLSCxmAAd59911OnDjBp59+yp49e5g9eza+vr50795dYwCLiIiIWDGT+UW57C8JO3bsYO7cuVy8eBEPDw86depEt27dMrosEREREclAL3QAFhERERF51AvbBUJEREREJCkKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYrJ5GApQXXVKfcX3uRcSaKQBLlhQSEkL16tVZt25dil9z//59Ro0axeHDh9OrTJF00aZNG8aMGZPkvLlz51K9enXj+ZEjRxg0aJDFMvPnz8fX1zc9SxSxKin5TpKMpQAsViswMJANGzYQGxub0aWIpJn27duzcOFC4/nq1au5cOGCxTJz5swhIiLieZcm8sLKkycPCxcupF69ehldiiRTtowuQERE0k6+fPnIly9fRpchYlXs7e15+eWXM7oMeQZqAZYM9+DBA2bMmMFrr71G7dq1adCgAf369SMwMNBYZvv27bzxxhvUrVuXN998k7///ttiHevWraN69eqEhIRYTH/cqeKDBw/St29fAPr27Uvv3r3TfsdEnpM1a9ZQo0YN5s+fb9EFYsyYMaxfv56rV68ap2fj582bN8+iq8TZs2cZPHgwDRo0oEGDBnz00UdcvnzZmH/w4EGqV6/O/v376d+/P3Xr1qV58+ZMmzaNmJiY57vDIs8gICCA9957jwYNGvDKK6/Qr18/jh8/bsw/fPgwvXv3pm7duvj4+DB69Gju3LljzF+3bh21atXixIkTdO/enTp16tC6dWuLbkRJdYG4dOkSH3/8Mc2bN6devXr06dOHI0eOJHrN4sWL6dChA3Xr1mXt2rXp+2aIQQFYMtzo0aNZu3Yt77zzDjNmzOCDDz7g/PnzjBgxArPZzJ9//sknn3xCqVKlmDhxIk2bNmXkyJGp2ma5cuX45JNPAPjkk0/49NNP02JXRJ67zZs3M378eHr27EnPnj0t5vXs2ZO6devi7u5unJ6N7x7Rrl074/HFixd59913+eeffxgzZgwjR47kypUrxrSERo4ciZeXF1OmTKF58+b89NNPrF69+rnsq8izCg0NZeDAgeTKlYtvv/2WL7/8koiICAYMGEBoaCj+/v689957ODo68vXXX/Phhx9y6NAh+vTpw4MHD4z1xMbG8umnn9KsWTOmTp1KlSpVmDp1Kvv27Utyu+fPn+ett97i6tWrDB06lC+++AKTyUTfvn05dOiQxbLz5s3jv//9L2PHjqVWrVrp+n7I/1EXCMlQUVFRhIeHM3ToUJo2bQpAtWrVCA0NZcqUKdy+fZv58+fz0ksvMW7cOABq164NwIwZM1K8XVdXV4oXLw5A8eLFKVGiRCr3ROT527VrF6NGjeKdd96hT58+ieYXKlQINzc3i9Ozbm5uAHh4eBjT5s2bh6OjI7NmzcLV1RWAGjVq0K5dO3x9fS0uomvfvr0RtGvUqMEff/zB7t276dChQ7ruq0hKXLhwgbt379K1a1cqV64MQLFixVi5ciVhYWHMmDGDokWL8t1332FrawvAyy+/TOfOnVm7di2dO3cG4kZN6dmzJ+3btwegcuXK7Nixg127dhnfSQnNmzcPOzs75syZg4uLCwD16tWjS5cuTJ06lZ9++slYtkmTJrRt2zY93wZJglqAJUPZ2dkxffp0mjZtyo0bNzh48CC//voru3fvBuICckBAAPXr17d4XXxYFrFWAQEBfPrpp3h4eBjdeVLqwIEDVK1aFUdHR6Kjo4mOjsbFxQUvLy/++usvi2Uf7efo4eGhC+ok0ypZsiRubm588MEHfPnll+zYsQN3d3fef/99cubMyYkTJ6hXrx5ms9n47BcsWJBixYol+uxXqlTJeGxvb0+uXLke+9k/dOgQ9evXN8IvQLZs2WjWrBkBAQGEh4cb08uUKZPGey3JoRZgyXD79u1j0qRJBAUF4eLiQunSpXF2dgbgxo0bmM1mcuXKZfGaPHnyZEClIpnHuXPnqFevHrt372bFihV07do1xeu6e/cuW7ZsYcuWLYnmxbcYx3N0dLR4bjKZNJKKZFrOzs7MmzePH374gS1btrBy5UocHBx49dVX6d69O7GxsSxatIhFixYleq2Dg4PF80c/+zY2No8dT/vevXu4u7snmu7u7o7ZbCYsLMyiRnn+FIAlQ12+fJmPPvqIBg0aMGXKFAoWLIjJZOLnn39m79695MyZExsbm0T9EO/du2fx3GQyAST6Ik74K1vkRVKnTh2mTJnCZ599xqxZs2jYsCH58+dP0bqyZ89OzZo16datW6J58aeFRbKqYsWKMW7cOGJiYjh58iQbNmzgl19+wcPDA5PJxH/+8x+aN2+e6HWPBt5nkTNnTm7fvp1oevy0nDlzcuvWrRSvX1JPXSAkQwUEBBAZGck777xDoUKFjCC7d+9eIO6UUaVKldi+fbvFL+0///zTYj3xp5muX79uTAsKCkoUlBPSF7tkZblz5wZgyJAh2NjY8PXXXye5nI1N4v/mH51WtWpVLly4QJkyZahQoQIVKlSgfPnyLFmyhJ07d6Z57SLPy9atW2nSpAm3bt3C1taWSpUq8emnn5I9e3Zu375NuXLlCAoKMj73FSpUoESJEsydOzfRxWrPomrVquzatcuipTcmJobff/+dChUqYG9vnxa7J6mgACwZqly5ctja2jJ9+nT8/PzYtWsXQ4cONfoAP3jwgP79+3P+/HmGDh3K3r17Wbp0KXPnzrVYT/Xq1XFwcGDKlCns2bOHzZs3M2TIEHLmzPnYbWfPnh2APXv2JBpWTSSryJMnD/3792f37t1s2rQp0fzs2bPzzz//sGfPHqPFKXv27Bw9ehR/f3/MZjO9evUiODiYDz74gJ07d7Jv3z4+/vhjNm/eTOnSpZ/3LomkmSpVqhAbG8tHH33Ezp07OXDgAOPHjyc0NJTGjRvTv39//Pz8GDFiBLt37+bPP//k/fff58CBA5QrVy7F2+3VqxeRkZH07duXrVu38scffzBw4ECuXLlC//7903APJaUUgCVDFS5cmPHjx3P9+nWGDBnCl19+CcTdztVkMnH48GG8vLyYNm0aN27cYOjQoaxcuZJRo0ZZrCd79uxMmDCBmJgYPvroI+bMmUOvXr2oUKHCY7ddokQJmjdvzooVKxgxYkS67qdIeurQoQMvvfQSkyZNSnTWo02bNhQoUIAhQ4awfv16ALp3705AQADvv/8+169fp3Tp0syfPx+TycTo0aP55JNPuHXrFhMnTsTHxycjdkkkTeTJk4fp06fj6urKuHHjGDx4MIGBgXz77bdUr14db29vpk+fzvXr1/nkk08YNWoUtra2zJo1K1U3tihZsiTz58/Hzc2NsWPHGt9Zc+fO1VBnmYTJ/Lge3CIiIiIiLyC1AIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlWyZXQBIiIvgl69enH48GEg7uYTo0ePzuCKEjt79iy//vor+/fv59atWzx8+BA3NzfKly9P27ZtadCgQUaXKCLyXOhGGCIiqXTx4kU6dOhgPHd0dGTTpk24urpmYFWWfvzxR+bMmUN0dPRjl2nZsiWff/45NjY6OSgiLzb9Lycikkpr1qyxeP7gwQM2bNiQQdUktmLFCmbMmEF0dDT58uVj2LBh/PzzzyxbtozBgwfj4uICwMaNG/nf//6XwdWKiKQ/tQCLiKRCdHQ0r776Krdv38bT05Pr168TExNDmTJlMkWYvHXrFm3atCEqKop8+fLx008/4e7ubrHMnj17GDRoEAB58+Zlw4YNmEymjChXROS5UB9gEZFU2L17N7dv3wagbdu2nDhxgt27d/P3339z4sQJKlasmOg1ISEhzJgxAz8/P6KiovDy8uLDDz/kyy+/xN/fn6pVq/L9998bywcFBTF37lwOHDhAeHg4BQoUoGXLlrz11ls4ODg8sb7169cTFRUFQM+ePROFX4C6desyePBgPD09qVChghF+161bx+effw7A5MmTWbRoEadOncLNzQ1fX1/c3d2Jiopi2bJlbNq0ieDgYABKlixJ+/btadu2rUWQ7t27N/7+/gAcPHjQmH7w4EH69u0LxPWl7tOnj8XyZcqU4ZtvvmHq1KkcOHAAk8lE7dq1GThwIJ6enk/cfxGRpCgAi4ikQsLuD82bN6dw4cLs3r0bgJUrVyYKwFevXuW///0vd+7cMabt3buXU6dOJdln+OTJk/Tr14+wsDBj2sWLF5kzZw779+9n1qxZZMv2+P/K4wMngLe392OX69at2xP2EkaPHs39+/cBcHd3x93dnfDwcHr37s3p06ctlj1+/DjHjx9nz549fPXVV9ja2j5x3U9z584dunfvzt27d41pW7Zswd/fn0WLFpE/f/5UrV9ErI/6AIuIpNDNmzfZu3cvABUqVKBw4cI0aNDA6FO7ZcsWQkNDLV4zY8YMI/y2bNmSpUuXMnv2bHLnzs3ly5ctljWbzYwdO5awsDBy5crFhAkT+PXXXxk6dCg2Njb4+/uzfPnyJ9Z4/fp143HevHkt5t26dYvr168n+vfw4cNE64mKimLy5Mn873//48MPPwRgypQpRvht1qwZixcvZsGCBdSqVQuA7du34+vr++Q3MRlu3rxJjhw5mDFjBkuXLqVly5YA3L59m+nTp6d6/SJifRSARURSaN26dcTExADQokULIG4EiEaNGgEQERHBpk2bjOVjY2ON1uF8+fIxevRoSpcuTY0aNRg/fnyi9Z85c4Zz584B0Lp1aypUqICjoyMNGzakatWqAPz2229PrDHhiA6PjgDx9ttv8+qrryb6d+zYsUTradKkCa+88gplypTBy8uLsLAwY9slS5Zk3LhxlCtXjkqVKjFx4kSjq8XTAnpyjRw5Em9vb0qXLs3o0aMpUKAAALt27TL+BiIiyaUALCKSAmazmbVr1xrPXV1d2bt3L3v37rU4Jb9q1Srj8Z07d4yuDBUqVLDoulC6dGmj5TjepUuXjMeLFy+2CKnxfWjPnTuXZIttvHz58hmPQ0JCnnU3DSVLlkxUW2RkJADVq1e36Obg5OREpUqVgLjW24RdF1LCZDJZdCXJli0bFSpUACA8PDzV6xcR66M+wCIiKXDo0CGLLgtjx45NcrnAwEBOnjzJSy+9hJ2dnTE9OQPwJKfvbExMDP/++y958uRJcn7NmjWNVufdu3dTokQJY17CodrGjBnD+vXrH7udR/snP622p+1fTEyMsY74IP2kdUVHRz/2/dOIFSLyrNQCLCKSAo+O/fsk8a3AOXLkIHv27AAEBARYdEk4ffq0xYVuAIULFzYe9+vXj4MHDxr/Fi9ezKZNmzh48OBjwy/E9c11dHQEYNGiRY9tBX5024969EK7ggULYm9vD8SN4hAbG2vMi4iI4Pjx40BcC3SuXLkAjOUf3d61a9eeuG2I+8ERLyYmhsDAQCAumMevX0QkuRSARUSe0f3799m+fTsAOXPmZN++fRbh9ODBg2zatMlo4dy8ebMR+Jo3bw7EXZz2+eefc/bsWfz8/Bg+fHii7ZQsWZIyZcoAcV0gfv/9dy5fvsyGDRv473//S4sWLRg6dOgTa82TJw8ffPABAPfu3aN79+78/PPPBAUFERQUxKZNm+jTpw87dux4pvfAxcWFxo0bA3HdMEaNGsXp06c5fvw4H3/8sTE0XOfOnY3XJLwIb+nSpcTGxhIYGMiiRYueur2vv/6aXbt2cfbsWb7++muuXLkCQMOGDXXnOhF5ZuoCISLyjDZu3Gictm/VqpXFqfl4efLkoUGDBmzfvp3w8HA2bdpEhw4d6NGjBzt27OD27dts3LiRjRs3ApA/f36cnJyIiIgwTumbTCaGDBnC+++/z7///psoJOfMmdMYM/dJOnToQFRUFFOnTuX27dt88803SS5na2tLu3btjP61TzN06FD+/vtvzp07x6ZNmywu+APw8fGxGF6tefPmrFu3DoB58+Yxf/58zGYzL7/88lP7J5vNZiPIx8ubNy8DBgxIVq0iIgnpZ7OIyDNK2P2hXbt2j12uQ4cOxuP4bhAeHh788MMPNGrUCBcXF1xcXPDx8WH+/PlGF4GEXQWqVavGjz/+SNOmTXF3d8fOzo58+fLRpk0bfvzxR0qVKpWsmrt27crPP/9M9+7dKVu2LDlz5sTOzo48efJQs2ZNBgwYwLp16xg2bBjOzs7JWmeOHDnw9fVl0KBBlC9fHmdnZxwdHalYsSIjRozgm2++segr7O3tzbhx4yhZsiT29vYUKFCAXr168d133z11W/HvmZOTE66urjRr1oyFCxc+sfuHiMjj6FbIIiLPkZ+fH/b29nh4eJA/f36jb21sbCz169cnMjKSZs2a8eWXX2ZwpRnvcXeOExFJLXWBEBF5jpYvX86uXbsAaN++Pf/97395+PAh69evN7pVJLcLgoiIpIwCsIjIc9SlSxf27NlDbGwsq1evZvXq1Rbz8+XLR9u2bTOmOBERK6E+wCIiz5G3tzezZs2ifv36uLu7Y2tri729PYUKFaJDhw78+OOP5MiRI6PLFBF5oakPsIiIiIhYFbUAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFX5f+uYZgYOAaeGAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a7528-860f-45b0-8c00-d0ddf10a0d8f",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "2924eee7-fd8b-4cee-9e43-bb0352e4c43b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    222      160     72.07\n",
      "1          M    337      258     76.56\n",
      "2          X    295      191     64.75\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "b7b2139e-f100-44e6-aff4-807f96753499",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58f49-cac6-40b1-8422-e3d95576c453",
   "metadata": {},
   "source": [
    "# RANDOM SEED 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "bf9d1b64-575e-47ca-bf37-e8916438d506",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[2]))\n",
    "np.random.seed(int(random_seeds[2]))\n",
    "tf.random.set_seed(int(random_seeds[2]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_13.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "91d5db1d-6c9f-4047-97b9-5a0eef4fbaaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "e435598c-cabc-4129-8504-68aac9cc06bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29b526-5098-4ce6-bc77-a93d1e6e1397",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "79a922e6-198c-4c43-a6f2-90dc580ae875",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "071A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "033A     9\n",
      "072A     9\n",
      "015A     9\n",
      "022A     9\n",
      "051B     9\n",
      "065A     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "108A     6\n",
      "109A     6\n",
      "037A     6\n",
      "007A     6\n",
      "008A     6\n",
      "025C     5\n",
      "070A     5\n",
      "021A     5\n",
      "034A     5\n",
      "075A     5\n",
      "023B     5\n",
      "035A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "058A     3\n",
      "014A     3\n",
      "011A     2\n",
      "061A     2\n",
      "032A     2\n",
      "093A     2\n",
      "025B     2\n",
      "087A     2\n",
      "069A     2\n",
      "073A     1\n",
      "115A     1\n",
      "088A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "019B     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "005A    10\n",
      "045A     9\n",
      "117A     7\n",
      "053A     6\n",
      "023A     6\n",
      "044A     5\n",
      "105A     4\n",
      "052A     4\n",
      "104A     4\n",
      "018A     2\n",
      "054A     2\n",
      "038A     2\n",
      "102A     2\n",
      "091A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    265\n",
      "M    256\n",
      "F    198\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    83\n",
      "M    81\n",
      "F    54\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 103A, 071A, 028A, 067...\n",
      "kitten    [014B, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [001A, 097B, 019A, 074A, 029A, 005A, 091A, 023...\n",
      "kitten                             [044A, 111A, 045A, 110A]\n",
      "senior                             [104A, 054A, 117A, 051A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "Length of X_train_val:\n",
      "719\n",
      "Length of y_train_val:\n",
      "719\n",
      "Length of groups_train_val:\n",
      "719\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 846, 2: 765, 1: 715})\n",
      "Epoch 1/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 1.0451 - accuracy: 0.5464\n",
      "Epoch 2/1500\n",
      "37/37 [==============================] - 0s 951us/step - loss: 0.8841 - accuracy: 0.6109\n",
      "Epoch 3/1500\n",
      "37/37 [==============================] - 0s 870us/step - loss: 0.7933 - accuracy: 0.6629\n",
      "Epoch 4/1500\n",
      "37/37 [==============================] - 0s 791us/step - loss: 0.7490 - accuracy: 0.6831\n",
      "Epoch 5/1500\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.7041 - accuracy: 0.7046\n",
      "Epoch 6/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.6784 - accuracy: 0.7046\n",
      "Epoch 7/1500\n",
      "37/37 [==============================] - 0s 851us/step - loss: 0.6607 - accuracy: 0.7201\n",
      "Epoch 8/1500\n",
      "37/37 [==============================] - 0s 928us/step - loss: 0.6389 - accuracy: 0.7373\n",
      "Epoch 9/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.6385 - accuracy: 0.7425\n",
      "Epoch 10/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.6146 - accuracy: 0.7489\n",
      "Epoch 11/1500\n",
      "37/37 [==============================] - 0s 788us/step - loss: 0.6116 - accuracy: 0.7468\n",
      "Epoch 12/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.5996 - accuracy: 0.7502\n",
      "Epoch 13/1500\n",
      "37/37 [==============================] - 0s 802us/step - loss: 0.5680 - accuracy: 0.7717\n",
      "Epoch 14/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.5803 - accuracy: 0.7674\n",
      "Epoch 15/1500\n",
      "37/37 [==============================] - 0s 792us/step - loss: 0.5561 - accuracy: 0.7721\n",
      "Epoch 16/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.5659 - accuracy: 0.7653\n",
      "Epoch 17/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.5579 - accuracy: 0.7683\n",
      "Epoch 18/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.5375 - accuracy: 0.7739\n",
      "Epoch 19/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.5358 - accuracy: 0.7829\n",
      "Epoch 20/1500\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.5219 - accuracy: 0.7842\n",
      "Epoch 21/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.4986 - accuracy: 0.7885\n",
      "Epoch 22/1500\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.5054 - accuracy: 0.7923\n",
      "Epoch 23/1500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.4910 - accuracy: 0.7962\n",
      "Epoch 24/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.4939 - accuracy: 0.7966\n",
      "Epoch 25/1500\n",
      "37/37 [==============================] - 0s 863us/step - loss: 0.4953 - accuracy: 0.7954\n",
      "Epoch 26/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.4739 - accuracy: 0.8061\n",
      "Epoch 27/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.4754 - accuracy: 0.8078\n",
      "Epoch 28/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.4709 - accuracy: 0.8113\n",
      "Epoch 29/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.4758 - accuracy: 0.8027\n",
      "Epoch 30/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.4582 - accuracy: 0.8087\n",
      "Epoch 31/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.4475 - accuracy: 0.8194\n",
      "Epoch 32/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.4490 - accuracy: 0.8186\n",
      "Epoch 33/1500\n",
      "37/37 [==============================] - 0s 769us/step - loss: 0.4601 - accuracy: 0.8087\n",
      "Epoch 34/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.4567 - accuracy: 0.8078\n",
      "Epoch 35/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.4559 - accuracy: 0.8147\n",
      "Epoch 36/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.4246 - accuracy: 0.8276\n",
      "Epoch 37/1500\n",
      "37/37 [==============================] - 0s 778us/step - loss: 0.4377 - accuracy: 0.8156\n",
      "Epoch 38/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.4217 - accuracy: 0.8409\n",
      "Epoch 39/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.4321 - accuracy: 0.8285\n",
      "Epoch 40/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.4097 - accuracy: 0.8285\n",
      "Epoch 41/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.4237 - accuracy: 0.8224\n",
      "Epoch 42/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.4131 - accuracy: 0.8345\n",
      "Epoch 43/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.4149 - accuracy: 0.8336\n",
      "Epoch 44/1500\n",
      "37/37 [==============================] - 0s 791us/step - loss: 0.4171 - accuracy: 0.8315\n",
      "Epoch 45/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.4087 - accuracy: 0.8267\n",
      "Epoch 46/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.4088 - accuracy: 0.8435\n",
      "Epoch 47/1500\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.3972 - accuracy: 0.8401\n",
      "Epoch 48/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.4081 - accuracy: 0.8439\n",
      "Epoch 49/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.3992 - accuracy: 0.8323\n",
      "Epoch 50/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.3836 - accuracy: 0.8512\n",
      "Epoch 51/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.3863 - accuracy: 0.8448\n",
      "Epoch 52/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.3946 - accuracy: 0.8396\n",
      "Epoch 53/1500\n",
      "37/37 [==============================] - 0s 769us/step - loss: 0.3883 - accuracy: 0.8465\n",
      "Epoch 54/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.3862 - accuracy: 0.8512\n",
      "Epoch 55/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.3900 - accuracy: 0.8396\n",
      "Epoch 56/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.3894 - accuracy: 0.8315\n",
      "Epoch 57/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.3946 - accuracy: 0.8452\n",
      "Epoch 58/1500\n",
      "37/37 [==============================] - 0s 788us/step - loss: 0.3706 - accuracy: 0.8517\n",
      "Epoch 59/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.3811 - accuracy: 0.8478\n",
      "Epoch 60/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.3872 - accuracy: 0.8405\n",
      "Epoch 61/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.3637 - accuracy: 0.8581\n",
      "Epoch 62/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.3637 - accuracy: 0.8534\n",
      "Epoch 63/1500\n",
      "37/37 [==============================] - 0s 769us/step - loss: 0.3745 - accuracy: 0.8547\n",
      "Epoch 64/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.3680 - accuracy: 0.8568\n",
      "Epoch 65/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.3636 - accuracy: 0.8568\n",
      "Epoch 66/1500\n",
      "37/37 [==============================] - 0s 880us/step - loss: 0.3637 - accuracy: 0.8555\n",
      "Epoch 67/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.3508 - accuracy: 0.8568\n",
      "Epoch 68/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.3748 - accuracy: 0.8512\n",
      "Epoch 69/1500\n",
      "37/37 [==============================] - 0s 802us/step - loss: 0.3515 - accuracy: 0.8564\n",
      "Epoch 70/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.3520 - accuracy: 0.8547\n",
      "Epoch 71/1500\n",
      "37/37 [==============================] - 0s 938us/step - loss: 0.3418 - accuracy: 0.8611\n",
      "Epoch 72/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.3573 - accuracy: 0.8564\n",
      "Epoch 73/1500\n",
      "37/37 [==============================] - 0s 887us/step - loss: 0.3309 - accuracy: 0.8667\n",
      "Epoch 74/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.3580 - accuracy: 0.8534\n",
      "Epoch 75/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.3435 - accuracy: 0.8702\n",
      "Epoch 76/1500\n",
      "37/37 [==============================] - 0s 922us/step - loss: 0.3462 - accuracy: 0.8659\n",
      "Epoch 77/1500\n",
      "37/37 [==============================] - 0s 979us/step - loss: 0.3211 - accuracy: 0.8749\n",
      "Epoch 78/1500\n",
      "37/37 [==============================] - 0s 906us/step - loss: 0.3433 - accuracy: 0.8672\n",
      "Epoch 79/1500\n",
      "37/37 [==============================] - 0s 899us/step - loss: 0.3489 - accuracy: 0.8564\n",
      "Epoch 80/1500\n",
      "37/37 [==============================] - 0s 938us/step - loss: 0.3452 - accuracy: 0.8672\n",
      "Epoch 81/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.3445 - accuracy: 0.8620\n",
      "Epoch 82/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.3289 - accuracy: 0.8697\n",
      "Epoch 83/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.3272 - accuracy: 0.8702\n",
      "Epoch 84/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.3258 - accuracy: 0.8706\n",
      "Epoch 85/1500\n",
      "37/37 [==============================] - 0s 792us/step - loss: 0.3372 - accuracy: 0.8650\n",
      "Epoch 86/1500\n",
      "37/37 [==============================] - 0s 890us/step - loss: 0.3389 - accuracy: 0.8624\n",
      "Epoch 87/1500\n",
      "37/37 [==============================] - 0s 920us/step - loss: 0.3435 - accuracy: 0.8633\n",
      "Epoch 88/1500\n",
      "37/37 [==============================] - 0s 891us/step - loss: 0.3282 - accuracy: 0.8753\n",
      "Epoch 89/1500\n",
      "37/37 [==============================] - 0s 950us/step - loss: 0.3334 - accuracy: 0.8710\n",
      "Epoch 90/1500\n",
      "37/37 [==============================] - 0s 927us/step - loss: 0.3293 - accuracy: 0.8633\n",
      "Epoch 91/1500\n",
      "37/37 [==============================] - 0s 949us/step - loss: 0.3239 - accuracy: 0.8719\n",
      "Epoch 92/1500\n",
      "37/37 [==============================] - 0s 914us/step - loss: 0.3242 - accuracy: 0.8770\n",
      "Epoch 93/1500\n",
      "37/37 [==============================] - 0s 807us/step - loss: 0.3207 - accuracy: 0.8732\n",
      "Epoch 94/1500\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.3100 - accuracy: 0.8736\n",
      "Epoch 95/1500\n",
      "37/37 [==============================] - 0s 807us/step - loss: 0.2998 - accuracy: 0.8783\n",
      "Epoch 96/1500\n",
      "37/37 [==============================] - 0s 807us/step - loss: 0.3289 - accuracy: 0.8706\n",
      "Epoch 97/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.3177 - accuracy: 0.8813\n",
      "Epoch 98/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.3385 - accuracy: 0.8603\n",
      "Epoch 99/1500\n",
      "37/37 [==============================] - 0s 792us/step - loss: 0.3289 - accuracy: 0.8646\n",
      "Epoch 100/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.3095 - accuracy: 0.8753\n",
      "Epoch 101/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.3153 - accuracy: 0.8736\n",
      "Epoch 102/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.3081 - accuracy: 0.8783\n",
      "Epoch 103/1500\n",
      "37/37 [==============================] - 0s 806us/step - loss: 0.3029 - accuracy: 0.8826\n",
      "Epoch 104/1500\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.3167 - accuracy: 0.8775\n",
      "Epoch 105/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.3037 - accuracy: 0.8770\n",
      "Epoch 106/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.3011 - accuracy: 0.8788\n",
      "Epoch 107/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.2883 - accuracy: 0.8925\n",
      "Epoch 108/1500\n",
      "37/37 [==============================] - 0s 791us/step - loss: 0.3043 - accuracy: 0.8801\n",
      "Epoch 109/1500\n",
      "37/37 [==============================] - 0s 815us/step - loss: 0.2852 - accuracy: 0.8891\n",
      "Epoch 110/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.3103 - accuracy: 0.8783\n",
      "Epoch 111/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.2913 - accuracy: 0.8835\n",
      "Epoch 112/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.3013 - accuracy: 0.8779\n",
      "Epoch 113/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.2957 - accuracy: 0.8831\n",
      "Epoch 114/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.2909 - accuracy: 0.8848\n",
      "Epoch 115/1500\n",
      "37/37 [==============================] - 0s 793us/step - loss: 0.2946 - accuracy: 0.8904\n",
      "Epoch 116/1500\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.2952 - accuracy: 0.8861\n",
      "Epoch 117/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.2912 - accuracy: 0.8848\n",
      "Epoch 118/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.2896 - accuracy: 0.8899\n",
      "Epoch 119/1500\n",
      "37/37 [==============================] - 0s 792us/step - loss: 0.3006 - accuracy: 0.8796\n",
      "Epoch 120/1500\n",
      "37/37 [==============================] - 0s 812us/step - loss: 0.2879 - accuracy: 0.8848\n",
      "Epoch 121/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.3015 - accuracy: 0.8826\n",
      "Epoch 122/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.2889 - accuracy: 0.8779\n",
      "Epoch 123/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.2940 - accuracy: 0.8788\n",
      "Epoch 124/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.3011 - accuracy: 0.8822\n",
      "Epoch 125/1500\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.2769 - accuracy: 0.8912\n",
      "Epoch 126/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.2897 - accuracy: 0.8904\n",
      "Epoch 127/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.2977 - accuracy: 0.8762\n",
      "Epoch 128/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.2754 - accuracy: 0.9007\n",
      "Epoch 129/1500\n",
      "37/37 [==============================] - 0s 986us/step - loss: 0.2951 - accuracy: 0.8835\n",
      "Epoch 130/1500\n",
      "37/37 [==============================] - 0s 909us/step - loss: 0.2774 - accuracy: 0.8899\n",
      "Epoch 131/1500\n",
      "37/37 [==============================] - 0s 921us/step - loss: 0.2782 - accuracy: 0.8869\n",
      "Epoch 132/1500\n",
      "37/37 [==============================] - 0s 916us/step - loss: 0.2797 - accuracy: 0.8942\n",
      "Epoch 133/1500\n",
      "37/37 [==============================] - 0s 908us/step - loss: 0.2970 - accuracy: 0.8792\n",
      "Epoch 134/1500\n",
      "37/37 [==============================] - 0s 925us/step - loss: 0.2743 - accuracy: 0.8938\n",
      "Epoch 135/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.2797 - accuracy: 0.8951\n",
      "Epoch 136/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.2696 - accuracy: 0.8972\n",
      "Epoch 137/1500\n",
      "37/37 [==============================] - 0s 873us/step - loss: 0.2837 - accuracy: 0.8899\n",
      "Epoch 138/1500\n",
      "37/37 [==============================] - 0s 890us/step - loss: 0.2815 - accuracy: 0.8908\n",
      "Epoch 139/1500\n",
      "37/37 [==============================] - 0s 944us/step - loss: 0.2668 - accuracy: 0.8904\n",
      "Epoch 140/1500\n",
      "37/37 [==============================] - 0s 915us/step - loss: 0.2729 - accuracy: 0.8861\n",
      "Epoch 141/1500\n",
      "37/37 [==============================] - 0s 935us/step - loss: 0.2669 - accuracy: 0.9015\n",
      "Epoch 142/1500\n",
      "37/37 [==============================] - 0s 916us/step - loss: 0.2596 - accuracy: 0.9024\n",
      "Epoch 143/1500\n",
      "37/37 [==============================] - 0s 912us/step - loss: 0.2754 - accuracy: 0.8960\n",
      "Epoch 144/1500\n",
      "37/37 [==============================] - 0s 833us/step - loss: 0.2660 - accuracy: 0.8921\n",
      "Epoch 145/1500\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.2824 - accuracy: 0.8848\n",
      "Epoch 146/1500\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2781 - accuracy: 0.8908\n",
      "Epoch 147/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.8934\n",
      "Epoch 148/1500\n",
      "37/37 [==============================] - 0s 850us/step - loss: 0.2667 - accuracy: 0.8921\n",
      "Epoch 149/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.2636 - accuracy: 0.8981\n",
      "Epoch 150/1500\n",
      "37/37 [==============================] - 0s 825us/step - loss: 0.2704 - accuracy: 0.8925\n",
      "Epoch 151/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.2519 - accuracy: 0.9067\n",
      "Epoch 152/1500\n",
      "37/37 [==============================] - 0s 745us/step - loss: 0.2679 - accuracy: 0.8899\n",
      "Epoch 153/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.2551 - accuracy: 0.9024\n",
      "Epoch 154/1500\n",
      "37/37 [==============================] - 0s 819us/step - loss: 0.2539 - accuracy: 0.9011\n",
      "Epoch 155/1500\n",
      "37/37 [==============================] - 0s 868us/step - loss: 0.2682 - accuracy: 0.8947\n",
      "Epoch 156/1500\n",
      "37/37 [==============================] - 0s 928us/step - loss: 0.2506 - accuracy: 0.9050\n",
      "Epoch 157/1500\n",
      "37/37 [==============================] - 0s 931us/step - loss: 0.2544 - accuracy: 0.8985\n",
      "Epoch 158/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.8998\n",
      "Epoch 159/1500\n",
      "37/37 [==============================] - 0s 939us/step - loss: 0.2557 - accuracy: 0.8964\n",
      "Epoch 160/1500\n",
      "37/37 [==============================] - 0s 983us/step - loss: 0.2499 - accuracy: 0.9024\n",
      "Epoch 161/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2549 - accuracy: 0.9028\n",
      "Epoch 162/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.8985\n",
      "Epoch 163/1500\n",
      "37/37 [==============================] - 0s 985us/step - loss: 0.2589 - accuracy: 0.8912\n",
      "Epoch 164/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.9007\n",
      "Epoch 165/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2552 - accuracy: 0.8994\n",
      "Epoch 166/1500\n",
      "37/37 [==============================] - 0s 990us/step - loss: 0.2453 - accuracy: 0.9020\n",
      "Epoch 167/1500\n",
      "37/37 [==============================] - 0s 930us/step - loss: 0.2496 - accuracy: 0.9080\n",
      "Epoch 168/1500\n",
      "37/37 [==============================] - 0s 858us/step - loss: 0.2455 - accuracy: 0.9058\n",
      "Epoch 169/1500\n",
      "37/37 [==============================] - 0s 856us/step - loss: 0.2500 - accuracy: 0.8968\n",
      "Epoch 170/1500\n",
      "37/37 [==============================] - 0s 912us/step - loss: 0.2488 - accuracy: 0.8998\n",
      "Epoch 171/1500\n",
      "37/37 [==============================] - 0s 997us/step - loss: 0.2422 - accuracy: 0.9046\n",
      "Epoch 172/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2648 - accuracy: 0.8947\n",
      "Epoch 173/1500\n",
      "37/37 [==============================] - 0s 968us/step - loss: 0.2328 - accuracy: 0.9132\n",
      "Epoch 174/1500\n",
      "37/37 [==============================] - 0s 976us/step - loss: 0.2533 - accuracy: 0.8960\n",
      "Epoch 175/1500\n",
      "37/37 [==============================] - 0s 975us/step - loss: 0.2462 - accuracy: 0.9046\n",
      "Epoch 176/1500\n",
      "37/37 [==============================] - 0s 952us/step - loss: 0.2403 - accuracy: 0.9011\n",
      "Epoch 177/1500\n",
      "37/37 [==============================] - 0s 916us/step - loss: 0.2422 - accuracy: 0.9024\n",
      "Epoch 178/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2331 - accuracy: 0.9067\n",
      "Epoch 179/1500\n",
      "37/37 [==============================] - 0s 910us/step - loss: 0.2419 - accuracy: 0.9028\n",
      "Epoch 180/1500\n",
      "37/37 [==============================] - 0s 911us/step - loss: 0.2347 - accuracy: 0.9097\n",
      "Epoch 181/1500\n",
      "37/37 [==============================] - 0s 935us/step - loss: 0.2520 - accuracy: 0.8977\n",
      "Epoch 182/1500\n",
      "37/37 [==============================] - 0s 912us/step - loss: 0.2494 - accuracy: 0.8981\n",
      "Epoch 183/1500\n",
      "37/37 [==============================] - 0s 920us/step - loss: 0.2449 - accuracy: 0.9003\n",
      "Epoch 184/1500\n",
      "37/37 [==============================] - 0s 857us/step - loss: 0.2399 - accuracy: 0.9067\n",
      "Epoch 185/1500\n",
      "37/37 [==============================] - 0s 929us/step - loss: 0.2502 - accuracy: 0.9106\n",
      "Epoch 186/1500\n",
      "37/37 [==============================] - 0s 907us/step - loss: 0.2354 - accuracy: 0.9097\n",
      "Epoch 187/1500\n",
      "37/37 [==============================] - 0s 897us/step - loss: 0.2221 - accuracy: 0.9166\n",
      "Epoch 188/1500\n",
      "37/37 [==============================] - 0s 904us/step - loss: 0.2274 - accuracy: 0.9140\n",
      "Epoch 189/1500\n",
      "37/37 [==============================] - 0s 896us/step - loss: 0.2321 - accuracy: 0.9119\n",
      "Epoch 190/1500\n",
      "37/37 [==============================] - 0s 878us/step - loss: 0.2362 - accuracy: 0.9093\n",
      "Epoch 191/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2392 - accuracy: 0.9119\n",
      "Epoch 192/1500\n",
      "37/37 [==============================] - 0s 983us/step - loss: 0.2367 - accuracy: 0.9058\n",
      "Epoch 193/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.2399 - accuracy: 0.9080\n",
      "Epoch 194/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.2190 - accuracy: 0.9175\n",
      "Epoch 195/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2229 - accuracy: 0.9084\n",
      "Epoch 196/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.2370 - accuracy: 0.9132\n",
      "Epoch 197/1500\n",
      "37/37 [==============================] - 0s 829us/step - loss: 0.2278 - accuracy: 0.9024\n",
      "Epoch 198/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.2450 - accuracy: 0.8985\n",
      "Epoch 199/1500\n",
      "37/37 [==============================] - 0s 808us/step - loss: 0.2269 - accuracy: 0.9149\n",
      "Epoch 200/1500\n",
      "37/37 [==============================] - 0s 817us/step - loss: 0.2336 - accuracy: 0.9110\n",
      "Epoch 201/1500\n",
      "37/37 [==============================] - 0s 902us/step - loss: 0.2410 - accuracy: 0.9011\n",
      "Epoch 202/1500\n",
      "37/37 [==============================] - 0s 907us/step - loss: 0.2289 - accuracy: 0.9170\n",
      "Epoch 203/1500\n",
      "37/37 [==============================] - 0s 849us/step - loss: 0.2205 - accuracy: 0.9140\n",
      "Epoch 204/1500\n",
      "37/37 [==============================] - 0s 797us/step - loss: 0.2280 - accuracy: 0.9067\n",
      "Epoch 205/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.2277 - accuracy: 0.9097\n",
      "Epoch 206/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.2044 - accuracy: 0.9235\n",
      "Epoch 207/1500\n",
      "37/37 [==============================] - 0s 788us/step - loss: 0.2306 - accuracy: 0.9136\n",
      "Epoch 208/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.2172 - accuracy: 0.9192\n",
      "Epoch 209/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.2327 - accuracy: 0.9110\n",
      "Epoch 210/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.2186 - accuracy: 0.9132\n",
      "Epoch 211/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.2302 - accuracy: 0.9123\n",
      "Epoch 212/1500\n",
      "37/37 [==============================] - 0s 890us/step - loss: 0.2388 - accuracy: 0.9046\n",
      "Epoch 213/1500\n",
      "37/37 [==============================] - 0s 881us/step - loss: 0.2211 - accuracy: 0.9119\n",
      "Epoch 214/1500\n",
      "37/37 [==============================] - 0s 838us/step - loss: 0.2231 - accuracy: 0.9123\n",
      "Epoch 215/1500\n",
      "37/37 [==============================] - 0s 794us/step - loss: 0.2110 - accuracy: 0.9157\n",
      "Epoch 216/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.2145 - accuracy: 0.9149\n",
      "Epoch 217/1500\n",
      "37/37 [==============================] - 0s 848us/step - loss: 0.2242 - accuracy: 0.9101\n",
      "Epoch 218/1500\n",
      "37/37 [==============================] - 0s 927us/step - loss: 0.2258 - accuracy: 0.9106\n",
      "Epoch 219/1500\n",
      "37/37 [==============================] - 0s 952us/step - loss: 0.2395 - accuracy: 0.9020\n",
      "Epoch 220/1500\n",
      "37/37 [==============================] - 0s 965us/step - loss: 0.2233 - accuracy: 0.9089\n",
      "Epoch 221/1500\n",
      "37/37 [==============================] - 0s 944us/step - loss: 0.2120 - accuracy: 0.9205\n",
      "Epoch 222/1500\n",
      "37/37 [==============================] - 0s 946us/step - loss: 0.2217 - accuracy: 0.9153\n",
      "Epoch 223/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.2084 - accuracy: 0.9149\n",
      "Epoch 224/1500\n",
      "37/37 [==============================] - 0s 818us/step - loss: 0.2315 - accuracy: 0.9054\n",
      "Epoch 225/1500\n",
      "37/37 [==============================] - 0s 923us/step - loss: 0.2171 - accuracy: 0.9140\n",
      "Epoch 226/1500\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.2090 - accuracy: 0.9166\n",
      "Epoch 227/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2150 - accuracy: 0.9149\n",
      "Epoch 228/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.2272 - accuracy: 0.9084\n",
      "Epoch 229/1500\n",
      "37/37 [==============================] - 0s 927us/step - loss: 0.2198 - accuracy: 0.9106\n",
      "Epoch 230/1500\n",
      "37/37 [==============================] - 0s 910us/step - loss: 0.2210 - accuracy: 0.9136\n",
      "Epoch 231/1500\n",
      "37/37 [==============================] - 0s 992us/step - loss: 0.2190 - accuracy: 0.9218\n",
      "Epoch 232/1500\n",
      "37/37 [==============================] - 0s 962us/step - loss: 0.2199 - accuracy: 0.9149\n",
      "Epoch 233/1500\n",
      "37/37 [==============================] - 0s 817us/step - loss: 0.2119 - accuracy: 0.9218\n",
      "Epoch 234/1500\n",
      "37/37 [==============================] - 0s 791us/step - loss: 0.2201 - accuracy: 0.9144\n",
      "Epoch 235/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.2020 - accuracy: 0.9200\n",
      "Epoch 236/1500\n",
      "37/37 [==============================] - 0s 824us/step - loss: 0.2266 - accuracy: 0.9084\n",
      "Epoch 237/1500\n",
      "37/37 [==============================] - 0s 885us/step - loss: 0.2038 - accuracy: 0.9256\n",
      "Epoch 238/1500\n",
      "37/37 [==============================] - 0s 937us/step - loss: 0.2165 - accuracy: 0.9110\n",
      "Epoch 239/1500\n",
      "37/37 [==============================] - 0s 890us/step - loss: 0.2064 - accuracy: 0.9205\n",
      "Epoch 240/1500\n",
      "37/37 [==============================] - 0s 866us/step - loss: 0.2021 - accuracy: 0.9261\n",
      "Epoch 241/1500\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.1938 - accuracy: 0.9286\n",
      "Epoch 242/1500\n",
      "37/37 [==============================] - 0s 863us/step - loss: 0.2131 - accuracy: 0.9196\n",
      "Epoch 243/1500\n",
      "37/37 [==============================] - 0s 802us/step - loss: 0.2187 - accuracy: 0.9157\n",
      "Epoch 244/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.2133 - accuracy: 0.9157\n",
      "Epoch 245/1500\n",
      "37/37 [==============================] - 0s 821us/step - loss: 0.2011 - accuracy: 0.9200\n",
      "Epoch 246/1500\n",
      "37/37 [==============================] - 0s 802us/step - loss: 0.2001 - accuracy: 0.9209\n",
      "Epoch 247/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.2163 - accuracy: 0.9132\n",
      "Epoch 248/1500\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.1993 - accuracy: 0.9192\n",
      "Epoch 249/1500\n",
      "37/37 [==============================] - 0s 802us/step - loss: 0.2069 - accuracy: 0.9140\n",
      "Epoch 250/1500\n",
      "37/37 [==============================] - 0s 777us/step - loss: 0.2192 - accuracy: 0.9144\n",
      "Epoch 251/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.2023 - accuracy: 0.9243\n",
      "Epoch 252/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.1972 - accuracy: 0.9235\n",
      "Epoch 253/1500\n",
      "37/37 [==============================] - 0s 758us/step - loss: 0.1956 - accuracy: 0.9235\n",
      "Epoch 254/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.1974 - accuracy: 0.9192\n",
      "Epoch 255/1500\n",
      "37/37 [==============================] - 0s 817us/step - loss: 0.1936 - accuracy: 0.9269\n",
      "Epoch 256/1500\n",
      "37/37 [==============================] - 0s 842us/step - loss: 0.2076 - accuracy: 0.9222\n",
      "Epoch 257/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.2010 - accuracy: 0.9235\n",
      "Epoch 258/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.1963 - accuracy: 0.9222\n",
      "Epoch 259/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.1946 - accuracy: 0.9269\n",
      "Epoch 260/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.1967 - accuracy: 0.9239\n",
      "Epoch 261/1500\n",
      "37/37 [==============================] - 0s 810us/step - loss: 0.1924 - accuracy: 0.9192\n",
      "Epoch 262/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.1948 - accuracy: 0.9183\n",
      "Epoch 263/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.2012 - accuracy: 0.9235\n",
      "Epoch 264/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.1896 - accuracy: 0.9295\n",
      "Epoch 265/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.1998 - accuracy: 0.9226\n",
      "Epoch 266/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.1930 - accuracy: 0.9273\n",
      "Epoch 267/1500\n",
      "37/37 [==============================] - 0s 2ms/step - loss: 0.1815 - accuracy: 0.9278\n",
      "Epoch 268/1500\n",
      "37/37 [==============================] - 0s 854us/step - loss: 0.1933 - accuracy: 0.9256\n",
      "Epoch 269/1500\n",
      "37/37 [==============================] - 0s 834us/step - loss: 0.2175 - accuracy: 0.9153\n",
      "Epoch 270/1500\n",
      "37/37 [==============================] - 0s 888us/step - loss: 0.2019 - accuracy: 0.9235\n",
      "Epoch 271/1500\n",
      "37/37 [==============================] - 0s 841us/step - loss: 0.1873 - accuracy: 0.9312\n",
      "Epoch 272/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.1936 - accuracy: 0.9235\n",
      "Epoch 273/1500\n",
      "37/37 [==============================] - 0s 859us/step - loss: 0.1895 - accuracy: 0.9278\n",
      "Epoch 274/1500\n",
      "37/37 [==============================] - 0s 861us/step - loss: 0.1843 - accuracy: 0.9273\n",
      "Epoch 275/1500\n",
      "37/37 [==============================] - 0s 931us/step - loss: 0.1930 - accuracy: 0.9269\n",
      "Epoch 276/1500\n",
      "37/37 [==============================] - 0s 999us/step - loss: 0.1713 - accuracy: 0.9372\n",
      "Epoch 277/1500\n",
      "37/37 [==============================] - 0s 962us/step - loss: 0.1946 - accuracy: 0.9325\n",
      "Epoch 278/1500\n",
      "37/37 [==============================] - 0s 925us/step - loss: 0.1928 - accuracy: 0.9243\n",
      "Epoch 279/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.1944 - accuracy: 0.9278\n",
      "Epoch 280/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.1890 - accuracy: 0.9261\n",
      "Epoch 281/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.1935 - accuracy: 0.9282\n",
      "Epoch 282/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.1895 - accuracy: 0.9278\n",
      "Epoch 283/1500\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.1830 - accuracy: 0.9295\n",
      "Epoch 284/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.1900 - accuracy: 0.9269\n",
      "Epoch 285/1500\n",
      "37/37 [==============================] - 0s 817us/step - loss: 0.1755 - accuracy: 0.9325\n",
      "Epoch 286/1500\n",
      "37/37 [==============================] - 0s 822us/step - loss: 0.1742 - accuracy: 0.9248\n",
      "Epoch 287/1500\n",
      "37/37 [==============================] - 0s 913us/step - loss: 0.1911 - accuracy: 0.9256\n",
      "Epoch 288/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.1908 - accuracy: 0.9239\n",
      "Epoch 289/1500\n",
      "37/37 [==============================] - 0s 802us/step - loss: 0.1840 - accuracy: 0.9265\n",
      "Epoch 290/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.1787 - accuracy: 0.9347\n",
      "Epoch 291/1500\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.1779 - accuracy: 0.9312\n",
      "Epoch 292/1500\n",
      "37/37 [==============================] - 0s 852us/step - loss: 0.1872 - accuracy: 0.9269\n",
      "Epoch 293/1500\n",
      "37/37 [==============================] - 0s 802us/step - loss: 0.1823 - accuracy: 0.9273\n",
      "Epoch 294/1500\n",
      "37/37 [==============================] - 0s 921us/step - loss: 0.1860 - accuracy: 0.9278\n",
      "Epoch 295/1500\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1967 - accuracy: 0.9243\n",
      "Epoch 296/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1941 - accuracy: 0.9235\n",
      "Epoch 297/1500\n",
      "37/37 [==============================] - 0s 1ms/step - loss: 0.1964 - accuracy: 0.9304\n",
      "Epoch 298/1500\n",
      "37/37 [==============================] - 0s 813us/step - loss: 0.1746 - accuracy: 0.9329\n",
      "Epoch 299/1500\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.1760 - accuracy: 0.9355\n",
      "Epoch 300/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.1824 - accuracy: 0.9261\n",
      "Epoch 301/1500\n",
      "37/37 [==============================] - 0s 814us/step - loss: 0.1982 - accuracy: 0.9226\n",
      "Epoch 302/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.1969 - accuracy: 0.9248\n",
      "Epoch 303/1500\n",
      "37/37 [==============================] - 0s 813us/step - loss: 0.1664 - accuracy: 0.9329\n",
      "Epoch 304/1500\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.1831 - accuracy: 0.9200\n",
      "Epoch 305/1500\n",
      "37/37 [==============================] - 0s 786us/step - loss: 0.1801 - accuracy: 0.9256\n",
      "Epoch 306/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.1745 - accuracy: 0.9299\n",
      "Epoch 307/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.1715 - accuracy: 0.9334\n",
      "Epoch 308/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.1801 - accuracy: 0.9265\n",
      "Epoch 309/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.1855 - accuracy: 0.9261\n",
      "Epoch 310/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.1708 - accuracy: 0.9342\n",
      "Epoch 311/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.1752 - accuracy: 0.9334\n",
      "Epoch 312/1500\n",
      "37/37 [==============================] - 0s 789us/step - loss: 0.1742 - accuracy: 0.9321\n",
      "Epoch 313/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.1772 - accuracy: 0.9312\n",
      "Epoch 314/1500\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.1649 - accuracy: 0.9368\n",
      "Epoch 315/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.1709 - accuracy: 0.9338\n",
      "Epoch 316/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.1664 - accuracy: 0.9355\n",
      "Epoch 317/1500\n",
      "37/37 [==============================] - 0s 764us/step - loss: 0.2067 - accuracy: 0.9218\n",
      "Epoch 318/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.1900 - accuracy: 0.9226\n",
      "Epoch 319/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.1781 - accuracy: 0.9299\n",
      "Epoch 320/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.1764 - accuracy: 0.9265\n",
      "Epoch 321/1500\n",
      "37/37 [==============================] - 0s 802us/step - loss: 0.1709 - accuracy: 0.9347\n",
      "Epoch 322/1500\n",
      "37/37 [==============================] - 0s 866us/step - loss: 0.1750 - accuracy: 0.9338\n",
      "Epoch 323/1500\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.1684 - accuracy: 0.9355\n",
      "Epoch 324/1500\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.1851 - accuracy: 0.9239\n",
      "Epoch 325/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.1722 - accuracy: 0.9312\n",
      "Epoch 326/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.1807 - accuracy: 0.9334\n",
      "Epoch 327/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.1798 - accuracy: 0.9291\n",
      "Epoch 328/1500\n",
      "37/37 [==============================] - 0s 757us/step - loss: 0.1831 - accuracy: 0.9325\n",
      "Epoch 329/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.1785 - accuracy: 0.9295\n",
      "Epoch 330/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.1733 - accuracy: 0.9312\n",
      "Epoch 331/1500\n",
      "37/37 [==============================] - 0s 816us/step - loss: 0.1762 - accuracy: 0.9316\n",
      "Epoch 332/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.1668 - accuracy: 0.9359\n",
      "Epoch 333/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.1659 - accuracy: 0.9359\n",
      "Epoch 334/1500\n",
      "37/37 [==============================] - 0s 804us/step - loss: 0.1610 - accuracy: 0.9377\n",
      "Epoch 335/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.1809 - accuracy: 0.9243\n",
      "Epoch 336/1500\n",
      "37/37 [==============================] - 0s 780us/step - loss: 0.1717 - accuracy: 0.9334\n",
      "Epoch 337/1500\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.1782 - accuracy: 0.9359\n",
      "Epoch 338/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.1741 - accuracy: 0.9329\n",
      "Epoch 339/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.1678 - accuracy: 0.9342\n",
      "Epoch 340/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.1685 - accuracy: 0.9286\n",
      "Epoch 341/1500\n",
      "37/37 [==============================] - 0s 765us/step - loss: 0.1709 - accuracy: 0.9312\n",
      "Epoch 342/1500\n",
      "37/37 [==============================] - 0s 760us/step - loss: 0.1687 - accuracy: 0.9316\n",
      "Epoch 343/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.1787 - accuracy: 0.9329\n",
      "Epoch 344/1500\n",
      "37/37 [==============================] - 0s 799us/step - loss: 0.1630 - accuracy: 0.9312\n",
      "Epoch 345/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.1747 - accuracy: 0.9316\n",
      "Epoch 346/1500\n",
      "37/37 [==============================] - 0s 775us/step - loss: 0.1631 - accuracy: 0.9415\n",
      "Epoch 347/1500\n",
      "37/37 [==============================] - 0s 778us/step - loss: 0.1830 - accuracy: 0.9278\n",
      "Epoch 348/1500\n",
      "37/37 [==============================] - 0s 761us/step - loss: 0.1661 - accuracy: 0.9359\n",
      "Epoch 349/1500\n",
      "37/37 [==============================] - 0s 823us/step - loss: 0.1651 - accuracy: 0.9364\n",
      "Epoch 350/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.1656 - accuracy: 0.9364\n",
      "Epoch 351/1500\n",
      "37/37 [==============================] - 0s 766us/step - loss: 0.1638 - accuracy: 0.9398\n",
      "Epoch 352/1500\n",
      "37/37 [==============================] - 0s 746us/step - loss: 0.1664 - accuracy: 0.9433\n",
      "Epoch 353/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.1466 - accuracy: 0.9510\n",
      "Epoch 354/1500\n",
      "37/37 [==============================] - 0s 5ms/step - loss: 0.1762 - accuracy: 0.9304\n",
      "Epoch 355/1500\n",
      "37/37 [==============================] - 0s 908us/step - loss: 0.1492 - accuracy: 0.9424\n",
      "Epoch 356/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.1747 - accuracy: 0.9295\n",
      "Epoch 357/1500\n",
      "37/37 [==============================] - 0s 806us/step - loss: 0.1832 - accuracy: 0.9269\n",
      "Epoch 358/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.1646 - accuracy: 0.9394\n",
      "Epoch 359/1500\n",
      "37/37 [==============================] - 0s 803us/step - loss: 0.1672 - accuracy: 0.9372\n",
      "Epoch 360/1500\n",
      "37/37 [==============================] - 0s 745us/step - loss: 0.1766 - accuracy: 0.9334\n",
      "Epoch 361/1500\n",
      "37/37 [==============================] - 0s 813us/step - loss: 0.1655 - accuracy: 0.9338\n",
      "Epoch 362/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.1586 - accuracy: 0.9402\n",
      "Epoch 363/1500\n",
      "37/37 [==============================] - 0s 754us/step - loss: 0.1754 - accuracy: 0.9321\n",
      "Epoch 364/1500\n",
      "37/37 [==============================] - 0s 800us/step - loss: 0.1533 - accuracy: 0.9407\n",
      "Epoch 365/1500\n",
      "37/37 [==============================] - 0s 773us/step - loss: 0.1663 - accuracy: 0.9334\n",
      "Epoch 366/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.1479 - accuracy: 0.9428\n",
      "Epoch 367/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.1694 - accuracy: 0.9394\n",
      "Epoch 368/1500\n",
      "37/37 [==============================] - 0s 759us/step - loss: 0.1580 - accuracy: 0.9407\n",
      "Epoch 369/1500\n",
      "37/37 [==============================] - 0s 811us/step - loss: 0.1481 - accuracy: 0.9428\n",
      "Epoch 370/1500\n",
      "37/37 [==============================] - 0s 753us/step - loss: 0.1677 - accuracy: 0.9347\n",
      "Epoch 371/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.1617 - accuracy: 0.9394\n",
      "Epoch 372/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.1690 - accuracy: 0.9342\n",
      "Epoch 373/1500\n",
      "37/37 [==============================] - 0s 790us/step - loss: 0.1585 - accuracy: 0.9390\n",
      "Epoch 374/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.1657 - accuracy: 0.9364\n",
      "Epoch 375/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.1590 - accuracy: 0.9351\n",
      "Epoch 376/1500\n",
      "37/37 [==============================] - 0s 788us/step - loss: 0.1585 - accuracy: 0.9385\n",
      "Epoch 377/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.1493 - accuracy: 0.9493\n",
      "Epoch 378/1500\n",
      "37/37 [==============================] - 0s 770us/step - loss: 0.1668 - accuracy: 0.9402\n",
      "Epoch 379/1500\n",
      "37/37 [==============================] - 0s 776us/step - loss: 0.1589 - accuracy: 0.9407\n",
      "Epoch 380/1500\n",
      "37/37 [==============================] - 0s 750us/step - loss: 0.1598 - accuracy: 0.9420\n",
      "Epoch 381/1500\n",
      "37/37 [==============================] - 0s 787us/step - loss: 0.1530 - accuracy: 0.9394\n",
      "Epoch 382/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.1591 - accuracy: 0.9355\n",
      "Epoch 383/1500\n",
      "37/37 [==============================] - 0s 788us/step - loss: 0.1425 - accuracy: 0.9480\n",
      "Epoch 384/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.1667 - accuracy: 0.9381\n",
      "Epoch 385/1500\n",
      "37/37 [==============================] - 0s 809us/step - loss: 0.1612 - accuracy: 0.9407\n",
      "Epoch 386/1500\n",
      "37/37 [==============================] - 0s 771us/step - loss: 0.1613 - accuracy: 0.9445\n",
      "Epoch 387/1500\n",
      "37/37 [==============================] - 0s 798us/step - loss: 0.1746 - accuracy: 0.9372\n",
      "Epoch 388/1500\n",
      "37/37 [==============================] - 0s 767us/step - loss: 0.1651 - accuracy: 0.9381\n",
      "Epoch 389/1500\n",
      "37/37 [==============================] - 0s 750us/step - loss: 0.1651 - accuracy: 0.9381\n",
      "Epoch 390/1500\n",
      "37/37 [==============================] - 0s 828us/step - loss: 0.1328 - accuracy: 0.9497\n",
      "Epoch 391/1500\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.1630 - accuracy: 0.9377\n",
      "Epoch 392/1500\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.1478 - accuracy: 0.9428\n",
      "Epoch 393/1500\n",
      "37/37 [==============================] - 0s 796us/step - loss: 0.1587 - accuracy: 0.9394\n",
      "Epoch 394/1500\n",
      "37/37 [==============================] - 0s 795us/step - loss: 0.1564 - accuracy: 0.9441\n",
      "Epoch 395/1500\n",
      "37/37 [==============================] - 0s 774us/step - loss: 0.1369 - accuracy: 0.9510\n",
      "Epoch 396/1500\n",
      "37/37 [==============================] - 0s 741us/step - loss: 0.1555 - accuracy: 0.9398\n",
      "Epoch 397/1500\n",
      "37/37 [==============================] - 0s 779us/step - loss: 0.1588 - accuracy: 0.9398\n",
      "Epoch 398/1500\n",
      "37/37 [==============================] - 0s 801us/step - loss: 0.1633 - accuracy: 0.9364\n",
      "Epoch 399/1500\n",
      "37/37 [==============================] - 0s 752us/step - loss: 0.1404 - accuracy: 0.9467\n",
      "Epoch 400/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.1568 - accuracy: 0.9377\n",
      "Epoch 401/1500\n",
      "37/37 [==============================] - 0s 792us/step - loss: 0.1557 - accuracy: 0.9420\n",
      "Epoch 402/1500\n",
      "37/37 [==============================] - 0s 750us/step - loss: 0.1536 - accuracy: 0.9437\n",
      "Epoch 403/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.1496 - accuracy: 0.9368\n",
      "Epoch 404/1500\n",
      "37/37 [==============================] - 0s 777us/step - loss: 0.1339 - accuracy: 0.9475\n",
      "Epoch 405/1500\n",
      "37/37 [==============================] - 0s 781us/step - loss: 0.1528 - accuracy: 0.9424\n",
      "Epoch 406/1500\n",
      "37/37 [==============================] - 0s 783us/step - loss: 0.1492 - accuracy: 0.9402\n",
      "Epoch 407/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.1480 - accuracy: 0.9445\n",
      "Epoch 408/1500\n",
      "37/37 [==============================] - 0s 762us/step - loss: 0.1463 - accuracy: 0.9437\n",
      "Epoch 409/1500\n",
      "37/37 [==============================] - 0s 753us/step - loss: 0.1470 - accuracy: 0.9385\n",
      "Epoch 410/1500\n",
      "37/37 [==============================] - 0s 785us/step - loss: 0.1491 - accuracy: 0.9441\n",
      "Epoch 411/1500\n",
      "37/37 [==============================] - 0s 747us/step - loss: 0.1375 - accuracy: 0.9467\n",
      "Epoch 412/1500\n",
      "37/37 [==============================] - 0s 4ms/step - loss: 0.1499 - accuracy: 0.9441\n",
      "Epoch 413/1500\n",
      "37/37 [==============================] - 0s 934us/step - loss: 0.1433 - accuracy: 0.9484\n",
      "Epoch 414/1500\n",
      "37/37 [==============================] - 0s 768us/step - loss: 0.1474 - accuracy: 0.9407\n",
      "Epoch 415/1500\n",
      "37/37 [==============================] - 0s 806us/step - loss: 0.1401 - accuracy: 0.9475\n",
      "Epoch 416/1500\n",
      "37/37 [==============================] - 0s 782us/step - loss: 0.1486 - accuracy: 0.9420\n",
      "Epoch 417/1500\n",
      "37/37 [==============================] - 0s 784us/step - loss: 0.1502 - accuracy: 0.9402\n",
      "Epoch 418/1500\n",
      "37/37 [==============================] - 0s 738us/step - loss: 0.1501 - accuracy: 0.9415\n",
      "Epoch 419/1500\n",
      "37/37 [==============================] - 0s 772us/step - loss: 0.1487 - accuracy: 0.9398\n",
      "Epoch 420/1500\n",
      " 1/37 [..............................] - ETA: 0s - loss: 0.0860 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 390.\n",
      "37/37 [==============================] - 0s 845us/step - loss: 0.1384 - accuracy: 0.9493\n",
      "Epoch 420: early stopping\n",
      "7/7 [==============================] - 0s 826us/step - loss: 0.8540 - accuracy: 0.6651\n",
      "7/7 [==============================] - 0s 649us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.60 (15/25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "Final Test Results - Loss: 0.8540095686912537, Accuracy: 0.6651375889778137, Precision: 0.625990675990676, Recall: 0.6348340548340549, F1 Score: 0.6092823661451113\n",
      "Confusion Matrix:\n",
      " [[112   4  49]\n",
      " [  6  22   0]\n",
      " [ 14   0  11]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "072A     9\n",
      "033A     9\n",
      "045A     9\n",
      "022A     9\n",
      "094A     8\n",
      "013B     8\n",
      "010A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "007A     6\n",
      "037A     6\n",
      "053A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "021A     5\n",
      "035A     4\n",
      "003A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "009A     4\n",
      "105A     4\n",
      "058A     3\n",
      "064A     3\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "014A     3\n",
      "093A     2\n",
      "025B     2\n",
      "038A     2\n",
      "087A     2\n",
      "102A     2\n",
      "032A     2\n",
      "054A     2\n",
      "018A     2\n",
      "115A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "110A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "088A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "042A    14\n",
      "106A    14\n",
      "116A    12\n",
      "039A    12\n",
      "063A    11\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "014B    10\n",
      "065A     9\n",
      "015A     9\n",
      "051B     9\n",
      "095A     8\n",
      "070A     5\n",
      "075A     5\n",
      "023B     5\n",
      "062A     4\n",
      "060A     3\n",
      "011A     2\n",
      "069A     2\n",
      "061A     2\n",
      "043A     1\n",
      "092A     1\n",
      "076A     1\n",
      "004A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    241\n",
      "F    181\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "X    75\n",
      "F    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 001A, 103A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 111A, 046A, 047A, 109A, 050A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 104A, 055A, 059A, 113A, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 071A, 062A, 002B, 095A, 065A, 039...\n",
      "kitten                             [014B, 040A, 042A, 043A]\n",
      "senior                 [106A, 116A, 051B, 016A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 12, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 4, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '018A' '019A' '020A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '004A' '011A' '014B' '015A' '016A' '019B' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'020A'}\n",
      "Moved to Test Set:\n",
      "{'020A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '018A' '019A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '011A' '014B' '015A' '016A' '019B' '020A' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "Length of X_train_val:\n",
      "711\n",
      "Length of y_train_val:\n",
      "711\n",
      "Length of groups_train_val:\n",
      "711\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     430\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     158\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     446\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     142\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 892, 1: 680, 2: 645})\n",
      "Epoch 1/1500\n",
      "35/35 [==============================] - 0s 993us/step - loss: 1.1279 - accuracy: 0.4876\n",
      "Epoch 2/1500\n",
      "35/35 [==============================] - 0s 954us/step - loss: 0.9161 - accuracy: 0.5949\n",
      "Epoch 3/1500\n",
      "35/35 [==============================] - 0s 898us/step - loss: 0.8565 - accuracy: 0.6324\n",
      "Epoch 4/1500\n",
      "35/35 [==============================] - 0s 760us/step - loss: 0.8089 - accuracy: 0.6504\n",
      "Epoch 5/1500\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.7757 - accuracy: 0.6590\n",
      "Epoch 6/1500\n",
      "35/35 [==============================] - 0s 793us/step - loss: 0.7293 - accuracy: 0.6964\n",
      "Epoch 7/1500\n",
      "35/35 [==============================] - 0s 785us/step - loss: 0.7235 - accuracy: 0.6811\n",
      "Epoch 8/1500\n",
      "35/35 [==============================] - 0s 802us/step - loss: 0.7026 - accuracy: 0.7005\n",
      "Epoch 9/1500\n",
      "35/35 [==============================] - 0s 806us/step - loss: 0.6632 - accuracy: 0.7149\n",
      "Epoch 10/1500\n",
      "35/35 [==============================] - 0s 800us/step - loss: 0.6762 - accuracy: 0.7046\n",
      "Epoch 11/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.6797 - accuracy: 0.7086\n",
      "Epoch 12/1500\n",
      "35/35 [==============================] - 0s 806us/step - loss: 0.6384 - accuracy: 0.7267\n",
      "Epoch 13/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.6070 - accuracy: 0.7442\n",
      "Epoch 14/1500\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.6040 - accuracy: 0.7470\n",
      "Epoch 15/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.5904 - accuracy: 0.7506\n",
      "Epoch 16/1500\n",
      "35/35 [==============================] - 0s 796us/step - loss: 0.5857 - accuracy: 0.7510\n",
      "Epoch 17/1500\n",
      "35/35 [==============================] - 0s 807us/step - loss: 0.5727 - accuracy: 0.7623\n",
      "Epoch 18/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.5590 - accuracy: 0.7632\n",
      "Epoch 19/1500\n",
      "35/35 [==============================] - 0s 806us/step - loss: 0.5577 - accuracy: 0.7695\n",
      "Epoch 20/1500\n",
      "35/35 [==============================] - 0s 758us/step - loss: 0.5664 - accuracy: 0.7537\n",
      "Epoch 21/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.5406 - accuracy: 0.7645\n",
      "Epoch 22/1500\n",
      "35/35 [==============================] - 0s 810us/step - loss: 0.5362 - accuracy: 0.7596\n",
      "Epoch 23/1500\n",
      "35/35 [==============================] - 0s 773us/step - loss: 0.5379 - accuracy: 0.7659\n",
      "Epoch 24/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.5190 - accuracy: 0.7767\n",
      "Epoch 25/1500\n",
      "35/35 [==============================] - 0s 803us/step - loss: 0.5293 - accuracy: 0.7686\n",
      "Epoch 26/1500\n",
      "35/35 [==============================] - 0s 813us/step - loss: 0.5043 - accuracy: 0.7808\n",
      "Epoch 27/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.5099 - accuracy: 0.7767\n",
      "Epoch 28/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.5010 - accuracy: 0.7839\n",
      "Epoch 29/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.4947 - accuracy: 0.7984\n",
      "Epoch 30/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.4847 - accuracy: 0.8011\n",
      "Epoch 31/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.4698 - accuracy: 0.7862\n",
      "Epoch 32/1500\n",
      "35/35 [==============================] - 0s 791us/step - loss: 0.4833 - accuracy: 0.7826\n",
      "Epoch 33/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.4676 - accuracy: 0.8060\n",
      "Epoch 34/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.4798 - accuracy: 0.8002\n",
      "Epoch 35/1500\n",
      "35/35 [==============================] - 0s 797us/step - loss: 0.4715 - accuracy: 0.8015\n",
      "Epoch 36/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.4590 - accuracy: 0.8092\n",
      "Epoch 37/1500\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.4611 - accuracy: 0.8069\n",
      "Epoch 38/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.4594 - accuracy: 0.8011\n",
      "Epoch 39/1500\n",
      "35/35 [==============================] - 0s 795us/step - loss: 0.4486 - accuracy: 0.8078\n",
      "Epoch 40/1500\n",
      "35/35 [==============================] - 0s 781us/step - loss: 0.4355 - accuracy: 0.8223\n",
      "Epoch 41/1500\n",
      "35/35 [==============================] - 0s 804us/step - loss: 0.4343 - accuracy: 0.8137\n",
      "Epoch 42/1500\n",
      "35/35 [==============================] - 0s 808us/step - loss: 0.4400 - accuracy: 0.8110\n",
      "Epoch 43/1500\n",
      "35/35 [==============================] - 0s 770us/step - loss: 0.4420 - accuracy: 0.8187\n",
      "Epoch 44/1500\n",
      "35/35 [==============================] - 0s 809us/step - loss: 0.4190 - accuracy: 0.8218\n",
      "Epoch 45/1500\n",
      "35/35 [==============================] - 0s 793us/step - loss: 0.4174 - accuracy: 0.8209\n",
      "Epoch 46/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.4251 - accuracy: 0.8214\n",
      "Epoch 47/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.4270 - accuracy: 0.8187\n",
      "Epoch 48/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.4202 - accuracy: 0.8259\n",
      "Epoch 49/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.3981 - accuracy: 0.8322\n",
      "Epoch 50/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.4074 - accuracy: 0.8209\n",
      "Epoch 51/1500\n",
      "35/35 [==============================] - 0s 854us/step - loss: 0.4016 - accuracy: 0.8309\n",
      "Epoch 52/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.3922 - accuracy: 0.8385\n",
      "Epoch 53/1500\n",
      "35/35 [==============================] - 0s 807us/step - loss: 0.3999 - accuracy: 0.8399\n",
      "Epoch 54/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.3795 - accuracy: 0.8403\n",
      "Epoch 55/1500\n",
      "35/35 [==============================] - 0s 833us/step - loss: 0.3872 - accuracy: 0.8367\n",
      "Epoch 56/1500\n",
      "35/35 [==============================] - 0s 835us/step - loss: 0.3911 - accuracy: 0.8385\n",
      "Epoch 57/1500\n",
      "35/35 [==============================] - 0s 940us/step - loss: 0.3840 - accuracy: 0.8430\n",
      "Epoch 58/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.3859 - accuracy: 0.8358\n",
      "Epoch 59/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.3863 - accuracy: 0.8426\n",
      "Epoch 60/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.3812 - accuracy: 0.8417\n",
      "Epoch 61/1500\n",
      "35/35 [==============================] - 0s 796us/step - loss: 0.3865 - accuracy: 0.8354\n",
      "Epoch 62/1500\n",
      "35/35 [==============================] - 0s 798us/step - loss: 0.3732 - accuracy: 0.8408\n",
      "Epoch 63/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.3702 - accuracy: 0.8412\n",
      "Epoch 64/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.3681 - accuracy: 0.8557\n",
      "Epoch 65/1500\n",
      "35/35 [==============================] - 0s 809us/step - loss: 0.3872 - accuracy: 0.8300\n",
      "Epoch 66/1500\n",
      "35/35 [==============================] - 0s 872us/step - loss: 0.3606 - accuracy: 0.8408\n",
      "Epoch 67/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.3648 - accuracy: 0.8552\n",
      "Epoch 68/1500\n",
      "35/35 [==============================] - 0s 813us/step - loss: 0.3589 - accuracy: 0.8439\n",
      "Epoch 69/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.3635 - accuracy: 0.8421\n",
      "Epoch 70/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.3559 - accuracy: 0.8448\n",
      "Epoch 71/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.3470 - accuracy: 0.8593\n",
      "Epoch 72/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.3518 - accuracy: 0.8530\n",
      "Epoch 73/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.3510 - accuracy: 0.8588\n",
      "Epoch 74/1500\n",
      "35/35 [==============================] - 0s 795us/step - loss: 0.3444 - accuracy: 0.8534\n",
      "Epoch 75/1500\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.3497 - accuracy: 0.8602\n",
      "Epoch 76/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.3576 - accuracy: 0.8530\n",
      "Epoch 77/1500\n",
      "35/35 [==============================] - 0s 781us/step - loss: 0.3601 - accuracy: 0.8484\n",
      "Epoch 78/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.3556 - accuracy: 0.8507\n",
      "Epoch 79/1500\n",
      "35/35 [==============================] - 0s 793us/step - loss: 0.3310 - accuracy: 0.8660\n",
      "Epoch 80/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.3450 - accuracy: 0.8539\n",
      "Epoch 81/1500\n",
      "35/35 [==============================] - 0s 813us/step - loss: 0.3286 - accuracy: 0.8714\n",
      "Epoch 82/1500\n",
      "35/35 [==============================] - 0s 745us/step - loss: 0.3348 - accuracy: 0.8606\n",
      "Epoch 83/1500\n",
      "35/35 [==============================] - 0s 785us/step - loss: 0.3287 - accuracy: 0.8651\n",
      "Epoch 84/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.3349 - accuracy: 0.8647\n",
      "Epoch 85/1500\n",
      "35/35 [==============================] - 0s 773us/step - loss: 0.3317 - accuracy: 0.8615\n",
      "Epoch 86/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.3154 - accuracy: 0.8701\n",
      "Epoch 87/1500\n",
      "35/35 [==============================] - 0s 778us/step - loss: 0.3310 - accuracy: 0.8656\n",
      "Epoch 88/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.3191 - accuracy: 0.8710\n",
      "Epoch 89/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.3300 - accuracy: 0.8692\n",
      "Epoch 90/1500\n",
      "35/35 [==============================] - 0s 797us/step - loss: 0.3145 - accuracy: 0.8714\n",
      "Epoch 91/1500\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.3230 - accuracy: 0.8651\n",
      "Epoch 92/1500\n",
      "35/35 [==============================] - 0s 777us/step - loss: 0.3067 - accuracy: 0.8728\n",
      "Epoch 93/1500\n",
      "35/35 [==============================] - 0s 773us/step - loss: 0.3167 - accuracy: 0.8728\n",
      "Epoch 94/1500\n",
      "35/35 [==============================] - 0s 791us/step - loss: 0.3156 - accuracy: 0.8724\n",
      "Epoch 95/1500\n",
      "35/35 [==============================] - 0s 793us/step - loss: 0.3277 - accuracy: 0.8660\n",
      "Epoch 96/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.2970 - accuracy: 0.8751\n",
      "Epoch 97/1500\n",
      "35/35 [==============================] - 0s 827us/step - loss: 0.3016 - accuracy: 0.8814\n",
      "Epoch 98/1500\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.3168 - accuracy: 0.8696\n",
      "Epoch 99/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3229 - accuracy: 0.8665\n",
      "Epoch 100/1500\n",
      "35/35 [==============================] - 0s 825us/step - loss: 0.3144 - accuracy: 0.8719\n",
      "Epoch 101/1500\n",
      "35/35 [==============================] - 0s 766us/step - loss: 0.3068 - accuracy: 0.8751\n",
      "Epoch 102/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.3089 - accuracy: 0.8746\n",
      "Epoch 103/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.3068 - accuracy: 0.8742\n",
      "Epoch 104/1500\n",
      "35/35 [==============================] - 0s 840us/step - loss: 0.2952 - accuracy: 0.8818\n",
      "Epoch 105/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.2996 - accuracy: 0.8724\n",
      "Epoch 106/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.2956 - accuracy: 0.8850\n",
      "Epoch 107/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.3041 - accuracy: 0.8746\n",
      "Epoch 108/1500\n",
      "35/35 [==============================] - 0s 773us/step - loss: 0.2997 - accuracy: 0.8823\n",
      "Epoch 109/1500\n",
      "35/35 [==============================] - 0s 797us/step - loss: 0.3127 - accuracy: 0.8642\n",
      "Epoch 110/1500\n",
      "35/35 [==============================] - 0s 804us/step - loss: 0.3011 - accuracy: 0.8796\n",
      "Epoch 111/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.2863 - accuracy: 0.8854\n",
      "Epoch 112/1500\n",
      "35/35 [==============================] - 0s 756us/step - loss: 0.2939 - accuracy: 0.8737\n",
      "Epoch 113/1500\n",
      "35/35 [==============================] - 0s 778us/step - loss: 0.3022 - accuracy: 0.8773\n",
      "Epoch 114/1500\n",
      "35/35 [==============================] - 0s 816us/step - loss: 0.2644 - accuracy: 0.8931\n",
      "Epoch 115/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.2898 - accuracy: 0.8805\n",
      "Epoch 116/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.2978 - accuracy: 0.8791\n",
      "Epoch 117/1500\n",
      "35/35 [==============================] - 0s 739us/step - loss: 0.2729 - accuracy: 0.9003\n",
      "Epoch 118/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.2926 - accuracy: 0.8805\n",
      "Epoch 119/1500\n",
      "35/35 [==============================] - 0s 793us/step - loss: 0.2714 - accuracy: 0.8854\n",
      "Epoch 120/1500\n",
      "35/35 [==============================] - 0s 803us/step - loss: 0.2835 - accuracy: 0.8796\n",
      "Epoch 121/1500\n",
      "35/35 [==============================] - 0s 778us/step - loss: 0.2848 - accuracy: 0.8809\n",
      "Epoch 122/1500\n",
      "35/35 [==============================] - 0s 803us/step - loss: 0.2844 - accuracy: 0.8778\n",
      "Epoch 123/1500\n",
      "35/35 [==============================] - 0s 831us/step - loss: 0.2644 - accuracy: 0.8931\n",
      "Epoch 124/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.2636 - accuracy: 0.8945\n",
      "Epoch 125/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.2741 - accuracy: 0.8890\n",
      "Epoch 126/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.2897 - accuracy: 0.8805\n",
      "Epoch 127/1500\n",
      "35/35 [==============================] - 0s 802us/step - loss: 0.2840 - accuracy: 0.8836\n",
      "Epoch 128/1500\n",
      "35/35 [==============================] - 0s 816us/step - loss: 0.2806 - accuracy: 0.8845\n",
      "Epoch 129/1500\n",
      "35/35 [==============================] - 0s 799us/step - loss: 0.2823 - accuracy: 0.8836\n",
      "Epoch 130/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.2699 - accuracy: 0.8958\n",
      "Epoch 131/1500\n",
      "35/35 [==============================] - 0s 819us/step - loss: 0.2718 - accuracy: 0.8872\n",
      "Epoch 132/1500\n",
      "35/35 [==============================] - 0s 809us/step - loss: 0.2752 - accuracy: 0.8859\n",
      "Epoch 133/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.2647 - accuracy: 0.9003\n",
      "Epoch 134/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.2675 - accuracy: 0.8868\n",
      "Epoch 135/1500\n",
      "35/35 [==============================] - 0s 777us/step - loss: 0.2693 - accuracy: 0.8845\n",
      "Epoch 136/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.2647 - accuracy: 0.8877\n",
      "Epoch 137/1500\n",
      "35/35 [==============================] - 0s 827us/step - loss: 0.2527 - accuracy: 0.8981\n",
      "Epoch 138/1500\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.2515 - accuracy: 0.8981\n",
      "Epoch 139/1500\n",
      "35/35 [==============================] - 0s 787us/step - loss: 0.2688 - accuracy: 0.8913\n",
      "Epoch 140/1500\n",
      "35/35 [==============================] - 0s 817us/step - loss: 0.2494 - accuracy: 0.8922\n",
      "Epoch 141/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.2551 - accuracy: 0.8999\n",
      "Epoch 142/1500\n",
      "35/35 [==============================] - 0s 799us/step - loss: 0.2446 - accuracy: 0.8990\n",
      "Epoch 143/1500\n",
      "35/35 [==============================] - 0s 789us/step - loss: 0.2387 - accuracy: 0.9057\n",
      "Epoch 144/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.2497 - accuracy: 0.8949\n",
      "Epoch 145/1500\n",
      "35/35 [==============================] - 0s 755us/step - loss: 0.2694 - accuracy: 0.8895\n",
      "Epoch 146/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.2373 - accuracy: 0.9071\n",
      "Epoch 147/1500\n",
      "35/35 [==============================] - 0s 736us/step - loss: 0.2512 - accuracy: 0.8935\n",
      "Epoch 148/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.2515 - accuracy: 0.9026\n",
      "Epoch 149/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.2308 - accuracy: 0.9093\n",
      "Epoch 150/1500\n",
      "35/35 [==============================] - 0s 801us/step - loss: 0.2528 - accuracy: 0.8949\n",
      "Epoch 151/1500\n",
      "35/35 [==============================] - 0s 806us/step - loss: 0.2409 - accuracy: 0.9066\n",
      "Epoch 152/1500\n",
      "35/35 [==============================] - 0s 799us/step - loss: 0.2432 - accuracy: 0.9075\n",
      "Epoch 153/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.2298 - accuracy: 0.9035\n",
      "Epoch 154/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.2464 - accuracy: 0.9026\n",
      "Epoch 155/1500\n",
      "35/35 [==============================] - 0s 777us/step - loss: 0.2527 - accuracy: 0.9017\n",
      "Epoch 156/1500\n",
      "35/35 [==============================] - 0s 795us/step - loss: 0.2416 - accuracy: 0.9057\n",
      "Epoch 157/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.2493 - accuracy: 0.9026\n",
      "Epoch 158/1500\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.2400 - accuracy: 0.9098\n",
      "Epoch 159/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.2382 - accuracy: 0.9008\n",
      "Epoch 160/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.2453 - accuracy: 0.9026\n",
      "Epoch 161/1500\n",
      "35/35 [==============================] - 0s 778us/step - loss: 0.2315 - accuracy: 0.9102\n",
      "Epoch 162/1500\n",
      "35/35 [==============================] - 0s 756us/step - loss: 0.2552 - accuracy: 0.8895\n",
      "Epoch 163/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.2333 - accuracy: 0.9066\n",
      "Epoch 164/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.2330 - accuracy: 0.9039\n",
      "Epoch 165/1500\n",
      "35/35 [==============================] - 0s 730us/step - loss: 0.2479 - accuracy: 0.8976\n",
      "Epoch 166/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.2422 - accuracy: 0.9098\n",
      "Epoch 167/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.2228 - accuracy: 0.9134\n",
      "Epoch 168/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.2618 - accuracy: 0.8922\n",
      "Epoch 169/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.2304 - accuracy: 0.9084\n",
      "Epoch 170/1500\n",
      "35/35 [==============================] - 0s 771us/step - loss: 0.2288 - accuracy: 0.9102\n",
      "Epoch 171/1500\n",
      "35/35 [==============================] - 0s 770us/step - loss: 0.2339 - accuracy: 0.9039\n",
      "Epoch 172/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.2323 - accuracy: 0.9048\n",
      "Epoch 173/1500\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.2460 - accuracy: 0.8999\n",
      "Epoch 174/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.2250 - accuracy: 0.9120\n",
      "Epoch 175/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.2228 - accuracy: 0.9143\n",
      "Epoch 176/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.2254 - accuracy: 0.9062\n",
      "Epoch 177/1500\n",
      "35/35 [==============================] - 0s 770us/step - loss: 0.2214 - accuracy: 0.9071\n",
      "Epoch 178/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.2347 - accuracy: 0.9044\n",
      "Epoch 179/1500\n",
      "35/35 [==============================] - 0s 777us/step - loss: 0.2206 - accuracy: 0.9120\n",
      "Epoch 180/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.2300 - accuracy: 0.9093\n",
      "Epoch 181/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.2231 - accuracy: 0.9089\n",
      "Epoch 182/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.2301 - accuracy: 0.9102\n",
      "Epoch 183/1500\n",
      "35/35 [==============================] - 0s 789us/step - loss: 0.2137 - accuracy: 0.9120\n",
      "Epoch 184/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.2285 - accuracy: 0.9089\n",
      "Epoch 185/1500\n",
      "35/35 [==============================] - 0s 800us/step - loss: 0.2165 - accuracy: 0.9143\n",
      "Epoch 186/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.2125 - accuracy: 0.9184\n",
      "Epoch 187/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.2357 - accuracy: 0.9035\n",
      "Epoch 188/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.2268 - accuracy: 0.9102\n",
      "Epoch 189/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.2185 - accuracy: 0.9089\n",
      "Epoch 190/1500\n",
      "35/35 [==============================] - 0s 817us/step - loss: 0.1969 - accuracy: 0.9242\n",
      "Epoch 191/1500\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.2236 - accuracy: 0.9102\n",
      "Epoch 192/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.9138\n",
      "Epoch 193/1500\n",
      "35/35 [==============================] - 0s 798us/step - loss: 0.2150 - accuracy: 0.9188\n",
      "Epoch 194/1500\n",
      "35/35 [==============================] - 0s 772us/step - loss: 0.2167 - accuracy: 0.9080\n",
      "Epoch 195/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.2293 - accuracy: 0.9053\n",
      "Epoch 196/1500\n",
      "35/35 [==============================] - 0s 755us/step - loss: 0.2091 - accuracy: 0.9229\n",
      "Epoch 197/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.2029 - accuracy: 0.9184\n",
      "Epoch 198/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.2186 - accuracy: 0.9138\n",
      "Epoch 199/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.2110 - accuracy: 0.9202\n",
      "Epoch 200/1500\n",
      "35/35 [==============================] - 0s 740us/step - loss: 0.2190 - accuracy: 0.9138\n",
      "Epoch 201/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.1996 - accuracy: 0.9238\n",
      "Epoch 202/1500\n",
      "35/35 [==============================] - 0s 796us/step - loss: 0.2276 - accuracy: 0.9053\n",
      "Epoch 203/1500\n",
      "35/35 [==============================] - 0s 802us/step - loss: 0.2062 - accuracy: 0.9120\n",
      "Epoch 204/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.1906 - accuracy: 0.9224\n",
      "Epoch 205/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.2171 - accuracy: 0.9147\n",
      "Epoch 206/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.2243 - accuracy: 0.9053\n",
      "Epoch 207/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.2073 - accuracy: 0.9211\n",
      "Epoch 208/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.2111 - accuracy: 0.9129\n",
      "Epoch 209/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.2002 - accuracy: 0.9215\n",
      "Epoch 210/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.2085 - accuracy: 0.9129\n",
      "Epoch 211/1500\n",
      "35/35 [==============================] - 0s 766us/step - loss: 0.2045 - accuracy: 0.9224\n",
      "Epoch 212/1500\n",
      "35/35 [==============================] - 0s 778us/step - loss: 0.2055 - accuracy: 0.9161\n",
      "Epoch 213/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.2035 - accuracy: 0.9179\n",
      "Epoch 214/1500\n",
      "35/35 [==============================] - 0s 798us/step - loss: 0.2035 - accuracy: 0.9206\n",
      "Epoch 215/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.1905 - accuracy: 0.9206\n",
      "Epoch 216/1500\n",
      "35/35 [==============================] - 0s 781us/step - loss: 0.1953 - accuracy: 0.9233\n",
      "Epoch 217/1500\n",
      "35/35 [==============================] - 0s 772us/step - loss: 0.1961 - accuracy: 0.9197\n",
      "Epoch 218/1500\n",
      "35/35 [==============================] - 0s 804us/step - loss: 0.1948 - accuracy: 0.9292\n",
      "Epoch 219/1500\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.1946 - accuracy: 0.9251\n",
      "Epoch 220/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.1981 - accuracy: 0.9211\n",
      "Epoch 221/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.1896 - accuracy: 0.9220\n",
      "Epoch 222/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.2007 - accuracy: 0.9233\n",
      "Epoch 223/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.1937 - accuracy: 0.9260\n",
      "Epoch 224/1500\n",
      "35/35 [==============================] - 0s 787us/step - loss: 0.1906 - accuracy: 0.9314\n",
      "Epoch 225/1500\n",
      "35/35 [==============================] - 0s 770us/step - loss: 0.1939 - accuracy: 0.9215\n",
      "Epoch 226/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.2190 - accuracy: 0.9129\n",
      "Epoch 227/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.2058 - accuracy: 0.9138\n",
      "Epoch 228/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.2032 - accuracy: 0.9215\n",
      "Epoch 229/1500\n",
      "35/35 [==============================] - 0s 773us/step - loss: 0.2019 - accuracy: 0.9143\n",
      "Epoch 230/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.1943 - accuracy: 0.9247\n",
      "Epoch 231/1500\n",
      "35/35 [==============================] - 0s 796us/step - loss: 0.1870 - accuracy: 0.9301\n",
      "Epoch 232/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.1825 - accuracy: 0.9350\n",
      "Epoch 233/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.1925 - accuracy: 0.9179\n",
      "Epoch 234/1500\n",
      "35/35 [==============================] - 0s 766us/step - loss: 0.1965 - accuracy: 0.9220\n",
      "Epoch 235/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.1893 - accuracy: 0.9224\n",
      "Epoch 236/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.1834 - accuracy: 0.9292\n",
      "Epoch 237/1500\n",
      "35/35 [==============================] - 0s 787us/step - loss: 0.1789 - accuracy: 0.9341\n",
      "Epoch 238/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.1823 - accuracy: 0.9305\n",
      "Epoch 239/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.1944 - accuracy: 0.9206\n",
      "Epoch 240/1500\n",
      "35/35 [==============================] - 0s 755us/step - loss: 0.1882 - accuracy: 0.9211\n",
      "Epoch 241/1500\n",
      "35/35 [==============================] - 0s 771us/step - loss: 0.1894 - accuracy: 0.9238\n",
      "Epoch 242/1500\n",
      "35/35 [==============================] - 0s 819us/step - loss: 0.1889 - accuracy: 0.9269\n",
      "Epoch 243/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.1813 - accuracy: 0.9332\n",
      "Epoch 244/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.1929 - accuracy: 0.9251\n",
      "Epoch 245/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.1841 - accuracy: 0.9265\n",
      "Epoch 246/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.1867 - accuracy: 0.9247\n",
      "Epoch 247/1500\n",
      "35/35 [==============================] - 0s 751us/step - loss: 0.2004 - accuracy: 0.9179\n",
      "Epoch 248/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9260\n",
      "Epoch 249/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.1908 - accuracy: 0.9278\n",
      "Epoch 250/1500\n",
      "35/35 [==============================] - 0s 781us/step - loss: 0.1925 - accuracy: 0.9233\n",
      "Epoch 251/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.1861 - accuracy: 0.9278\n",
      "Epoch 252/1500\n",
      "35/35 [==============================] - 0s 766us/step - loss: 0.1803 - accuracy: 0.9359\n",
      "Epoch 253/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.1933 - accuracy: 0.9193\n",
      "Epoch 254/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.1640 - accuracy: 0.9332\n",
      "Epoch 255/1500\n",
      "35/35 [==============================] - 0s 758us/step - loss: 0.1920 - accuracy: 0.9269\n",
      "Epoch 256/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.1856 - accuracy: 0.9220\n",
      "Epoch 257/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.1703 - accuracy: 0.9373\n",
      "Epoch 258/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.1690 - accuracy: 0.9350\n",
      "Epoch 259/1500\n",
      "35/35 [==============================] - 0s 734us/step - loss: 0.1688 - accuracy: 0.9341\n",
      "Epoch 260/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.1914 - accuracy: 0.9265\n",
      "Epoch 261/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.1842 - accuracy: 0.9256\n",
      "Epoch 262/1500\n",
      "35/35 [==============================] - 0s 745us/step - loss: 0.1780 - accuracy: 0.9346\n",
      "Epoch 263/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.1865 - accuracy: 0.9319\n",
      "Epoch 264/1500\n",
      "35/35 [==============================] - 0s 4ms/step - loss: 0.1815 - accuracy: 0.9247\n",
      "Epoch 265/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.9296\n",
      "Epoch 266/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.1821 - accuracy: 0.9287\n",
      "Epoch 267/1500\n",
      "35/35 [==============================] - 0s 754us/step - loss: 0.1761 - accuracy: 0.9305\n",
      "Epoch 268/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.1713 - accuracy: 0.9305\n",
      "Epoch 269/1500\n",
      "35/35 [==============================] - 0s 770us/step - loss: 0.1702 - accuracy: 0.9269\n",
      "Epoch 270/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.1600 - accuracy: 0.9405\n",
      "Epoch 271/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.1776 - accuracy: 0.9283\n",
      "Epoch 272/1500\n",
      "35/35 [==============================] - 0s 734us/step - loss: 0.1869 - accuracy: 0.9251\n",
      "Epoch 273/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.1671 - accuracy: 0.9391\n",
      "Epoch 274/1500\n",
      "35/35 [==============================] - 0s 781us/step - loss: 0.1635 - accuracy: 0.9387\n",
      "Epoch 275/1500\n",
      "35/35 [==============================] - 0s 766us/step - loss: 0.1688 - accuracy: 0.9369\n",
      "Epoch 276/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.1537 - accuracy: 0.9382\n",
      "Epoch 277/1500\n",
      "35/35 [==============================] - 0s 734us/step - loss: 0.1663 - accuracy: 0.9373\n",
      "Epoch 278/1500\n",
      "35/35 [==============================] - 0s 777us/step - loss: 0.1685 - accuracy: 0.9341\n",
      "Epoch 279/1500\n",
      "35/35 [==============================] - 0s 772us/step - loss: 0.1646 - accuracy: 0.9396\n",
      "Epoch 280/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.1768 - accuracy: 0.9305\n",
      "Epoch 281/1500\n",
      "35/35 [==============================] - 0s 758us/step - loss: 0.1610 - accuracy: 0.9400\n",
      "Epoch 282/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.1713 - accuracy: 0.9364\n",
      "Epoch 283/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.1786 - accuracy: 0.9283\n",
      "Epoch 284/1500\n",
      "35/35 [==============================] - 0s 731us/step - loss: 0.1651 - accuracy: 0.9332\n",
      "Epoch 285/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.1610 - accuracy: 0.9414\n",
      "Epoch 286/1500\n",
      "35/35 [==============================] - 0s 768us/step - loss: 0.1855 - accuracy: 0.9287\n",
      "Epoch 287/1500\n",
      "35/35 [==============================] - 0s 738us/step - loss: 0.1814 - accuracy: 0.9260\n",
      "Epoch 288/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.1660 - accuracy: 0.9391\n",
      "Epoch 289/1500\n",
      "35/35 [==============================] - 0s 738us/step - loss: 0.1709 - accuracy: 0.9323\n",
      "Epoch 290/1500\n",
      "35/35 [==============================] - 0s 729us/step - loss: 0.1543 - accuracy: 0.9432\n",
      "Epoch 291/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.1557 - accuracy: 0.9450\n",
      "Epoch 292/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.1695 - accuracy: 0.9364\n",
      "Epoch 293/1500\n",
      "35/35 [==============================] - 0s 807us/step - loss: 0.1627 - accuracy: 0.9350\n",
      "Epoch 294/1500\n",
      "35/35 [==============================] - 0s 764us/step - loss: 0.1559 - accuracy: 0.9450\n",
      "Epoch 295/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.1673 - accuracy: 0.9328\n",
      "Epoch 296/1500\n",
      "35/35 [==============================] - 0s 724us/step - loss: 0.1543 - accuracy: 0.9427\n",
      "Epoch 297/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.1627 - accuracy: 0.9405\n",
      "Epoch 298/1500\n",
      "35/35 [==============================] - 0s 744us/step - loss: 0.1601 - accuracy: 0.9400\n",
      "Epoch 299/1500\n",
      "35/35 [==============================] - 0s 727us/step - loss: 0.1529 - accuracy: 0.9445\n",
      "Epoch 300/1500\n",
      "35/35 [==============================] - 0s 765us/step - loss: 0.1473 - accuracy: 0.9418\n",
      "Epoch 301/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.1635 - accuracy: 0.9364\n",
      "Epoch 302/1500\n",
      "35/35 [==============================] - 0s 764us/step - loss: 0.1563 - accuracy: 0.9418\n",
      "Epoch 303/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.1647 - accuracy: 0.9319\n",
      "Epoch 304/1500\n",
      "35/35 [==============================] - 0s 847us/step - loss: 0.1545 - accuracy: 0.9414\n",
      "Epoch 305/1500\n",
      "35/35 [==============================] - 0s 830us/step - loss: 0.1816 - accuracy: 0.9269\n",
      "Epoch 306/1500\n",
      "35/35 [==============================] - 0s 919us/step - loss: 0.1662 - accuracy: 0.9405\n",
      "Epoch 307/1500\n",
      "35/35 [==============================] - 0s 899us/step - loss: 0.1459 - accuracy: 0.9499\n",
      "Epoch 308/1500\n",
      "35/35 [==============================] - 0s 903us/step - loss: 0.1623 - accuracy: 0.9414\n",
      "Epoch 309/1500\n",
      "35/35 [==============================] - 0s 888us/step - loss: 0.1557 - accuracy: 0.9369\n",
      "Epoch 310/1500\n",
      "35/35 [==============================] - 0s 906us/step - loss: 0.1579 - accuracy: 0.9405\n",
      "Epoch 311/1500\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.1530 - accuracy: 0.9441\n",
      "Epoch 312/1500\n",
      "35/35 [==============================] - 0s 894us/step - loss: 0.1484 - accuracy: 0.9459\n",
      "Epoch 313/1500\n",
      "35/35 [==============================] - 0s 864us/step - loss: 0.1690 - accuracy: 0.9341\n",
      "Epoch 314/1500\n",
      "35/35 [==============================] - 0s 847us/step - loss: 0.1655 - accuracy: 0.9355\n",
      "Epoch 315/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.1545 - accuracy: 0.9463\n",
      "Epoch 316/1500\n",
      "35/35 [==============================] - 0s 865us/step - loss: 0.1506 - accuracy: 0.9400\n",
      "Epoch 317/1500\n",
      "35/35 [==============================] - 0s 806us/step - loss: 0.1669 - accuracy: 0.9441\n",
      "Epoch 318/1500\n",
      "35/35 [==============================] - 0s 798us/step - loss: 0.1448 - accuracy: 0.9409\n",
      "Epoch 319/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.1703 - accuracy: 0.9305\n",
      "Epoch 320/1500\n",
      "35/35 [==============================] - 0s 800us/step - loss: 0.1423 - accuracy: 0.9508\n",
      "Epoch 321/1500\n",
      "35/35 [==============================] - 0s 815us/step - loss: 0.1507 - accuracy: 0.9405\n",
      "Epoch 322/1500\n",
      "35/35 [==============================] - 0s 772us/step - loss: 0.1513 - accuracy: 0.9414\n",
      "Epoch 323/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.1442 - accuracy: 0.9427\n",
      "Epoch 324/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.1517 - accuracy: 0.9436\n",
      "Epoch 325/1500\n",
      "35/35 [==============================] - 0s 954us/step - loss: 0.1460 - accuracy: 0.9468\n",
      "Epoch 326/1500\n",
      "35/35 [==============================] - 0s 964us/step - loss: 0.1628 - accuracy: 0.9355\n",
      "Epoch 327/1500\n",
      "35/35 [==============================] - 0s 948us/step - loss: 0.1523 - accuracy: 0.9369\n",
      "Epoch 328/1500\n",
      "35/35 [==============================] - 0s 926us/step - loss: 0.1586 - accuracy: 0.9481\n",
      "Epoch 329/1500\n",
      "35/35 [==============================] - 0s 5ms/step - loss: 0.1580 - accuracy: 0.9405\n",
      "Epoch 330/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9387\n",
      "Epoch 331/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.1354 - accuracy: 0.9517\n",
      "Epoch 332/1500\n",
      "35/35 [==============================] - 0s 900us/step - loss: 0.1382 - accuracy: 0.9513\n",
      "Epoch 333/1500\n",
      "35/35 [==============================] - 0s 914us/step - loss: 0.1555 - accuracy: 0.9427\n",
      "Epoch 334/1500\n",
      "35/35 [==============================] - 0s 850us/step - loss: 0.1541 - accuracy: 0.9441\n",
      "Epoch 335/1500\n",
      "35/35 [==============================] - 0s 871us/step - loss: 0.1549 - accuracy: 0.9423\n",
      "Epoch 336/1500\n",
      "35/35 [==============================] - 0s 864us/step - loss: 0.1464 - accuracy: 0.9436\n",
      "Epoch 337/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.1289 - accuracy: 0.9526\n",
      "Epoch 338/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.1487 - accuracy: 0.9436\n",
      "Epoch 339/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.1365 - accuracy: 0.9535\n",
      "Epoch 340/1500\n",
      "35/35 [==============================] - 0s 772us/step - loss: 0.1458 - accuracy: 0.9450\n",
      "Epoch 341/1500\n",
      "35/35 [==============================] - 0s 798us/step - loss: 0.1534 - accuracy: 0.9378\n",
      "Epoch 342/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.1618 - accuracy: 0.9414\n",
      "Epoch 343/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.1308 - accuracy: 0.9477\n",
      "Epoch 344/1500\n",
      "35/35 [==============================] - 0s 761us/step - loss: 0.1406 - accuracy: 0.9504\n",
      "Epoch 345/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.1532 - accuracy: 0.9396\n",
      "Epoch 346/1500\n",
      "35/35 [==============================] - 0s 742us/step - loss: 0.1359 - accuracy: 0.9522\n",
      "Epoch 347/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.1473 - accuracy: 0.9463\n",
      "Epoch 348/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.1455 - accuracy: 0.9445\n",
      "Epoch 349/1500\n",
      "35/35 [==============================] - 0s 765us/step - loss: 0.1544 - accuracy: 0.9391\n",
      "Epoch 350/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.1332 - accuracy: 0.9459\n",
      "Epoch 351/1500\n",
      "35/35 [==============================] - 0s 772us/step - loss: 0.1489 - accuracy: 0.9423\n",
      "Epoch 352/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.1495 - accuracy: 0.9432\n",
      "Epoch 353/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.1436 - accuracy: 0.9463\n",
      "Epoch 354/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.1454 - accuracy: 0.9477\n",
      "Epoch 355/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.1451 - accuracy: 0.9450\n",
      "Epoch 356/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.1464 - accuracy: 0.9414\n",
      "Epoch 357/1500\n",
      "35/35 [==============================] - 0s 785us/step - loss: 0.1438 - accuracy: 0.9454\n",
      "Epoch 358/1500\n",
      "35/35 [==============================] - 0s 881us/step - loss: 0.1487 - accuracy: 0.9454\n",
      "Epoch 359/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.1323 - accuracy: 0.9508\n",
      "Epoch 360/1500\n",
      "35/35 [==============================] - 0s 974us/step - loss: 0.1376 - accuracy: 0.9450\n",
      "Epoch 361/1500\n",
      "35/35 [==============================] - 0s 834us/step - loss: 0.1440 - accuracy: 0.9409\n",
      "Epoch 362/1500\n",
      "35/35 [==============================] - 0s 764us/step - loss: 0.1424 - accuracy: 0.9490\n",
      "Epoch 363/1500\n",
      "35/35 [==============================] - 0s 963us/step - loss: 0.1433 - accuracy: 0.9472\n",
      "Epoch 364/1500\n",
      "35/35 [==============================] - 0s 951us/step - loss: 0.1434 - accuracy: 0.9459\n",
      "Epoch 365/1500\n",
      "35/35 [==============================] - 0s 868us/step - loss: 0.1490 - accuracy: 0.9477\n",
      "Epoch 366/1500\n",
      "35/35 [==============================] - 0s 867us/step - loss: 0.1474 - accuracy: 0.9441\n",
      "Epoch 367/1500\n",
      " 1/35 [..............................] - ETA: 0s - loss: 0.1671 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 337.\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9499\n",
      "Epoch 367: early stopping\n",
      "8/8 [==============================] - 0s 815us/step - loss: 0.8361 - accuracy: 0.7301\n",
      "8/8 [==============================] - 0s 640us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.71 (20/28)\n",
      "Before appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "After appending - Cat IDs: 444, Predictions: 444, Actuals: 444, Gender: 444\n",
      "Final Test Results - Loss: 0.8360629677772522, Accuracy: 0.730088472366333, Precision: 0.7217402610020032, Recall: 0.6979112771869311, F1 Score: 0.7085143062773552\n",
      "Confusion Matrix:\n",
      " [[113   2  27]\n",
      " [  3  29   3]\n",
      " [ 26   0  23]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "042A    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "040A    10\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "015A     9\n",
      "045A     9\n",
      "095A     8\n",
      "117A     7\n",
      "099A     7\n",
      "031A     7\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "012A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "060A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "038A     2\n",
      "093A     2\n",
      "018A     2\n",
      "054A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "091A     1\n",
      "019B     1\n",
      "092A     1\n",
      "004A     1\n",
      "049A     1\n",
      "076A     1\n",
      "043A     1\n",
      "026C     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "020A    23\n",
      "067A    19\n",
      "097A    16\n",
      "059A    14\n",
      "022A     9\n",
      "072A     9\n",
      "010A     8\n",
      "094A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "037A     6\n",
      "109A     6\n",
      "008A     6\n",
      "009A     4\n",
      "003A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "025B     2\n",
      "087A     2\n",
      "032A     2\n",
      "066A     1\n",
      "048A     1\n",
      "041A     1\n",
      "115A     1\n",
      "096A     1\n",
      "100A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    243\n",
      "X    229\n",
      "F    226\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    119\n",
      "M     94\n",
      "F     26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 043A, 049...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [067A, 020A, 022A, 072A, 009A, 027A, 013B, 014...\n",
      "kitten                 [046A, 109A, 050A, 041A, 048A, 115A]\n",
      "senior                       [097A, 059A, 058A, 094A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '044A' '045A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '046A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'044A'}\n",
      "Moved to Test Set:\n",
      "{'044A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '045A' '046A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '044A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "Length of X_train_val:\n",
      "756\n",
      "Length of y_train_val:\n",
      "756\n",
      "Length of groups_train_val:\n",
      "756\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     470\n",
      "senior    136\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     118\n",
      "kitten     79\n",
      "senior     42\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     470\n",
      "kitten    150\n",
      "senior    136\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     118\n",
      "senior     42\n",
      "kitten     21\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 940, 1: 750, 2: 680})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 1s 1ms/step - loss: 1.0787 - accuracy: 0.4852\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.8662 - accuracy: 0.6097\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 980us/step - loss: 0.7845 - accuracy: 0.6540\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 979us/step - loss: 0.7710 - accuracy: 0.6561\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.7357 - accuracy: 0.6789\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 995us/step - loss: 0.7123 - accuracy: 0.6895\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 919us/step - loss: 0.7021 - accuracy: 0.6899\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.6608 - accuracy: 0.7068\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.6856 - accuracy: 0.7008\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.6756 - accuracy: 0.7101\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 912us/step - loss: 0.6527 - accuracy: 0.7063\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.6339 - accuracy: 0.7287\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 997us/step - loss: 0.6219 - accuracy: 0.7367\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 894us/step - loss: 0.6231 - accuracy: 0.7295\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 833us/step - loss: 0.5923 - accuracy: 0.7418\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.6115 - accuracy: 0.7325\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.5833 - accuracy: 0.7371\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.5968 - accuracy: 0.7418\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.5850 - accuracy: 0.7498\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 990us/step - loss: 0.5705 - accuracy: 0.7511\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.5689 - accuracy: 0.7527\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 942us/step - loss: 0.5624 - accuracy: 0.7599\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 910us/step - loss: 0.5538 - accuracy: 0.7637\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.5547 - accuracy: 0.7578\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.5504 - accuracy: 0.7561\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.5316 - accuracy: 0.7700\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 959us/step - loss: 0.5415 - accuracy: 0.7570\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.5244 - accuracy: 0.7603\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 920us/step - loss: 0.5123 - accuracy: 0.7717\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.5314 - accuracy: 0.7654\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 985us/step - loss: 0.5226 - accuracy: 0.7570\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.5119 - accuracy: 0.7755\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.5006 - accuracy: 0.7679\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.5210 - accuracy: 0.7709\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.4965 - accuracy: 0.7764\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.4956 - accuracy: 0.7954\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.5019 - accuracy: 0.7932\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.4810 - accuracy: 0.7924\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.4950 - accuracy: 0.7835\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.4849 - accuracy: 0.7857\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.4936 - accuracy: 0.7878\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.4929 - accuracy: 0.7920\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.5003 - accuracy: 0.7785\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.4821 - accuracy: 0.7844\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.4752 - accuracy: 0.7861\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.4547 - accuracy: 0.7975\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.4911 - accuracy: 0.7861\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.4626 - accuracy: 0.7916\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.4702 - accuracy: 0.7861\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.4709 - accuracy: 0.8008\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.4551 - accuracy: 0.7996\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.4611 - accuracy: 0.7954\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.4484 - accuracy: 0.7992\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.4557 - accuracy: 0.7992\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.4454 - accuracy: 0.7992\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.4391 - accuracy: 0.8072\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.4479 - accuracy: 0.7979\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.4327 - accuracy: 0.8068\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.4356 - accuracy: 0.8101\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.4397 - accuracy: 0.8084\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.4377 - accuracy: 0.7979\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 940us/step - loss: 0.4357 - accuracy: 0.8089\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4308 - accuracy: 0.8122\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.4335 - accuracy: 0.8038\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.4202 - accuracy: 0.8139\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.4298 - accuracy: 0.8131\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.4223 - accuracy: 0.8122\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.4261 - accuracy: 0.8165\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 957us/step - loss: 0.4278 - accuracy: 0.8148\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 967us/step - loss: 0.4273 - accuracy: 0.8173\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.4191 - accuracy: 0.8198\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.4219 - accuracy: 0.8173\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.4146 - accuracy: 0.8173\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.4166 - accuracy: 0.8207\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.3991 - accuracy: 0.8329\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.4153 - accuracy: 0.8203\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.4137 - accuracy: 0.8156\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.4240 - accuracy: 0.8110\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.4261 - accuracy: 0.8148\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.4067 - accuracy: 0.8181\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.4091 - accuracy: 0.8245\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.4055 - accuracy: 0.8194\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8300\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.4128 - accuracy: 0.8232\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.4024 - accuracy: 0.8295\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.3913 - accuracy: 0.8190\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.3958 - accuracy: 0.8316\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.3921 - accuracy: 0.8325\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.3863 - accuracy: 0.8312\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.3980 - accuracy: 0.8304\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.3894 - accuracy: 0.8262\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.3942 - accuracy: 0.8308\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 866us/step - loss: 0.3861 - accuracy: 0.8295\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.3879 - accuracy: 0.8308\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3947 - accuracy: 0.8262\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 900us/step - loss: 0.3677 - accuracy: 0.8401\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.3848 - accuracy: 0.8287\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.3763 - accuracy: 0.8350\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.3693 - accuracy: 0.8405\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.3818 - accuracy: 0.8346\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.3830 - accuracy: 0.8270\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 874us/step - loss: 0.3802 - accuracy: 0.8384\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 911us/step - loss: 0.3743 - accuracy: 0.8414\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.3714 - accuracy: 0.8388\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.3651 - accuracy: 0.8464\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.3857 - accuracy: 0.8325\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.3657 - accuracy: 0.8456\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.3576 - accuracy: 0.8464\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.3633 - accuracy: 0.8430\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.3591 - accuracy: 0.8464\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.3748 - accuracy: 0.8443\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.3694 - accuracy: 0.8380\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.3720 - accuracy: 0.8414\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.3708 - accuracy: 0.8397\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.3507 - accuracy: 0.8502\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.3549 - accuracy: 0.8481\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.3625 - accuracy: 0.8443\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.3603 - accuracy: 0.8430\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.3588 - accuracy: 0.8435\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.3620 - accuracy: 0.8430\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.3613 - accuracy: 0.8371\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.3561 - accuracy: 0.8443\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.3686 - accuracy: 0.8430\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 878us/step - loss: 0.3449 - accuracy: 0.8515\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.3463 - accuracy: 0.8527\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.3431 - accuracy: 0.8536\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.3565 - accuracy: 0.8401\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.3495 - accuracy: 0.8489\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.3691 - accuracy: 0.8346\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.3546 - accuracy: 0.8414\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.3523 - accuracy: 0.8485\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.3499 - accuracy: 0.8540\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.3495 - accuracy: 0.8506\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.3601 - accuracy: 0.8460\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.3416 - accuracy: 0.8578\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.3420 - accuracy: 0.8544\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.3335 - accuracy: 0.8595\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.3381 - accuracy: 0.8540\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.3413 - accuracy: 0.8586\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.3464 - accuracy: 0.8494\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.3355 - accuracy: 0.8570\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3388 - accuracy: 0.8527\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.3545 - accuracy: 0.8405\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.3502 - accuracy: 0.8549\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.3408 - accuracy: 0.8515\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.3403 - accuracy: 0.8561\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.3437 - accuracy: 0.8523\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.3240 - accuracy: 0.8599\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.3362 - accuracy: 0.8519\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.3261 - accuracy: 0.8595\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.3301 - accuracy: 0.8671\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.3309 - accuracy: 0.8523\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.3431 - accuracy: 0.8523\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.3252 - accuracy: 0.8629\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.3293 - accuracy: 0.8641\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 862us/step - loss: 0.3204 - accuracy: 0.8684\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.3248 - accuracy: 0.8620\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.3295 - accuracy: 0.8650\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.3325 - accuracy: 0.8549\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.3257 - accuracy: 0.8599\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.3200 - accuracy: 0.8624\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.3266 - accuracy: 0.8684\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.3300 - accuracy: 0.8629\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.3144 - accuracy: 0.8679\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.3256 - accuracy: 0.8641\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.3365 - accuracy: 0.8565\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.3066 - accuracy: 0.8726\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.3247 - accuracy: 0.8532\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.3133 - accuracy: 0.8646\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.3032 - accuracy: 0.8722\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.3053 - accuracy: 0.8730\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.3094 - accuracy: 0.8747\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.3188 - accuracy: 0.8591\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.3238 - accuracy: 0.8620\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.3156 - accuracy: 0.8692\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.3221 - accuracy: 0.8586\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.3134 - accuracy: 0.8700\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.3160 - accuracy: 0.8624\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.3015 - accuracy: 0.8831\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3092 - accuracy: 0.8759\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3095 - accuracy: 0.8688\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.3150 - accuracy: 0.8679\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.3082 - accuracy: 0.8759\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2982 - accuracy: 0.8751\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.3013 - accuracy: 0.8747\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.3116 - accuracy: 0.8759\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.3225 - accuracy: 0.8612\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.3114 - accuracy: 0.8641\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.3161 - accuracy: 0.8637\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.3019 - accuracy: 0.8743\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.3220 - accuracy: 0.8616\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.3052 - accuracy: 0.8700\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.3028 - accuracy: 0.8730\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2970 - accuracy: 0.8797\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2914 - accuracy: 0.8789\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.3051 - accuracy: 0.8684\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.2967 - accuracy: 0.8793\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 843us/step - loss: 0.2900 - accuracy: 0.8781\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.3096 - accuracy: 0.8679\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2959 - accuracy: 0.8747\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2855 - accuracy: 0.8793\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.2897 - accuracy: 0.8848\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.2941 - accuracy: 0.8696\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2899 - accuracy: 0.8772\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2923 - accuracy: 0.8840\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2864 - accuracy: 0.8852\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2958 - accuracy: 0.8768\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2955 - accuracy: 0.8831\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.3038 - accuracy: 0.8743\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2892 - accuracy: 0.8831\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2933 - accuracy: 0.8772\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2881 - accuracy: 0.8835\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.3034 - accuracy: 0.8650\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2884 - accuracy: 0.8848\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2877 - accuracy: 0.8840\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2871 - accuracy: 0.8789\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2809 - accuracy: 0.8882\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2888 - accuracy: 0.8806\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2748 - accuracy: 0.8789\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2967 - accuracy: 0.8759\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2946 - accuracy: 0.8755\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2838 - accuracy: 0.8781\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 846us/step - loss: 0.2790 - accuracy: 0.8895\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2714 - accuracy: 0.8878\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.2681 - accuracy: 0.8890\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 871us/step - loss: 0.2824 - accuracy: 0.8844\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2793 - accuracy: 0.8844\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2901 - accuracy: 0.8819\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2910 - accuracy: 0.8738\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2802 - accuracy: 0.8831\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2792 - accuracy: 0.8810\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2685 - accuracy: 0.8903\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2764 - accuracy: 0.8768\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2640 - accuracy: 0.8932\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2768 - accuracy: 0.8848\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2746 - accuracy: 0.8802\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.2639 - accuracy: 0.8945\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2903 - accuracy: 0.8755\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2741 - accuracy: 0.8932\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2796 - accuracy: 0.8878\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2779 - accuracy: 0.8886\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2846 - accuracy: 0.8840\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2685 - accuracy: 0.8983\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2852 - accuracy: 0.8831\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.2731 - accuracy: 0.8861\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2565 - accuracy: 0.8966\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.2653 - accuracy: 0.8827\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 715us/step - loss: 0.2559 - accuracy: 0.8949\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 824us/step - loss: 0.2619 - accuracy: 0.8869\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2648 - accuracy: 0.8895\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2705 - accuracy: 0.8962\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2586 - accuracy: 0.8899\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.2689 - accuracy: 0.8941\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 865us/step - loss: 0.2808 - accuracy: 0.8810\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2649 - accuracy: 0.8903\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.2652 - accuracy: 0.8882\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2589 - accuracy: 0.8992\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2527 - accuracy: 0.8962\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.2667 - accuracy: 0.8878\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2627 - accuracy: 0.8899\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 983us/step - loss: 0.2407 - accuracy: 0.9025\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2563 - accuracy: 0.8954\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2602 - accuracy: 0.8941\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2446 - accuracy: 0.9030\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2627 - accuracy: 0.8941\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2661 - accuracy: 0.8924\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.2683 - accuracy: 0.8886\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2485 - accuracy: 0.9004\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.2607 - accuracy: 0.8928\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2658 - accuracy: 0.8840\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2684 - accuracy: 0.8869\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2639 - accuracy: 0.8945\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2561 - accuracy: 0.8987\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2555 - accuracy: 0.9000\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2500 - accuracy: 0.8970\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2692 - accuracy: 0.8945\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2680 - accuracy: 0.8886\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.2529 - accuracy: 0.9063\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2481 - accuracy: 0.9000\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2635 - accuracy: 0.8890\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.2387 - accuracy: 0.9021\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2784 - accuracy: 0.8814\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2519 - accuracy: 0.8928\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2439 - accuracy: 0.9059\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2488 - accuracy: 0.9004\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2467 - accuracy: 0.8987\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2571 - accuracy: 0.8916\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2496 - accuracy: 0.9034\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2374 - accuracy: 0.8979\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2440 - accuracy: 0.9000\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2542 - accuracy: 0.8962\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2409 - accuracy: 0.8996\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2605 - accuracy: 0.8958\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2545 - accuracy: 0.8958\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2542 - accuracy: 0.8954\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2357 - accuracy: 0.9013\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.2484 - accuracy: 0.8992\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2385 - accuracy: 0.9080\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2480 - accuracy: 0.8958\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.2509 - accuracy: 0.8987\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.2256 - accuracy: 0.9105\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2535 - accuracy: 0.8958\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2302 - accuracy: 0.9063\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2404 - accuracy: 0.9025\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2610 - accuracy: 0.8924\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2472 - accuracy: 0.9030\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.2421 - accuracy: 0.9059\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.2398 - accuracy: 0.9089\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2467 - accuracy: 0.8996\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2178 - accuracy: 0.9148\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2383 - accuracy: 0.9105\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.2372 - accuracy: 0.9004\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.2408 - accuracy: 0.8970\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2284 - accuracy: 0.9101\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2291 - accuracy: 0.9105\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2246 - accuracy: 0.9093\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2389 - accuracy: 0.9038\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2197 - accuracy: 0.9089\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2335 - accuracy: 0.9084\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.2296 - accuracy: 0.9076\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.2251 - accuracy: 0.9072\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2488 - accuracy: 0.8962\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.2428 - accuracy: 0.9068\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.2251 - accuracy: 0.9135\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2276 - accuracy: 0.9110\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2289 - accuracy: 0.9008\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2262 - accuracy: 0.9063\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2369 - accuracy: 0.9084\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2335 - accuracy: 0.9089\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.2220 - accuracy: 0.9101\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 825us/step - loss: 0.2111 - accuracy: 0.9219\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2316 - accuracy: 0.9110\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2290 - accuracy: 0.9025\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2297 - accuracy: 0.9093\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2398 - accuracy: 0.8996\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2182 - accuracy: 0.9131\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2465 - accuracy: 0.9004\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2233 - accuracy: 0.9093\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2154 - accuracy: 0.9152\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.2273 - accuracy: 0.9068\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2285 - accuracy: 0.9055\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.2212 - accuracy: 0.9118\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 856us/step - loss: 0.2419 - accuracy: 0.9080\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.2087 - accuracy: 0.9241\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 895us/step - loss: 0.2296 - accuracy: 0.9101\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 859us/step - loss: 0.2366 - accuracy: 0.9093\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.2242 - accuracy: 0.9127\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2290 - accuracy: 0.9017\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2273 - accuracy: 0.9152\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.2186 - accuracy: 0.9101\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.2134 - accuracy: 0.9135\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.2207 - accuracy: 0.9055\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 959us/step - loss: 0.2264 - accuracy: 0.9080\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2316 - accuracy: 0.9034\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2254 - accuracy: 0.9097\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2312 - accuracy: 0.9080\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2249 - accuracy: 0.9063\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2242 - accuracy: 0.9118\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.2161 - accuracy: 0.9127\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2082 - accuracy: 0.9165\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2213 - accuracy: 0.9110\n",
      "Epoch 362/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2213 - accuracy: 0.9097\n",
      "Epoch 363/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.2149 - accuracy: 0.9173\n",
      "Epoch 364/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2331 - accuracy: 0.9034\n",
      "Epoch 365/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2090 - accuracy: 0.9160\n",
      "Epoch 366/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2134 - accuracy: 0.9110\n",
      "Epoch 367/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2307 - accuracy: 0.9042\n",
      "Epoch 368/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2262 - accuracy: 0.9076\n",
      "Epoch 369/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2050 - accuracy: 0.9160\n",
      "Epoch 370/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2169 - accuracy: 0.9139\n",
      "Epoch 371/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2015 - accuracy: 0.9211\n",
      "Epoch 372/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2205 - accuracy: 0.9110\n",
      "Epoch 373/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2296 - accuracy: 0.9084\n",
      "Epoch 374/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2178 - accuracy: 0.9084\n",
      "Epoch 375/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2059 - accuracy: 0.9139\n",
      "Epoch 376/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2249 - accuracy: 0.9080\n",
      "Epoch 377/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2066 - accuracy: 0.9181\n",
      "Epoch 378/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.2171 - accuracy: 0.9114\n",
      "Epoch 379/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.2358 - accuracy: 0.9030\n",
      "Epoch 380/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.2164 - accuracy: 0.9084\n",
      "Epoch 381/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2312 - accuracy: 0.9046\n",
      "Epoch 382/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2261 - accuracy: 0.9122\n",
      "Epoch 383/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.2321 - accuracy: 0.9021\n",
      "Epoch 384/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.1954 - accuracy: 0.9207\n",
      "Epoch 385/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.1994 - accuracy: 0.9143\n",
      "Epoch 386/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2121 - accuracy: 0.9143\n",
      "Epoch 387/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2035 - accuracy: 0.9203\n",
      "Epoch 388/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2126 - accuracy: 0.9114\n",
      "Epoch 389/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.1998 - accuracy: 0.9177\n",
      "Epoch 390/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2158 - accuracy: 0.9228\n",
      "Epoch 391/1500\n",
      "38/38 [==============================] - 0s 800us/step - loss: 0.2069 - accuracy: 0.9152\n",
      "Epoch 392/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2033 - accuracy: 0.9131\n",
      "Epoch 393/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2097 - accuracy: 0.9194\n",
      "Epoch 394/1500\n",
      "38/38 [==============================] - 0s 882us/step - loss: 0.2149 - accuracy: 0.9097\n",
      "Epoch 395/1500\n",
      "38/38 [==============================] - 0s 837us/step - loss: 0.2118 - accuracy: 0.9160\n",
      "Epoch 396/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2004 - accuracy: 0.9207\n",
      "Epoch 397/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2033 - accuracy: 0.9211\n",
      "Epoch 398/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2035 - accuracy: 0.9236\n",
      "Epoch 399/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2077 - accuracy: 0.9135\n",
      "Epoch 400/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2093 - accuracy: 0.9203\n",
      "Epoch 401/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.2106 - accuracy: 0.9135\n",
      "Epoch 402/1500\n",
      "38/38 [==============================] - 0s 958us/step - loss: 0.2162 - accuracy: 0.9093\n",
      "Epoch 403/1500\n",
      "38/38 [==============================] - 0s 968us/step - loss: 0.2057 - accuracy: 0.9177\n",
      "Epoch 404/1500\n",
      "38/38 [==============================] - 0s 928us/step - loss: 0.2037 - accuracy: 0.9194\n",
      "Epoch 405/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.2098 - accuracy: 0.9118\n",
      "Epoch 406/1500\n",
      "38/38 [==============================] - 0s 842us/step - loss: 0.2016 - accuracy: 0.9211\n",
      "Epoch 407/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2086 - accuracy: 0.9152\n",
      "Epoch 408/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2092 - accuracy: 0.9122\n",
      "Epoch 409/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2093 - accuracy: 0.9190\n",
      "Epoch 410/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.2189 - accuracy: 0.9089\n",
      "Epoch 411/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2048 - accuracy: 0.9262\n",
      "Epoch 412/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2079 - accuracy: 0.9160\n",
      "Epoch 413/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.2183 - accuracy: 0.9101\n",
      "Epoch 414/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.3668 - accuracy: 0.8594Restoring model weights from the end of the best epoch: 384.\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.2050 - accuracy: 0.9215\n",
      "Epoch 414: early stopping\n",
      "6/6 [==============================] - 0s 812us/step - loss: 0.4344 - accuracy: 0.8122\n",
      "6/6 [==============================] - 0s 683us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (25/30)\n",
      "Before appending - Cat IDs: 444, Predictions: 444, Actuals: 444, Gender: 444\n",
      "After appending - Cat IDs: 625, Predictions: 625, Actuals: 625, Gender: 625\n",
      "Final Test Results - Loss: 0.4344176948070526, Accuracy: 0.8121547102928162, Precision: 0.8091126946088778, Recall: 0.7019101425881087, F1 Score: 0.7411298208470516\n",
      "Confusion Matrix:\n",
      " [[108   2   8]\n",
      " [  9  11   1]\n",
      " [ 14   0  28]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "116A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "016A    10\n",
      "005A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "050A     7\n",
      "117A     7\n",
      "037A     6\n",
      "053A     6\n",
      "008A     6\n",
      "109A     6\n",
      "023A     6\n",
      "044A     5\n",
      "023B     5\n",
      "070A     5\n",
      "075A     5\n",
      "009A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "060A     3\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "025B     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "032A     2\n",
      "018A     2\n",
      "069A     2\n",
      "092A     1\n",
      "100A     1\n",
      "096A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "041A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "066A     1\n",
      "091A     1\n",
      "076A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "055A    20\n",
      "101A    15\n",
      "028A    13\n",
      "002A    13\n",
      "025A    11\n",
      "033A     9\n",
      "031A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "034A     5\n",
      "025C     5\n",
      "021A     5\n",
      "026A     4\n",
      "035A     4\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "093A     2\n",
      "049A     1\n",
      "026C     1\n",
      "073A     1\n",
      "088A     1\n",
      "090A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    277\n",
      "M    271\n",
      "F    151\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    101\n",
      "X     71\n",
      "M     66\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 071A, 097B, 019A, 074A, 067...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 050...\n",
      "senior    [097A, 106A, 104A, 059A, 116A, 051B, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 103A, 028A, 101A, 034A, 002A, 099...\n",
      "kitten                                         [047A, 049A]\n",
      "senior           [093A, 057A, 055A, 113A, 056A, 108A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 14, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 2, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "No common groups found between train and test sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "Length of X_train_val:\n",
      "699\n",
      "Length of y_train_val:\n",
      "699\n",
      "Length of groups_train_val:\n",
      "699\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 882, 1: 710, 2: 580})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.3466 - accuracy: 0.3711\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.0416 - accuracy: 0.5308\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 926us/step - loss: 0.9238 - accuracy: 0.5787\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.8543 - accuracy: 0.6321\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.8066 - accuracy: 0.6492\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.8008 - accuracy: 0.6524\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.7585 - accuracy: 0.6727\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.7429 - accuracy: 0.6837\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.7110 - accuracy: 0.6878\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.7020 - accuracy: 0.6971\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.6836 - accuracy: 0.6980\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.6743 - accuracy: 0.6989\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.6514 - accuracy: 0.7150\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.6321 - accuracy: 0.7320\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.6075 - accuracy: 0.7426\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.5814 - accuracy: 0.7376\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.5799 - accuracy: 0.7569\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.6000 - accuracy: 0.7436\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.5771 - accuracy: 0.7698\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.5515 - accuracy: 0.7597\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.5484 - accuracy: 0.7597\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.5558 - accuracy: 0.7634\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.5342 - accuracy: 0.7790\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.5593 - accuracy: 0.7587\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.5368 - accuracy: 0.7818\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.5164 - accuracy: 0.7735\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.5192 - accuracy: 0.7855\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.5199 - accuracy: 0.7772\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.5106 - accuracy: 0.7790\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 912us/step - loss: 0.5168 - accuracy: 0.7776\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 953us/step - loss: 0.5142 - accuracy: 0.7873\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 983us/step - loss: 0.4984 - accuracy: 0.7956\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 966us/step - loss: 0.4913 - accuracy: 0.7956\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 924us/step - loss: 0.4936 - accuracy: 0.7947\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.4817 - accuracy: 0.7951\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.4668 - accuracy: 0.8043\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.4627 - accuracy: 0.8062\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.4787 - accuracy: 0.7997\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.4584 - accuracy: 0.8158\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 852us/step - loss: 0.4571 - accuracy: 0.8025\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.4503 - accuracy: 0.8126\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4607 - accuracy: 0.8057\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.4469 - accuracy: 0.8135\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.4553 - accuracy: 0.8043\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.4533 - accuracy: 0.8112\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.4552 - accuracy: 0.8135\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.4393 - accuracy: 0.8135\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.4284 - accuracy: 0.8227\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.4427 - accuracy: 0.8145\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.4512 - accuracy: 0.8057\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.4216 - accuracy: 0.8232\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.4275 - accuracy: 0.8246\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.4415 - accuracy: 0.8163\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.4404 - accuracy: 0.8177\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.4259 - accuracy: 0.8241\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.4152 - accuracy: 0.8255\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.4132 - accuracy: 0.8246\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.3977 - accuracy: 0.8287\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 752us/step - loss: 0.4029 - accuracy: 0.8320\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.4048 - accuracy: 0.8287\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.3993 - accuracy: 0.8356\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.4256 - accuracy: 0.8204\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.4007 - accuracy: 0.8338\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.4087 - accuracy: 0.8310\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.4000 - accuracy: 0.8324\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.4065 - accuracy: 0.8255\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3906 - accuracy: 0.8389\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3951 - accuracy: 0.8352\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.3955 - accuracy: 0.8292\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.3813 - accuracy: 0.8407\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.3901 - accuracy: 0.8435\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.3722 - accuracy: 0.8467\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.3839 - accuracy: 0.8412\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.4015 - accuracy: 0.8389\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.3870 - accuracy: 0.8379\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3718 - accuracy: 0.8485\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.3742 - accuracy: 0.8444\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.3832 - accuracy: 0.8425\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.3924 - accuracy: 0.8370\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.3655 - accuracy: 0.8439\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.3649 - accuracy: 0.8462\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.3520 - accuracy: 0.8605\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.3481 - accuracy: 0.8476\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.3713 - accuracy: 0.8522\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 970us/step - loss: 0.3632 - accuracy: 0.8517\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 984us/step - loss: 0.3591 - accuracy: 0.8508\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 951us/step - loss: 0.3705 - accuracy: 0.8444\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 980us/step - loss: 0.3522 - accuracy: 0.8605\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3620 - accuracy: 0.8471\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3536 - accuracy: 0.8490\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3439 - accuracy: 0.8582\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 996us/step - loss: 0.3600 - accuracy: 0.8513\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 975us/step - loss: 0.3454 - accuracy: 0.8577\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 976us/step - loss: 0.3607 - accuracy: 0.8476\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 958us/step - loss: 0.3500 - accuracy: 0.8508\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.3386 - accuracy: 0.8614\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3345 - accuracy: 0.8605\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8591\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8614\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 978us/step - loss: 0.3384 - accuracy: 0.8610\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.3383 - accuracy: 0.8628\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.3339 - accuracy: 0.8706\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.3349 - accuracy: 0.8564\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.3358 - accuracy: 0.8628\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.3297 - accuracy: 0.8633\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.3281 - accuracy: 0.8610\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.3436 - accuracy: 0.8633\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.3278 - accuracy: 0.8715\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.3213 - accuracy: 0.8706\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.3278 - accuracy: 0.8623\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.3300 - accuracy: 0.8669\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.3297 - accuracy: 0.8692\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.3174 - accuracy: 0.8692\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3254 - accuracy: 0.8656\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.3191 - accuracy: 0.8679\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.3148 - accuracy: 0.8706\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.3063 - accuracy: 0.8780\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.3257 - accuracy: 0.8651\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.3142 - accuracy: 0.8734\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3315 - accuracy: 0.8623\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.3163 - accuracy: 0.8669\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.3166 - accuracy: 0.8688\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.3225 - accuracy: 0.8651\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.3166 - accuracy: 0.8720\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.3060 - accuracy: 0.8794\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.3008 - accuracy: 0.8762\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.3052 - accuracy: 0.8817\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.3061 - accuracy: 0.8757\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.3001 - accuracy: 0.8715\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.3164 - accuracy: 0.8702\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2974 - accuracy: 0.8766\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.2962 - accuracy: 0.8798\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.3175 - accuracy: 0.8762\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.2916 - accuracy: 0.8789\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.3176 - accuracy: 0.8752\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.3028 - accuracy: 0.8780\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.2875 - accuracy: 0.8771\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.2938 - accuracy: 0.8752\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.2950 - accuracy: 0.8715\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.2867 - accuracy: 0.8854\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.2787 - accuracy: 0.8932\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2874 - accuracy: 0.8840\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2868 - accuracy: 0.8858\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.2805 - accuracy: 0.8867\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2904 - accuracy: 0.8752\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.3024 - accuracy: 0.8794\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.2980 - accuracy: 0.8803\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.2865 - accuracy: 0.8789\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.2753 - accuracy: 0.8867\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2926 - accuracy: 0.8831\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.2900 - accuracy: 0.8789\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2879 - accuracy: 0.8849\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.2732 - accuracy: 0.8858\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.2918 - accuracy: 0.8817\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.2676 - accuracy: 0.8886\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2836 - accuracy: 0.8808\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.2720 - accuracy: 0.8863\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 812us/step - loss: 0.2820 - accuracy: 0.8812\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.2653 - accuracy: 0.8927\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2670 - accuracy: 0.8987\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2808 - accuracy: 0.8854\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.2785 - accuracy: 0.8835\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.2602 - accuracy: 0.8941\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.2648 - accuracy: 0.8909\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.2734 - accuracy: 0.8877\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.2668 - accuracy: 0.8950\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.2669 - accuracy: 0.8932\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2598 - accuracy: 0.8969\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2744 - accuracy: 0.8877\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2736 - accuracy: 0.8867\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2685 - accuracy: 0.8877\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.2620 - accuracy: 0.8936\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.2631 - accuracy: 0.8996\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.2490 - accuracy: 0.9042\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2636 - accuracy: 0.8886\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.2678 - accuracy: 0.8978\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.2729 - accuracy: 0.8918\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.2641 - accuracy: 0.8932\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2623 - accuracy: 0.8900\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2640 - accuracy: 0.8936\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2349 - accuracy: 0.9042\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2730 - accuracy: 0.8936\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2497 - accuracy: 0.9019\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2644 - accuracy: 0.8987\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2611 - accuracy: 0.8964\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.2456 - accuracy: 0.9033\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.2568 - accuracy: 0.9019\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.2551 - accuracy: 0.8987\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.2590 - accuracy: 0.8950\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2564 - accuracy: 0.8923\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2353 - accuracy: 0.9084\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.9033\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.2365 - accuracy: 0.9088\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.2544 - accuracy: 0.9001\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.2516 - accuracy: 0.8909\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.2510 - accuracy: 0.8983\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.2541 - accuracy: 0.9038\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.2304 - accuracy: 0.9015\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.2410 - accuracy: 0.9088\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2406 - accuracy: 0.8973\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2534 - accuracy: 0.8964\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2447 - accuracy: 0.8987\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.2329 - accuracy: 0.9042\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.2349 - accuracy: 0.9056\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 826us/step - loss: 0.2504 - accuracy: 0.9019\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.2442 - accuracy: 0.9024\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2498 - accuracy: 0.9015\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.2273 - accuracy: 0.9070\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.2403 - accuracy: 0.9029\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.2419 - accuracy: 0.9001\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.2338 - accuracy: 0.9061\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.2422 - accuracy: 0.9130\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2353 - accuracy: 0.9061\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.2459 - accuracy: 0.9015\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.2214 - accuracy: 0.9121\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2286 - accuracy: 0.9052\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.2195 - accuracy: 0.9134\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2323 - accuracy: 0.9070\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.2189 - accuracy: 0.9171\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.2396 - accuracy: 0.9033\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.2408 - accuracy: 0.9065\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2256 - accuracy: 0.9088\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.2266 - accuracy: 0.9107\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.2227 - accuracy: 0.9148\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2296 - accuracy: 0.9070\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.2255 - accuracy: 0.9070\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.2177 - accuracy: 0.9070\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.2234 - accuracy: 0.9162\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2218 - accuracy: 0.9134\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2262 - accuracy: 0.9015\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2240 - accuracy: 0.9084\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.2172 - accuracy: 0.9139\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2240 - accuracy: 0.9171\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 809us/step - loss: 0.2348 - accuracy: 0.9042\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.2162 - accuracy: 0.9134\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.2285 - accuracy: 0.9084\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.2213 - accuracy: 0.9148\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2338 - accuracy: 0.9056\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2305 - accuracy: 0.9019\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.2183 - accuracy: 0.9134\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2077 - accuracy: 0.9222\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.2240 - accuracy: 0.9130\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 742us/step - loss: 0.2167 - accuracy: 0.9134\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.2130 - accuracy: 0.9185\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.2034 - accuracy: 0.9157\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.2219 - accuracy: 0.9079\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.2286 - accuracy: 0.9056\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.2229 - accuracy: 0.9107\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.2260 - accuracy: 0.9098\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2116 - accuracy: 0.9153\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2103 - accuracy: 0.9208\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.2085 - accuracy: 0.9208\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2229 - accuracy: 0.9176\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.2251 - accuracy: 0.9065\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.2083 - accuracy: 0.9116\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 783us/step - loss: 0.1980 - accuracy: 0.9273\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1995 - accuracy: 0.9180\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2158 - accuracy: 0.9102\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2050 - accuracy: 0.9130\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2084 - accuracy: 0.9153\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2058 - accuracy: 0.9208\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.2061 - accuracy: 0.9203\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.2172 - accuracy: 0.9139\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.1949 - accuracy: 0.9309\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.2104 - accuracy: 0.9190\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.2167 - accuracy: 0.9125\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 885us/step - loss: 0.1934 - accuracy: 0.9194\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.2236 - accuracy: 0.9038\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 944us/step - loss: 0.2005 - accuracy: 0.9190\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.1998 - accuracy: 0.9236\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.1982 - accuracy: 0.9208\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2036 - accuracy: 0.9217\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.1981 - accuracy: 0.9199\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.1918 - accuracy: 0.9208\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1951 - accuracy: 0.9208\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.1990 - accuracy: 0.9240\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.2075 - accuracy: 0.9167\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2014 - accuracy: 0.9254\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2110 - accuracy: 0.9194\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1812 - accuracy: 0.9259\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2081 - accuracy: 0.9199\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.2008 - accuracy: 0.9227\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.1937 - accuracy: 0.9259\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.1965 - accuracy: 0.9263\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.2143 - accuracy: 0.9153\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.2066 - accuracy: 0.9222\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 767us/step - loss: 0.1946 - accuracy: 0.9213\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.1963 - accuracy: 0.9296\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2056 - accuracy: 0.9203\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.1889 - accuracy: 0.9203\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.1914 - accuracy: 0.9300\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.2009 - accuracy: 0.9194\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.2228 - accuracy: 0.9116\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2000 - accuracy: 0.9236\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.1895 - accuracy: 0.9254\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.2005 - accuracy: 0.9227\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.1982 - accuracy: 0.9263\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1904 - accuracy: 0.9254\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1994 - accuracy: 0.9236\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.1838 - accuracy: 0.9263\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.1753 - accuracy: 0.9300\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.2018 - accuracy: 0.9231\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2032 - accuracy: 0.9185\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 729us/step - loss: 0.1842 - accuracy: 0.9282\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1917 - accuracy: 0.9319\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.1708 - accuracy: 0.9300\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.1789 - accuracy: 0.9319\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.1987 - accuracy: 0.9231\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 924us/step - loss: 0.1804 - accuracy: 0.9263\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9263\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1837 - accuracy: 0.9286\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.1774 - accuracy: 0.9259\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.1775 - accuracy: 0.9355\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.1904 - accuracy: 0.9245\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.1771 - accuracy: 0.9332\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.2047 - accuracy: 0.9190\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.1744 - accuracy: 0.9365\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 788us/step - loss: 0.1854 - accuracy: 0.9277\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.1823 - accuracy: 0.9296\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.1694 - accuracy: 0.9388\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.1593 - accuracy: 0.9397\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.1793 - accuracy: 0.9314\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.1878 - accuracy: 0.9236\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.1899 - accuracy: 0.9286\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.1698 - accuracy: 0.9346\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.1775 - accuracy: 0.9300\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1840 - accuracy: 0.9236\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.1824 - accuracy: 0.9286\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.1949 - accuracy: 0.9273\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.1620 - accuracy: 0.9378\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.1782 - accuracy: 0.9346\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1755 - accuracy: 0.9286\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1753 - accuracy: 0.9337\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.1708 - accuracy: 0.9309\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.1865 - accuracy: 0.9328\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1773 - accuracy: 0.9314\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.1802 - accuracy: 0.9259\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.1799 - accuracy: 0.9277\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.1856 - accuracy: 0.9332\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.1721 - accuracy: 0.9351\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.1774 - accuracy: 0.9355\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.1922 - accuracy: 0.9199\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.1701 - accuracy: 0.9369\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1792 - accuracy: 0.9319\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.1816 - accuracy: 0.9282\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.1576 - accuracy: 0.9429\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.1624 - accuracy: 0.9401\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.1727 - accuracy: 0.9309\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.1687 - accuracy: 0.9351\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.1692 - accuracy: 0.9300\n",
      "Epoch 351/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.1728 - accuracy: 0.9300\n",
      "Epoch 352/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.1807 - accuracy: 0.9332\n",
      "Epoch 353/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1797 - accuracy: 0.9305\n",
      "Epoch 354/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9282\n",
      "Epoch 355/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.1715 - accuracy: 0.9332\n",
      "Epoch 356/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.1556 - accuracy: 0.9355\n",
      "Epoch 357/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1814 - accuracy: 0.9286\n",
      "Epoch 358/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.1776 - accuracy: 0.9296\n",
      "Epoch 359/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.1823 - accuracy: 0.9231\n",
      "Epoch 360/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.9314\n",
      "Epoch 361/1500\n",
      "34/34 [==============================] - 0s 931us/step - loss: 0.1633 - accuracy: 0.9374\n",
      "Epoch 362/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9365\n",
      "Epoch 363/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.1778 - accuracy: 0.9328\n",
      "Epoch 364/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.1543 - accuracy: 0.9466\n",
      "Epoch 365/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.1534 - accuracy: 0.9369\n",
      "Epoch 366/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.1696 - accuracy: 0.9378\n",
      "Epoch 367/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.1703 - accuracy: 0.9397\n",
      "Epoch 368/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.1643 - accuracy: 0.9342\n",
      "Epoch 369/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.1743 - accuracy: 0.9328\n",
      "Epoch 370/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.1509 - accuracy: 0.9415\n",
      "Epoch 371/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1632 - accuracy: 0.9401\n",
      "Epoch 372/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1694 - accuracy: 0.9355\n",
      "Epoch 373/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.1590 - accuracy: 0.9397\n",
      "Epoch 374/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.1666 - accuracy: 0.9355\n",
      "Epoch 375/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.1699 - accuracy: 0.9305\n",
      "Epoch 376/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.1696 - accuracy: 0.9337\n",
      "Epoch 377/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.1567 - accuracy: 0.9397\n",
      "Epoch 378/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1720 - accuracy: 0.9365\n",
      "Epoch 379/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.1608 - accuracy: 0.9346\n",
      "Epoch 380/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.1694 - accuracy: 0.9406\n",
      "Epoch 381/1500\n",
      "34/34 [==============================] - 0s 758us/step - loss: 0.1715 - accuracy: 0.9282\n",
      "Epoch 382/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1596 - accuracy: 0.9383\n",
      "Epoch 383/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1595 - accuracy: 0.9429\n",
      "Epoch 384/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.1599 - accuracy: 0.9383\n",
      "Epoch 385/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1795 - accuracy: 0.9328\n",
      "Epoch 386/1500\n",
      "34/34 [==============================] - 0s 750us/step - loss: 0.1655 - accuracy: 0.9332\n",
      "Epoch 387/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.1754 - accuracy: 0.9305\n",
      "Epoch 388/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.1595 - accuracy: 0.9424\n",
      "Epoch 389/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.1752 - accuracy: 0.9282\n",
      "Epoch 390/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.1702 - accuracy: 0.9305\n",
      "Epoch 391/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1676 - accuracy: 0.9328\n",
      "Epoch 392/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.1536 - accuracy: 0.9420\n",
      "Epoch 393/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.1727 - accuracy: 0.9332\n",
      "Epoch 394/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.1690 - accuracy: 0.9291\n",
      "Epoch 395/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.1487 - accuracy: 0.9406\n",
      "Epoch 396/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.1599 - accuracy: 0.9378\n",
      "Epoch 397/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.1578 - accuracy: 0.9406\n",
      "Epoch 398/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1606 - accuracy: 0.9388\n",
      "Epoch 399/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.1413 - accuracy: 0.9461\n",
      "Epoch 400/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1683 - accuracy: 0.9346\n",
      "Epoch 401/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.1535 - accuracy: 0.9420\n",
      "Epoch 402/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.1584 - accuracy: 0.9365\n",
      "Epoch 403/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.1325 - accuracy: 0.9498\n",
      "Epoch 404/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.1660 - accuracy: 0.9328\n",
      "Epoch 405/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1688 - accuracy: 0.9346\n",
      "Epoch 406/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.1542 - accuracy: 0.9424\n",
      "Epoch 407/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.1683 - accuracy: 0.9332\n",
      "Epoch 408/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.1634 - accuracy: 0.9388\n",
      "Epoch 409/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1543 - accuracy: 0.9420\n",
      "Epoch 410/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1689 - accuracy: 0.9360\n",
      "Epoch 411/1500\n",
      "34/34 [==============================] - 0s 773us/step - loss: 0.1579 - accuracy: 0.9374\n",
      "Epoch 412/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.1599 - accuracy: 0.9411\n",
      "Epoch 413/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.1512 - accuracy: 0.9429\n",
      "Epoch 414/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.1617 - accuracy: 0.9378\n",
      "Epoch 415/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.1509 - accuracy: 0.9420\n",
      "Epoch 416/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.1380 - accuracy: 0.9494\n",
      "Epoch 417/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.1492 - accuracy: 0.9401\n",
      "Epoch 418/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1620 - accuracy: 0.9415\n",
      "Epoch 419/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.9346\n",
      "Epoch 420/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.1466 - accuracy: 0.9392\n",
      "Epoch 421/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.1443 - accuracy: 0.9461\n",
      "Epoch 422/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.1489 - accuracy: 0.9420\n",
      "Epoch 423/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1445 - accuracy: 0.9475\n",
      "Epoch 424/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.1601 - accuracy: 0.9378\n",
      "Epoch 425/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.1445 - accuracy: 0.9429\n",
      "Epoch 426/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.1505 - accuracy: 0.9448\n",
      "Epoch 427/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.1448 - accuracy: 0.9438\n",
      "Epoch 428/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.1599 - accuracy: 0.9360\n",
      "Epoch 429/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.1531 - accuracy: 0.9415\n",
      "Epoch 430/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.1566 - accuracy: 0.9360\n",
      "Epoch 431/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.1552 - accuracy: 0.9415\n",
      "Epoch 432/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1539 - accuracy: 0.9392\n",
      "Epoch 433/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.0790 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 403.\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.1415 - accuracy: 0.9484\n",
      "Epoch 433: early stopping\n",
      "8/8 [==============================] - 0s 780us/step - loss: 0.8491 - accuracy: 0.6891\n",
      "8/8 [==============================] - 0s 653us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (22/29)\n",
      "Before appending - Cat IDs: 625, Predictions: 625, Actuals: 625, Gender: 625\n",
      "After appending - Cat IDs: 863, Predictions: 863, Actuals: 863, Gender: 863\n",
      "Final Test Results - Loss: 0.8490554690361023, Accuracy: 0.6890756487846375, Precision: 0.7083557468172853, Recall: 0.6585069326210277, F1 Score: 0.6771237840818288\n",
      "Confusion Matrix:\n",
      " [[121   0  26]\n",
      " [  4  25   0]\n",
      " [ 44   0  18]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6840125693378367\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.7433864250779152\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7241141051054001\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.7162998446047106\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6732906018075305\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[2]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be692ae4-6d3f-4353-b5bf-554d20da4df3",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "8da9a092-ed2e-4397-a6c8-2c4888735265",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 863, Predictions: 863, Actuals: 863, Gender: 863\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "51cf386a-c49e-4716-ba15-aa3b7930419a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "a8d43ac5-d50e-430d-98a1-ff4f45006bae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.73 (80/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "ccc9acb7-bb1b-42a6-bb25-cdf5a3356315",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "95e69b27-cae1-4a3a-ba70-5244a11aadf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, adult, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, adult, kitten, adult, kitten, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[senior, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, adult, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, senior, kitten, kitten, kitten, adult...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, adult...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[adult, adult, kitten, adult, kitten, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, senior, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[senior, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, kitten, adult, adult, adult, ad...         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A              [adult, adult, senior, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, senior, senior, senior, adult, adult,...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "56    051A  [adult, senior, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, senior, senior, ...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "51    045A  [adult, adult, kitten, kitten, kitten, adult, ...        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "48    042A  [kitten, adult, kitten, adult, kitten, kitten,...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, adult,...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A     [adult, adult, senior, senior, senior, senior]        senior           senior                   True\n",
       "100   105A                     [senior, adult, adult, senior]         adult            adult                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "92    097A  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, adult, adult, senior,...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "55    050A  [kitten, senior, kitten, kitten, kitten, adult...        kitten           kitten                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "32    026B                                            [adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "4     003A                     [adult, senior, adult, senior]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "17    015A  [adult, adult, senior, senior, senior, adult, ...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "8     007A       [adult, senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "103   109A        [adult, adult, kitten, adult, adult, adult]         adult           kitten                  False\n",
       "101   106A  [adult, senior, senior, senior, adult, adult, ...         adult           senior                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "90    095A  [senior, senior, senior, senior, senior, adult...        senior            adult                  False\n",
       "99    104A                     [adult, senior, adult, senior]         adult           senior                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "18    016A  [senior, adult, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, senior, adult, s...         adult           senior                  False\n",
       "42    036A  [adult, senior, senior, senior, senior, senior...        senior            adult                  False\n",
       "50    044A  [adult, adult, kitten, adult, kitten, adult, a...         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "57    051B  [senior, adult, senior, adult, senior, adult, ...         adult           senior                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "61    055A  [adult, adult, senior, adult, adult, adult, se...         adult           senior                  False\n",
       "62    056A                              [adult, adult, adult]         adult           senior                  False\n",
       "64    058A                             [adult, senior, adult]         adult           senior                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "66    060A                           [senior, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                    [senior, adult]         adult           senior                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "69    063A  [senior, senior, senior, adult, senior, senior...        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, a...        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "85    090A                                            [adult]         adult           senior                  False\n",
       "109   117A  [senior, senior, adult, adult, adult, adult, a...         adult           senior                  False"
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "d36b3c54-3377-4249-a774-6d31557e36da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     61\n",
      "kitten    12\n",
      "senior     7\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "b36eb8a4-57f3-48c0-b92c-4a8e5a52c59e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             61  83.561644\n",
      "1           kitten           15             12  80.000000\n",
      "2           senior           22              7  31.818182\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750e2da-df8c-4f00-b860-539dd822864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6b34e-2368-4956-8241-8e6ab6e8cf81",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "fdefd9be-6e6d-4903-a0ec-7824da4313d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          572            454  79.370629\n",
      "1           kitten          113             87  76.991150\n",
      "2           senior          178             80  44.943820\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57576e25-349c-46e0-b57b-8bf3cee3b5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b69785-d4c1-49ab-ba75-4a733326cdc1",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "15c5b6e5-d7b0-4089-ae78-3dafe1cead3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      148     69.48\n",
      "1          M    360      273     75.83\n",
      "2          X    290      200     68.97\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6001f1b-3f01-42e9-8761-4253acbed5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b5d44-f489-4de0-8785-4bfa52d09797",
   "metadata": {},
   "source": [
    "# RANDOM SEED 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "c4439fc4-670c-4009-8ca9-23c419ee99ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[3])) \n",
    "np.random.seed(int(random_seeds[3]))\n",
    "tf.random.set_seed(int(random_seeds[3]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_13.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "5fe04e3d-fd90-43c1-97af-5eac75e35f85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "2ed5261d-2aeb-412e-a017-65b461a2cf5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70aa6-1716-400a-be48-cccb3511542e",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "fb9d0248-481e-4807-85dd-407fa88963f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "072A     9\n",
      "015A     9\n",
      "051B     9\n",
      "022A     9\n",
      "033A     9\n",
      "045A     9\n",
      "095A     8\n",
      "013B     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "109A     6\n",
      "023A     6\n",
      "037A     6\n",
      "075A     5\n",
      "070A     5\n",
      "025C     5\n",
      "021A     5\n",
      "034A     5\n",
      "044A     5\n",
      "023B     5\n",
      "003A     4\n",
      "105A     4\n",
      "035A     4\n",
      "026A     4\n",
      "052A     4\n",
      "062A     4\n",
      "012A     3\n",
      "113A     3\n",
      "058A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "025B     2\n",
      "032A     2\n",
      "093A     2\n",
      "054A     2\n",
      "069A     2\n",
      "087A     2\n",
      "038A     2\n",
      "073A     1\n",
      "004A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "091A     1\n",
      "019B     1\n",
      "066A     1\n",
      "048A     1\n",
      "092A     1\n",
      "026C     1\n",
      "076A     1\n",
      "043A     1\n",
      "041A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "019A    17\n",
      "101A    15\n",
      "039A    12\n",
      "063A    11\n",
      "071A    10\n",
      "005A    10\n",
      "065A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "099A     7\n",
      "008A     6\n",
      "009A     4\n",
      "104A     4\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "096A     1\n",
      "049A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    296\n",
      "X    286\n",
      "F    208\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    62\n",
      "F    44\n",
      "M    41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 028...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 019A, 101A, 005A, 065A, 039A, 009A, 063...\n",
      "kitten                                         [050A, 049A]\n",
      "senior                       [104A, 056A, 094A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 14, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 2, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "Length of X_train_val:\n",
      "790\n",
      "Length of y_train_val:\n",
      "790\n",
      "Length of groups_train_val:\n",
      "790\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age group distribution: Counter({0: 936, 1: 815, 2: 795})\n",
      "Epoch 1/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 1.1863 - accuracy: 0.5051\n",
      "Epoch 2/1500\n",
      "40/40 [==============================] - 0s 908us/step - loss: 0.8941 - accuracy: 0.6053\n",
      "Epoch 3/1500\n",
      "40/40 [==============================] - 0s 815us/step - loss: 0.8061 - accuracy: 0.6493\n",
      "Epoch 4/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.7830 - accuracy: 0.6606\n",
      "Epoch 5/1500\n",
      "40/40 [==============================] - 0s 800us/step - loss: 0.7264 - accuracy: 0.6740\n",
      "Epoch 6/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.6965 - accuracy: 0.6956\n",
      "Epoch 7/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.6726 - accuracy: 0.6984\n",
      "Epoch 8/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.6670 - accuracy: 0.6999\n",
      "Epoch 9/1500\n",
      "40/40 [==============================] - 0s 835us/step - loss: 0.6343 - accuracy: 0.7231\n",
      "Epoch 10/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.6168 - accuracy: 0.7231\n",
      "Epoch 11/1500\n",
      "40/40 [==============================] - 0s 769us/step - loss: 0.6005 - accuracy: 0.7455\n",
      "Epoch 12/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.5951 - accuracy: 0.7313\n",
      "Epoch 13/1500\n",
      "40/40 [==============================] - 0s 754us/step - loss: 0.5984 - accuracy: 0.7498\n",
      "Epoch 14/1500\n",
      "40/40 [==============================] - 0s 775us/step - loss: 0.5722 - accuracy: 0.7463\n",
      "Epoch 15/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.5608 - accuracy: 0.7451\n",
      "Epoch 16/1500\n",
      "40/40 [==============================] - 0s 754us/step - loss: 0.5640 - accuracy: 0.7569\n",
      "Epoch 17/1500\n",
      "40/40 [==============================] - 0s 784us/step - loss: 0.5647 - accuracy: 0.7624\n",
      "Epoch 18/1500\n",
      "40/40 [==============================] - 0s 770us/step - loss: 0.5315 - accuracy: 0.7667\n",
      "Epoch 19/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.5361 - accuracy: 0.7581\n",
      "Epoch 20/1500\n",
      "40/40 [==============================] - 0s 930us/step - loss: 0.5269 - accuracy: 0.7671\n",
      "Epoch 21/1500\n",
      "40/40 [==============================] - 0s 948us/step - loss: 0.5148 - accuracy: 0.7804\n",
      "Epoch 22/1500\n",
      "40/40 [==============================] - 0s 974us/step - loss: 0.5102 - accuracy: 0.7722\n",
      "Epoch 23/1500\n",
      "40/40 [==============================] - 0s 993us/step - loss: 0.5205 - accuracy: 0.7749\n",
      "Epoch 24/1500\n",
      "40/40 [==============================] - 0s 976us/step - loss: 0.4953 - accuracy: 0.7855\n",
      "Epoch 25/1500\n",
      "40/40 [==============================] - 0s 864us/step - loss: 0.4999 - accuracy: 0.7757\n",
      "Epoch 26/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.4893 - accuracy: 0.7844\n",
      "Epoch 27/1500\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.5154 - accuracy: 0.7734\n",
      "Epoch 28/1500\n",
      "40/40 [==============================] - 0s 790us/step - loss: 0.4881 - accuracy: 0.7914\n",
      "Epoch 29/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.4775 - accuracy: 0.7910\n",
      "Epoch 30/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.4760 - accuracy: 0.7907\n",
      "Epoch 31/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.4807 - accuracy: 0.7977\n",
      "Epoch 32/1500\n",
      "40/40 [==============================] - 0s 821us/step - loss: 0.4683 - accuracy: 0.7910\n",
      "Epoch 33/1500\n",
      "40/40 [==============================] - 0s 859us/step - loss: 0.4642 - accuracy: 0.7958\n",
      "Epoch 34/1500\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.4739 - accuracy: 0.8016\n",
      "Epoch 35/1500\n",
      "40/40 [==============================] - 0s 956us/step - loss: 0.4452 - accuracy: 0.8087\n",
      "Epoch 36/1500\n",
      "40/40 [==============================] - 0s 897us/step - loss: 0.4505 - accuracy: 0.8064\n",
      "Epoch 37/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.4435 - accuracy: 0.8138\n",
      "Epoch 38/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.4577 - accuracy: 0.7962\n",
      "Epoch 39/1500\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.4489 - accuracy: 0.7973\n",
      "Epoch 40/1500\n",
      "40/40 [==============================] - 0s 760us/step - loss: 0.4522 - accuracy: 0.7950\n",
      "Epoch 41/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.4413 - accuracy: 0.8028\n",
      "Epoch 42/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.4430 - accuracy: 0.8060\n",
      "Epoch 43/1500\n",
      "40/40 [==============================] - 0s 855us/step - loss: 0.4385 - accuracy: 0.8095\n",
      "Epoch 44/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.4284 - accuracy: 0.8174\n",
      "Epoch 45/1500\n",
      "40/40 [==============================] - 0s 910us/step - loss: 0.4494 - accuracy: 0.8013\n",
      "Epoch 46/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4301 - accuracy: 0.8217\n",
      "Epoch 47/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.4234 - accuracy: 0.8236\n",
      "Epoch 48/1500\n",
      "40/40 [==============================] - 0s 876us/step - loss: 0.4387 - accuracy: 0.8150\n",
      "Epoch 49/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.4318 - accuracy: 0.8103\n",
      "Epoch 50/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.4375 - accuracy: 0.8091\n",
      "Epoch 51/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.4127 - accuracy: 0.8236\n",
      "Epoch 52/1500\n",
      "40/40 [==============================] - 0s 804us/step - loss: 0.4158 - accuracy: 0.8209\n",
      "Epoch 53/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.4120 - accuracy: 0.8244\n",
      "Epoch 54/1500\n",
      "40/40 [==============================] - 0s 804us/step - loss: 0.4062 - accuracy: 0.8303\n",
      "Epoch 55/1500\n",
      "40/40 [==============================] - 0s 801us/step - loss: 0.4191 - accuracy: 0.8189\n",
      "Epoch 56/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.4144 - accuracy: 0.8205\n",
      "Epoch 57/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.4009 - accuracy: 0.8248\n",
      "Epoch 58/1500\n",
      "40/40 [==============================] - 0s 836us/step - loss: 0.3978 - accuracy: 0.8307\n",
      "Epoch 59/1500\n",
      "40/40 [==============================] - 0s 915us/step - loss: 0.4018 - accuracy: 0.8264\n",
      "Epoch 60/1500\n",
      "40/40 [==============================] - 0s 920us/step - loss: 0.3988 - accuracy: 0.8288\n",
      "Epoch 61/1500\n",
      "40/40 [==============================] - 0s 948us/step - loss: 0.3958 - accuracy: 0.8346\n",
      "Epoch 62/1500\n",
      "40/40 [==============================] - 0s 983us/step - loss: 0.3950 - accuracy: 0.8268\n",
      "Epoch 63/1500\n",
      "40/40 [==============================] - 0s 963us/step - loss: 0.4065 - accuracy: 0.8311\n",
      "Epoch 64/1500\n",
      "40/40 [==============================] - 0s 860us/step - loss: 0.3908 - accuracy: 0.8303\n",
      "Epoch 65/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.3857 - accuracy: 0.8331\n",
      "Epoch 66/1500\n",
      "40/40 [==============================] - 0s 790us/step - loss: 0.4008 - accuracy: 0.8252\n",
      "Epoch 67/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.3886 - accuracy: 0.8264\n",
      "Epoch 68/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.3786 - accuracy: 0.8397\n",
      "Epoch 69/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.4022 - accuracy: 0.8331\n",
      "Epoch 70/1500\n",
      "40/40 [==============================] - 0s 756us/step - loss: 0.3826 - accuracy: 0.8394\n",
      "Epoch 71/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.3740 - accuracy: 0.8335\n",
      "Epoch 72/1500\n",
      "40/40 [==============================] - 0s 808us/step - loss: 0.3762 - accuracy: 0.8339\n",
      "Epoch 73/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.3667 - accuracy: 0.8488\n",
      "Epoch 74/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.3676 - accuracy: 0.8429\n",
      "Epoch 75/1500\n",
      "40/40 [==============================] - 0s 764us/step - loss: 0.3673 - accuracy: 0.8445\n",
      "Epoch 76/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.3685 - accuracy: 0.8476\n",
      "Epoch 77/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.3768 - accuracy: 0.8335\n",
      "Epoch 78/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.3682 - accuracy: 0.8382\n",
      "Epoch 79/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.3660 - accuracy: 0.8445\n",
      "Epoch 80/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.3633 - accuracy: 0.8449\n",
      "Epoch 81/1500\n",
      "40/40 [==============================] - 0s 817us/step - loss: 0.3606 - accuracy: 0.8417\n",
      "Epoch 82/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.3597 - accuracy: 0.8445\n",
      "Epoch 83/1500\n",
      "40/40 [==============================] - 0s 957us/step - loss: 0.3531 - accuracy: 0.8519\n",
      "Epoch 84/1500\n",
      "40/40 [==============================] - 0s 986us/step - loss: 0.3578 - accuracy: 0.8488\n",
      "Epoch 85/1500\n",
      "40/40 [==============================] - 0s 935us/step - loss: 0.3600 - accuracy: 0.8421\n",
      "Epoch 86/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.3697 - accuracy: 0.8358\n",
      "Epoch 87/1500\n",
      "40/40 [==============================] - 0s 782us/step - loss: 0.3585 - accuracy: 0.8504\n",
      "Epoch 88/1500\n",
      "40/40 [==============================] - 0s 795us/step - loss: 0.3497 - accuracy: 0.8504\n",
      "Epoch 89/1500\n",
      "40/40 [==============================] - 0s 801us/step - loss: 0.3533 - accuracy: 0.8519\n",
      "Epoch 90/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.3618 - accuracy: 0.8346\n",
      "Epoch 91/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.3522 - accuracy: 0.8456\n",
      "Epoch 92/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.3446 - accuracy: 0.8500\n",
      "Epoch 93/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.3551 - accuracy: 0.8472\n",
      "Epoch 94/1500\n",
      "40/40 [==============================] - 0s 836us/step - loss: 0.3334 - accuracy: 0.8657\n",
      "Epoch 95/1500\n",
      "40/40 [==============================] - 0s 954us/step - loss: 0.3395 - accuracy: 0.8523\n",
      "Epoch 96/1500\n",
      "40/40 [==============================] - 0s 992us/step - loss: 0.3255 - accuracy: 0.8665\n",
      "Epoch 97/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.3417 - accuracy: 0.8586\n",
      "Epoch 98/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.3456 - accuracy: 0.8527\n",
      "Epoch 99/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.3430 - accuracy: 0.8496\n",
      "Epoch 100/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.3371 - accuracy: 0.8578\n",
      "Epoch 101/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.3413 - accuracy: 0.8614\n",
      "Epoch 102/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.3399 - accuracy: 0.8551\n",
      "Epoch 103/1500\n",
      "40/40 [==============================] - 0s 894us/step - loss: 0.3451 - accuracy: 0.8500\n",
      "Epoch 104/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3343 - accuracy: 0.8602\n",
      "Epoch 105/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8614\n",
      "Epoch 106/1500\n",
      "40/40 [==============================] - 0s 898us/step - loss: 0.3378 - accuracy: 0.8614\n",
      "Epoch 107/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.3265 - accuracy: 0.8606\n",
      "Epoch 108/1500\n",
      "40/40 [==============================] - 0s 890us/step - loss: 0.3172 - accuracy: 0.8625\n",
      "Epoch 109/1500\n",
      "40/40 [==============================] - 0s 806us/step - loss: 0.3242 - accuracy: 0.8562\n",
      "Epoch 110/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.3227 - accuracy: 0.8657\n",
      "Epoch 111/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.3318 - accuracy: 0.8562\n",
      "Epoch 112/1500\n",
      "40/40 [==============================] - 0s 877us/step - loss: 0.3384 - accuracy: 0.8566\n",
      "Epoch 113/1500\n",
      "40/40 [==============================] - 0s 953us/step - loss: 0.3278 - accuracy: 0.8566\n",
      "Epoch 114/1500\n",
      "40/40 [==============================] - 0s 978us/step - loss: 0.3276 - accuracy: 0.8574\n",
      "Epoch 115/1500\n",
      "40/40 [==============================] - 0s 963us/step - loss: 0.3185 - accuracy: 0.8649\n",
      "Epoch 116/1500\n",
      "40/40 [==============================] - 0s 998us/step - loss: 0.3369 - accuracy: 0.8566\n",
      "Epoch 117/1500\n",
      "40/40 [==============================] - 0s 841us/step - loss: 0.3081 - accuracy: 0.8641\n",
      "Epoch 118/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.3109 - accuracy: 0.8668\n",
      "Epoch 119/1500\n",
      "40/40 [==============================] - 0s 835us/step - loss: 0.3112 - accuracy: 0.8688\n",
      "Epoch 120/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.3303 - accuracy: 0.8629\n",
      "Epoch 121/1500\n",
      "40/40 [==============================] - 0s 792us/step - loss: 0.3185 - accuracy: 0.8661\n",
      "Epoch 122/1500\n",
      "40/40 [==============================] - 0s 865us/step - loss: 0.3340 - accuracy: 0.8598\n",
      "Epoch 123/1500\n",
      "40/40 [==============================] - 0s 936us/step - loss: 0.3038 - accuracy: 0.8720\n",
      "Epoch 124/1500\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.3235 - accuracy: 0.8661\n",
      "Epoch 125/1500\n",
      "40/40 [==============================] - 0s 906us/step - loss: 0.3043 - accuracy: 0.8822\n",
      "Epoch 126/1500\n",
      "40/40 [==============================] - 0s 857us/step - loss: 0.3158 - accuracy: 0.8641\n",
      "Epoch 127/1500\n",
      "40/40 [==============================] - 0s 828us/step - loss: 0.3293 - accuracy: 0.8676\n",
      "Epoch 128/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.3139 - accuracy: 0.8653\n",
      "Epoch 129/1500\n",
      "40/40 [==============================] - 0s 856us/step - loss: 0.3136 - accuracy: 0.8676\n",
      "Epoch 130/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3110 - accuracy: 0.8661\n",
      "Epoch 131/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2930 - accuracy: 0.8802\n",
      "Epoch 132/1500\n",
      "40/40 [==============================] - 0s 998us/step - loss: 0.3130 - accuracy: 0.8582\n",
      "Epoch 133/1500\n",
      "40/40 [==============================] - 0s 981us/step - loss: 0.3150 - accuracy: 0.8661\n",
      "Epoch 134/1500\n",
      "40/40 [==============================] - 0s 860us/step - loss: 0.3141 - accuracy: 0.8665\n",
      "Epoch 135/1500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2909 - accuracy: 0.8818\n",
      "Epoch 136/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3017 - accuracy: 0.8763\n",
      "Epoch 137/1500\n",
      "40/40 [==============================] - 0s 984us/step - loss: 0.3071 - accuracy: 0.8720\n",
      "Epoch 138/1500\n",
      "40/40 [==============================] - 0s 897us/step - loss: 0.2945 - accuracy: 0.8841\n",
      "Epoch 139/1500\n",
      "40/40 [==============================] - 0s 914us/step - loss: 0.2998 - accuracy: 0.8723\n",
      "Epoch 140/1500\n",
      "40/40 [==============================] - 0s 943us/step - loss: 0.2852 - accuracy: 0.8790\n",
      "Epoch 141/1500\n",
      "40/40 [==============================] - 0s 901us/step - loss: 0.2988 - accuracy: 0.8739\n",
      "Epoch 142/1500\n",
      "40/40 [==============================] - 0s 942us/step - loss: 0.2894 - accuracy: 0.8775\n",
      "Epoch 143/1500\n",
      "40/40 [==============================] - 0s 914us/step - loss: 0.2975 - accuracy: 0.8790\n",
      "Epoch 144/1500\n",
      "40/40 [==============================] - 0s 957us/step - loss: 0.2969 - accuracy: 0.8716\n",
      "Epoch 145/1500\n",
      "40/40 [==============================] - 0s 856us/step - loss: 0.2885 - accuracy: 0.8877\n",
      "Epoch 146/1500\n",
      "40/40 [==============================] - 0s 833us/step - loss: 0.2800 - accuracy: 0.8869\n",
      "Epoch 147/1500\n",
      "40/40 [==============================] - 0s 804us/step - loss: 0.3005 - accuracy: 0.8771\n",
      "Epoch 148/1500\n",
      "40/40 [==============================] - 0s 810us/step - loss: 0.2958 - accuracy: 0.8755\n",
      "Epoch 149/1500\n",
      "40/40 [==============================] - 0s 801us/step - loss: 0.2871 - accuracy: 0.8767\n",
      "Epoch 150/1500\n",
      "40/40 [==============================] - 0s 802us/step - loss: 0.2810 - accuracy: 0.8806\n",
      "Epoch 151/1500\n",
      "40/40 [==============================] - 0s 887us/step - loss: 0.3000 - accuracy: 0.8743\n",
      "Epoch 152/1500\n",
      "40/40 [==============================] - 0s 968us/step - loss: 0.2720 - accuracy: 0.8830\n",
      "Epoch 153/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.2906 - accuracy: 0.8837\n",
      "Epoch 154/1500\n",
      "40/40 [==============================] - 0s 853us/step - loss: 0.2754 - accuracy: 0.8826\n",
      "Epoch 155/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.2764 - accuracy: 0.8869\n",
      "Epoch 156/1500\n",
      "40/40 [==============================] - 0s 810us/step - loss: 0.2841 - accuracy: 0.8798\n",
      "Epoch 157/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.2719 - accuracy: 0.8881\n",
      "Epoch 158/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.2742 - accuracy: 0.8888\n",
      "Epoch 159/1500\n",
      "40/40 [==============================] - 0s 800us/step - loss: 0.2760 - accuracy: 0.8853\n",
      "Epoch 160/1500\n",
      "40/40 [==============================] - 0s 802us/step - loss: 0.2821 - accuracy: 0.8794\n",
      "Epoch 161/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.2917 - accuracy: 0.8755\n",
      "Epoch 162/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.2796 - accuracy: 0.8881\n",
      "Epoch 163/1500\n",
      "40/40 [==============================] - 0s 836us/step - loss: 0.2539 - accuracy: 0.8951\n",
      "Epoch 164/1500\n",
      "40/40 [==============================] - 0s 813us/step - loss: 0.2656 - accuracy: 0.8853\n",
      "Epoch 165/1500\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.2708 - accuracy: 0.8896\n",
      "Epoch 166/1500\n",
      "40/40 [==============================] - 0s 823us/step - loss: 0.2800 - accuracy: 0.8818\n",
      "Epoch 167/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.2622 - accuracy: 0.8896\n",
      "Epoch 168/1500\n",
      "40/40 [==============================] - 0s 927us/step - loss: 0.2634 - accuracy: 0.8920\n",
      "Epoch 169/1500\n",
      "40/40 [==============================] - 0s 974us/step - loss: 0.2785 - accuracy: 0.8837\n",
      "Epoch 170/1500\n",
      "40/40 [==============================] - 0s 956us/step - loss: 0.2637 - accuracy: 0.8928\n",
      "Epoch 171/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2770 - accuracy: 0.8877\n",
      "Epoch 172/1500\n",
      "40/40 [==============================] - 0s 933us/step - loss: 0.2664 - accuracy: 0.8888\n",
      "Epoch 173/1500\n",
      "40/40 [==============================] - 0s 874us/step - loss: 0.2628 - accuracy: 0.8912\n",
      "Epoch 174/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.2623 - accuracy: 0.8932\n",
      "Epoch 175/1500\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.2683 - accuracy: 0.8928\n",
      "Epoch 176/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.2624 - accuracy: 0.8877\n",
      "Epoch 177/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.2655 - accuracy: 0.8916\n",
      "Epoch 178/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.2682 - accuracy: 0.8904\n",
      "Epoch 179/1500\n",
      "40/40 [==============================] - 0s 858us/step - loss: 0.2726 - accuracy: 0.8920\n",
      "Epoch 180/1500\n",
      "40/40 [==============================] - 0s 918us/step - loss: 0.2630 - accuracy: 0.8947\n",
      "Epoch 181/1500\n",
      "40/40 [==============================] - 0s 955us/step - loss: 0.2635 - accuracy: 0.8920\n",
      "Epoch 182/1500\n",
      "40/40 [==============================] - 0s 922us/step - loss: 0.2588 - accuracy: 0.8967\n",
      "Epoch 183/1500\n",
      "40/40 [==============================] - 0s 817us/step - loss: 0.2659 - accuracy: 0.8943\n",
      "Epoch 184/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.2497 - accuracy: 0.9002\n",
      "Epoch 185/1500\n",
      "40/40 [==============================] - 0s 804us/step - loss: 0.2574 - accuracy: 0.8951\n",
      "Epoch 186/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.2591 - accuracy: 0.9022\n",
      "Epoch 187/1500\n",
      "40/40 [==============================] - 0s 790us/step - loss: 0.2660 - accuracy: 0.8900\n",
      "Epoch 188/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.2549 - accuracy: 0.8940\n",
      "Epoch 189/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.2627 - accuracy: 0.8924\n",
      "Epoch 190/1500\n",
      "40/40 [==============================] - 0s 814us/step - loss: 0.2577 - accuracy: 0.8951\n",
      "Epoch 191/1500\n",
      "40/40 [==============================] - 0s 830us/step - loss: 0.2541 - accuracy: 0.8900\n",
      "Epoch 192/1500\n",
      "40/40 [==============================] - 0s 829us/step - loss: 0.2746 - accuracy: 0.8861\n",
      "Epoch 193/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.2593 - accuracy: 0.8943\n",
      "Epoch 194/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.2427 - accuracy: 0.9046\n",
      "Epoch 195/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.2517 - accuracy: 0.9042\n",
      "Epoch 196/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.2486 - accuracy: 0.8943\n",
      "Epoch 197/1500\n",
      "40/40 [==============================] - 0s 821us/step - loss: 0.2414 - accuracy: 0.9026\n",
      "Epoch 198/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.2435 - accuracy: 0.9014\n",
      "Epoch 199/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.2538 - accuracy: 0.8920\n",
      "Epoch 200/1500\n",
      "40/40 [==============================] - 0s 849us/step - loss: 0.2590 - accuracy: 0.8928\n",
      "Epoch 201/1500\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.2605 - accuracy: 0.8900\n",
      "Epoch 202/1500\n",
      "40/40 [==============================] - 0s 974us/step - loss: 0.2399 - accuracy: 0.9018\n",
      "Epoch 203/1500\n",
      "40/40 [==============================] - 0s 965us/step - loss: 0.2571 - accuracy: 0.9002\n",
      "Epoch 204/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.2402 - accuracy: 0.9049\n",
      "Epoch 205/1500\n",
      "40/40 [==============================] - 0s 843us/step - loss: 0.2430 - accuracy: 0.9069\n",
      "Epoch 206/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.2438 - accuracy: 0.9030\n",
      "Epoch 207/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.2408 - accuracy: 0.9010\n",
      "Epoch 208/1500\n",
      "40/40 [==============================] - 0s 900us/step - loss: 0.2485 - accuracy: 0.8936\n",
      "Epoch 209/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.9034\n",
      "Epoch 210/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2433 - accuracy: 0.8983\n",
      "Epoch 211/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.2323 - accuracy: 0.9057\n",
      "Epoch 212/1500\n",
      "40/40 [==============================] - 0s 830us/step - loss: 0.2436 - accuracy: 0.9053\n",
      "Epoch 213/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.2425 - accuracy: 0.9038\n",
      "Epoch 214/1500\n",
      "40/40 [==============================] - 0s 823us/step - loss: 0.2343 - accuracy: 0.9053\n",
      "Epoch 215/1500\n",
      "40/40 [==============================] - 0s 881us/step - loss: 0.2469 - accuracy: 0.8983\n",
      "Epoch 216/1500\n",
      "40/40 [==============================] - 0s 985us/step - loss: 0.2415 - accuracy: 0.9014\n",
      "Epoch 217/1500\n",
      "40/40 [==============================] - 0s 934us/step - loss: 0.2457 - accuracy: 0.8928\n",
      "Epoch 218/1500\n",
      "40/40 [==============================] - 0s 891us/step - loss: 0.2440 - accuracy: 0.9042\n",
      "Epoch 219/1500\n",
      "40/40 [==============================] - 0s 814us/step - loss: 0.2393 - accuracy: 0.8979\n",
      "Epoch 220/1500\n",
      "40/40 [==============================] - 0s 801us/step - loss: 0.2414 - accuracy: 0.9034\n",
      "Epoch 221/1500\n",
      "40/40 [==============================] - 0s 803us/step - loss: 0.2351 - accuracy: 0.8971\n",
      "Epoch 222/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.2211 - accuracy: 0.9112\n",
      "Epoch 223/1500\n",
      "40/40 [==============================] - 0s 828us/step - loss: 0.2477 - accuracy: 0.9002\n",
      "Epoch 224/1500\n",
      "40/40 [==============================] - 0s 841us/step - loss: 0.2360 - accuracy: 0.9046\n",
      "Epoch 225/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.2355 - accuracy: 0.9030\n",
      "Epoch 226/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.2366 - accuracy: 0.9010\n",
      "Epoch 227/1500\n",
      "40/40 [==============================] - 0s 810us/step - loss: 0.2364 - accuracy: 0.9049\n",
      "Epoch 228/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.2315 - accuracy: 0.9034\n",
      "Epoch 229/1500\n",
      "40/40 [==============================] - 0s 778us/step - loss: 0.2443 - accuracy: 0.8979\n",
      "Epoch 230/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.2175 - accuracy: 0.9159\n",
      "Epoch 231/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.2410 - accuracy: 0.9026\n",
      "Epoch 232/1500\n",
      "40/40 [==============================] - 0s 774us/step - loss: 0.2368 - accuracy: 0.9065\n",
      "Epoch 233/1500\n",
      "40/40 [==============================] - 0s 800us/step - loss: 0.2248 - accuracy: 0.9049\n",
      "Epoch 234/1500\n",
      "40/40 [==============================] - 0s 784us/step - loss: 0.2459 - accuracy: 0.9057\n",
      "Epoch 235/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.2236 - accuracy: 0.9136\n",
      "Epoch 236/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.2312 - accuracy: 0.9053\n",
      "Epoch 237/1500\n",
      "40/40 [==============================] - 0s 801us/step - loss: 0.2230 - accuracy: 0.9073\n",
      "Epoch 238/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.2381 - accuracy: 0.9065\n",
      "Epoch 239/1500\n",
      "40/40 [==============================] - 0s 817us/step - loss: 0.2276 - accuracy: 0.9034\n",
      "Epoch 240/1500\n",
      "40/40 [==============================] - 0s 800us/step - loss: 0.2218 - accuracy: 0.9097\n",
      "Epoch 241/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.2317 - accuracy: 0.9116\n",
      "Epoch 242/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.2194 - accuracy: 0.9128\n",
      "Epoch 243/1500\n",
      "40/40 [==============================] - 0s 813us/step - loss: 0.2033 - accuracy: 0.9207\n",
      "Epoch 244/1500\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.2328 - accuracy: 0.9077\n",
      "Epoch 245/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.2386 - accuracy: 0.9014\n",
      "Epoch 246/1500\n",
      "40/40 [==============================] - 0s 765us/step - loss: 0.2129 - accuracy: 0.9148\n",
      "Epoch 247/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.2192 - accuracy: 0.9093\n",
      "Epoch 248/1500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2228 - accuracy: 0.9022\n",
      "Epoch 249/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2314 - accuracy: 0.9077\n",
      "Epoch 250/1500\n",
      "40/40 [==============================] - 0s 883us/step - loss: 0.2188 - accuracy: 0.9097\n",
      "Epoch 251/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.2205 - accuracy: 0.9116\n",
      "Epoch 252/1500\n",
      "40/40 [==============================] - 0s 803us/step - loss: 0.2193 - accuracy: 0.9124\n",
      "Epoch 253/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.2183 - accuracy: 0.9069\n",
      "Epoch 254/1500\n",
      "40/40 [==============================] - 0s 803us/step - loss: 0.2044 - accuracy: 0.9203\n",
      "Epoch 255/1500\n",
      "40/40 [==============================] - 0s 783us/step - loss: 0.2174 - accuracy: 0.9085\n",
      "Epoch 256/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.2117 - accuracy: 0.9226\n",
      "Epoch 257/1500\n",
      "40/40 [==============================] - 0s 771us/step - loss: 0.2176 - accuracy: 0.9112\n",
      "Epoch 258/1500\n",
      "40/40 [==============================] - 0s 799us/step - loss: 0.2291 - accuracy: 0.9077\n",
      "Epoch 259/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.2139 - accuracy: 0.9081\n",
      "Epoch 260/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.2241 - accuracy: 0.9061\n",
      "Epoch 261/1500\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.2177 - accuracy: 0.9120\n",
      "Epoch 262/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.2135 - accuracy: 0.9116\n",
      "Epoch 263/1500\n",
      "40/40 [==============================] - 0s 815us/step - loss: 0.2056 - accuracy: 0.9167\n",
      "Epoch 264/1500\n",
      "40/40 [==============================] - 0s 833us/step - loss: 0.2050 - accuracy: 0.9175\n",
      "Epoch 265/1500\n",
      "40/40 [==============================] - 0s 874us/step - loss: 0.1965 - accuracy: 0.9273\n",
      "Epoch 266/1500\n",
      "40/40 [==============================] - 0s 828us/step - loss: 0.2143 - accuracy: 0.9175\n",
      "Epoch 267/1500\n",
      "40/40 [==============================] - 0s 790us/step - loss: 0.2082 - accuracy: 0.9124\n",
      "Epoch 268/1500\n",
      "40/40 [==============================] - 0s 781us/step - loss: 0.2093 - accuracy: 0.9163\n",
      "Epoch 269/1500\n",
      "40/40 [==============================] - 0s 793us/step - loss: 0.2118 - accuracy: 0.9207\n",
      "Epoch 270/1500\n",
      "40/40 [==============================] - 0s 821us/step - loss: 0.2010 - accuracy: 0.9199\n",
      "Epoch 271/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.2031 - accuracy: 0.9195\n",
      "Epoch 272/1500\n",
      "40/40 [==============================] - 0s 814us/step - loss: 0.2090 - accuracy: 0.9128\n",
      "Epoch 273/1500\n",
      "40/40 [==============================] - 0s 964us/step - loss: 0.2099 - accuracy: 0.9179\n",
      "Epoch 274/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9108\n",
      "Epoch 275/1500\n",
      "40/40 [==============================] - 0s 924us/step - loss: 0.2069 - accuracy: 0.9222\n",
      "Epoch 276/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.2101 - accuracy: 0.9128\n",
      "Epoch 277/1500\n",
      "40/40 [==============================] - 0s 968us/step - loss: 0.2177 - accuracy: 0.9101\n",
      "Epoch 278/1500\n",
      "40/40 [==============================] - 0s 849us/step - loss: 0.2054 - accuracy: 0.9171\n",
      "Epoch 279/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.2141 - accuracy: 0.9136\n",
      "Epoch 280/1500\n",
      "40/40 [==============================] - 0s 802us/step - loss: 0.2026 - accuracy: 0.9207\n",
      "Epoch 281/1500\n",
      "40/40 [==============================] - 0s 826us/step - loss: 0.2011 - accuracy: 0.9211\n",
      "Epoch 282/1500\n",
      "40/40 [==============================] - 0s 876us/step - loss: 0.2102 - accuracy: 0.9108\n",
      "Epoch 283/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.2060 - accuracy: 0.9183\n",
      "Epoch 284/1500\n",
      "40/40 [==============================] - 0s 790us/step - loss: 0.2174 - accuracy: 0.9148\n",
      "Epoch 285/1500\n",
      "40/40 [==============================] - 0s 797us/step - loss: 0.2054 - accuracy: 0.9218\n",
      "Epoch 286/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.2032 - accuracy: 0.9159\n",
      "Epoch 287/1500\n",
      "40/40 [==============================] - 0s 803us/step - loss: 0.2084 - accuracy: 0.9144\n",
      "Epoch 288/1500\n",
      "40/40 [==============================] - 0s 755us/step - loss: 0.2078 - accuracy: 0.9163\n",
      "Epoch 289/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.1906 - accuracy: 0.9246\n",
      "Epoch 290/1500\n",
      "40/40 [==============================] - 0s 800us/step - loss: 0.2070 - accuracy: 0.9128\n",
      "Epoch 291/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.2018 - accuracy: 0.9167\n",
      "Epoch 292/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.1961 - accuracy: 0.9254\n",
      "Epoch 293/1500\n",
      "40/40 [==============================] - 0s 772us/step - loss: 0.2008 - accuracy: 0.9203\n",
      "Epoch 294/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.1926 - accuracy: 0.9191\n",
      "Epoch 295/1500\n",
      "40/40 [==============================] - 0s 791us/step - loss: 0.2137 - accuracy: 0.9132\n",
      "Epoch 296/1500\n",
      "40/40 [==============================] - 0s 776us/step - loss: 0.1944 - accuracy: 0.9250\n",
      "Epoch 297/1500\n",
      "40/40 [==============================] - 0s 789us/step - loss: 0.1970 - accuracy: 0.9218\n",
      "Epoch 298/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.2005 - accuracy: 0.9242\n",
      "Epoch 299/1500\n",
      "40/40 [==============================] - 0s 814us/step - loss: 0.1964 - accuracy: 0.9246\n",
      "Epoch 300/1500\n",
      "40/40 [==============================] - 0s 856us/step - loss: 0.2033 - accuracy: 0.9159\n",
      "Epoch 301/1500\n",
      "40/40 [==============================] - 0s 808us/step - loss: 0.1958 - accuracy: 0.9183\n",
      "Epoch 302/1500\n",
      "40/40 [==============================] - 0s 787us/step - loss: 0.2014 - accuracy: 0.9203\n",
      "Epoch 303/1500\n",
      "40/40 [==============================] - 0s 817us/step - loss: 0.1817 - accuracy: 0.9269\n",
      "Epoch 304/1500\n",
      "40/40 [==============================] - 0s 777us/step - loss: 0.1996 - accuracy: 0.9234\n",
      "Epoch 305/1500\n",
      "40/40 [==============================] - 0s 766us/step - loss: 0.1947 - accuracy: 0.9266\n",
      "Epoch 306/1500\n",
      "40/40 [==============================] - 0s 779us/step - loss: 0.1998 - accuracy: 0.9234\n",
      "Epoch 307/1500\n",
      "40/40 [==============================] - 0s 803us/step - loss: 0.2096 - accuracy: 0.9140\n",
      "Epoch 308/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.2048 - accuracy: 0.9159\n",
      "Epoch 309/1500\n",
      "40/40 [==============================] - 0s 826us/step - loss: 0.1857 - accuracy: 0.9305\n",
      "Epoch 310/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.1974 - accuracy: 0.9222\n",
      "Epoch 311/1500\n",
      "40/40 [==============================] - 0s 828us/step - loss: 0.1946 - accuracy: 0.9238\n",
      "Epoch 312/1500\n",
      "40/40 [==============================] - 0s 810us/step - loss: 0.2041 - accuracy: 0.9179\n",
      "Epoch 313/1500\n",
      "40/40 [==============================] - 0s 792us/step - loss: 0.2036 - accuracy: 0.9171\n",
      "Epoch 314/1500\n",
      "40/40 [==============================] - 0s 806us/step - loss: 0.1978 - accuracy: 0.9171\n",
      "Epoch 315/1500\n",
      "40/40 [==============================] - 0s 794us/step - loss: 0.1996 - accuracy: 0.9242\n",
      "Epoch 316/1500\n",
      "40/40 [==============================] - 0s 822us/step - loss: 0.1741 - accuracy: 0.9344\n",
      "Epoch 317/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.2023 - accuracy: 0.9159\n",
      "Epoch 318/1500\n",
      "40/40 [==============================] - 0s 843us/step - loss: 0.1943 - accuracy: 0.9222\n",
      "Epoch 319/1500\n",
      "40/40 [==============================] - 0s 929us/step - loss: 0.2160 - accuracy: 0.9136\n",
      "Epoch 320/1500\n",
      "40/40 [==============================] - 0s 965us/step - loss: 0.1970 - accuracy: 0.9230\n",
      "Epoch 321/1500\n",
      "40/40 [==============================] - 0s 960us/step - loss: 0.1969 - accuracy: 0.9250\n",
      "Epoch 322/1500\n",
      "40/40 [==============================] - 0s 924us/step - loss: 0.2038 - accuracy: 0.9199\n",
      "Epoch 323/1500\n",
      "40/40 [==============================] - 0s 823us/step - loss: 0.1941 - accuracy: 0.9207\n",
      "Epoch 324/1500\n",
      "40/40 [==============================] - 0s 878us/step - loss: 0.1938 - accuracy: 0.9273\n",
      "Epoch 325/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.1935 - accuracy: 0.9262\n",
      "Epoch 326/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.1894 - accuracy: 0.9246\n",
      "Epoch 327/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2066 - accuracy: 0.9175\n",
      "Epoch 328/1500\n",
      "40/40 [==============================] - 0s 984us/step - loss: 0.1941 - accuracy: 0.9214\n",
      "Epoch 329/1500\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.1860 - accuracy: 0.9222\n",
      "Epoch 330/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.1875 - accuracy: 0.9258\n",
      "Epoch 331/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.1871 - accuracy: 0.9250\n",
      "Epoch 332/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.1999 - accuracy: 0.9163\n",
      "Epoch 333/1500\n",
      "40/40 [==============================] - 0s 843us/step - loss: 0.1900 - accuracy: 0.9214\n",
      "Epoch 334/1500\n",
      "40/40 [==============================] - 0s 862us/step - loss: 0.1892 - accuracy: 0.9218\n",
      "Epoch 335/1500\n",
      "40/40 [==============================] - 0s 852us/step - loss: 0.1965 - accuracy: 0.9242\n",
      "Epoch 336/1500\n",
      "40/40 [==============================] - 0s 786us/step - loss: 0.1845 - accuracy: 0.9222\n",
      "Epoch 337/1500\n",
      "40/40 [==============================] - 0s 806us/step - loss: 0.1802 - accuracy: 0.9332\n",
      "Epoch 338/1500\n",
      "40/40 [==============================] - 0s 814us/step - loss: 0.1892 - accuracy: 0.9246\n",
      "Epoch 339/1500\n",
      "40/40 [==============================] - 0s 935us/step - loss: 0.1953 - accuracy: 0.9214\n",
      "Epoch 340/1500\n",
      "40/40 [==============================] - 0s 946us/step - loss: 0.1946 - accuracy: 0.9207\n",
      "Epoch 341/1500\n",
      "40/40 [==============================] - 0s 968us/step - loss: 0.1975 - accuracy: 0.9218\n",
      "Epoch 342/1500\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.1862 - accuracy: 0.9281\n",
      "Epoch 343/1500\n",
      "40/40 [==============================] - 0s 889us/step - loss: 0.1851 - accuracy: 0.9226\n",
      "Epoch 344/1500\n",
      "40/40 [==============================] - 0s 880us/step - loss: 0.1835 - accuracy: 0.9266\n",
      "Epoch 345/1500\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.1783 - accuracy: 0.9262\n",
      "Epoch 346/1500\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.1588 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 316.\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.9266\n",
      "Epoch 346: early stopping\n",
      "5/5 [==============================] - 0s 967us/step - loss: 0.4721 - accuracy: 0.8231\n",
      "5/5 [==============================] - 0s 764us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.84 (21/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "Final Test Results - Loss: 0.47211113572120667, Accuracy: 0.8231292366981506, Precision: 0.750858890564773, Recall: 0.8539473684210526, F1 Score: 0.7844689762697531\n",
      "Confusion Matrix:\n",
      " [[99  1 20]\n",
      " [ 0  8  0]\n",
      " [ 5  0 14]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "042A    14\n",
      "059A    14\n",
      "001A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "025A    11\n",
      "036A    11\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "065A     9\n",
      "051B     9\n",
      "072A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "099A     7\n",
      "008A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "026A     4\n",
      "009A     4\n",
      "105A     4\n",
      "035A     4\n",
      "003A     4\n",
      "104A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "014A     3\n",
      "012A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "018A     2\n",
      "032A     2\n",
      "093A     2\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "048A     1\n",
      "096A     1\n",
      "088A     1\n",
      "076A     1\n",
      "091A     1\n",
      "115A     1\n",
      "110A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "097B    14\n",
      "068A    11\n",
      "015A     9\n",
      "045A     9\n",
      "033A     9\n",
      "109A     6\n",
      "053A     6\n",
      "075A     5\n",
      "025C     5\n",
      "062A     4\n",
      "060A     3\n",
      "025B     2\n",
      "087A     2\n",
      "038A     2\n",
      "054A     2\n",
      "041A     1\n",
      "043A     1\n",
      "026C     1\n",
      "066A     1\n",
      "004A     1\n",
      "019B     1\n",
      "090A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    299\n",
      "F    216\n",
      "M    214\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    123\n",
      "X     49\n",
      "F     36\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 071A, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 050...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 097B, 067A, 020A, 062A, 002B, 029...\n",
      "kitten                             [109A, 043A, 041A, 045A]\n",
      "senior                             [055A, 054A, 090A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "Length of X_train_val:\n",
      "729\n",
      "Length of y_train_val:\n",
      "729\n",
      "Length of groups_train_val:\n",
      "729\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 842, 1: 770, 2: 770})\n",
      "Epoch 1/1500\n",
      "38/38 [==============================] - 0s 953us/step - loss: 1.1723 - accuracy: 0.4924\n",
      "Epoch 2/1500\n",
      "38/38 [==============================] - 0s 972us/step - loss: 0.9608 - accuracy: 0.5709\n",
      "Epoch 3/1500\n",
      "38/38 [==============================] - 0s 877us/step - loss: 0.8868 - accuracy: 0.6113\n",
      "Epoch 4/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.8282 - accuracy: 0.6331\n",
      "Epoch 5/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.7781 - accuracy: 0.6599\n",
      "Epoch 6/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.7483 - accuracy: 0.6738\n",
      "Epoch 7/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.7111 - accuracy: 0.6935\n",
      "Epoch 8/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.6882 - accuracy: 0.6977\n",
      "Epoch 9/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.6678 - accuracy: 0.7179\n",
      "Epoch 10/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.6905 - accuracy: 0.7011\n",
      "Epoch 11/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.6561 - accuracy: 0.7183\n",
      "Epoch 12/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.6403 - accuracy: 0.7280\n",
      "Epoch 13/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.6416 - accuracy: 0.7259\n",
      "Epoch 14/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.6075 - accuracy: 0.7519\n",
      "Epoch 15/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.5940 - accuracy: 0.7410\n",
      "Epoch 16/1500\n",
      "38/38 [==============================] - 0s 997us/step - loss: 0.5885 - accuracy: 0.7536\n",
      "Epoch 17/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.5950 - accuracy: 0.7406\n",
      "Epoch 18/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.5768 - accuracy: 0.7452\n",
      "Epoch 19/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.5522 - accuracy: 0.7573\n",
      "Epoch 20/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.5832 - accuracy: 0.7494\n",
      "Epoch 21/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.5571 - accuracy: 0.7557\n",
      "Epoch 22/1500\n",
      "38/38 [==============================] - 0s 976us/step - loss: 0.5304 - accuracy: 0.7666\n",
      "Epoch 23/1500\n",
      "38/38 [==============================] - 0s 969us/step - loss: 0.5482 - accuracy: 0.7599\n",
      "Epoch 24/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.5298 - accuracy: 0.7582\n",
      "Epoch 25/1500\n",
      "38/38 [==============================] - 0s 873us/step - loss: 0.5224 - accuracy: 0.7641\n",
      "Epoch 26/1500\n",
      "38/38 [==============================] - 0s 867us/step - loss: 0.5162 - accuracy: 0.7809\n",
      "Epoch 27/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.5255 - accuracy: 0.7708\n",
      "Epoch 28/1500\n",
      "38/38 [==============================] - 0s 956us/step - loss: 0.5132 - accuracy: 0.7762\n",
      "Epoch 29/1500\n",
      "38/38 [==============================] - 0s 940us/step - loss: 0.5117 - accuracy: 0.7809\n",
      "Epoch 30/1500\n",
      "38/38 [==============================] - 0s 955us/step - loss: 0.5129 - accuracy: 0.7746\n",
      "Epoch 31/1500\n",
      "38/38 [==============================] - 0s 944us/step - loss: 0.5079 - accuracy: 0.7746\n",
      "Epoch 32/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.5047 - accuracy: 0.7909\n",
      "Epoch 33/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.4921 - accuracy: 0.7947\n",
      "Epoch 34/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.5031 - accuracy: 0.7872\n",
      "Epoch 35/1500\n",
      "38/38 [==============================] - 0s 959us/step - loss: 0.4805 - accuracy: 0.7909\n",
      "Epoch 36/1500\n",
      "38/38 [==============================] - 0s 964us/step - loss: 0.5010 - accuracy: 0.7846\n",
      "Epoch 37/1500\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.4772 - accuracy: 0.7880\n",
      "Epoch 38/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.4894 - accuracy: 0.7851\n",
      "Epoch 39/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.4854 - accuracy: 0.7901\n",
      "Epoch 40/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.4762 - accuracy: 0.7893\n",
      "Epoch 41/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.4720 - accuracy: 0.7939\n",
      "Epoch 42/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.4732 - accuracy: 0.7909\n",
      "Epoch 43/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.4498 - accuracy: 0.8039\n",
      "Epoch 44/1500\n",
      "38/38 [==============================] - 0s 969us/step - loss: 0.4650 - accuracy: 0.7989\n",
      "Epoch 45/1500\n",
      "38/38 [==============================] - 0s 966us/step - loss: 0.4578 - accuracy: 0.8010\n",
      "Epoch 46/1500\n",
      "38/38 [==============================] - 0s 967us/step - loss: 0.4525 - accuracy: 0.8023\n",
      "Epoch 47/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4416 - accuracy: 0.8081\n",
      "Epoch 48/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.4580 - accuracy: 0.7872\n",
      "Epoch 49/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.4322 - accuracy: 0.8098\n",
      "Epoch 50/1500\n",
      "38/38 [==============================] - 0s 968us/step - loss: 0.4316 - accuracy: 0.8157\n",
      "Epoch 51/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.4464 - accuracy: 0.8027\n",
      "Epoch 52/1500\n",
      "38/38 [==============================] - 0s 954us/step - loss: 0.4417 - accuracy: 0.8115\n",
      "Epoch 53/1500\n",
      "38/38 [==============================] - 0s 949us/step - loss: 0.4399 - accuracy: 0.7955\n",
      "Epoch 54/1500\n",
      "38/38 [==============================] - 0s 984us/step - loss: 0.4429 - accuracy: 0.8098\n",
      "Epoch 55/1500\n",
      "38/38 [==============================] - 0s 937us/step - loss: 0.4416 - accuracy: 0.8077\n",
      "Epoch 56/1500\n",
      "38/38 [==============================] - 0s 950us/step - loss: 0.4373 - accuracy: 0.8065\n",
      "Epoch 57/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.4371 - accuracy: 0.8065\n",
      "Epoch 58/1500\n",
      "38/38 [==============================] - 0s 962us/step - loss: 0.4216 - accuracy: 0.8140\n",
      "Epoch 59/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.4306 - accuracy: 0.8132\n",
      "Epoch 60/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.4227 - accuracy: 0.8165\n",
      "Epoch 61/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.4236 - accuracy: 0.8132\n",
      "Epoch 62/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.4373 - accuracy: 0.8060\n",
      "Epoch 63/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.4222 - accuracy: 0.8153\n",
      "Epoch 64/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.4245 - accuracy: 0.8220\n",
      "Epoch 65/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.4026 - accuracy: 0.8237\n",
      "Epoch 66/1500\n",
      "38/38 [==============================] - 0s 922us/step - loss: 0.4082 - accuracy: 0.8161\n",
      "Epoch 67/1500\n",
      "38/38 [==============================] - 0s 929us/step - loss: 0.4069 - accuracy: 0.8245\n",
      "Epoch 68/1500\n",
      "38/38 [==============================] - 0s 978us/step - loss: 0.4116 - accuracy: 0.8161\n",
      "Epoch 69/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4072 - accuracy: 0.8258\n",
      "Epoch 70/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.4104 - accuracy: 0.8308\n",
      "Epoch 71/1500\n",
      "38/38 [==============================] - 0s 972us/step - loss: 0.4056 - accuracy: 0.8165\n",
      "Epoch 72/1500\n",
      "38/38 [==============================] - 0s 942us/step - loss: 0.3973 - accuracy: 0.8203\n",
      "Epoch 73/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.3965 - accuracy: 0.8258\n",
      "Epoch 74/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.3971 - accuracy: 0.8233\n",
      "Epoch 75/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.3840 - accuracy: 0.8317\n",
      "Epoch 76/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.4074 - accuracy: 0.8203\n",
      "Epoch 77/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.3895 - accuracy: 0.8338\n",
      "Epoch 78/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.4072 - accuracy: 0.8300\n",
      "Epoch 79/1500\n",
      "38/38 [==============================] - 0s 818us/step - loss: 0.3740 - accuracy: 0.8434\n",
      "Epoch 80/1500\n",
      "38/38 [==============================] - 0s 958us/step - loss: 0.4031 - accuracy: 0.8275\n",
      "Epoch 81/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3915 - accuracy: 0.8308\n",
      "Epoch 82/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8375\n",
      "Epoch 83/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.3936 - accuracy: 0.8300\n",
      "Epoch 84/1500\n",
      "38/38 [==============================] - 0s 969us/step - loss: 0.3836 - accuracy: 0.8329\n",
      "Epoch 85/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.3910 - accuracy: 0.8275\n",
      "Epoch 86/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.3819 - accuracy: 0.8321\n",
      "Epoch 87/1500\n",
      "38/38 [==============================] - 0s 849us/step - loss: 0.3766 - accuracy: 0.8384\n",
      "Epoch 88/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.3812 - accuracy: 0.8325\n",
      "Epoch 89/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.3745 - accuracy: 0.8279\n",
      "Epoch 90/1500\n",
      "38/38 [==============================] - 0s 809us/step - loss: 0.3700 - accuracy: 0.8417\n",
      "Epoch 91/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.3577 - accuracy: 0.8438\n",
      "Epoch 92/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.3687 - accuracy: 0.8459\n",
      "Epoch 93/1500\n",
      "38/38 [==============================] - 0s 908us/step - loss: 0.3653 - accuracy: 0.8442\n",
      "Epoch 94/1500\n",
      "38/38 [==============================] - 0s 910us/step - loss: 0.3714 - accuracy: 0.8338\n",
      "Epoch 95/1500\n",
      "38/38 [==============================] - 0s 976us/step - loss: 0.3531 - accuracy: 0.8396\n",
      "Epoch 96/1500\n",
      "38/38 [==============================] - 0s 931us/step - loss: 0.3647 - accuracy: 0.8396\n",
      "Epoch 97/1500\n",
      "38/38 [==============================] - 0s 977us/step - loss: 0.3505 - accuracy: 0.8447\n",
      "Epoch 98/1500\n",
      "38/38 [==============================] - 0s 908us/step - loss: 0.3614 - accuracy: 0.8438\n",
      "Epoch 99/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.3669 - accuracy: 0.8413\n",
      "Epoch 100/1500\n",
      "38/38 [==============================] - 0s 970us/step - loss: 0.3356 - accuracy: 0.8556\n",
      "Epoch 101/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.3586 - accuracy: 0.8367\n",
      "Epoch 102/1500\n",
      "38/38 [==============================] - 0s 838us/step - loss: 0.3559 - accuracy: 0.8552\n",
      "Epoch 103/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.3599 - accuracy: 0.8409\n",
      "Epoch 104/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.3509 - accuracy: 0.8472\n",
      "Epoch 105/1500\n",
      "38/38 [==============================] - 0s 872us/step - loss: 0.3500 - accuracy: 0.8472\n",
      "Epoch 106/1500\n",
      "38/38 [==============================] - 0s 987us/step - loss: 0.3542 - accuracy: 0.8421\n",
      "Epoch 107/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.3286 - accuracy: 0.8652\n",
      "Epoch 108/1500\n",
      "38/38 [==============================] - 0s 963us/step - loss: 0.3486 - accuracy: 0.8497\n",
      "Epoch 109/1500\n",
      "38/38 [==============================] - 0s 988us/step - loss: 0.3653 - accuracy: 0.8447\n",
      "Epoch 110/1500\n",
      "38/38 [==============================] - 0s 887us/step - loss: 0.3407 - accuracy: 0.8514\n",
      "Epoch 111/1500\n",
      "38/38 [==============================] - 0s 917us/step - loss: 0.3409 - accuracy: 0.8480\n",
      "Epoch 112/1500\n",
      "38/38 [==============================] - 0s 905us/step - loss: 0.3486 - accuracy: 0.8489\n",
      "Epoch 113/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.3410 - accuracy: 0.8556\n",
      "Epoch 114/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.3440 - accuracy: 0.8484\n",
      "Epoch 115/1500\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.3425 - accuracy: 0.8602\n",
      "Epoch 116/1500\n",
      "38/38 [==============================] - 0s 952us/step - loss: 0.3433 - accuracy: 0.8560\n",
      "Epoch 117/1500\n",
      "38/38 [==============================] - 0s 961us/step - loss: 0.3445 - accuracy: 0.8510\n",
      "Epoch 118/1500\n",
      "38/38 [==============================] - 0s 917us/step - loss: 0.3364 - accuracy: 0.8535\n",
      "Epoch 119/1500\n",
      "38/38 [==============================] - 0s 962us/step - loss: 0.3414 - accuracy: 0.8531\n",
      "Epoch 120/1500\n",
      "38/38 [==============================] - 0s 840us/step - loss: 0.3389 - accuracy: 0.8526\n",
      "Epoch 121/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.3354 - accuracy: 0.8577\n",
      "Epoch 122/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.3512 - accuracy: 0.8510\n",
      "Epoch 123/1500\n",
      "38/38 [==============================] - 0s 816us/step - loss: 0.3254 - accuracy: 0.8526\n",
      "Epoch 124/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.3268 - accuracy: 0.8640\n",
      "Epoch 125/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.3113 - accuracy: 0.8673\n",
      "Epoch 126/1500\n",
      "38/38 [==============================] - 0s 848us/step - loss: 0.3417 - accuracy: 0.8526\n",
      "Epoch 127/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.3278 - accuracy: 0.8615\n",
      "Epoch 128/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.3273 - accuracy: 0.8589\n",
      "Epoch 129/1500\n",
      "38/38 [==============================] - 0s 965us/step - loss: 0.3146 - accuracy: 0.8678\n",
      "Epoch 130/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.3291 - accuracy: 0.8552\n",
      "Epoch 131/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.3233 - accuracy: 0.8631\n",
      "Epoch 132/1500\n",
      "38/38 [==============================] - 0s 935us/step - loss: 0.3270 - accuracy: 0.8640\n",
      "Epoch 133/1500\n",
      "38/38 [==============================] - 0s 947us/step - loss: 0.3202 - accuracy: 0.8757\n",
      "Epoch 134/1500\n",
      "38/38 [==============================] - 0s 916us/step - loss: 0.3248 - accuracy: 0.8652\n",
      "Epoch 135/1500\n",
      "38/38 [==============================] - 0s 910us/step - loss: 0.3296 - accuracy: 0.8543\n",
      "Epoch 136/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.3207 - accuracy: 0.8640\n",
      "Epoch 137/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.3096 - accuracy: 0.8657\n",
      "Epoch 138/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.3142 - accuracy: 0.8703\n",
      "Epoch 139/1500\n",
      "38/38 [==============================] - 0s 836us/step - loss: 0.3093 - accuracy: 0.8732\n",
      "Epoch 140/1500\n",
      "38/38 [==============================] - 0s 855us/step - loss: 0.2983 - accuracy: 0.8783\n",
      "Epoch 141/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.3021 - accuracy: 0.8711\n",
      "Epoch 142/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.3153 - accuracy: 0.8707\n",
      "Epoch 143/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.3027 - accuracy: 0.8728\n",
      "Epoch 144/1500\n",
      "38/38 [==============================] - 0s 868us/step - loss: 0.3111 - accuracy: 0.8669\n",
      "Epoch 145/1500\n",
      "38/38 [==============================] - 0s 897us/step - loss: 0.3102 - accuracy: 0.8678\n",
      "Epoch 146/1500\n",
      "38/38 [==============================] - 0s 847us/step - loss: 0.3043 - accuracy: 0.8715\n",
      "Epoch 147/1500\n",
      "38/38 [==============================] - 0s 999us/step - loss: 0.2990 - accuracy: 0.8728\n",
      "Epoch 148/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2939 - accuracy: 0.8799\n",
      "Epoch 149/1500\n",
      "38/38 [==============================] - 0s 997us/step - loss: 0.3017 - accuracy: 0.8686\n",
      "Epoch 150/1500\n",
      "38/38 [==============================] - 0s 956us/step - loss: 0.2986 - accuracy: 0.8657\n",
      "Epoch 151/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.3060 - accuracy: 0.8686\n",
      "Epoch 152/1500\n",
      "38/38 [==============================] - 0s 883us/step - loss: 0.2902 - accuracy: 0.8753\n",
      "Epoch 153/1500\n",
      "38/38 [==============================] - 0s 880us/step - loss: 0.3047 - accuracy: 0.8669\n",
      "Epoch 154/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2996 - accuracy: 0.8745\n",
      "Epoch 155/1500\n",
      "38/38 [==============================] - 0s 875us/step - loss: 0.2976 - accuracy: 0.8816\n",
      "Epoch 156/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2935 - accuracy: 0.8757\n",
      "Epoch 157/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.2941 - accuracy: 0.8812\n",
      "Epoch 158/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.3008 - accuracy: 0.8745\n",
      "Epoch 159/1500\n",
      "38/38 [==============================] - 0s 819us/step - loss: 0.2827 - accuracy: 0.8808\n",
      "Epoch 160/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.2958 - accuracy: 0.8724\n",
      "Epoch 161/1500\n",
      "38/38 [==============================] - 0s 976us/step - loss: 0.2915 - accuracy: 0.8820\n",
      "Epoch 162/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2882 - accuracy: 0.8816\n",
      "Epoch 163/1500\n",
      "38/38 [==============================] - 0s 955us/step - loss: 0.2948 - accuracy: 0.8762\n",
      "Epoch 164/1500\n",
      "38/38 [==============================] - 0s 962us/step - loss: 0.2857 - accuracy: 0.8753\n",
      "Epoch 165/1500\n",
      "38/38 [==============================] - 0s 968us/step - loss: 0.2922 - accuracy: 0.8766\n",
      "Epoch 166/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.2917 - accuracy: 0.8711\n",
      "Epoch 167/1500\n",
      "38/38 [==============================] - 0s 942us/step - loss: 0.2938 - accuracy: 0.8757\n",
      "Epoch 168/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.2890 - accuracy: 0.8791\n",
      "Epoch 169/1500\n",
      "38/38 [==============================] - 0s 946us/step - loss: 0.3007 - accuracy: 0.8757\n",
      "Epoch 170/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.2846 - accuracy: 0.8774\n",
      "Epoch 171/1500\n",
      "38/38 [==============================] - 0s 901us/step - loss: 0.2881 - accuracy: 0.8757\n",
      "Epoch 172/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.2800 - accuracy: 0.8875\n",
      "Epoch 173/1500\n",
      "38/38 [==============================] - 0s 929us/step - loss: 0.2867 - accuracy: 0.8829\n",
      "Epoch 174/1500\n",
      "38/38 [==============================] - 0s 983us/step - loss: 0.2758 - accuracy: 0.8854\n",
      "Epoch 175/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2945 - accuracy: 0.8808\n",
      "Epoch 176/1500\n",
      "38/38 [==============================] - 0s 973us/step - loss: 0.2796 - accuracy: 0.8795\n",
      "Epoch 177/1500\n",
      "38/38 [==============================] - 0s 959us/step - loss: 0.2795 - accuracy: 0.8908\n",
      "Epoch 178/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2732 - accuracy: 0.8900\n",
      "Epoch 179/1500\n",
      "38/38 [==============================] - 0s 993us/step - loss: 0.2794 - accuracy: 0.8850\n",
      "Epoch 180/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2791 - accuracy: 0.8854\n",
      "Epoch 181/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2696 - accuracy: 0.8980\n",
      "Epoch 182/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2729 - accuracy: 0.8887\n",
      "Epoch 183/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.2806 - accuracy: 0.8778\n",
      "Epoch 184/1500\n",
      "38/38 [==============================] - 0s 902us/step - loss: 0.2783 - accuracy: 0.8871\n",
      "Epoch 185/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2790 - accuracy: 0.8812\n",
      "Epoch 186/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2697 - accuracy: 0.8875\n",
      "Epoch 187/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2679 - accuracy: 0.8904\n",
      "Epoch 188/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.8892\n",
      "Epoch 189/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2631 - accuracy: 0.8833\n",
      "Epoch 190/1500\n",
      "38/38 [==============================] - 0s 988us/step - loss: 0.2617 - accuracy: 0.8866\n",
      "Epoch 191/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2709 - accuracy: 0.8871\n",
      "Epoch 192/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2558 - accuracy: 0.8959\n",
      "Epoch 193/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2710 - accuracy: 0.8921\n",
      "Epoch 194/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.8938\n",
      "Epoch 195/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.8913\n",
      "Epoch 196/1500\n",
      "38/38 [==============================] - 0s 960us/step - loss: 0.2860 - accuracy: 0.8787\n",
      "Epoch 197/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2655 - accuracy: 0.8896\n",
      "Epoch 198/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2540 - accuracy: 0.8963\n",
      "Epoch 199/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2617 - accuracy: 0.8883\n",
      "Epoch 200/1500\n",
      "38/38 [==============================] - 0s 969us/step - loss: 0.2600 - accuracy: 0.8921\n",
      "Epoch 201/1500\n",
      "38/38 [==============================] - 0s 884us/step - loss: 0.2555 - accuracy: 0.8988\n",
      "Epoch 202/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2652 - accuracy: 0.8938\n",
      "Epoch 203/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.2700 - accuracy: 0.8938\n",
      "Epoch 204/1500\n",
      "38/38 [==============================] - 0s 956us/step - loss: 0.2690 - accuracy: 0.8942\n",
      "Epoch 205/1500\n",
      "38/38 [==============================] - 0s 956us/step - loss: 0.2720 - accuracy: 0.8866\n",
      "Epoch 206/1500\n",
      "38/38 [==============================] - 0s 943us/step - loss: 0.2584 - accuracy: 0.8946\n",
      "Epoch 207/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2636 - accuracy: 0.8854\n",
      "Epoch 208/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.2605 - accuracy: 0.8988\n",
      "Epoch 209/1500\n",
      "38/38 [==============================] - 0s 931us/step - loss: 0.2532 - accuracy: 0.8913\n",
      "Epoch 210/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.2603 - accuracy: 0.8921\n",
      "Epoch 211/1500\n",
      "38/38 [==============================] - 0s 915us/step - loss: 0.2607 - accuracy: 0.8913\n",
      "Epoch 212/1500\n",
      "38/38 [==============================] - 0s 964us/step - loss: 0.2431 - accuracy: 0.9013\n",
      "Epoch 213/1500\n",
      "38/38 [==============================] - 0s 930us/step - loss: 0.2569 - accuracy: 0.8971\n",
      "Epoch 214/1500\n",
      "38/38 [==============================] - 0s 966us/step - loss: 0.2513 - accuracy: 0.8967\n",
      "Epoch 215/1500\n",
      "38/38 [==============================] - 0s 939us/step - loss: 0.2539 - accuracy: 0.8997\n",
      "Epoch 216/1500\n",
      "38/38 [==============================] - 0s 869us/step - loss: 0.2672 - accuracy: 0.8934\n",
      "Epoch 217/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.2385 - accuracy: 0.9089\n",
      "Epoch 218/1500\n",
      "38/38 [==============================] - 0s 963us/step - loss: 0.2556 - accuracy: 0.8913\n",
      "Epoch 219/1500\n",
      "38/38 [==============================] - 0s 991us/step - loss: 0.2367 - accuracy: 0.9009\n",
      "Epoch 220/1500\n",
      "38/38 [==============================] - 0s 914us/step - loss: 0.2433 - accuracy: 0.9001\n",
      "Epoch 221/1500\n",
      "38/38 [==============================] - 0s 905us/step - loss: 0.2343 - accuracy: 0.9055\n",
      "Epoch 222/1500\n",
      "38/38 [==============================] - 0s 841us/step - loss: 0.2438 - accuracy: 0.9043\n",
      "Epoch 223/1500\n",
      "38/38 [==============================] - 0s 839us/step - loss: 0.2452 - accuracy: 0.8963\n",
      "Epoch 224/1500\n",
      "38/38 [==============================] - 0s 903us/step - loss: 0.2423 - accuracy: 0.8971\n",
      "Epoch 225/1500\n",
      "38/38 [==============================] - 0s 881us/step - loss: 0.2347 - accuracy: 0.9060\n",
      "Epoch 226/1500\n",
      "38/38 [==============================] - 0s 858us/step - loss: 0.2473 - accuracy: 0.8967\n",
      "Epoch 227/1500\n",
      "38/38 [==============================] - 0s 898us/step - loss: 0.2414 - accuracy: 0.8976\n",
      "Epoch 228/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2376 - accuracy: 0.8988\n",
      "Epoch 229/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2371 - accuracy: 0.9051\n",
      "Epoch 230/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.2395 - accuracy: 0.9089\n",
      "Epoch 231/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2331 - accuracy: 0.9064\n",
      "Epoch 232/1500\n",
      "38/38 [==============================] - 0s 948us/step - loss: 0.2408 - accuracy: 0.9043\n",
      "Epoch 233/1500\n",
      "38/38 [==============================] - 0s 975us/step - loss: 0.2365 - accuracy: 0.9060\n",
      "Epoch 234/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2305 - accuracy: 0.9135\n",
      "Epoch 235/1500\n",
      "38/38 [==============================] - 0s 2ms/step - loss: 0.2197 - accuracy: 0.9139\n",
      "Epoch 236/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2280 - accuracy: 0.9110\n",
      "Epoch 237/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.2351 - accuracy: 0.8984\n",
      "Epoch 238/1500\n",
      "38/38 [==============================] - 0s 936us/step - loss: 0.2399 - accuracy: 0.8992\n",
      "Epoch 239/1500\n",
      "38/38 [==============================] - 0s 992us/step - loss: 0.2198 - accuracy: 0.9076\n",
      "Epoch 240/1500\n",
      "38/38 [==============================] - 0s 854us/step - loss: 0.2385 - accuracy: 0.9013\n",
      "Epoch 241/1500\n",
      "38/38 [==============================] - 0s 857us/step - loss: 0.2338 - accuracy: 0.9030\n",
      "Epoch 242/1500\n",
      "38/38 [==============================] - 0s 876us/step - loss: 0.2384 - accuracy: 0.8980\n",
      "Epoch 243/1500\n",
      "38/38 [==============================] - 0s 924us/step - loss: 0.2269 - accuracy: 0.9118\n",
      "Epoch 244/1500\n",
      "38/38 [==============================] - 0s 938us/step - loss: 0.2325 - accuracy: 0.9076\n",
      "Epoch 245/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.2264 - accuracy: 0.9051\n",
      "Epoch 246/1500\n",
      "38/38 [==============================] - 0s 982us/step - loss: 0.2344 - accuracy: 0.9034\n",
      "Epoch 247/1500\n",
      "38/38 [==============================] - 0s 949us/step - loss: 0.2286 - accuracy: 0.9064\n",
      "Epoch 248/1500\n",
      "38/38 [==============================] - 0s 994us/step - loss: 0.2254 - accuracy: 0.9055\n",
      "Epoch 249/1500\n",
      "38/38 [==============================] - 0s 923us/step - loss: 0.2345 - accuracy: 0.9085\n",
      "Epoch 250/1500\n",
      "38/38 [==============================] - 0s 947us/step - loss: 0.2325 - accuracy: 0.9055\n",
      "Epoch 251/1500\n",
      "38/38 [==============================] - 0s 927us/step - loss: 0.2278 - accuracy: 0.9034\n",
      "Epoch 252/1500\n",
      "38/38 [==============================] - 0s 933us/step - loss: 0.2204 - accuracy: 0.9030\n",
      "Epoch 253/1500\n",
      "38/38 [==============================] - 0s 889us/step - loss: 0.2311 - accuracy: 0.9064\n",
      "Epoch 254/1500\n",
      "38/38 [==============================] - 0s 851us/step - loss: 0.2145 - accuracy: 0.9165\n",
      "Epoch 255/1500\n",
      "38/38 [==============================] - 0s 934us/step - loss: 0.2364 - accuracy: 0.9005\n",
      "Epoch 256/1500\n",
      "38/38 [==============================] - 0s 892us/step - loss: 0.2202 - accuracy: 0.9118\n",
      "Epoch 257/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.2260 - accuracy: 0.9102\n",
      "Epoch 258/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.2339 - accuracy: 0.9072\n",
      "Epoch 259/1500\n",
      "38/38 [==============================] - 0s 926us/step - loss: 0.2254 - accuracy: 0.9047\n",
      "Epoch 260/1500\n",
      "38/38 [==============================] - 0s 932us/step - loss: 0.2183 - accuracy: 0.9127\n",
      "Epoch 261/1500\n",
      "38/38 [==============================] - 0s 860us/step - loss: 0.2180 - accuracy: 0.9085\n",
      "Epoch 262/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.2212 - accuracy: 0.9072\n",
      "Epoch 263/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2425 - accuracy: 0.9013\n",
      "Epoch 264/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.2282 - accuracy: 0.9106\n",
      "Epoch 265/1500\n",
      "38/38 [==============================] - 0s 830us/step - loss: 0.2130 - accuracy: 0.9127\n",
      "Epoch 266/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2202 - accuracy: 0.9148\n",
      "Epoch 267/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.2491 - accuracy: 0.9005\n",
      "Epoch 268/1500\n",
      "38/38 [==============================] - 0s 870us/step - loss: 0.2265 - accuracy: 0.9106\n",
      "Epoch 269/1500\n",
      "38/38 [==============================] - 0s 845us/step - loss: 0.2226 - accuracy: 0.9114\n",
      "Epoch 270/1500\n",
      "38/38 [==============================] - 0s 885us/step - loss: 0.2168 - accuracy: 0.9148\n",
      "Epoch 271/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.2303 - accuracy: 0.9114\n",
      "Epoch 272/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2108 - accuracy: 0.9194\n",
      "Epoch 273/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2228 - accuracy: 0.9118\n",
      "Epoch 274/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.2231 - accuracy: 0.9118\n",
      "Epoch 275/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2149 - accuracy: 0.9139\n",
      "Epoch 276/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2244 - accuracy: 0.9068\n",
      "Epoch 277/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.2099 - accuracy: 0.9148\n",
      "Epoch 278/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.2125 - accuracy: 0.9093\n",
      "Epoch 279/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.2236 - accuracy: 0.9135\n",
      "Epoch 280/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.2246 - accuracy: 0.9097\n",
      "Epoch 281/1500\n",
      "38/38 [==============================] - 0s 863us/step - loss: 0.2202 - accuracy: 0.9148\n",
      "Epoch 282/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.2255 - accuracy: 0.9085\n",
      "Epoch 283/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.2063 - accuracy: 0.9160\n",
      "Epoch 284/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2089 - accuracy: 0.9190\n",
      "Epoch 285/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2023 - accuracy: 0.9207\n",
      "Epoch 286/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2110 - accuracy: 0.9186\n",
      "Epoch 287/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.2162 - accuracy: 0.9093\n",
      "Epoch 288/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.2053 - accuracy: 0.9152\n",
      "Epoch 289/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2062 - accuracy: 0.9160\n",
      "Epoch 290/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.2032 - accuracy: 0.9173\n",
      "Epoch 291/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2081 - accuracy: 0.9156\n",
      "Epoch 292/1500\n",
      "38/38 [==============================] - 0s 814us/step - loss: 0.2063 - accuracy: 0.9127\n",
      "Epoch 293/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1999 - accuracy: 0.9202\n",
      "Epoch 294/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.2078 - accuracy: 0.9131\n",
      "Epoch 295/1500\n",
      "38/38 [==============================] - 0s 744us/step - loss: 0.2199 - accuracy: 0.9085\n",
      "Epoch 296/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2017 - accuracy: 0.9207\n",
      "Epoch 297/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.2023 - accuracy: 0.9181\n",
      "Epoch 298/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.2019 - accuracy: 0.9236\n",
      "Epoch 299/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.2032 - accuracy: 0.9207\n",
      "Epoch 300/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2129 - accuracy: 0.9156\n",
      "Epoch 301/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.2087 - accuracy: 0.9127\n",
      "Epoch 302/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2138 - accuracy: 0.9123\n",
      "Epoch 303/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.1990 - accuracy: 0.9215\n",
      "Epoch 304/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.2083 - accuracy: 0.9160\n",
      "Epoch 305/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2027 - accuracy: 0.9215\n",
      "Epoch 306/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.2093 - accuracy: 0.9228\n",
      "Epoch 307/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.2042 - accuracy: 0.9152\n",
      "Epoch 308/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.2085 - accuracy: 0.9127\n",
      "Epoch 309/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.1938 - accuracy: 0.9240\n",
      "Epoch 310/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.2113 - accuracy: 0.9118\n",
      "Epoch 311/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.2029 - accuracy: 0.9156\n",
      "Epoch 312/1500\n",
      "38/38 [==============================] - 0s 909us/step - loss: 0.2108 - accuracy: 0.9160\n",
      "Epoch 313/1500\n",
      "38/38 [==============================] - 0s 893us/step - loss: 0.2066 - accuracy: 0.9177\n",
      "Epoch 314/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.2053 - accuracy: 0.9160\n",
      "Epoch 315/1500\n",
      "38/38 [==============================] - 0s 746us/step - loss: 0.1957 - accuracy: 0.9228\n",
      "Epoch 316/1500\n",
      "38/38 [==============================] - 0s 798us/step - loss: 0.2099 - accuracy: 0.9186\n",
      "Epoch 317/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.2014 - accuracy: 0.9186\n",
      "Epoch 318/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.2067 - accuracy: 0.9139\n",
      "Epoch 319/1500\n",
      "38/38 [==============================] - 0s 729us/step - loss: 0.1973 - accuracy: 0.9194\n",
      "Epoch 320/1500\n",
      "38/38 [==============================] - 0s 5ms/step - loss: 0.2030 - accuracy: 0.9215\n",
      "Epoch 321/1500\n",
      "38/38 [==============================] - 0s 921us/step - loss: 0.1899 - accuracy: 0.9219\n",
      "Epoch 322/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.1966 - accuracy: 0.9198\n",
      "Epoch 323/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.2001 - accuracy: 0.9190\n",
      "Epoch 324/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.1994 - accuracy: 0.9173\n",
      "Epoch 325/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.2034 - accuracy: 0.9181\n",
      "Epoch 326/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1974 - accuracy: 0.9211\n",
      "Epoch 327/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.2097 - accuracy: 0.9152\n",
      "Epoch 328/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2090 - accuracy: 0.9165\n",
      "Epoch 329/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.2030 - accuracy: 0.9181\n",
      "Epoch 330/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1960 - accuracy: 0.9181\n",
      "Epoch 331/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.2069 - accuracy: 0.9160\n",
      "Epoch 332/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.1912 - accuracy: 0.9249\n",
      "Epoch 333/1500\n",
      "38/38 [==============================] - 0s 815us/step - loss: 0.1891 - accuracy: 0.9228\n",
      "Epoch 334/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.1978 - accuracy: 0.9181\n",
      "Epoch 335/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.1967 - accuracy: 0.9211\n",
      "Epoch 336/1500\n",
      "38/38 [==============================] - 0s 906us/step - loss: 0.1964 - accuracy: 0.9202\n",
      "Epoch 337/1500\n",
      "38/38 [==============================] - 0s 852us/step - loss: 0.2068 - accuracy: 0.9211\n",
      "Epoch 338/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.1927 - accuracy: 0.9194\n",
      "Epoch 339/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.2055 - accuracy: 0.9202\n",
      "Epoch 340/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1999 - accuracy: 0.9190\n",
      "Epoch 341/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.2016 - accuracy: 0.9232\n",
      "Epoch 342/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.1950 - accuracy: 0.9219\n",
      "Epoch 343/1500\n",
      "38/38 [==============================] - 0s 767us/step - loss: 0.2001 - accuracy: 0.9190\n",
      "Epoch 344/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1851 - accuracy: 0.9215\n",
      "Epoch 345/1500\n",
      "38/38 [==============================] - 0s 812us/step - loss: 0.1911 - accuracy: 0.9223\n",
      "Epoch 346/1500\n",
      "38/38 [==============================] - 0s 772us/step - loss: 0.1849 - accuracy: 0.9219\n",
      "Epoch 347/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1968 - accuracy: 0.9257\n",
      "Epoch 348/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.1922 - accuracy: 0.9169\n",
      "Epoch 349/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.2063 - accuracy: 0.9190\n",
      "Epoch 350/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.1853 - accuracy: 0.9240\n",
      "Epoch 351/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.1894 - accuracy: 0.9345\n",
      "Epoch 352/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.2009 - accuracy: 0.9207\n",
      "Epoch 353/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.1994 - accuracy: 0.9215\n",
      "Epoch 354/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.1948 - accuracy: 0.9274\n",
      "Epoch 355/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.1863 - accuracy: 0.9270\n",
      "Epoch 356/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.1795 - accuracy: 0.9274\n",
      "Epoch 357/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1749 - accuracy: 0.9270\n",
      "Epoch 358/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.1964 - accuracy: 0.9211\n",
      "Epoch 359/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1966 - accuracy: 0.9215\n",
      "Epoch 360/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.2159 - accuracy: 0.9135\n",
      "Epoch 361/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.1940 - accuracy: 0.9215\n",
      "Epoch 362/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.1832 - accuracy: 0.9257\n",
      "Epoch 363/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1785 - accuracy: 0.9332\n",
      "Epoch 364/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1930 - accuracy: 0.9181\n",
      "Epoch 365/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.1785 - accuracy: 0.9253\n",
      "Epoch 366/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.1986 - accuracy: 0.9198\n",
      "Epoch 367/1500\n",
      "38/38 [==============================] - 0s 763us/step - loss: 0.1860 - accuracy: 0.9257\n",
      "Epoch 368/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1823 - accuracy: 0.9194\n",
      "Epoch 369/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.1682 - accuracy: 0.9324\n",
      "Epoch 370/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1760 - accuracy: 0.9316\n",
      "Epoch 371/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.1842 - accuracy: 0.9261\n",
      "Epoch 372/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1899 - accuracy: 0.9295\n",
      "Epoch 373/1500\n",
      "38/38 [==============================] - 0s 832us/step - loss: 0.1834 - accuracy: 0.9307\n",
      "Epoch 374/1500\n",
      "38/38 [==============================] - 0s 807us/step - loss: 0.1895 - accuracy: 0.9265\n",
      "Epoch 375/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.1757 - accuracy: 0.9337\n",
      "Epoch 376/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.1906 - accuracy: 0.9358\n",
      "Epoch 377/1500\n",
      "38/38 [==============================] - 0s 4ms/step - loss: 0.1870 - accuracy: 0.9282\n",
      "Epoch 378/1500\n",
      "38/38 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9291\n",
      "Epoch 379/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1760 - accuracy: 0.9278\n",
      "Epoch 380/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.1941 - accuracy: 0.9228\n",
      "Epoch 381/1500\n",
      "38/38 [==============================] - 0s 805us/step - loss: 0.1774 - accuracy: 0.9332\n",
      "Epoch 382/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.1869 - accuracy: 0.9274\n",
      "Epoch 383/1500\n",
      "38/38 [==============================] - 0s 826us/step - loss: 0.1755 - accuracy: 0.9307\n",
      "Epoch 384/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.1773 - accuracy: 0.9286\n",
      "Epoch 385/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.1653 - accuracy: 0.9291\n",
      "Epoch 386/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1697 - accuracy: 0.9299\n",
      "Epoch 387/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1773 - accuracy: 0.9274\n",
      "Epoch 388/1500\n",
      "38/38 [==============================] - 0s 793us/step - loss: 0.1811 - accuracy: 0.9265\n",
      "Epoch 389/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1714 - accuracy: 0.9312\n",
      "Epoch 390/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.1818 - accuracy: 0.9282\n",
      "Epoch 391/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1695 - accuracy: 0.9328\n",
      "Epoch 392/1500\n",
      "38/38 [==============================] - 0s 728us/step - loss: 0.1791 - accuracy: 0.9303\n",
      "Epoch 393/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.1669 - accuracy: 0.9337\n",
      "Epoch 394/1500\n",
      "38/38 [==============================] - 0s 781us/step - loss: 0.1838 - accuracy: 0.9278\n",
      "Epoch 395/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.1817 - accuracy: 0.9307\n",
      "Epoch 396/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.1613 - accuracy: 0.9400\n",
      "Epoch 397/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.1761 - accuracy: 0.9345\n",
      "Epoch 398/1500\n",
      "38/38 [==============================] - 0s 770us/step - loss: 0.1675 - accuracy: 0.9307\n",
      "Epoch 399/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1861 - accuracy: 0.9274\n",
      "Epoch 400/1500\n",
      "38/38 [==============================] - 0s 757us/step - loss: 0.1816 - accuracy: 0.9307\n",
      "Epoch 401/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.1676 - accuracy: 0.9349\n",
      "Epoch 402/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.1790 - accuracy: 0.9257\n",
      "Epoch 403/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1714 - accuracy: 0.9274\n",
      "Epoch 404/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.1760 - accuracy: 0.9349\n",
      "Epoch 405/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1832 - accuracy: 0.9295\n",
      "Epoch 406/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.1811 - accuracy: 0.9270\n",
      "Epoch 407/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1701 - accuracy: 0.9307\n",
      "Epoch 408/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.1655 - accuracy: 0.9291\n",
      "Epoch 409/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.1782 - accuracy: 0.9345\n",
      "Epoch 410/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.1622 - accuracy: 0.9337\n",
      "Epoch 411/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.1775 - accuracy: 0.9244\n",
      "Epoch 412/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.1747 - accuracy: 0.9320\n",
      "Epoch 413/1500\n",
      "38/38 [==============================] - 0s 784us/step - loss: 0.1715 - accuracy: 0.9353\n",
      "Epoch 414/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.1799 - accuracy: 0.9286\n",
      "Epoch 415/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1702 - accuracy: 0.9328\n",
      "Epoch 416/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1664 - accuracy: 0.9353\n",
      "Epoch 417/1500\n",
      "38/38 [==============================] - 0s 766us/step - loss: 0.1771 - accuracy: 0.9257\n",
      "Epoch 418/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1736 - accuracy: 0.9270\n",
      "Epoch 419/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.1627 - accuracy: 0.9383\n",
      "Epoch 420/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.1674 - accuracy: 0.9332\n",
      "Epoch 421/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1782 - accuracy: 0.9228\n",
      "Epoch 422/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.1692 - accuracy: 0.9370\n",
      "Epoch 423/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1609 - accuracy: 0.9345\n",
      "Epoch 424/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1748 - accuracy: 0.9312\n",
      "Epoch 425/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.1573 - accuracy: 0.9341\n",
      "Epoch 426/1500\n",
      "38/38 [==============================] - 0s 738us/step - loss: 0.1698 - accuracy: 0.9320\n",
      "Epoch 427/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.1526 - accuracy: 0.9391\n",
      "Epoch 428/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1704 - accuracy: 0.9332\n",
      "Epoch 429/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.1638 - accuracy: 0.9324\n",
      "Epoch 430/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.1625 - accuracy: 0.9345\n",
      "Epoch 431/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.1650 - accuracy: 0.9303\n",
      "Epoch 432/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1666 - accuracy: 0.9307\n",
      "Epoch 433/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1747 - accuracy: 0.9324\n",
      "Epoch 434/1500\n",
      "38/38 [==============================] - 0s 821us/step - loss: 0.1713 - accuracy: 0.9265\n",
      "Epoch 435/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1747 - accuracy: 0.9324\n",
      "Epoch 436/1500\n",
      "38/38 [==============================] - 0s 890us/step - loss: 0.1564 - accuracy: 0.9425\n",
      "Epoch 437/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1659 - accuracy: 0.9307\n",
      "Epoch 438/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.1771 - accuracy: 0.9286\n",
      "Epoch 439/1500\n",
      "38/38 [==============================] - 0s 790us/step - loss: 0.1643 - accuracy: 0.9332\n",
      "Epoch 440/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.1692 - accuracy: 0.9353\n",
      "Epoch 441/1500\n",
      "38/38 [==============================] - 0s 788us/step - loss: 0.1851 - accuracy: 0.9295\n",
      "Epoch 442/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.1666 - accuracy: 0.9332\n",
      "Epoch 443/1500\n",
      "38/38 [==============================] - 0s 791us/step - loss: 0.1593 - accuracy: 0.9366\n",
      "Epoch 444/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.1682 - accuracy: 0.9353\n",
      "Epoch 445/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.1658 - accuracy: 0.9337\n",
      "Epoch 446/1500\n",
      "38/38 [==============================] - 0s 829us/step - loss: 0.1498 - accuracy: 0.9437\n",
      "Epoch 447/1500\n",
      "38/38 [==============================] - 0s 758us/step - loss: 0.1587 - accuracy: 0.9421\n",
      "Epoch 448/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.1567 - accuracy: 0.9341\n",
      "Epoch 449/1500\n",
      "38/38 [==============================] - 0s 740us/step - loss: 0.1564 - accuracy: 0.9358\n",
      "Epoch 450/1500\n",
      "38/38 [==============================] - 0s 778us/step - loss: 0.1603 - accuracy: 0.9370\n",
      "Epoch 451/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1719 - accuracy: 0.9299\n",
      "Epoch 452/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.1494 - accuracy: 0.9404\n",
      "Epoch 453/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1687 - accuracy: 0.9320\n",
      "Epoch 454/1500\n",
      "38/38 [==============================] - 0s 804us/step - loss: 0.1707 - accuracy: 0.9324\n",
      "Epoch 455/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.1583 - accuracy: 0.9353\n",
      "Epoch 456/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.1647 - accuracy: 0.9341\n",
      "Epoch 457/1500\n",
      "38/38 [==============================] - 0s 813us/step - loss: 0.1516 - accuracy: 0.9421\n",
      "Epoch 458/1500\n",
      "38/38 [==============================] - 0s 748us/step - loss: 0.1537 - accuracy: 0.9383\n",
      "Epoch 459/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1657 - accuracy: 0.9320\n",
      "Epoch 460/1500\n",
      "38/38 [==============================] - 0s 802us/step - loss: 0.1681 - accuracy: 0.9324\n",
      "Epoch 461/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.1601 - accuracy: 0.9358\n",
      "Epoch 462/1500\n",
      "38/38 [==============================] - 0s 775us/step - loss: 0.1555 - accuracy: 0.9379\n",
      "Epoch 463/1500\n",
      "38/38 [==============================] - 0s 822us/step - loss: 0.1590 - accuracy: 0.9404\n",
      "Epoch 464/1500\n",
      "38/38 [==============================] - 0s 776us/step - loss: 0.1689 - accuracy: 0.9278\n",
      "Epoch 465/1500\n",
      "38/38 [==============================] - 0s 828us/step - loss: 0.1489 - accuracy: 0.9450\n",
      "Epoch 466/1500\n",
      "38/38 [==============================] - 0s 879us/step - loss: 0.1597 - accuracy: 0.9358\n",
      "Epoch 467/1500\n",
      "38/38 [==============================] - 0s 891us/step - loss: 0.1533 - accuracy: 0.9387\n",
      "Epoch 468/1500\n",
      "38/38 [==============================] - 0s 904us/step - loss: 0.1407 - accuracy: 0.9400\n",
      "Epoch 469/1500\n",
      "38/38 [==============================] - 0s 774us/step - loss: 0.1676 - accuracy: 0.9370\n",
      "Epoch 470/1500\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.1528 - accuracy: 0.9349\n",
      "Epoch 471/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.1539 - accuracy: 0.9425\n",
      "Epoch 472/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1541 - accuracy: 0.9425\n",
      "Epoch 473/1500\n",
      "38/38 [==============================] - 0s 971us/step - loss: 0.1426 - accuracy: 0.9433\n",
      "Epoch 474/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1538 - accuracy: 0.9421\n",
      "Epoch 475/1500\n",
      "38/38 [==============================] - 0s 831us/step - loss: 0.1694 - accuracy: 0.9337\n",
      "Epoch 476/1500\n",
      "38/38 [==============================] - 0s 811us/step - loss: 0.1623 - accuracy: 0.9395\n",
      "Epoch 477/1500\n",
      "38/38 [==============================] - 0s 779us/step - loss: 0.1504 - accuracy: 0.9379\n",
      "Epoch 478/1500\n",
      "38/38 [==============================] - 0s 997us/step - loss: 0.1610 - accuracy: 0.9374\n",
      "Epoch 479/1500\n",
      "38/38 [==============================] - 0s 760us/step - loss: 0.1626 - accuracy: 0.9337\n",
      "Epoch 480/1500\n",
      "38/38 [==============================] - 0s 755us/step - loss: 0.1562 - accuracy: 0.9379\n",
      "Epoch 481/1500\n",
      "38/38 [==============================] - 0s 769us/step - loss: 0.1572 - accuracy: 0.9408\n",
      "Epoch 482/1500\n",
      "38/38 [==============================] - 0s 752us/step - loss: 0.1474 - accuracy: 0.9429\n",
      "Epoch 483/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1344 - accuracy: 0.9505\n",
      "Epoch 484/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1592 - accuracy: 0.9416\n",
      "Epoch 485/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1434 - accuracy: 0.9400\n",
      "Epoch 486/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1611 - accuracy: 0.9379\n",
      "Epoch 487/1500\n",
      "38/38 [==============================] - 0s 789us/step - loss: 0.1620 - accuracy: 0.9391\n",
      "Epoch 488/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1644 - accuracy: 0.9362\n",
      "Epoch 489/1500\n",
      "38/38 [==============================] - 0s 787us/step - loss: 0.1540 - accuracy: 0.9391\n",
      "Epoch 490/1500\n",
      "38/38 [==============================] - 0s 3ms/step - loss: 0.1665 - accuracy: 0.9349\n",
      "Epoch 491/1500\n",
      "38/38 [==============================] - 0s 911us/step - loss: 0.1545 - accuracy: 0.9353\n",
      "Epoch 492/1500\n",
      "38/38 [==============================] - 0s 797us/step - loss: 0.1517 - accuracy: 0.9349\n",
      "Epoch 493/1500\n",
      "38/38 [==============================] - 0s 834us/step - loss: 0.1511 - accuracy: 0.9387\n",
      "Epoch 494/1500\n",
      "38/38 [==============================] - 0s 796us/step - loss: 0.1481 - accuracy: 0.9404\n",
      "Epoch 495/1500\n",
      "38/38 [==============================] - 0s 745us/step - loss: 0.1416 - accuracy: 0.9446\n",
      "Epoch 496/1500\n",
      "38/38 [==============================] - 0s 762us/step - loss: 0.1395 - accuracy: 0.9425\n",
      "Epoch 497/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1487 - accuracy: 0.9370\n",
      "Epoch 498/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.1542 - accuracy: 0.9412\n",
      "Epoch 499/1500\n",
      "38/38 [==============================] - 0s 792us/step - loss: 0.1557 - accuracy: 0.9454\n",
      "Epoch 500/1500\n",
      "38/38 [==============================] - 0s 782us/step - loss: 0.1465 - accuracy: 0.9408\n",
      "Epoch 501/1500\n",
      "38/38 [==============================] - 0s 808us/step - loss: 0.1535 - accuracy: 0.9395\n",
      "Epoch 502/1500\n",
      "38/38 [==============================] - 0s 765us/step - loss: 0.1312 - accuracy: 0.9484\n",
      "Epoch 503/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1611 - accuracy: 0.9391\n",
      "Epoch 504/1500\n",
      "38/38 [==============================] - 0s 761us/step - loss: 0.1509 - accuracy: 0.9433\n",
      "Epoch 505/1500\n",
      "38/38 [==============================] - 0s 794us/step - loss: 0.1515 - accuracy: 0.9358\n",
      "Epoch 506/1500\n",
      "38/38 [==============================] - 0s 823us/step - loss: 0.1338 - accuracy: 0.9450\n",
      "Epoch 507/1500\n",
      "38/38 [==============================] - 0s 786us/step - loss: 0.1519 - accuracy: 0.9412\n",
      "Epoch 508/1500\n",
      "38/38 [==============================] - 0s 783us/step - loss: 0.1563 - accuracy: 0.9387\n",
      "Epoch 509/1500\n",
      "38/38 [==============================] - 0s 820us/step - loss: 0.1457 - accuracy: 0.9458\n",
      "Epoch 510/1500\n",
      "38/38 [==============================] - 0s 780us/step - loss: 0.1587 - accuracy: 0.9366\n",
      "Epoch 511/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.1487 - accuracy: 0.9395\n",
      "Epoch 512/1500\n",
      "38/38 [==============================] - 0s 771us/step - loss: 0.1568 - accuracy: 0.9425\n",
      "Epoch 513/1500\n",
      "38/38 [==============================] - 0s 734us/step - loss: 0.1425 - accuracy: 0.9433\n",
      "Epoch 514/1500\n",
      "38/38 [==============================] - 0s 739us/step - loss: 0.1482 - accuracy: 0.9404\n",
      "Epoch 515/1500\n",
      "38/38 [==============================] - 0s 754us/step - loss: 0.1614 - accuracy: 0.9353\n",
      "Epoch 516/1500\n",
      "38/38 [==============================] - 0s 773us/step - loss: 0.1472 - accuracy: 0.9454\n",
      "Epoch 517/1500\n",
      "38/38 [==============================] - 0s 795us/step - loss: 0.1393 - accuracy: 0.9467\n",
      "Epoch 518/1500\n",
      "38/38 [==============================] - 0s 751us/step - loss: 0.1509 - accuracy: 0.9416\n",
      "Epoch 519/1500\n",
      "38/38 [==============================] - 0s 764us/step - loss: 0.1660 - accuracy: 0.9324\n",
      "Epoch 520/1500\n",
      "38/38 [==============================] - 0s 777us/step - loss: 0.1464 - accuracy: 0.9458\n",
      "Epoch 521/1500\n",
      "38/38 [==============================] - 0s 810us/step - loss: 0.1577 - accuracy: 0.9395\n",
      "Epoch 522/1500\n",
      "38/38 [==============================] - 0s 803us/step - loss: 0.1486 - accuracy: 0.9429\n",
      "Epoch 523/1500\n",
      "38/38 [==============================] - 0s 801us/step - loss: 0.1457 - accuracy: 0.9433\n",
      "Epoch 524/1500\n",
      "38/38 [==============================] - 0s 785us/step - loss: 0.1357 - accuracy: 0.9538\n",
      "Epoch 525/1500\n",
      "38/38 [==============================] - 0s 817us/step - loss: 0.1445 - accuracy: 0.9437\n",
      "Epoch 526/1500\n",
      "38/38 [==============================] - 0s 827us/step - loss: 0.1464 - accuracy: 0.9471\n",
      "Epoch 527/1500\n",
      "38/38 [==============================] - 0s 753us/step - loss: 0.1435 - accuracy: 0.9408\n",
      "Epoch 528/1500\n",
      "38/38 [==============================] - 0s 806us/step - loss: 0.1476 - accuracy: 0.9400\n",
      "Epoch 529/1500\n",
      "38/38 [==============================] - 0s 747us/step - loss: 0.1507 - accuracy: 0.9429\n",
      "Epoch 530/1500\n",
      "38/38 [==============================] - 0s 768us/step - loss: 0.1435 - accuracy: 0.9437\n",
      "Epoch 531/1500\n",
      "38/38 [==============================] - 0s 756us/step - loss: 0.1474 - accuracy: 0.9450\n",
      "Epoch 532/1500\n",
      " 1/38 [..............................] - ETA: 0s - loss: 0.1693 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 502.\n",
      "38/38 [==============================] - 0s 799us/step - loss: 0.1396 - accuracy: 0.9467\n",
      "Epoch 532: early stopping\n",
      "7/7 [==============================] - 0s 871us/step - loss: 0.6299 - accuracy: 0.7933\n",
      "7/7 [==============================] - 0s 611us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.82 (23/28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "After appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "Final Test Results - Loss: 0.6299256682395935, Accuracy: 0.7932692170143127, Precision: 0.6605805628703094, Recall: 0.8903858948769128, F1 Score: 0.721770115832108\n",
      "Confusion Matrix:\n",
      " [[126  14  27]\n",
      " [  0  17   0]\n",
      " [  2   0  22]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "016A    10\n",
      "071A    10\n",
      "065A     9\n",
      "045A     9\n",
      "022A     9\n",
      "015A     9\n",
      "033A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "031A     7\n",
      "117A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "053A     6\n",
      "075A     5\n",
      "044A     5\n",
      "021A     5\n",
      "025C     5\n",
      "034A     5\n",
      "023B     5\n",
      "009A     4\n",
      "003A     4\n",
      "104A     4\n",
      "062A     4\n",
      "113A     3\n",
      "014A     3\n",
      "056A     3\n",
      "060A     3\n",
      "064A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "018A     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "011A     2\n",
      "100A     1\n",
      "090A     1\n",
      "115A     1\n",
      "088A     1\n",
      "024A     1\n",
      "019B     1\n",
      "096A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "043A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "116A    12\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "037A     6\n",
      "023A     6\n",
      "070A     5\n",
      "052A     4\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "012A     3\n",
      "006A     3\n",
      "058A     3\n",
      "032A     2\n",
      "076A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    235\n",
      "X    186\n",
      "F    169\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    162\n",
      "M    102\n",
      "F     83\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 071A, 097B, 028A, 019A, 074A, 067...\n",
      "kitten    [044A, 047A, 109A, 050A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 095A, 072A, 023A, 027...\n",
      "kitten                 [014B, 111A, 040A, 046A, 042A, 110A]\n",
      "senior                       [057A, 106A, 116A, 051B, 058A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A' '010A'\n",
      " '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '031A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '047A' '048A'\n",
      " '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A' '073A'\n",
      " '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A' '096A'\n",
      " '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '109A' '113A'\n",
      " '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A'\n",
      " '032A' '035A' '037A' '040A' '042A' '046A' '051B' '052A' '057A' '058A'\n",
      " '070A' '072A' '076A' '095A' '103A' '105A' '106A' '110A' '111A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A', '000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'109A', '031A'}\n",
      "Moved to Test Set:\n",
      "{'109A', '031A'}\n",
      "Removed from Test Set\n",
      "{'046A', '000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A'\n",
      " '010A' '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A'\n",
      " '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A'\n",
      " '073A' '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '096A' '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '113A'\n",
      " '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A' '031A'\n",
      " '032A' '035A' '037A' '040A' '042A' '051B' '052A' '057A' '058A' '070A'\n",
      " '072A' '076A' '095A' '103A' '105A' '106A' '109A' '110A' '111A' '116A']\n",
      "Length of X_train_val:\n",
      "679\n",
      "Length of y_train_val:\n",
      "679\n",
      "Length of groups_train_val:\n",
      "679\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     417\n",
      "senior    113\n",
      "kitten     60\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     171\n",
      "kitten    111\n",
      "senior     65\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     449\n",
      "kitten    117\n",
      "senior    113\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     139\n",
      "senior     65\n",
      "kitten     54\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 898, 1: 585, 2: 565})\n",
      "Epoch 1/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.1902 - accuracy: 0.4395\n",
      "Epoch 2/1500\n",
      "32/32 [==============================] - 0s 939us/step - loss: 0.9925 - accuracy: 0.5537\n",
      "Epoch 3/1500\n",
      "32/32 [==============================] - 0s 907us/step - loss: 0.8717 - accuracy: 0.6006\n",
      "Epoch 4/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.8276 - accuracy: 0.6279\n",
      "Epoch 5/1500\n",
      "32/32 [==============================] - 0s 904us/step - loss: 0.7951 - accuracy: 0.6704\n",
      "Epoch 6/1500\n",
      "32/32 [==============================] - 0s 882us/step - loss: 0.7369 - accuracy: 0.6831\n",
      "Epoch 7/1500\n",
      "32/32 [==============================] - 0s 810us/step - loss: 0.7493 - accuracy: 0.6704\n",
      "Epoch 8/1500\n",
      "32/32 [==============================] - 0s 803us/step - loss: 0.6891 - accuracy: 0.7046\n",
      "Epoch 9/1500\n",
      "32/32 [==============================] - 0s 866us/step - loss: 0.6826 - accuracy: 0.7070\n",
      "Epoch 10/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.6757 - accuracy: 0.7139\n",
      "Epoch 11/1500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.6431 - accuracy: 0.7192\n",
      "Epoch 12/1500\n",
      "32/32 [==============================] - 0s 816us/step - loss: 0.6174 - accuracy: 0.7295\n",
      "Epoch 13/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.6362 - accuracy: 0.7261\n",
      "Epoch 14/1500\n",
      "32/32 [==============================] - 0s 790us/step - loss: 0.6182 - accuracy: 0.7495\n",
      "Epoch 15/1500\n",
      "32/32 [==============================] - 0s 893us/step - loss: 0.5795 - accuracy: 0.7539\n",
      "Epoch 16/1500\n",
      "32/32 [==============================] - 0s 898us/step - loss: 0.5900 - accuracy: 0.7432\n",
      "Epoch 17/1500\n",
      "32/32 [==============================] - 0s 987us/step - loss: 0.5879 - accuracy: 0.7485\n",
      "Epoch 18/1500\n",
      "32/32 [==============================] - 0s 984us/step - loss: 0.5598 - accuracy: 0.7656\n",
      "Epoch 19/1500\n",
      "32/32 [==============================] - 0s 989us/step - loss: 0.5472 - accuracy: 0.7778\n",
      "Epoch 20/1500\n",
      "32/32 [==============================] - 0s 894us/step - loss: 0.5536 - accuracy: 0.7588\n",
      "Epoch 21/1500\n",
      "32/32 [==============================] - 0s 914us/step - loss: 0.5328 - accuracy: 0.7739\n",
      "Epoch 22/1500\n",
      "32/32 [==============================] - 0s 943us/step - loss: 0.5183 - accuracy: 0.7817\n",
      "Epoch 23/1500\n",
      "32/32 [==============================] - 0s 896us/step - loss: 0.5189 - accuracy: 0.7817\n",
      "Epoch 24/1500\n",
      "32/32 [==============================] - 0s 829us/step - loss: 0.5412 - accuracy: 0.7788\n",
      "Epoch 25/1500\n",
      "32/32 [==============================] - 0s 995us/step - loss: 0.5199 - accuracy: 0.7842\n",
      "Epoch 26/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5008 - accuracy: 0.7896\n",
      "Epoch 27/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.5080 - accuracy: 0.7832\n",
      "Epoch 28/1500\n",
      "32/32 [==============================] - 0s 991us/step - loss: 0.5102 - accuracy: 0.7954\n",
      "Epoch 29/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4841 - accuracy: 0.8037\n",
      "Epoch 30/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4808 - accuracy: 0.8091\n",
      "Epoch 31/1500\n",
      "32/32 [==============================] - 0s 915us/step - loss: 0.4784 - accuracy: 0.8062\n",
      "Epoch 32/1500\n",
      "32/32 [==============================] - 0s 866us/step - loss: 0.4855 - accuracy: 0.8052\n",
      "Epoch 33/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4878 - accuracy: 0.7983\n",
      "Epoch 34/1500\n",
      "32/32 [==============================] - 0s 968us/step - loss: 0.4832 - accuracy: 0.8101\n",
      "Epoch 35/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4658 - accuracy: 0.7979\n",
      "Epoch 36/1500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.4783 - accuracy: 0.8057\n",
      "Epoch 37/1500\n",
      "32/32 [==============================] - 0s 799us/step - loss: 0.4556 - accuracy: 0.8145\n",
      "Epoch 38/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4566 - accuracy: 0.8159\n",
      "Epoch 39/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4365 - accuracy: 0.8164\n",
      "Epoch 40/1500\n",
      "32/32 [==============================] - 0s 959us/step - loss: 0.4596 - accuracy: 0.8066\n",
      "Epoch 41/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.4506 - accuracy: 0.8047\n",
      "Epoch 42/1500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.4349 - accuracy: 0.8203\n",
      "Epoch 43/1500\n",
      "32/32 [==============================] - 0s 812us/step - loss: 0.4469 - accuracy: 0.8164\n",
      "Epoch 44/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.4329 - accuracy: 0.8149\n",
      "Epoch 45/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.4294 - accuracy: 0.8301\n",
      "Epoch 46/1500\n",
      "32/32 [==============================] - 0s 927us/step - loss: 0.4246 - accuracy: 0.8247\n",
      "Epoch 47/1500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.4185 - accuracy: 0.8252\n",
      "Epoch 48/1500\n",
      "32/32 [==============================] - 0s 881us/step - loss: 0.4315 - accuracy: 0.8286\n",
      "Epoch 49/1500\n",
      "32/32 [==============================] - 0s 827us/step - loss: 0.4228 - accuracy: 0.8140\n",
      "Epoch 50/1500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.4240 - accuracy: 0.8208\n",
      "Epoch 51/1500\n",
      "32/32 [==============================] - 0s 890us/step - loss: 0.4129 - accuracy: 0.8247\n",
      "Epoch 52/1500\n",
      "32/32 [==============================] - 0s 910us/step - loss: 0.4186 - accuracy: 0.8291\n",
      "Epoch 53/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.4062 - accuracy: 0.8286\n",
      "Epoch 54/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3999 - accuracy: 0.8330\n",
      "Epoch 55/1500\n",
      "32/32 [==============================] - 0s 926us/step - loss: 0.3968 - accuracy: 0.8364\n",
      "Epoch 56/1500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.4135 - accuracy: 0.8271\n",
      "Epoch 57/1500\n",
      "32/32 [==============================] - 0s 855us/step - loss: 0.3972 - accuracy: 0.8408\n",
      "Epoch 58/1500\n",
      "32/32 [==============================] - 0s 879us/step - loss: 0.3876 - accuracy: 0.8428\n",
      "Epoch 59/1500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.4155 - accuracy: 0.8247\n",
      "Epoch 60/1500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.4062 - accuracy: 0.8374\n",
      "Epoch 61/1500\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.3954 - accuracy: 0.8438\n",
      "Epoch 62/1500\n",
      "32/32 [==============================] - 0s 881us/step - loss: 0.3831 - accuracy: 0.8423\n",
      "Epoch 63/1500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.4029 - accuracy: 0.8394\n",
      "Epoch 64/1500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.4042 - accuracy: 0.8359\n",
      "Epoch 65/1500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.3867 - accuracy: 0.8398\n",
      "Epoch 66/1500\n",
      "32/32 [==============================] - 0s 886us/step - loss: 0.3897 - accuracy: 0.8486\n",
      "Epoch 67/1500\n",
      "32/32 [==============================] - 0s 889us/step - loss: 0.3718 - accuracy: 0.8486\n",
      "Epoch 68/1500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.3746 - accuracy: 0.8350\n",
      "Epoch 69/1500\n",
      "32/32 [==============================] - 0s 860us/step - loss: 0.3880 - accuracy: 0.8418\n",
      "Epoch 70/1500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.3736 - accuracy: 0.8496\n",
      "Epoch 71/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3658 - accuracy: 0.8481\n",
      "Epoch 72/1500\n",
      "32/32 [==============================] - 0s 898us/step - loss: 0.3866 - accuracy: 0.8447\n",
      "Epoch 73/1500\n",
      "32/32 [==============================] - 0s 938us/step - loss: 0.3665 - accuracy: 0.8496\n",
      "Epoch 74/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3648 - accuracy: 0.8530\n",
      "Epoch 75/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3632 - accuracy: 0.8477\n",
      "Epoch 76/1500\n",
      "32/32 [==============================] - 0s 932us/step - loss: 0.3651 - accuracy: 0.8438\n",
      "Epoch 77/1500\n",
      "32/32 [==============================] - 0s 930us/step - loss: 0.3644 - accuracy: 0.8467\n",
      "Epoch 78/1500\n",
      "32/32 [==============================] - 0s 936us/step - loss: 0.3474 - accuracy: 0.8633\n",
      "Epoch 79/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3555 - accuracy: 0.8491\n",
      "Epoch 80/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3532 - accuracy: 0.8530\n",
      "Epoch 81/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3530 - accuracy: 0.8584\n",
      "Epoch 82/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3559 - accuracy: 0.8574\n",
      "Epoch 83/1500\n",
      "32/32 [==============================] - 0s 981us/step - loss: 0.3332 - accuracy: 0.8608\n",
      "Epoch 84/1500\n",
      "32/32 [==============================] - 0s 909us/step - loss: 0.3511 - accuracy: 0.8599\n",
      "Epoch 85/1500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.3481 - accuracy: 0.8560\n",
      "Epoch 86/1500\n",
      "32/32 [==============================] - 0s 934us/step - loss: 0.3303 - accuracy: 0.8677\n",
      "Epoch 87/1500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.3395 - accuracy: 0.8613\n",
      "Epoch 88/1500\n",
      "32/32 [==============================] - 0s 943us/step - loss: 0.3388 - accuracy: 0.8628\n",
      "Epoch 89/1500\n",
      "32/32 [==============================] - 0s 922us/step - loss: 0.3537 - accuracy: 0.8477\n",
      "Epoch 90/1500\n",
      "32/32 [==============================] - 0s 877us/step - loss: 0.3640 - accuracy: 0.8540\n",
      "Epoch 91/1500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.3337 - accuracy: 0.8687\n",
      "Epoch 92/1500\n",
      "32/32 [==============================] - 0s 966us/step - loss: 0.3260 - accuracy: 0.8716\n",
      "Epoch 93/1500\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.3335 - accuracy: 0.8579\n",
      "Epoch 94/1500\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3339 - accuracy: 0.8696\n",
      "Epoch 95/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3313 - accuracy: 0.8657\n",
      "Epoch 96/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3293 - accuracy: 0.8682\n",
      "Epoch 97/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3469 - accuracy: 0.8525\n",
      "Epoch 98/1500\n",
      "32/32 [==============================] - 0s 971us/step - loss: 0.3344 - accuracy: 0.8599\n",
      "Epoch 99/1500\n",
      "32/32 [==============================] - 0s 979us/step - loss: 0.3162 - accuracy: 0.8755\n",
      "Epoch 100/1500\n",
      "32/32 [==============================] - 0s 924us/step - loss: 0.3224 - accuracy: 0.8647\n",
      "Epoch 101/1500\n",
      "32/32 [==============================] - 0s 952us/step - loss: 0.3264 - accuracy: 0.8721\n",
      "Epoch 102/1500\n",
      "32/32 [==============================] - 0s 924us/step - loss: 0.3407 - accuracy: 0.8560\n",
      "Epoch 103/1500\n",
      "32/32 [==============================] - 0s 943us/step - loss: 0.3141 - accuracy: 0.8774\n",
      "Epoch 104/1500\n",
      "32/32 [==============================] - 0s 838us/step - loss: 0.3154 - accuracy: 0.8721\n",
      "Epoch 105/1500\n",
      "32/32 [==============================] - 0s 879us/step - loss: 0.3139 - accuracy: 0.8735\n",
      "Epoch 106/1500\n",
      "32/32 [==============================] - 0s 899us/step - loss: 0.3044 - accuracy: 0.8813\n",
      "Epoch 107/1500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.3067 - accuracy: 0.8691\n",
      "Epoch 108/1500\n",
      "32/32 [==============================] - 0s 913us/step - loss: 0.3162 - accuracy: 0.8755\n",
      "Epoch 109/1500\n",
      "32/32 [==============================] - 0s 967us/step - loss: 0.3215 - accuracy: 0.8672\n",
      "Epoch 110/1500\n",
      "32/32 [==============================] - 0s 941us/step - loss: 0.3089 - accuracy: 0.8818\n",
      "Epoch 111/1500\n",
      "32/32 [==============================] - 0s 951us/step - loss: 0.3041 - accuracy: 0.8853\n",
      "Epoch 112/1500\n",
      "32/32 [==============================] - 0s 977us/step - loss: 0.3303 - accuracy: 0.8672\n",
      "Epoch 113/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3094 - accuracy: 0.8765\n",
      "Epoch 114/1500\n",
      "32/32 [==============================] - 0s 895us/step - loss: 0.2929 - accuracy: 0.8813\n",
      "Epoch 115/1500\n",
      "32/32 [==============================] - 0s 912us/step - loss: 0.2982 - accuracy: 0.8813\n",
      "Epoch 116/1500\n",
      "32/32 [==============================] - 0s 940us/step - loss: 0.3023 - accuracy: 0.8828\n",
      "Epoch 117/1500\n",
      "32/32 [==============================] - 0s 961us/step - loss: 0.3139 - accuracy: 0.8755\n",
      "Epoch 118/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8716\n",
      "Epoch 119/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3018 - accuracy: 0.8794\n",
      "Epoch 120/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3095 - accuracy: 0.8755\n",
      "Epoch 121/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.3109 - accuracy: 0.8716\n",
      "Epoch 122/1500\n",
      "32/32 [==============================] - 0s 957us/step - loss: 0.3107 - accuracy: 0.8804\n",
      "Epoch 123/1500\n",
      "32/32 [==============================] - 0s 948us/step - loss: 0.2937 - accuracy: 0.8799\n",
      "Epoch 124/1500\n",
      "32/32 [==============================] - 0s 938us/step - loss: 0.3072 - accuracy: 0.8809\n",
      "Epoch 125/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2949 - accuracy: 0.8867\n",
      "Epoch 126/1500\n",
      "32/32 [==============================] - 0s 928us/step - loss: 0.2986 - accuracy: 0.8770\n",
      "Epoch 127/1500\n",
      "32/32 [==============================] - 0s 989us/step - loss: 0.3056 - accuracy: 0.8774\n",
      "Epoch 128/1500\n",
      "32/32 [==============================] - 0s 974us/step - loss: 0.2884 - accuracy: 0.8896\n",
      "Epoch 129/1500\n",
      "32/32 [==============================] - 0s 967us/step - loss: 0.2873 - accuracy: 0.8877\n",
      "Epoch 130/1500\n",
      "32/32 [==============================] - 0s 899us/step - loss: 0.2957 - accuracy: 0.8823\n",
      "Epoch 131/1500\n",
      "32/32 [==============================] - 0s 815us/step - loss: 0.2964 - accuracy: 0.8755\n",
      "Epoch 132/1500\n",
      "32/32 [==============================] - 0s 868us/step - loss: 0.2843 - accuracy: 0.8833\n",
      "Epoch 133/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2792 - accuracy: 0.8877\n",
      "Epoch 134/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.2942 - accuracy: 0.8818\n",
      "Epoch 135/1500\n",
      "32/32 [==============================] - 0s 786us/step - loss: 0.3015 - accuracy: 0.8804\n",
      "Epoch 136/1500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.2860 - accuracy: 0.8877\n",
      "Epoch 137/1500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.2908 - accuracy: 0.8809\n",
      "Epoch 138/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.2735 - accuracy: 0.8955\n",
      "Epoch 139/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.2799 - accuracy: 0.8906\n",
      "Epoch 140/1500\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.2740 - accuracy: 0.8862\n",
      "Epoch 141/1500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.2768 - accuracy: 0.8877\n",
      "Epoch 142/1500\n",
      "32/32 [==============================] - 0s 893us/step - loss: 0.2693 - accuracy: 0.8950\n",
      "Epoch 143/1500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.2801 - accuracy: 0.8901\n",
      "Epoch 144/1500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.2803 - accuracy: 0.8872\n",
      "Epoch 145/1500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.2736 - accuracy: 0.8892\n",
      "Epoch 146/1500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.2515 - accuracy: 0.9004\n",
      "Epoch 147/1500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.2577 - accuracy: 0.8911\n",
      "Epoch 148/1500\n",
      "32/32 [==============================] - 0s 813us/step - loss: 0.2698 - accuracy: 0.8911\n",
      "Epoch 149/1500\n",
      "32/32 [==============================] - 0s 814us/step - loss: 0.2616 - accuracy: 0.9019\n",
      "Epoch 150/1500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.2625 - accuracy: 0.8862\n",
      "Epoch 151/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.2784 - accuracy: 0.8926\n",
      "Epoch 152/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.2638 - accuracy: 0.8892\n",
      "Epoch 153/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.2480 - accuracy: 0.9028\n",
      "Epoch 154/1500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.2548 - accuracy: 0.9019\n",
      "Epoch 155/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.2567 - accuracy: 0.9009\n",
      "Epoch 156/1500\n",
      "32/32 [==============================] - 0s 827us/step - loss: 0.2611 - accuracy: 0.8975\n",
      "Epoch 157/1500\n",
      "32/32 [==============================] - 0s 829us/step - loss: 0.2696 - accuracy: 0.8906\n",
      "Epoch 158/1500\n",
      "32/32 [==============================] - 0s 772us/step - loss: 0.2493 - accuracy: 0.8999\n",
      "Epoch 159/1500\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.2575 - accuracy: 0.9014\n",
      "Epoch 160/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.2564 - accuracy: 0.8965\n",
      "Epoch 161/1500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.2509 - accuracy: 0.9058\n",
      "Epoch 162/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.2562 - accuracy: 0.9004\n",
      "Epoch 163/1500\n",
      "32/32 [==============================] - 0s 779us/step - loss: 0.2527 - accuracy: 0.9043\n",
      "Epoch 164/1500\n",
      "32/32 [==============================] - 0s 881us/step - loss: 0.2491 - accuracy: 0.9048\n",
      "Epoch 165/1500\n",
      "32/32 [==============================] - 0s 850us/step - loss: 0.2406 - accuracy: 0.9023\n",
      "Epoch 166/1500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.2437 - accuracy: 0.9014\n",
      "Epoch 167/1500\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.2385 - accuracy: 0.9106\n",
      "Epoch 168/1500\n",
      "32/32 [==============================] - 0s 770us/step - loss: 0.2594 - accuracy: 0.8940\n",
      "Epoch 169/1500\n",
      "32/32 [==============================] - 0s 816us/step - loss: 0.2326 - accuracy: 0.9062\n",
      "Epoch 170/1500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.2377 - accuracy: 0.9053\n",
      "Epoch 171/1500\n",
      "32/32 [==============================] - 0s 826us/step - loss: 0.2458 - accuracy: 0.9038\n",
      "Epoch 172/1500\n",
      "32/32 [==============================] - 0s 827us/step - loss: 0.2529 - accuracy: 0.8911\n",
      "Epoch 173/1500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.2406 - accuracy: 0.9116\n",
      "Epoch 174/1500\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2361 - accuracy: 0.9146\n",
      "Epoch 175/1500\n",
      "32/32 [==============================] - 0s 805us/step - loss: 0.2505 - accuracy: 0.8994\n",
      "Epoch 176/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.2306 - accuracy: 0.9141\n",
      "Epoch 177/1500\n",
      "32/32 [==============================] - 0s 829us/step - loss: 0.2516 - accuracy: 0.9087\n",
      "Epoch 178/1500\n",
      "32/32 [==============================] - 0s 870us/step - loss: 0.2288 - accuracy: 0.9077\n",
      "Epoch 179/1500\n",
      "32/32 [==============================] - 0s 836us/step - loss: 0.2318 - accuracy: 0.9092\n",
      "Epoch 180/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2414 - accuracy: 0.9028\n",
      "Epoch 181/1500\n",
      "32/32 [==============================] - 0s 815us/step - loss: 0.2387 - accuracy: 0.9053\n",
      "Epoch 182/1500\n",
      "32/32 [==============================] - 0s 790us/step - loss: 0.2422 - accuracy: 0.9043\n",
      "Epoch 183/1500\n",
      "32/32 [==============================] - 0s 832us/step - loss: 0.2341 - accuracy: 0.9038\n",
      "Epoch 184/1500\n",
      "32/32 [==============================] - 0s 771us/step - loss: 0.2304 - accuracy: 0.9155\n",
      "Epoch 185/1500\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.2304 - accuracy: 0.9072\n",
      "Epoch 186/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.2357 - accuracy: 0.9126\n",
      "Epoch 187/1500\n",
      "32/32 [==============================] - 0s 807us/step - loss: 0.2416 - accuracy: 0.8984\n",
      "Epoch 188/1500\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.2362 - accuracy: 0.9077\n",
      "Epoch 189/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9106\n",
      "Epoch 190/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.2541 - accuracy: 0.8945\n",
      "Epoch 191/1500\n",
      "32/32 [==============================] - 0s 829us/step - loss: 0.2347 - accuracy: 0.9067\n",
      "Epoch 192/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2332 - accuracy: 0.9048\n",
      "Epoch 193/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.2203 - accuracy: 0.9175\n",
      "Epoch 194/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.2355 - accuracy: 0.9092\n",
      "Epoch 195/1500\n",
      "32/32 [==============================] - 0s 793us/step - loss: 0.2230 - accuracy: 0.9146\n",
      "Epoch 196/1500\n",
      "32/32 [==============================] - 0s 797us/step - loss: 0.2301 - accuracy: 0.9111\n",
      "Epoch 197/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.2397 - accuracy: 0.9082\n",
      "Epoch 198/1500\n",
      "32/32 [==============================] - 0s 747us/step - loss: 0.2253 - accuracy: 0.9160\n",
      "Epoch 199/1500\n",
      "32/32 [==============================] - 0s 793us/step - loss: 0.2147 - accuracy: 0.9106\n",
      "Epoch 200/1500\n",
      "32/32 [==============================] - 0s 796us/step - loss: 0.2270 - accuracy: 0.9082\n",
      "Epoch 201/1500\n",
      "32/32 [==============================] - 0s 802us/step - loss: 0.2120 - accuracy: 0.9189\n",
      "Epoch 202/1500\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.2198 - accuracy: 0.9155\n",
      "Epoch 203/1500\n",
      "32/32 [==============================] - 0s 817us/step - loss: 0.2194 - accuracy: 0.9150\n",
      "Epoch 204/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.2259 - accuracy: 0.9146\n",
      "Epoch 205/1500\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.2137 - accuracy: 0.9238\n",
      "Epoch 206/1500\n",
      "32/32 [==============================] - 0s 789us/step - loss: 0.2060 - accuracy: 0.9141\n",
      "Epoch 207/1500\n",
      "32/32 [==============================] - 0s 780us/step - loss: 0.2214 - accuracy: 0.9170\n",
      "Epoch 208/1500\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.2243 - accuracy: 0.9092\n",
      "Epoch 209/1500\n",
      "32/32 [==============================] - 0s 826us/step - loss: 0.2372 - accuracy: 0.9043\n",
      "Epoch 210/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.2164 - accuracy: 0.9165\n",
      "Epoch 211/1500\n",
      "32/32 [==============================] - 0s 829us/step - loss: 0.2023 - accuracy: 0.9233\n",
      "Epoch 212/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.2099 - accuracy: 0.9136\n",
      "Epoch 213/1500\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.2225 - accuracy: 0.9097\n",
      "Epoch 214/1500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.2170 - accuracy: 0.9155\n",
      "Epoch 215/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.2065 - accuracy: 0.9180\n",
      "Epoch 216/1500\n",
      "32/32 [==============================] - 0s 816us/step - loss: 0.2233 - accuracy: 0.9102\n",
      "Epoch 217/1500\n",
      "32/32 [==============================] - 0s 873us/step - loss: 0.2096 - accuracy: 0.9170\n",
      "Epoch 218/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.2164 - accuracy: 0.9116\n",
      "Epoch 219/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.1935 - accuracy: 0.9326\n",
      "Epoch 220/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.2180 - accuracy: 0.9229\n",
      "Epoch 221/1500\n",
      "32/32 [==============================] - 0s 784us/step - loss: 0.2205 - accuracy: 0.9106\n",
      "Epoch 222/1500\n",
      "32/32 [==============================] - 0s 854us/step - loss: 0.1984 - accuracy: 0.9185\n",
      "Epoch 223/1500\n",
      "32/32 [==============================] - 0s 878us/step - loss: 0.2021 - accuracy: 0.9248\n",
      "Epoch 224/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.2086 - accuracy: 0.9204\n",
      "Epoch 225/1500\n",
      "32/32 [==============================] - 0s 859us/step - loss: 0.2163 - accuracy: 0.9131\n",
      "Epoch 226/1500\n",
      "32/32 [==============================] - 0s 886us/step - loss: 0.1951 - accuracy: 0.9170\n",
      "Epoch 227/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.1932 - accuracy: 0.9272\n",
      "Epoch 228/1500\n",
      "32/32 [==============================] - 0s 901us/step - loss: 0.1929 - accuracy: 0.9238\n",
      "Epoch 229/1500\n",
      "32/32 [==============================] - 0s 801us/step - loss: 0.1973 - accuracy: 0.9277\n",
      "Epoch 230/1500\n",
      "32/32 [==============================] - 0s 794us/step - loss: 0.1902 - accuracy: 0.9248\n",
      "Epoch 231/1500\n",
      "32/32 [==============================] - 0s 839us/step - loss: 0.1945 - accuracy: 0.9287\n",
      "Epoch 232/1500\n",
      "32/32 [==============================] - 0s 841us/step - loss: 0.1991 - accuracy: 0.9185\n",
      "Epoch 233/1500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.1857 - accuracy: 0.9307\n",
      "Epoch 234/1500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.2059 - accuracy: 0.9155\n",
      "Epoch 235/1500\n",
      "32/32 [==============================] - 0s 813us/step - loss: 0.2094 - accuracy: 0.9165\n",
      "Epoch 236/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.2087 - accuracy: 0.9175\n",
      "Epoch 237/1500\n",
      "32/32 [==============================] - 0s 803us/step - loss: 0.1874 - accuracy: 0.9268\n",
      "Epoch 238/1500\n",
      "32/32 [==============================] - 0s 827us/step - loss: 0.1999 - accuracy: 0.9180\n",
      "Epoch 239/1500\n",
      "32/32 [==============================] - 0s 822us/step - loss: 0.1960 - accuracy: 0.9253\n",
      "Epoch 240/1500\n",
      "32/32 [==============================] - 0s 853us/step - loss: 0.1929 - accuracy: 0.9258\n",
      "Epoch 241/1500\n",
      "32/32 [==============================] - 0s 857us/step - loss: 0.1917 - accuracy: 0.9253\n",
      "Epoch 242/1500\n",
      "32/32 [==============================] - 0s 863us/step - loss: 0.1781 - accuracy: 0.9297\n",
      "Epoch 243/1500\n",
      "32/32 [==============================] - 0s 812us/step - loss: 0.1979 - accuracy: 0.9170\n",
      "Epoch 244/1500\n",
      "32/32 [==============================] - 0s 848us/step - loss: 0.1966 - accuracy: 0.9263\n",
      "Epoch 245/1500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.2078 - accuracy: 0.9185\n",
      "Epoch 246/1500\n",
      "32/32 [==============================] - 0s 819us/step - loss: 0.1845 - accuracy: 0.9258\n",
      "Epoch 247/1500\n",
      "32/32 [==============================] - 0s 758us/step - loss: 0.2159 - accuracy: 0.9189\n",
      "Epoch 248/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.1999 - accuracy: 0.9214\n",
      "Epoch 249/1500\n",
      "32/32 [==============================] - 0s 774us/step - loss: 0.1995 - accuracy: 0.9224\n",
      "Epoch 250/1500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.1867 - accuracy: 0.9302\n",
      "Epoch 251/1500\n",
      "32/32 [==============================] - 0s 813us/step - loss: 0.1941 - accuracy: 0.9248\n",
      "Epoch 252/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.1990 - accuracy: 0.9292\n",
      "Epoch 253/1500\n",
      "32/32 [==============================] - 0s 792us/step - loss: 0.1982 - accuracy: 0.9238\n",
      "Epoch 254/1500\n",
      "32/32 [==============================] - 0s 814us/step - loss: 0.1820 - accuracy: 0.9277\n",
      "Epoch 255/1500\n",
      "32/32 [==============================] - 0s 824us/step - loss: 0.1697 - accuracy: 0.9312\n",
      "Epoch 256/1500\n",
      "32/32 [==============================] - 0s 781us/step - loss: 0.1876 - accuracy: 0.9243\n",
      "Epoch 257/1500\n",
      "32/32 [==============================] - 0s 763us/step - loss: 0.1939 - accuracy: 0.9272\n",
      "Epoch 258/1500\n",
      "32/32 [==============================] - 0s 818us/step - loss: 0.1992 - accuracy: 0.9219\n",
      "Epoch 259/1500\n",
      "32/32 [==============================] - 0s 799us/step - loss: 0.1874 - accuracy: 0.9321\n",
      "Epoch 260/1500\n",
      "32/32 [==============================] - 0s 843us/step - loss: 0.1783 - accuracy: 0.9316\n",
      "Epoch 261/1500\n",
      "32/32 [==============================] - 0s 865us/step - loss: 0.1778 - accuracy: 0.9307\n",
      "Epoch 262/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.1920 - accuracy: 0.9253\n",
      "Epoch 263/1500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.1902 - accuracy: 0.9253\n",
      "Epoch 264/1500\n",
      "32/32 [==============================] - 0s 802us/step - loss: 0.1892 - accuracy: 0.9287\n",
      "Epoch 265/1500\n",
      "32/32 [==============================] - 0s 776us/step - loss: 0.1756 - accuracy: 0.9312\n",
      "Epoch 266/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.1867 - accuracy: 0.9199\n",
      "Epoch 267/1500\n",
      "32/32 [==============================] - 0s 799us/step - loss: 0.1837 - accuracy: 0.9355\n",
      "Epoch 268/1500\n",
      "32/32 [==============================] - 0s 882us/step - loss: 0.1886 - accuracy: 0.9224\n",
      "Epoch 269/1500\n",
      "32/32 [==============================] - 0s 916us/step - loss: 0.1700 - accuracy: 0.9351\n",
      "Epoch 270/1500\n",
      "32/32 [==============================] - 0s 867us/step - loss: 0.1757 - accuracy: 0.9316\n",
      "Epoch 271/1500\n",
      "32/32 [==============================] - 0s 870us/step - loss: 0.1848 - accuracy: 0.9316\n",
      "Epoch 272/1500\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.1896 - accuracy: 0.9238\n",
      "Epoch 273/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9380\n",
      "Epoch 274/1500\n",
      "32/32 [==============================] - 0s 910us/step - loss: 0.1667 - accuracy: 0.9424\n",
      "Epoch 275/1500\n",
      "32/32 [==============================] - 0s 817us/step - loss: 0.1905 - accuracy: 0.9268\n",
      "Epoch 276/1500\n",
      "32/32 [==============================] - 0s 820us/step - loss: 0.1717 - accuracy: 0.9341\n",
      "Epoch 277/1500\n",
      "32/32 [==============================] - 0s 804us/step - loss: 0.1650 - accuracy: 0.9370\n",
      "Epoch 278/1500\n",
      "32/32 [==============================] - 0s 837us/step - loss: 0.1721 - accuracy: 0.9365\n",
      "Epoch 279/1500\n",
      "32/32 [==============================] - 0s 783us/step - loss: 0.1779 - accuracy: 0.9312\n",
      "Epoch 280/1500\n",
      "32/32 [==============================] - 0s 846us/step - loss: 0.1876 - accuracy: 0.9380\n",
      "Epoch 281/1500\n",
      "32/32 [==============================] - 0s 940us/step - loss: 0.1835 - accuracy: 0.9277\n",
      "Epoch 282/1500\n",
      "32/32 [==============================] - 0s 929us/step - loss: 0.1811 - accuracy: 0.9282\n",
      "Epoch 283/1500\n",
      "32/32 [==============================] - 0s 856us/step - loss: 0.1683 - accuracy: 0.9370\n",
      "Epoch 284/1500\n",
      "32/32 [==============================] - 0s 904us/step - loss: 0.1801 - accuracy: 0.9297\n",
      "Epoch 285/1500\n",
      "32/32 [==============================] - 0s 899us/step - loss: 0.1620 - accuracy: 0.9360\n",
      "Epoch 286/1500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.1701 - accuracy: 0.9346\n",
      "Epoch 287/1500\n",
      "32/32 [==============================] - 0s 825us/step - loss: 0.1631 - accuracy: 0.9399\n",
      "Epoch 288/1500\n",
      "32/32 [==============================] - 0s 782us/step - loss: 0.1895 - accuracy: 0.9219\n",
      "Epoch 289/1500\n",
      "32/32 [==============================] - 0s 802us/step - loss: 0.1822 - accuracy: 0.9326\n",
      "Epoch 290/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.1718 - accuracy: 0.9336\n",
      "Epoch 291/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.1652 - accuracy: 0.9390\n",
      "Epoch 292/1500\n",
      "32/32 [==============================] - 0s 779us/step - loss: 0.1695 - accuracy: 0.9390\n",
      "Epoch 293/1500\n",
      "32/32 [==============================] - 0s 803us/step - loss: 0.1847 - accuracy: 0.9268\n",
      "Epoch 294/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.1594 - accuracy: 0.9409\n",
      "Epoch 295/1500\n",
      "32/32 [==============================] - 0s 775us/step - loss: 0.1652 - accuracy: 0.9336\n",
      "Epoch 296/1500\n",
      "32/32 [==============================] - 0s 785us/step - loss: 0.1709 - accuracy: 0.9399\n",
      "Epoch 297/1500\n",
      "32/32 [==============================] - 0s 816us/step - loss: 0.1917 - accuracy: 0.9209\n",
      "Epoch 298/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.1628 - accuracy: 0.9336\n",
      "Epoch 299/1500\n",
      "32/32 [==============================] - 0s 916us/step - loss: 0.1646 - accuracy: 0.9365\n",
      "Epoch 300/1500\n",
      "32/32 [==============================] - 0s 861us/step - loss: 0.1754 - accuracy: 0.9287\n",
      "Epoch 301/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.1699 - accuracy: 0.9360\n",
      "Epoch 302/1500\n",
      "32/32 [==============================] - 0s 844us/step - loss: 0.1657 - accuracy: 0.9346\n",
      "Epoch 303/1500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.1544 - accuracy: 0.9375\n",
      "Epoch 304/1500\n",
      "32/32 [==============================] - 0s 880us/step - loss: 0.1732 - accuracy: 0.9385\n",
      "Epoch 305/1500\n",
      "32/32 [==============================] - 0s 877us/step - loss: 0.1601 - accuracy: 0.9414\n",
      "Epoch 306/1500\n",
      "32/32 [==============================] - 0s 835us/step - loss: 0.1663 - accuracy: 0.9385\n",
      "Epoch 307/1500\n",
      "32/32 [==============================] - 0s 871us/step - loss: 0.1637 - accuracy: 0.9375\n",
      "Epoch 308/1500\n",
      "32/32 [==============================] - 0s 890us/step - loss: 0.1853 - accuracy: 0.9282\n",
      "Epoch 309/1500\n",
      "32/32 [==============================] - 0s 867us/step - loss: 0.1666 - accuracy: 0.9360\n",
      "Epoch 310/1500\n",
      "32/32 [==============================] - 0s 744us/step - loss: 0.1627 - accuracy: 0.9424\n",
      "Epoch 311/1500\n",
      "32/32 [==============================] - 0s 788us/step - loss: 0.1406 - accuracy: 0.9497\n",
      "Epoch 312/1500\n",
      "32/32 [==============================] - 0s 757us/step - loss: 0.1858 - accuracy: 0.9307\n",
      "Epoch 313/1500\n",
      "32/32 [==============================] - 0s 877us/step - loss: 0.1557 - accuracy: 0.9375\n",
      "Epoch 314/1500\n",
      "32/32 [==============================] - 0s 926us/step - loss: 0.1621 - accuracy: 0.9360\n",
      "Epoch 315/1500\n",
      "32/32 [==============================] - 0s 971us/step - loss: 0.1619 - accuracy: 0.9390\n",
      "Epoch 316/1500\n",
      "32/32 [==============================] - 0s 886us/step - loss: 0.1612 - accuracy: 0.9385\n",
      "Epoch 317/1500\n",
      "32/32 [==============================] - 0s 834us/step - loss: 0.1699 - accuracy: 0.9321\n",
      "Epoch 318/1500\n",
      "32/32 [==============================] - 0s 842us/step - loss: 0.1637 - accuracy: 0.9360\n",
      "Epoch 319/1500\n",
      "32/32 [==============================] - 0s 821us/step - loss: 0.1579 - accuracy: 0.9365\n",
      "Epoch 320/1500\n",
      "32/32 [==============================] - 0s 764us/step - loss: 0.1566 - accuracy: 0.9438\n",
      "Epoch 321/1500\n",
      "32/32 [==============================] - 0s 778us/step - loss: 0.1483 - accuracy: 0.9414\n",
      "Epoch 322/1500\n",
      "32/32 [==============================] - 0s 833us/step - loss: 0.1497 - accuracy: 0.9473\n",
      "Epoch 323/1500\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9438\n",
      "Epoch 324/1500\n",
      "32/32 [==============================] - 0s 877us/step - loss: 0.1609 - accuracy: 0.9399\n",
      "Epoch 325/1500\n",
      "32/32 [==============================] - 0s 875us/step - loss: 0.1443 - accuracy: 0.9438\n",
      "Epoch 326/1500\n",
      "32/32 [==============================] - 0s 773us/step - loss: 0.1473 - accuracy: 0.9478\n",
      "Epoch 327/1500\n",
      "32/32 [==============================] - 0s 768us/step - loss: 0.1509 - accuracy: 0.9414\n",
      "Epoch 328/1500\n",
      "32/32 [==============================] - 0s 845us/step - loss: 0.1583 - accuracy: 0.9390\n",
      "Epoch 329/1500\n",
      "32/32 [==============================] - 0s 811us/step - loss: 0.1677 - accuracy: 0.9321\n",
      "Epoch 330/1500\n",
      "32/32 [==============================] - 0s 790us/step - loss: 0.1708 - accuracy: 0.9341\n",
      "Epoch 331/1500\n",
      "32/32 [==============================] - 0s 831us/step - loss: 0.1507 - accuracy: 0.9365\n",
      "Epoch 332/1500\n",
      "32/32 [==============================] - 0s 935us/step - loss: 0.1583 - accuracy: 0.9438\n",
      "Epoch 333/1500\n",
      "32/32 [==============================] - 0s 821us/step - loss: 0.1589 - accuracy: 0.9438\n",
      "Epoch 334/1500\n",
      "32/32 [==============================] - 0s 851us/step - loss: 0.1562 - accuracy: 0.9404\n",
      "Epoch 335/1500\n",
      "32/32 [==============================] - 0s 806us/step - loss: 0.1420 - accuracy: 0.9434\n",
      "Epoch 336/1500\n",
      "32/32 [==============================] - 0s 786us/step - loss: 0.1454 - accuracy: 0.9482\n",
      "Epoch 337/1500\n",
      "32/32 [==============================] - 0s 753us/step - loss: 0.1729 - accuracy: 0.9355\n",
      "Epoch 338/1500\n",
      "32/32 [==============================] - 0s 761us/step - loss: 0.1643 - accuracy: 0.9424\n",
      "Epoch 339/1500\n",
      "32/32 [==============================] - 0s 847us/step - loss: 0.1516 - accuracy: 0.9453\n",
      "Epoch 340/1500\n",
      "32/32 [==============================] - 0s 791us/step - loss: 0.1492 - accuracy: 0.9429\n",
      "Epoch 341/1500\n",
      " 1/32 [..............................] - ETA: 0s - loss: 0.1981 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 311.\n",
      "32/32 [==============================] - 0s 858us/step - loss: 0.1745 - accuracy: 0.9263\n",
      "Epoch 341: early stopping\n",
      "9/9 [==============================] - 0s 1ms/step - loss: 0.8499 - accuracy: 0.7093\n",
      "9/9 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.87 (26/30)\n",
      "Before appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "After appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "Final Test Results - Loss: 0.8498671054840088, Accuracy: 0.7093023061752319, Precision: 0.7355230230230231, Recall: 0.6860344203269864, F1 Score: 0.7038189912696563\n",
      "Confusion Matrix:\n",
      " [[107   2  30]\n",
      " [ 10  38   6]\n",
      " [ 27   0  38]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "097B    14\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "068A    11\n",
      "025A    11\n",
      "063A    11\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "005A    10\n",
      "072A     9\n",
      "065A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "099A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "062A     4\n",
      "009A     4\n",
      "012A     3\n",
      "058A     3\n",
      "060A     3\n",
      "006A     3\n",
      "056A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "032A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "066A     1\n",
      "026C     1\n",
      "076A     1\n",
      "096A     1\n",
      "041A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "047A    28\n",
      "074A    25\n",
      "000B    19\n",
      "097A    16\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "036A    11\n",
      "016A    10\n",
      "022A     9\n",
      "117A     7\n",
      "031A     7\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "003A     4\n",
      "064A     3\n",
      "113A     3\n",
      "093A     2\n",
      "069A     2\n",
      "073A     1\n",
      "091A     1\n",
      "092A     1\n",
      "048A     1\n",
      "115A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    266\n",
      "F    163\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    89\n",
      "X    75\n",
      "M    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [014B, 111A, 040A, 046A, 042A, 109A, 050A, 043...\n",
      "senior    [057A, 106A, 104A, 055A, 116A, 051B, 054A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [028A, 074A, 022A, 034A, 091A, 002A, 007A, 069...\n",
      "kitten                             [044A, 047A, 048A, 115A]\n",
      "senior     [093A, 097A, 059A, 113A, 117A, 051A, 016A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "Length of X_train_val:\n",
      "702\n",
      "Length of y_train_val:\n",
      "702\n",
      "Length of groups_train_val:\n",
      "702\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 916, 1: 680, 2: 540})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 1.1059 - accuracy: 0.5037\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.9642 - accuracy: 0.5796\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8740 - accuracy: 0.6320\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.8648 - accuracy: 0.6316\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8221 - accuracy: 0.6526\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 976us/step - loss: 0.7939 - accuracy: 0.6568\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 908us/step - loss: 0.7563 - accuracy: 0.6732\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.7028 - accuracy: 0.6877\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.7371 - accuracy: 0.6840\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 921us/step - loss: 0.6996 - accuracy: 0.6859\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 796us/step - loss: 0.6928 - accuracy: 0.7083\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.6926 - accuracy: 0.7027\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 886us/step - loss: 0.6790 - accuracy: 0.7149\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.6630 - accuracy: 0.7168\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.6369 - accuracy: 0.7257\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.6357 - accuracy: 0.7266\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.6410 - accuracy: 0.7210\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 937us/step - loss: 0.6106 - accuracy: 0.7285\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6086 - accuracy: 0.7406\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 960us/step - loss: 0.5996 - accuracy: 0.7397\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5986 - accuracy: 0.7500\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5890 - accuracy: 0.7449\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5705 - accuracy: 0.7598\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 991us/step - loss: 0.5945 - accuracy: 0.7406\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.5772 - accuracy: 0.7566\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 932us/step - loss: 0.5773 - accuracy: 0.7570\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.5333 - accuracy: 0.7636\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 938us/step - loss: 0.5377 - accuracy: 0.7566\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 947us/step - loss: 0.5624 - accuracy: 0.7500\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5394 - accuracy: 0.7734\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5323 - accuracy: 0.7650\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5277 - accuracy: 0.7795\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5032 - accuracy: 0.7875\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 979us/step - loss: 0.5099 - accuracy: 0.7875\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 885us/step - loss: 0.5265 - accuracy: 0.7650\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 941us/step - loss: 0.5046 - accuracy: 0.7856\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5082 - accuracy: 0.7804\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4918 - accuracy: 0.7898\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5130 - accuracy: 0.7748\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4994 - accuracy: 0.7786\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4886 - accuracy: 0.7907\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4998 - accuracy: 0.7860\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4907 - accuracy: 0.7879\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4823 - accuracy: 0.8010\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4981 - accuracy: 0.7781\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4872 - accuracy: 0.7889\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4860 - accuracy: 0.7870\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4714 - accuracy: 0.7987\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4589 - accuracy: 0.7992\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4650 - accuracy: 0.7893\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 985us/step - loss: 0.4646 - accuracy: 0.7987\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4509 - accuracy: 0.8029\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4636 - accuracy: 0.8043\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4603 - accuracy: 0.8090\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4488 - accuracy: 0.8029\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4635 - accuracy: 0.8066\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.4195 - accuracy: 0.8184\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.4435 - accuracy: 0.8132\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.4579 - accuracy: 0.8052\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4206 - accuracy: 0.8221\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4143 - accuracy: 0.8305\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 946us/step - loss: 0.4280 - accuracy: 0.8226\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4339 - accuracy: 0.8202\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 973us/step - loss: 0.4083 - accuracy: 0.8258\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 923us/step - loss: 0.4097 - accuracy: 0.8305\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.4210 - accuracy: 0.8310\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.4271 - accuracy: 0.8202\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.4132 - accuracy: 0.8216\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4232 - accuracy: 0.8109\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4010 - accuracy: 0.8277\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4131 - accuracy: 0.8216\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.4147 - accuracy: 0.8249\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.4139 - accuracy: 0.8179\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.4131 - accuracy: 0.8244\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.4029 - accuracy: 0.8287\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3949 - accuracy: 0.8380\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 916us/step - loss: 0.3896 - accuracy: 0.8333\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3913 - accuracy: 0.8287\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3853 - accuracy: 0.8287\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 956us/step - loss: 0.3815 - accuracy: 0.8483\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.4026 - accuracy: 0.8352\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.3919 - accuracy: 0.8240\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.3892 - accuracy: 0.8361\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.3894 - accuracy: 0.8329\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.3780 - accuracy: 0.8380\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 854us/step - loss: 0.3806 - accuracy: 0.8418\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.3732 - accuracy: 0.8497\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.4088 - accuracy: 0.8235\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.3616 - accuracy: 0.8507\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.3672 - accuracy: 0.8441\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.3807 - accuracy: 0.8352\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.3739 - accuracy: 0.8432\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 973us/step - loss: 0.3637 - accuracy: 0.8474\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 975us/step - loss: 0.3589 - accuracy: 0.8436\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3600 - accuracy: 0.8427\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3546 - accuracy: 0.8488\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 931us/step - loss: 0.3708 - accuracy: 0.8413\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.3791 - accuracy: 0.8408\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.3633 - accuracy: 0.8390\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.3342 - accuracy: 0.8610\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.3567 - accuracy: 0.8553\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.3597 - accuracy: 0.8375\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.3461 - accuracy: 0.8558\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 979us/step - loss: 0.3621 - accuracy: 0.8539\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 998us/step - loss: 0.3427 - accuracy: 0.8591\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.3571 - accuracy: 0.8558\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.3391 - accuracy: 0.8558\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.3511 - accuracy: 0.8478\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.3613 - accuracy: 0.8469\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 992us/step - loss: 0.3495 - accuracy: 0.8511\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.3530 - accuracy: 0.8516\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8535\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.3500 - accuracy: 0.8464\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.3414 - accuracy: 0.8478\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 949us/step - loss: 0.3303 - accuracy: 0.8722\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.3419 - accuracy: 0.8549\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.3447 - accuracy: 0.8581\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.3464 - accuracy: 0.8539\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.3444 - accuracy: 0.8567\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 964us/step - loss: 0.3410 - accuracy: 0.8572\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 995us/step - loss: 0.3301 - accuracy: 0.8549\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3401 - accuracy: 0.8614\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 989us/step - loss: 0.3148 - accuracy: 0.8670\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 977us/step - loss: 0.3297 - accuracy: 0.8633\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3265 - accuracy: 0.8596\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.8764\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3182 - accuracy: 0.8652\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3298 - accuracy: 0.8610\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.3107 - accuracy: 0.8661\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.3330 - accuracy: 0.8680\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.3246 - accuracy: 0.8591\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.3142 - accuracy: 0.8652\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.3241 - accuracy: 0.8684\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 899us/step - loss: 0.3105 - accuracy: 0.8764\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3170 - accuracy: 0.8675\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2993 - accuracy: 0.8699\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3123 - accuracy: 0.8727\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3120 - accuracy: 0.8628\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3039 - accuracy: 0.8689\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 931us/step - loss: 0.3049 - accuracy: 0.8731\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3013 - accuracy: 0.8745\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.8731\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3102 - accuracy: 0.8741\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3020 - accuracy: 0.8792\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3093 - accuracy: 0.8708\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2973 - accuracy: 0.8787\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3158 - accuracy: 0.8680\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 974us/step - loss: 0.2957 - accuracy: 0.8745\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.2963 - accuracy: 0.8769\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.3072 - accuracy: 0.8689\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.3053 - accuracy: 0.8675\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.3089 - accuracy: 0.8656\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.3112 - accuracy: 0.8759\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2982 - accuracy: 0.8792\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.3010 - accuracy: 0.8759\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.3084 - accuracy: 0.8647\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.2970 - accuracy: 0.8727\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 957us/step - loss: 0.2931 - accuracy: 0.8755\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 979us/step - loss: 0.2904 - accuracy: 0.8801\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2881 - accuracy: 0.8797\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 941us/step - loss: 0.2985 - accuracy: 0.8727\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.2783 - accuracy: 0.8834\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.2956 - accuracy: 0.8778\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.2792 - accuracy: 0.8783\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 872us/step - loss: 0.2601 - accuracy: 0.8867\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.2969 - accuracy: 0.8816\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.2839 - accuracy: 0.8755\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.2834 - accuracy: 0.8816\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.2746 - accuracy: 0.8862\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2748 - accuracy: 0.8858\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2922 - accuracy: 0.8778\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2587 - accuracy: 0.8979\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 976us/step - loss: 0.2814 - accuracy: 0.8801\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2767 - accuracy: 0.8900\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2694 - accuracy: 0.8806\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2731 - accuracy: 0.8834\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 967us/step - loss: 0.2652 - accuracy: 0.8923\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2696 - accuracy: 0.8919\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2719 - accuracy: 0.8937\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8806\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 982us/step - loss: 0.2711 - accuracy: 0.8961\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2745 - accuracy: 0.8858\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8872\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.2655 - accuracy: 0.8928\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 872us/step - loss: 0.2657 - accuracy: 0.8979\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 912us/step - loss: 0.2688 - accuracy: 0.8937\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2596 - accuracy: 0.8853\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 976us/step - loss: 0.2562 - accuracy: 0.9003\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 944us/step - loss: 0.2544 - accuracy: 0.8993\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.2681 - accuracy: 0.8900\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.2662 - accuracy: 0.8820\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.2701 - accuracy: 0.8895\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 939us/step - loss: 0.2376 - accuracy: 0.9078\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 942us/step - loss: 0.2748 - accuracy: 0.8876\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 979us/step - loss: 0.2702 - accuracy: 0.8942\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2527 - accuracy: 0.9017\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2583 - accuracy: 0.8975\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 982us/step - loss: 0.2592 - accuracy: 0.8909\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.2653 - accuracy: 0.8979\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 886us/step - loss: 0.2523 - accuracy: 0.8900\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.2461 - accuracy: 0.8965\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.2703 - accuracy: 0.8872\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2303 - accuracy: 0.9120\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 957us/step - loss: 0.2569 - accuracy: 0.8951\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.2641 - accuracy: 0.8989\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 956us/step - loss: 0.2617 - accuracy: 0.8853\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 979us/step - loss: 0.2589 - accuracy: 0.9012\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.2569 - accuracy: 0.8914\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 986us/step - loss: 0.2564 - accuracy: 0.8970\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 915us/step - loss: 0.2424 - accuracy: 0.9007\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.2492 - accuracy: 0.8965\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.2473 - accuracy: 0.9007\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.2507 - accuracy: 0.8984\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.2432 - accuracy: 0.9007\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.2479 - accuracy: 0.8961\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.2612 - accuracy: 0.8919\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.2524 - accuracy: 0.8909\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.2505 - accuracy: 0.8984\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.2321 - accuracy: 0.9054\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.2448 - accuracy: 0.9040\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2593 - accuracy: 0.8928\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.2420 - accuracy: 0.8984\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.2527 - accuracy: 0.8970\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2284 - accuracy: 0.9120\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 940us/step - loss: 0.2304 - accuracy: 0.9092\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 960us/step - loss: 0.2265 - accuracy: 0.9068\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9120\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 961us/step - loss: 0.2350 - accuracy: 0.9012\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 961us/step - loss: 0.2434 - accuracy: 0.9012\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2459 - accuracy: 0.9026\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 974us/step - loss: 0.2408 - accuracy: 0.8979\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.2356 - accuracy: 0.9101\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.2321 - accuracy: 0.9087\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.2487 - accuracy: 0.8975\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.2400 - accuracy: 0.9059\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.2259 - accuracy: 0.9087\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.2545 - accuracy: 0.8970\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 975us/step - loss: 0.2274 - accuracy: 0.9110\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.2395 - accuracy: 0.9068\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.2426 - accuracy: 0.9040\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.2322 - accuracy: 0.9125\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.2382 - accuracy: 0.9054\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.2402 - accuracy: 0.8984\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 896us/step - loss: 0.2403 - accuracy: 0.9036\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2365 - accuracy: 0.9068\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2259 - accuracy: 0.9167\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2271 - accuracy: 0.9017\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2273 - accuracy: 0.9139\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2328 - accuracy: 0.9036\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.2131 - accuracy: 0.9228\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 976us/step - loss: 0.2150 - accuracy: 0.9185\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.2224 - accuracy: 0.9110\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 903us/step - loss: 0.2185 - accuracy: 0.9139\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.2154 - accuracy: 0.9171\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.2378 - accuracy: 0.8975\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.2242 - accuracy: 0.9087\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.2175 - accuracy: 0.9143\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.2250 - accuracy: 0.9087\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.2231 - accuracy: 0.9176\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.2259 - accuracy: 0.9101\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.2177 - accuracy: 0.9148\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 950us/step - loss: 0.2268 - accuracy: 0.9059\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.2326 - accuracy: 0.9068\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.2494 - accuracy: 0.9026\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2330 - accuracy: 0.8970\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.2353 - accuracy: 0.9106\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2312 - accuracy: 0.9096\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.2185 - accuracy: 0.9157\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 927us/step - loss: 0.2105 - accuracy: 0.9162\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.2187 - accuracy: 0.9181\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.2177 - accuracy: 0.9176\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.2092 - accuracy: 0.9218\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 923us/step - loss: 0.2108 - accuracy: 0.9232\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.2249 - accuracy: 0.9082\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.2290 - accuracy: 0.9078\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.2138 - accuracy: 0.9157\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.2222 - accuracy: 0.9176\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2219 - accuracy: 0.9139\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.2132 - accuracy: 0.9148\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2192 - accuracy: 0.9139\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2233 - accuracy: 0.9068\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2129 - accuracy: 0.9232\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2184 - accuracy: 0.9181\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9293\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2005 - accuracy: 0.9199\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.2235 - accuracy: 0.9157\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.2024 - accuracy: 0.9256\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.2105 - accuracy: 0.9125\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.2051 - accuracy: 0.9228\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.2051 - accuracy: 0.9218\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.2131 - accuracy: 0.9181\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.2032 - accuracy: 0.9213\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.2032 - accuracy: 0.9153\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.2062 - accuracy: 0.9223\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.2200 - accuracy: 0.9078\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9204\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9143\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2157 - accuracy: 0.9162\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 971us/step - loss: 0.1886 - accuracy: 0.9279\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.2111 - accuracy: 0.9195\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 949us/step - loss: 0.2131 - accuracy: 0.9195\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2032 - accuracy: 0.9199\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.2165 - accuracy: 0.9153\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 938us/step - loss: 0.1959 - accuracy: 0.9288\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.2078 - accuracy: 0.9176\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1934 - accuracy: 0.9232\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 934us/step - loss: 0.1976 - accuracy: 0.9204\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 990us/step - loss: 0.2027 - accuracy: 0.9237\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 967us/step - loss: 0.1938 - accuracy: 0.9204\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 923us/step - loss: 0.2017 - accuracy: 0.9162\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.2052 - accuracy: 0.9223\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1869 - accuracy: 0.9279\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 890us/step - loss: 0.1789 - accuracy: 0.9293\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 908us/step - loss: 0.1985 - accuracy: 0.9209\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.2072 - accuracy: 0.9190\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 940us/step - loss: 0.2146 - accuracy: 0.9157\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.1877 - accuracy: 0.9199\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 954us/step - loss: 0.2100 - accuracy: 0.9228\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 989us/step - loss: 0.1894 - accuracy: 0.9302\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 918us/step - loss: 0.1954 - accuracy: 0.9251\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 950us/step - loss: 0.1913 - accuracy: 0.9242\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 957us/step - loss: 0.1826 - accuracy: 0.9298\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 910us/step - loss: 0.2163 - accuracy: 0.9143\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.2027 - accuracy: 0.9129\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.1822 - accuracy: 0.9274\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.1912 - accuracy: 0.9232\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1921 - accuracy: 0.9232\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.2034 - accuracy: 0.9213\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.1946 - accuracy: 0.9242\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1925 - accuracy: 0.9288\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.1792 - accuracy: 0.9340\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.1917 - accuracy: 0.9260\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 876us/step - loss: 0.2041 - accuracy: 0.9260\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 918us/step - loss: 0.2034 - accuracy: 0.9213\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.1769 - accuracy: 0.9331\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.1871 - accuracy: 0.9270\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 906us/step - loss: 0.1923 - accuracy: 0.9237\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 860us/step - loss: 0.1823 - accuracy: 0.9302\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.9256\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 940us/step - loss: 0.1952 - accuracy: 0.9270\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 950us/step - loss: 0.1838 - accuracy: 0.9251\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 909us/step - loss: 0.1850 - accuracy: 0.9288\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 964us/step - loss: 0.1829 - accuracy: 0.9274\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 978us/step - loss: 0.1819 - accuracy: 0.9260\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 965us/step - loss: 0.1860 - accuracy: 0.9270\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1983 - accuracy: 0.9185\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.1701 - accuracy: 0.9326\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 4ms/step - loss: 0.1934 - accuracy: 0.9232\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1927 - accuracy: 0.9270\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.1952 - accuracy: 0.9199\n",
      "Epoch 351/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.1861 - accuracy: 0.9260\n",
      "Epoch 352/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.1983 - accuracy: 0.9228\n",
      "Epoch 353/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1665 - accuracy: 0.9359\n",
      "Epoch 354/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1785 - accuracy: 0.9312\n",
      "Epoch 355/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.1687 - accuracy: 0.9302\n",
      "Epoch 356/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1789 - accuracy: 0.9316\n",
      "Epoch 357/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.1932 - accuracy: 0.9298\n",
      "Epoch 358/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1757 - accuracy: 0.9373\n",
      "Epoch 359/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.1827 - accuracy: 0.9288\n",
      "Epoch 360/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1856 - accuracy: 0.9251\n",
      "Epoch 361/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.1803 - accuracy: 0.9260\n",
      "Epoch 362/1500\n",
      "34/34 [==============================] - 0s 898us/step - loss: 0.1778 - accuracy: 0.9256\n",
      "Epoch 363/1500\n",
      "34/34 [==============================] - 0s 929us/step - loss: 0.1844 - accuracy: 0.9312\n",
      "Epoch 364/1500\n",
      "34/34 [==============================] - 0s 895us/step - loss: 0.1756 - accuracy: 0.9331\n",
      "Epoch 365/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.1927 - accuracy: 0.9246\n",
      "Epoch 366/1500\n",
      "34/34 [==============================] - 0s 927us/step - loss: 0.1900 - accuracy: 0.9228\n",
      "Epoch 367/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1691 - accuracy: 0.9401\n",
      "Epoch 368/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.1821 - accuracy: 0.9302\n",
      "Epoch 369/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1849 - accuracy: 0.9256\n",
      "Epoch 370/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.1819 - accuracy: 0.9279\n",
      "Epoch 371/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.1791 - accuracy: 0.9270\n",
      "Epoch 372/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.1650 - accuracy: 0.9373\n",
      "Epoch 373/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.1779 - accuracy: 0.9316\n",
      "Epoch 374/1500\n",
      "34/34 [==============================] - 0s 814us/step - loss: 0.1875 - accuracy: 0.9270\n",
      "Epoch 375/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1647 - accuracy: 0.9331\n",
      "Epoch 376/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.1817 - accuracy: 0.9331\n",
      "Epoch 377/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.1817 - accuracy: 0.9274\n",
      "Epoch 378/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.1653 - accuracy: 0.9387\n",
      "Epoch 379/1500\n",
      "34/34 [==============================] - 0s 915us/step - loss: 0.1699 - accuracy: 0.9377\n",
      "Epoch 380/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.1730 - accuracy: 0.9349\n",
      "Epoch 381/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.1852 - accuracy: 0.9293\n",
      "Epoch 382/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1706 - accuracy: 0.9316\n",
      "Epoch 383/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.1730 - accuracy: 0.9321\n",
      "Epoch 384/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.1792 - accuracy: 0.9307\n",
      "Epoch 385/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.1756 - accuracy: 0.9368\n",
      "Epoch 386/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1706 - accuracy: 0.9265\n",
      "Epoch 387/1500\n",
      "34/34 [==============================] - 0s 821us/step - loss: 0.1831 - accuracy: 0.9298\n",
      "Epoch 388/1500\n",
      "34/34 [==============================] - 0s 793us/step - loss: 0.1727 - accuracy: 0.9265\n",
      "Epoch 389/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.1760 - accuracy: 0.9270\n",
      "Epoch 390/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.1679 - accuracy: 0.9363\n",
      "Epoch 391/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.1621 - accuracy: 0.9401\n",
      "Epoch 392/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.1783 - accuracy: 0.9326\n",
      "Epoch 393/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.1687 - accuracy: 0.9363\n",
      "Epoch 394/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.1672 - accuracy: 0.9391\n",
      "Epoch 395/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.1686 - accuracy: 0.9345\n",
      "Epoch 396/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.1456 - accuracy: 0.9424\n",
      "Epoch 397/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.1718 - accuracy: 0.9288\n",
      "Epoch 398/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.1781 - accuracy: 0.9316\n",
      "Epoch 399/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.1595 - accuracy: 0.9354\n",
      "Epoch 400/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.1589 - accuracy: 0.9419\n",
      "Epoch 401/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.1562 - accuracy: 0.9396\n",
      "Epoch 402/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.1551 - accuracy: 0.9434\n",
      "Epoch 403/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.1688 - accuracy: 0.9316\n",
      "Epoch 404/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.1736 - accuracy: 0.9335\n",
      "Epoch 405/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1556 - accuracy: 0.9419\n",
      "Epoch 406/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.1526 - accuracy: 0.9443\n",
      "Epoch 407/1500\n",
      "34/34 [==============================] - 0s 5ms/step - loss: 0.1576 - accuracy: 0.9391\n",
      "Epoch 408/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9448\n",
      "Epoch 409/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.1659 - accuracy: 0.9316\n",
      "Epoch 410/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.1577 - accuracy: 0.9391\n",
      "Epoch 411/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1645 - accuracy: 0.9345\n",
      "Epoch 412/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.1668 - accuracy: 0.9359\n",
      "Epoch 413/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.1592 - accuracy: 0.9354\n",
      "Epoch 414/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.1532 - accuracy: 0.9396\n",
      "Epoch 415/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.1607 - accuracy: 0.9354\n",
      "Epoch 416/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.1601 - accuracy: 0.9391\n",
      "Epoch 417/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1580 - accuracy: 0.9335\n",
      "Epoch 418/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1885 - accuracy: 0.9223\n",
      "Epoch 419/1500\n",
      "34/34 [==============================] - 0s 858us/step - loss: 0.1567 - accuracy: 0.9434\n",
      "Epoch 420/1500\n",
      "34/34 [==============================] - 0s 881us/step - loss: 0.1789 - accuracy: 0.9256\n",
      "Epoch 421/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.1606 - accuracy: 0.9345\n",
      "Epoch 422/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1565 - accuracy: 0.9373\n",
      "Epoch 423/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.1619 - accuracy: 0.9382\n",
      "Epoch 424/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1632 - accuracy: 0.9340\n",
      "Epoch 425/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.1431 - accuracy: 0.9457\n",
      "Epoch 426/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.1577 - accuracy: 0.9434\n",
      "Epoch 427/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.1680 - accuracy: 0.9359\n",
      "Epoch 428/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1622 - accuracy: 0.9401\n",
      "Epoch 429/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1569 - accuracy: 0.9363\n",
      "Epoch 430/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.1607 - accuracy: 0.9391\n",
      "Epoch 431/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1614 - accuracy: 0.9410\n",
      "Epoch 432/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.1623 - accuracy: 0.9326\n",
      "Epoch 433/1500\n",
      "34/34 [==============================] - 0s 820us/step - loss: 0.1524 - accuracy: 0.9373\n",
      "Epoch 434/1500\n",
      "34/34 [==============================] - 0s 851us/step - loss: 0.1663 - accuracy: 0.9349\n",
      "Epoch 435/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.1489 - accuracy: 0.9466\n",
      "Epoch 436/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.1521 - accuracy: 0.9405\n",
      "Epoch 437/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.1600 - accuracy: 0.9335\n",
      "Epoch 438/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.1557 - accuracy: 0.9307\n",
      "Epoch 439/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.1622 - accuracy: 0.9349\n",
      "Epoch 440/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.1459 - accuracy: 0.9424\n",
      "Epoch 441/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.1509 - accuracy: 0.9457\n",
      "Epoch 442/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.1646 - accuracy: 0.9345\n",
      "Epoch 443/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.1695 - accuracy: 0.9335\n",
      "Epoch 444/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.1600 - accuracy: 0.9391\n",
      "Epoch 445/1500\n",
      "34/34 [==============================] - 0s 853us/step - loss: 0.1459 - accuracy: 0.9448\n",
      "Epoch 446/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.1477 - accuracy: 0.9429\n",
      "Epoch 447/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.1571 - accuracy: 0.9354\n",
      "Epoch 448/1500\n",
      "34/34 [==============================] - 0s 856us/step - loss: 0.1531 - accuracy: 0.9396\n",
      "Epoch 449/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1508 - accuracy: 0.9452\n",
      "Epoch 450/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.1582 - accuracy: 0.9359\n",
      "Epoch 451/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1456 - accuracy: 0.9410\n",
      "Epoch 452/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.1441 - accuracy: 0.9419\n",
      "Epoch 453/1500\n",
      "34/34 [==============================] - 0s 848us/step - loss: 0.1551 - accuracy: 0.9438\n",
      "Epoch 454/1500\n",
      "34/34 [==============================] - 0s 852us/step - loss: 0.1434 - accuracy: 0.9415\n",
      "Epoch 455/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.0628 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 425.\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.1601 - accuracy: 0.9368\n",
      "Epoch 455: early stopping\n",
      "8/8 [==============================] - 0s 852us/step - loss: 0.6356 - accuracy: 0.7404\n",
      "8/8 [==============================] - 0s 602us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (23/29)\n",
      "Before appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "After appending - Cat IDs: 848, Predictions: 848, Actuals: 848, Gender: 848\n",
      "Final Test Results - Loss: 0.6356417536735535, Accuracy: 0.7404255270957947, Precision: 0.7799589183236982, Recall: 0.6684981684981685, F1 Score: 0.7056281082958167\n",
      "Confusion Matrix:\n",
      " [[114   3  13]\n",
      " [ 16  19   0]\n",
      " [ 29   0  41]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.7289215479168335\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.6468864157795906\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7665315717458725\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.731730348695451\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.77471646303078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[3]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0522ad-89f9-479c-b073-c5e720fc6221",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "ecac477e-680c-4a82-abcb-2b3a2dbfbea4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 848, Predictions: 848, Actuals: 848, Gender: 848\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "ce33a298-d7a3-40ec-8f78-00c109118230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "23d083a1-d61a-4fce-9b56-667a9930bfe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.84 (92/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "431e9ecf-be90-466a-a1b7-dd98f7fb2a9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "42e4a5c0-56dd-4771-820f-bcecaf391f0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[kitten, adult, adult, adult, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten, kitten, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[senior, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, senior, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "69    063A  [adult, senior, senior, adult, senior, adult, ...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A              [adult, adult, senior, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                      [kitten, adult, adult, adult]         adult            adult                   True\n",
       "50    044A             [kitten, adult, adult, kitten, kitten]        kitten           kitten                   True\n",
       "63    057A  [senior, adult, adult, senior, adult, adult, s...        senior           senior                   True\n",
       "62    056A                            [adult, senior, senior]        senior           senior                   True\n",
       "61    055A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "59    053A       [kitten, adult, adult, adult, kitten, adult]         adult            adult                   True\n",
       "58    052A                     [adult, senior, senior, adult]         adult            adult                   True\n",
       "57    051B  [senior, adult, senior, adult, senior, senior,...        senior           senior                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, adult, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, adult, adult, kitten, kitten, kitten, ...        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "103   109A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "102   108A    [senior, senior, adult, senior, senior, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                    [senior, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, adult, senior, adult, a...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "16    014B  [senior, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "25    023A        [adult, kitten, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "18    016A  [senior, adult, senior, senior, senior, senior...        senior           senior                   True\n",
       "17    015A  [senior, senior, adult, adult, senior, adult, ...         adult            adult                   True\n",
       "15    014A                             [adult, adult, kitten]         adult            adult                   True\n",
       "29    025B                                     [adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, senior, adult]         adult            adult                   True\n",
       "4     003A                     [adult, senior, adult, senior]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "46    040A  [kitten, kitten, senior, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "44    038A                                    [adult, kitten]         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "40    034A               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, se...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "106   113A                             [adult, senior, adult]         adult           senior                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "101   106A  [adult, senior, adult, senior, senior, adult, ...         adult           senior                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "42    036A  [adult, senior, senior, senior, adult, adult, ...        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "64    058A                             [senior, adult, adult]         adult           senior                  False\n",
       "65    059A  [adult, adult, adult, adult, senior, senior, s...         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "bc2e5ff6-b761-4b71-b7cd-9eb24aae0ebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     66\n",
      "kitten    13\n",
      "senior    13\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "ff925f9d-efd5-46a0-89a0-1fb3da66a46a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             66  90.410959\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22             13  59.090909\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "d8f08817-c78c-4654-b195-911f2a9ccb2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6f284-fe35-4071-8054-5e5a086b4ed9",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "4394c6ec-6da8-42de-aaaa-bc1b17bbf74b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          556            446  80.215827\n",
      "1           kitten          114             82  71.929825\n",
      "2           senior          178            115  64.606742\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "8d541c77-590d-4b79-b526-85520ea5c4a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c98d75-f483-4819-b292-975bd229cfd2",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "c7325b11-ab06-45bd-8e77-d498737b7552",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    220      177     80.45\n",
      "1          M    337      256     75.96\n",
      "2          X    291      210     72.16\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "777ae0eb-dab5-47c0-86a7-0d3c6b01b9c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40270db-853d-46a1-acb4-8dab8a4f9c81",
   "metadata": {},
   "source": [
    "# RANDOM SEED 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "588e6cfb-6ee5-4b51-a9ee-79dcbfbb208c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[4]))\n",
    "np.random.seed(int(random_seeds[4]))\n",
    "tf.random.set_seed(int(random_seeds[4]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_13.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "432a9515-e7f1-482c-bdaf-73cb07a9132e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "8231e441-4472-4d9b-8ddf-dc285c07923b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9ed6b-6d2f-491e-95d8-73d33edeef6a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "6cd63258-8a3b-4bc0-9ac6-ae8aa4b515b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "106A    14\n",
      "097B    14\n",
      "028A    13\n",
      "111A    13\n",
      "051A    12\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "068A    11\n",
      "014B    10\n",
      "016A    10\n",
      "071A    10\n",
      "005A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "045A     9\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "095A     8\n",
      "010A     8\n",
      "050A     7\n",
      "027A     7\n",
      "031A     7\n",
      "099A     7\n",
      "117A     7\n",
      "053A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "075A     5\n",
      "021A     5\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "062A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "009A     4\n",
      "035A     4\n",
      "056A     3\n",
      "014A     3\n",
      "058A     3\n",
      "060A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "032A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "073A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "041A     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "000B    19\n",
      "001A    14\n",
      "002A    13\n",
      "040A    10\n",
      "022A     9\n",
      "065A     9\n",
      "109A     6\n",
      "108A     6\n",
      "044A     5\n",
      "026A     4\n",
      "113A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "011A     2\n",
      "054A     2\n",
      "018A     2\n",
      "092A     1\n",
      "049A     1\n",
      "004A     1\n",
      "019B     1\n",
      "115A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    280\n",
      "X    256\n",
      "F    186\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    92\n",
      "F    66\n",
      "M    57\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [014B, 111A, 047A, 042A, 050A, 043A, 041A, 045...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 001A, 103A, 022A, 065A, 002A, 000B, 026...\n",
      "kitten                 [044A, 040A, 046A, 109A, 049A, 115A]\n",
      "senior                             [113A, 054A, 108A, 011A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 59, 'kitten': 10, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 15, 'kitten': 6, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '041A' '042A' '043A' '045A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '044A' '046A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'041A'}\n",
      "Moved to Test Set:\n",
      "{'041A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '042A' '043A' '045A' '046A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '041A' '044A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "Length of X_train_val:\n",
      "784\n",
      "Length of y_train_val:\n",
      "784\n",
      "Length of groups_train_val:\n",
      "784\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten     85\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     116\n",
      "kitten     86\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten    147\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     116\n",
      "kitten     24\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 944, 2: 825, 1: 735})\n",
      "Epoch 1/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 1.1033 - accuracy: 0.5088\n",
      "Epoch 2/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.9567 - accuracy: 0.5855\n",
      "Epoch 3/1500\n",
      "40/40 [==============================] - 0s 858us/step - loss: 0.8878 - accuracy: 0.6106\n",
      "Epoch 4/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.8273 - accuracy: 0.6338\n",
      "Epoch 5/1500\n",
      "40/40 [==============================] - 0s 817us/step - loss: 0.7915 - accuracy: 0.6494\n",
      "Epoch 6/1500\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.7650 - accuracy: 0.6585\n",
      "Epoch 7/1500\n",
      "40/40 [==============================] - 0s 808us/step - loss: 0.7702 - accuracy: 0.6797\n",
      "Epoch 8/1500\n",
      "40/40 [==============================] - 0s 826us/step - loss: 0.7410 - accuracy: 0.6749\n",
      "Epoch 9/1500\n",
      "40/40 [==============================] - 0s 811us/step - loss: 0.7309 - accuracy: 0.6805\n",
      "Epoch 10/1500\n",
      "40/40 [==============================] - 0s 798us/step - loss: 0.7156 - accuracy: 0.7033\n",
      "Epoch 11/1500\n",
      "40/40 [==============================] - 0s 813us/step - loss: 0.6738 - accuracy: 0.7137\n",
      "Epoch 12/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.6741 - accuracy: 0.7109\n",
      "Epoch 13/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.6645 - accuracy: 0.7081\n",
      "Epoch 14/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.6301 - accuracy: 0.7228\n",
      "Epoch 15/1500\n",
      "40/40 [==============================] - 0s 903us/step - loss: 0.6117 - accuracy: 0.7396\n",
      "Epoch 16/1500\n",
      "40/40 [==============================] - 0s 838us/step - loss: 0.6068 - accuracy: 0.7416\n",
      "Epoch 17/1500\n",
      "40/40 [==============================] - 0s 863us/step - loss: 0.5973 - accuracy: 0.7480\n",
      "Epoch 18/1500\n",
      "40/40 [==============================] - 0s 882us/step - loss: 0.6014 - accuracy: 0.7444\n",
      "Epoch 19/1500\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.5890 - accuracy: 0.7476\n",
      "Epoch 20/1500\n",
      "40/40 [==============================] - 0s 902us/step - loss: 0.5758 - accuracy: 0.7620\n",
      "Epoch 21/1500\n",
      "40/40 [==============================] - 0s 847us/step - loss: 0.5813 - accuracy: 0.7532\n",
      "Epoch 22/1500\n",
      "40/40 [==============================] - 0s 848us/step - loss: 0.5612 - accuracy: 0.7600\n",
      "Epoch 23/1500\n",
      "40/40 [==============================] - 0s 843us/step - loss: 0.5585 - accuracy: 0.7600\n",
      "Epoch 24/1500\n",
      "40/40 [==============================] - 0s 847us/step - loss: 0.5634 - accuracy: 0.7628\n",
      "Epoch 25/1500\n",
      "40/40 [==============================] - 0s 867us/step - loss: 0.5548 - accuracy: 0.7624\n",
      "Epoch 26/1500\n",
      "40/40 [==============================] - 0s 841us/step - loss: 0.5465 - accuracy: 0.7632\n",
      "Epoch 27/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.5419 - accuracy: 0.7768\n",
      "Epoch 28/1500\n",
      "40/40 [==============================] - 0s 830us/step - loss: 0.5305 - accuracy: 0.7760\n",
      "Epoch 29/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.5175 - accuracy: 0.7752\n",
      "Epoch 30/1500\n",
      "40/40 [==============================] - 0s 825us/step - loss: 0.5261 - accuracy: 0.7696\n",
      "Epoch 31/1500\n",
      "40/40 [==============================] - 0s 815us/step - loss: 0.5221 - accuracy: 0.7839\n",
      "Epoch 32/1500\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.5087 - accuracy: 0.7915\n",
      "Epoch 33/1500\n",
      "40/40 [==============================] - 0s 855us/step - loss: 0.5102 - accuracy: 0.7780\n",
      "Epoch 34/1500\n",
      "40/40 [==============================] - 0s 823us/step - loss: 0.4935 - accuracy: 0.7935\n",
      "Epoch 35/1500\n",
      "40/40 [==============================] - 0s 807us/step - loss: 0.5009 - accuracy: 0.7879\n",
      "Epoch 36/1500\n",
      "40/40 [==============================] - 0s 865us/step - loss: 0.5094 - accuracy: 0.7835\n",
      "Epoch 37/1500\n",
      "40/40 [==============================] - 0s 785us/step - loss: 0.4988 - accuracy: 0.7955\n",
      "Epoch 38/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.4888 - accuracy: 0.7943\n",
      "Epoch 39/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.4805 - accuracy: 0.8015\n",
      "Epoch 40/1500\n",
      "40/40 [==============================] - 0s 813us/step - loss: 0.4770 - accuracy: 0.7991\n",
      "Epoch 41/1500\n",
      "40/40 [==============================] - 0s 851us/step - loss: 0.4886 - accuracy: 0.7931\n",
      "Epoch 42/1500\n",
      "40/40 [==============================] - 0s 810us/step - loss: 0.4774 - accuracy: 0.7943\n",
      "Epoch 43/1500\n",
      "40/40 [==============================] - 0s 830us/step - loss: 0.4834 - accuracy: 0.7819\n",
      "Epoch 44/1500\n",
      "40/40 [==============================] - 0s 804us/step - loss: 0.4744 - accuracy: 0.7891\n",
      "Epoch 45/1500\n",
      "40/40 [==============================] - 0s 843us/step - loss: 0.4622 - accuracy: 0.8043\n",
      "Epoch 46/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.4808 - accuracy: 0.7983\n",
      "Epoch 47/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.4691 - accuracy: 0.8011\n",
      "Epoch 48/1500\n",
      "40/40 [==============================] - 0s 805us/step - loss: 0.4594 - accuracy: 0.8079\n",
      "Epoch 49/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.4449 - accuracy: 0.8147\n",
      "Epoch 50/1500\n",
      "40/40 [==============================] - 0s 759us/step - loss: 0.4579 - accuracy: 0.8075\n",
      "Epoch 51/1500\n",
      "40/40 [==============================] - 0s 889us/step - loss: 0.4592 - accuracy: 0.8023\n",
      "Epoch 52/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.4580 - accuracy: 0.8175\n",
      "Epoch 53/1500\n",
      "40/40 [==============================] - 0s 816us/step - loss: 0.4565 - accuracy: 0.8051\n",
      "Epoch 54/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.4627 - accuracy: 0.8003\n",
      "Epoch 55/1500\n",
      "40/40 [==============================] - 0s 853us/step - loss: 0.4318 - accuracy: 0.8151\n",
      "Epoch 56/1500\n",
      "40/40 [==============================] - 0s 864us/step - loss: 0.4358 - accuracy: 0.8147\n",
      "Epoch 57/1500\n",
      "40/40 [==============================] - 0s 837us/step - loss: 0.4294 - accuracy: 0.8227\n",
      "Epoch 58/1500\n",
      "40/40 [==============================] - 0s 847us/step - loss: 0.4411 - accuracy: 0.8131\n",
      "Epoch 59/1500\n",
      "40/40 [==============================] - 0s 857us/step - loss: 0.4118 - accuracy: 0.8259\n",
      "Epoch 60/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.4104 - accuracy: 0.8247\n",
      "Epoch 61/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.4365 - accuracy: 0.8243\n",
      "Epoch 62/1500\n",
      "40/40 [==============================] - 0s 835us/step - loss: 0.4321 - accuracy: 0.8111\n",
      "Epoch 63/1500\n",
      "40/40 [==============================] - 0s 861us/step - loss: 0.4086 - accuracy: 0.8283\n",
      "Epoch 64/1500\n",
      "40/40 [==============================] - 0s 842us/step - loss: 0.4217 - accuracy: 0.8139\n",
      "Epoch 65/1500\n",
      "40/40 [==============================] - 0s 848us/step - loss: 0.4267 - accuracy: 0.8075\n",
      "Epoch 66/1500\n",
      "40/40 [==============================] - 0s 847us/step - loss: 0.4200 - accuracy: 0.8219\n",
      "Epoch 67/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.4129 - accuracy: 0.8267\n",
      "Epoch 68/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.3967 - accuracy: 0.8259\n",
      "Epoch 69/1500\n",
      "40/40 [==============================] - 0s 812us/step - loss: 0.4069 - accuracy: 0.8223\n",
      "Epoch 70/1500\n",
      "40/40 [==============================] - 0s 806us/step - loss: 0.4142 - accuracy: 0.8287\n",
      "Epoch 71/1500\n",
      "40/40 [==============================] - 0s 818us/step - loss: 0.4035 - accuracy: 0.8235\n",
      "Epoch 72/1500\n",
      "40/40 [==============================] - 0s 796us/step - loss: 0.4124 - accuracy: 0.8255\n",
      "Epoch 73/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.3929 - accuracy: 0.8403\n",
      "Epoch 74/1500\n",
      "40/40 [==============================] - 0s 858us/step - loss: 0.3856 - accuracy: 0.8339\n",
      "Epoch 75/1500\n",
      "40/40 [==============================] - 0s 801us/step - loss: 0.3882 - accuracy: 0.8391\n",
      "Epoch 76/1500\n",
      "40/40 [==============================] - 0s 821us/step - loss: 0.3905 - accuracy: 0.8295\n",
      "Epoch 77/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.3844 - accuracy: 0.8383\n",
      "Epoch 78/1500\n",
      "40/40 [==============================] - 0s 828us/step - loss: 0.4026 - accuracy: 0.8367\n",
      "Epoch 79/1500\n",
      "40/40 [==============================] - 0s 970us/step - loss: 0.3835 - accuracy: 0.8363\n",
      "Epoch 80/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3887 - accuracy: 0.8379\n",
      "Epoch 81/1500\n",
      "40/40 [==============================] - 0s 900us/step - loss: 0.3750 - accuracy: 0.8411\n",
      "Epoch 82/1500\n",
      "40/40 [==============================] - 0s 855us/step - loss: 0.3823 - accuracy: 0.8395\n",
      "Epoch 83/1500\n",
      "40/40 [==============================] - 0s 863us/step - loss: 0.3929 - accuracy: 0.8383\n",
      "Epoch 84/1500\n",
      "40/40 [==============================] - 0s 859us/step - loss: 0.3693 - accuracy: 0.8482\n",
      "Epoch 85/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.3745 - accuracy: 0.8407\n",
      "Epoch 86/1500\n",
      "40/40 [==============================] - 0s 885us/step - loss: 0.3793 - accuracy: 0.8335\n",
      "Epoch 87/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3647 - accuracy: 0.8450\n",
      "Epoch 88/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8427\n",
      "Epoch 89/1500\n",
      "40/40 [==============================] - 0s 928us/step - loss: 0.3759 - accuracy: 0.8442\n",
      "Epoch 90/1500\n",
      "40/40 [==============================] - 0s 854us/step - loss: 0.3886 - accuracy: 0.8395\n",
      "Epoch 91/1500\n",
      "40/40 [==============================] - 0s 857us/step - loss: 0.3664 - accuracy: 0.8522\n",
      "Epoch 92/1500\n",
      "40/40 [==============================] - 0s 846us/step - loss: 0.3710 - accuracy: 0.8419\n",
      "Epoch 93/1500\n",
      "40/40 [==============================] - 0s 865us/step - loss: 0.3635 - accuracy: 0.8502\n",
      "Epoch 94/1500\n",
      "40/40 [==============================] - 0s 863us/step - loss: 0.3732 - accuracy: 0.8431\n",
      "Epoch 95/1500\n",
      "40/40 [==============================] - 0s 951us/step - loss: 0.3853 - accuracy: 0.8407\n",
      "Epoch 96/1500\n",
      "40/40 [==============================] - 0s 970us/step - loss: 0.3791 - accuracy: 0.8435\n",
      "Epoch 97/1500\n",
      "40/40 [==============================] - 0s 957us/step - loss: 0.3573 - accuracy: 0.8538\n",
      "Epoch 98/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3484 - accuracy: 0.8435\n",
      "Epoch 99/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3527 - accuracy: 0.8522\n",
      "Epoch 100/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3770 - accuracy: 0.8419\n",
      "Epoch 101/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3569 - accuracy: 0.8498\n",
      "Epoch 102/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3422 - accuracy: 0.8606\n",
      "Epoch 103/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3628 - accuracy: 0.8438\n",
      "Epoch 104/1500\n",
      "40/40 [==============================] - 0s 977us/step - loss: 0.3420 - accuracy: 0.8474\n",
      "Epoch 105/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3576 - accuracy: 0.8490\n",
      "Epoch 106/1500\n",
      "40/40 [==============================] - 0s 963us/step - loss: 0.3473 - accuracy: 0.8550\n",
      "Epoch 107/1500\n",
      "40/40 [==============================] - 0s 937us/step - loss: 0.3263 - accuracy: 0.8666\n",
      "Epoch 108/1500\n",
      "40/40 [==============================] - 0s 960us/step - loss: 0.3538 - accuracy: 0.8514\n",
      "Epoch 109/1500\n",
      "40/40 [==============================] - 0s 933us/step - loss: 0.3457 - accuracy: 0.8534\n",
      "Epoch 110/1500\n",
      "40/40 [==============================] - 0s 887us/step - loss: 0.3330 - accuracy: 0.8586\n",
      "Epoch 111/1500\n",
      "40/40 [==============================] - 0s 926us/step - loss: 0.3355 - accuracy: 0.8590\n",
      "Epoch 112/1500\n",
      "40/40 [==============================] - 0s 966us/step - loss: 0.3399 - accuracy: 0.8590\n",
      "Epoch 113/1500\n",
      "40/40 [==============================] - 0s 992us/step - loss: 0.3409 - accuracy: 0.8598\n",
      "Epoch 114/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3311 - accuracy: 0.8622\n",
      "Epoch 115/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3348 - accuracy: 0.8602\n",
      "Epoch 116/1500\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.3399 - accuracy: 0.8622\n",
      "Epoch 117/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3303 - accuracy: 0.8606\n",
      "Epoch 118/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8590\n",
      "Epoch 119/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3206 - accuracy: 0.8642\n",
      "Epoch 120/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3305 - accuracy: 0.8706\n",
      "Epoch 121/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3249 - accuracy: 0.8714\n",
      "Epoch 122/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.3305 - accuracy: 0.8602\n",
      "Epoch 123/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.3245 - accuracy: 0.8642\n",
      "Epoch 124/1500\n",
      "40/40 [==============================] - 0s 849us/step - loss: 0.3299 - accuracy: 0.8626\n",
      "Epoch 125/1500\n",
      "40/40 [==============================] - 0s 862us/step - loss: 0.3301 - accuracy: 0.8566\n",
      "Epoch 126/1500\n",
      "40/40 [==============================] - 0s 842us/step - loss: 0.3237 - accuracy: 0.8590\n",
      "Epoch 127/1500\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.3144 - accuracy: 0.8758\n",
      "Epoch 128/1500\n",
      "40/40 [==============================] - 0s 918us/step - loss: 0.3234 - accuracy: 0.8646\n",
      "Epoch 129/1500\n",
      "40/40 [==============================] - 0s 879us/step - loss: 0.3261 - accuracy: 0.8626\n",
      "Epoch 130/1500\n",
      "40/40 [==============================] - 0s 880us/step - loss: 0.3208 - accuracy: 0.8766\n",
      "Epoch 131/1500\n",
      "40/40 [==============================] - 0s 890us/step - loss: 0.3170 - accuracy: 0.8614\n",
      "Epoch 132/1500\n",
      "40/40 [==============================] - 0s 848us/step - loss: 0.3262 - accuracy: 0.8614\n",
      "Epoch 133/1500\n",
      "40/40 [==============================] - 0s 862us/step - loss: 0.3263 - accuracy: 0.8642\n",
      "Epoch 134/1500\n",
      "40/40 [==============================] - 0s 878us/step - loss: 0.2925 - accuracy: 0.8774\n",
      "Epoch 135/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.3326 - accuracy: 0.8570\n",
      "Epoch 136/1500\n",
      "40/40 [==============================] - 0s 848us/step - loss: 0.3189 - accuracy: 0.8678\n",
      "Epoch 137/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3056 - accuracy: 0.8714\n",
      "Epoch 138/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3003 - accuracy: 0.8678\n",
      "Epoch 139/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3113 - accuracy: 0.8742\n",
      "Epoch 140/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3157 - accuracy: 0.8666\n",
      "Epoch 141/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2904 - accuracy: 0.8818\n",
      "Epoch 142/1500\n",
      "40/40 [==============================] - 0s 917us/step - loss: 0.3134 - accuracy: 0.8710\n",
      "Epoch 143/1500\n",
      "40/40 [==============================] - 0s 889us/step - loss: 0.2988 - accuracy: 0.8782\n",
      "Epoch 144/1500\n",
      "40/40 [==============================] - 0s 901us/step - loss: 0.3069 - accuracy: 0.8718\n",
      "Epoch 145/1500\n",
      "40/40 [==============================] - 0s 876us/step - loss: 0.2902 - accuracy: 0.8786\n",
      "Epoch 146/1500\n",
      "40/40 [==============================] - 0s 860us/step - loss: 0.2987 - accuracy: 0.8754\n",
      "Epoch 147/1500\n",
      "40/40 [==============================] - 0s 903us/step - loss: 0.3062 - accuracy: 0.8678\n",
      "Epoch 148/1500\n",
      "40/40 [==============================] - 0s 954us/step - loss: 0.3000 - accuracy: 0.8822\n",
      "Epoch 149/1500\n",
      "40/40 [==============================] - 0s 946us/step - loss: 0.2843 - accuracy: 0.8874\n",
      "Epoch 150/1500\n",
      "40/40 [==============================] - 0s 911us/step - loss: 0.2938 - accuracy: 0.8786\n",
      "Epoch 151/1500\n",
      "40/40 [==============================] - 0s 957us/step - loss: 0.3060 - accuracy: 0.8634\n",
      "Epoch 152/1500\n",
      "40/40 [==============================] - 0s 991us/step - loss: 0.2986 - accuracy: 0.8746\n",
      "Epoch 153/1500\n",
      "40/40 [==============================] - 0s 987us/step - loss: 0.3126 - accuracy: 0.8734\n",
      "Epoch 154/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2820 - accuracy: 0.8786\n",
      "Epoch 155/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2964 - accuracy: 0.8734\n",
      "Epoch 156/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.3015 - accuracy: 0.8722\n",
      "Epoch 157/1500\n",
      "40/40 [==============================] - 0s 890us/step - loss: 0.2794 - accuracy: 0.8866\n",
      "Epoch 158/1500\n",
      "40/40 [==============================] - 0s 971us/step - loss: 0.2924 - accuracy: 0.8806\n",
      "Epoch 159/1500\n",
      "40/40 [==============================] - 0s 994us/step - loss: 0.3070 - accuracy: 0.8742\n",
      "Epoch 160/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2987 - accuracy: 0.8790\n",
      "Epoch 161/1500\n",
      "40/40 [==============================] - 0s 959us/step - loss: 0.2849 - accuracy: 0.8818\n",
      "Epoch 162/1500\n",
      "40/40 [==============================] - 0s 984us/step - loss: 0.3050 - accuracy: 0.8778\n",
      "Epoch 163/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2929 - accuracy: 0.8822\n",
      "Epoch 164/1500\n",
      "40/40 [==============================] - 0s 978us/step - loss: 0.2898 - accuracy: 0.8754\n",
      "Epoch 165/1500\n",
      "40/40 [==============================] - 0s 999us/step - loss: 0.2758 - accuracy: 0.8878\n",
      "Epoch 166/1500\n",
      "40/40 [==============================] - 0s 948us/step - loss: 0.2844 - accuracy: 0.8886\n",
      "Epoch 167/1500\n",
      "40/40 [==============================] - 0s 900us/step - loss: 0.2803 - accuracy: 0.8902\n",
      "Epoch 168/1500\n",
      "40/40 [==============================] - 0s 930us/step - loss: 0.2866 - accuracy: 0.8878\n",
      "Epoch 169/1500\n",
      "40/40 [==============================] - 0s 911us/step - loss: 0.2816 - accuracy: 0.8850\n",
      "Epoch 170/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.2860 - accuracy: 0.8798\n",
      "Epoch 171/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.2957 - accuracy: 0.8826\n",
      "Epoch 172/1500\n",
      "40/40 [==============================] - 0s 914us/step - loss: 0.2933 - accuracy: 0.8822\n",
      "Epoch 173/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2873 - accuracy: 0.8734\n",
      "Epoch 174/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2722 - accuracy: 0.8898\n",
      "Epoch 175/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2874 - accuracy: 0.8794\n",
      "Epoch 176/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2777 - accuracy: 0.8890\n",
      "Epoch 177/1500\n",
      "40/40 [==============================] - 0s 918us/step - loss: 0.2758 - accuracy: 0.8890\n",
      "Epoch 178/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2877 - accuracy: 0.8810\n",
      "Epoch 179/1500\n",
      "40/40 [==============================] - 0s 953us/step - loss: 0.2824 - accuracy: 0.8822\n",
      "Epoch 180/1500\n",
      "40/40 [==============================] - 0s 911us/step - loss: 0.2670 - accuracy: 0.8902\n",
      "Epoch 181/1500\n",
      "40/40 [==============================] - 0s 890us/step - loss: 0.2700 - accuracy: 0.8866\n",
      "Epoch 182/1500\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.2708 - accuracy: 0.8882\n",
      "Epoch 183/1500\n",
      "40/40 [==============================] - 0s 871us/step - loss: 0.2719 - accuracy: 0.8866\n",
      "Epoch 184/1500\n",
      "40/40 [==============================] - 0s 913us/step - loss: 0.2664 - accuracy: 0.8942\n",
      "Epoch 185/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2874 - accuracy: 0.8838\n",
      "Epoch 186/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2679 - accuracy: 0.8934\n",
      "Epoch 187/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2711 - accuracy: 0.8902\n",
      "Epoch 188/1500\n",
      "40/40 [==============================] - 0s 978us/step - loss: 0.2856 - accuracy: 0.8798\n",
      "Epoch 189/1500\n",
      "40/40 [==============================] - 0s 967us/step - loss: 0.2570 - accuracy: 0.8994\n",
      "Epoch 190/1500\n",
      "40/40 [==============================] - 0s 861us/step - loss: 0.2670 - accuracy: 0.8946\n",
      "Epoch 191/1500\n",
      "40/40 [==============================] - 0s 817us/step - loss: 0.2637 - accuracy: 0.9026\n",
      "Epoch 192/1500\n",
      "40/40 [==============================] - 0s 854us/step - loss: 0.2589 - accuracy: 0.8954\n",
      "Epoch 193/1500\n",
      "40/40 [==============================] - 0s 948us/step - loss: 0.2734 - accuracy: 0.8882\n",
      "Epoch 194/1500\n",
      "40/40 [==============================] - 0s 983us/step - loss: 0.2725 - accuracy: 0.8914\n",
      "Epoch 195/1500\n",
      "40/40 [==============================] - 0s 970us/step - loss: 0.2649 - accuracy: 0.8906\n",
      "Epoch 196/1500\n",
      "40/40 [==============================] - 0s 984us/step - loss: 0.2652 - accuracy: 0.8958\n",
      "Epoch 197/1500\n",
      "40/40 [==============================] - 0s 999us/step - loss: 0.2643 - accuracy: 0.8954\n",
      "Epoch 198/1500\n",
      "40/40 [==============================] - 0s 995us/step - loss: 0.2486 - accuracy: 0.8986\n",
      "Epoch 199/1500\n",
      "40/40 [==============================] - 0s 1000us/step - loss: 0.2594 - accuracy: 0.9026\n",
      "Epoch 200/1500\n",
      "40/40 [==============================] - 0s 977us/step - loss: 0.2549 - accuracy: 0.8974\n",
      "Epoch 201/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2528 - accuracy: 0.8970\n",
      "Epoch 202/1500\n",
      "40/40 [==============================] - 0s 931us/step - loss: 0.2557 - accuracy: 0.8918\n",
      "Epoch 203/1500\n",
      "40/40 [==============================] - 0s 876us/step - loss: 0.2589 - accuracy: 0.8934\n",
      "Epoch 204/1500\n",
      "40/40 [==============================] - 0s 869us/step - loss: 0.2639 - accuracy: 0.8978\n",
      "Epoch 205/1500\n",
      "40/40 [==============================] - 0s 850us/step - loss: 0.2580 - accuracy: 0.8958\n",
      "Epoch 206/1500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.2585 - accuracy: 0.8954\n",
      "Epoch 207/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.2527 - accuracy: 0.9054\n",
      "Epoch 208/1500\n",
      "40/40 [==============================] - 0s 960us/step - loss: 0.2679 - accuracy: 0.8906\n",
      "Epoch 209/1500\n",
      "40/40 [==============================] - 0s 895us/step - loss: 0.2457 - accuracy: 0.9014\n",
      "Epoch 210/1500\n",
      "40/40 [==============================] - 0s 849us/step - loss: 0.2741 - accuracy: 0.8878\n",
      "Epoch 211/1500\n",
      "40/40 [==============================] - 0s 820us/step - loss: 0.2531 - accuracy: 0.8970\n",
      "Epoch 212/1500\n",
      "40/40 [==============================] - 0s 866us/step - loss: 0.2471 - accuracy: 0.8990\n",
      "Epoch 213/1500\n",
      "40/40 [==============================] - 0s 904us/step - loss: 0.2602 - accuracy: 0.8906\n",
      "Epoch 214/1500\n",
      "40/40 [==============================] - 0s 958us/step - loss: 0.2608 - accuracy: 0.8982\n",
      "Epoch 215/1500\n",
      "40/40 [==============================] - 0s 991us/step - loss: 0.2539 - accuracy: 0.8954\n",
      "Epoch 216/1500\n",
      "40/40 [==============================] - 0s 928us/step - loss: 0.2640 - accuracy: 0.8918\n",
      "Epoch 217/1500\n",
      "40/40 [==============================] - 0s 954us/step - loss: 0.2543 - accuracy: 0.8994\n",
      "Epoch 218/1500\n",
      "40/40 [==============================] - 0s 969us/step - loss: 0.2408 - accuracy: 0.9018\n",
      "Epoch 219/1500\n",
      "40/40 [==============================] - 0s 937us/step - loss: 0.2654 - accuracy: 0.8950\n",
      "Epoch 220/1500\n",
      "40/40 [==============================] - 0s 910us/step - loss: 0.2457 - accuracy: 0.9014\n",
      "Epoch 221/1500\n",
      "40/40 [==============================] - 0s 859us/step - loss: 0.2533 - accuracy: 0.8946\n",
      "Epoch 222/1500\n",
      "40/40 [==============================] - 0s 856us/step - loss: 0.2606 - accuracy: 0.8978\n",
      "Epoch 223/1500\n",
      "40/40 [==============================] - 0s 841us/step - loss: 0.2451 - accuracy: 0.8986\n",
      "Epoch 224/1500\n",
      "40/40 [==============================] - 0s 851us/step - loss: 0.2538 - accuracy: 0.8990\n",
      "Epoch 225/1500\n",
      "40/40 [==============================] - 0s 890us/step - loss: 0.2411 - accuracy: 0.9077\n",
      "Epoch 226/1500\n",
      "40/40 [==============================] - 0s 833us/step - loss: 0.2604 - accuracy: 0.8918\n",
      "Epoch 227/1500\n",
      "40/40 [==============================] - 0s 819us/step - loss: 0.2549 - accuracy: 0.8990\n",
      "Epoch 228/1500\n",
      "40/40 [==============================] - 0s 857us/step - loss: 0.2626 - accuracy: 0.8946\n",
      "Epoch 229/1500\n",
      "40/40 [==============================] - 0s 877us/step - loss: 0.2279 - accuracy: 0.9117\n",
      "Epoch 230/1500\n",
      "40/40 [==============================] - 0s 885us/step - loss: 0.2380 - accuracy: 0.9117\n",
      "Epoch 231/1500\n",
      "40/40 [==============================] - 0s 863us/step - loss: 0.2344 - accuracy: 0.9133\n",
      "Epoch 232/1500\n",
      "40/40 [==============================] - 0s 864us/step - loss: 0.2390 - accuracy: 0.9113\n",
      "Epoch 233/1500\n",
      "40/40 [==============================] - 0s 839us/step - loss: 0.2392 - accuracy: 0.9042\n",
      "Epoch 234/1500\n",
      "40/40 [==============================] - 0s 960us/step - loss: 0.2431 - accuracy: 0.8986\n",
      "Epoch 235/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2437 - accuracy: 0.8978\n",
      "Epoch 236/1500\n",
      "40/40 [==============================] - 0s 936us/step - loss: 0.2526 - accuracy: 0.9014\n",
      "Epoch 237/1500\n",
      "40/40 [==============================] - 0s 944us/step - loss: 0.2449 - accuracy: 0.9006\n",
      "Epoch 238/1500\n",
      "40/40 [==============================] - 0s 934us/step - loss: 0.2533 - accuracy: 0.8994\n",
      "Epoch 239/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2348 - accuracy: 0.8986\n",
      "Epoch 240/1500\n",
      "40/40 [==============================] - 0s 977us/step - loss: 0.2461 - accuracy: 0.9018\n",
      "Epoch 241/1500\n",
      "40/40 [==============================] - 0s 942us/step - loss: 0.2436 - accuracy: 0.9038\n",
      "Epoch 242/1500\n",
      "40/40 [==============================] - 0s 908us/step - loss: 0.2486 - accuracy: 0.9014\n",
      "Epoch 243/1500\n",
      "40/40 [==============================] - 0s 837us/step - loss: 0.2293 - accuracy: 0.9002\n",
      "Epoch 244/1500\n",
      "40/40 [==============================] - 0s 853us/step - loss: 0.2332 - accuracy: 0.9121\n",
      "Epoch 245/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2430 - accuracy: 0.9034\n",
      "Epoch 246/1500\n",
      "40/40 [==============================] - 0s 999us/step - loss: 0.2340 - accuracy: 0.9093\n",
      "Epoch 247/1500\n",
      "40/40 [==============================] - 0s 924us/step - loss: 0.2464 - accuracy: 0.9006\n",
      "Epoch 248/1500\n",
      "40/40 [==============================] - 0s 934us/step - loss: 0.2274 - accuracy: 0.9153\n",
      "Epoch 249/1500\n",
      "40/40 [==============================] - 0s 851us/step - loss: 0.2318 - accuracy: 0.9109\n",
      "Epoch 250/1500\n",
      "40/40 [==============================] - 0s 840us/step - loss: 0.2364 - accuracy: 0.8982\n",
      "Epoch 251/1500\n",
      "40/40 [==============================] - 0s 876us/step - loss: 0.2305 - accuracy: 0.9109\n",
      "Epoch 252/1500\n",
      "40/40 [==============================] - 0s 894us/step - loss: 0.2268 - accuracy: 0.9181\n",
      "Epoch 253/1500\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.2373 - accuracy: 0.9054\n",
      "Epoch 254/1500\n",
      "40/40 [==============================] - 0s 881us/step - loss: 0.2249 - accuracy: 0.9117\n",
      "Epoch 255/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2245 - accuracy: 0.9109\n",
      "Epoch 256/1500\n",
      "40/40 [==============================] - 0s 953us/step - loss: 0.2335 - accuracy: 0.9022\n",
      "Epoch 257/1500\n",
      "40/40 [==============================] - 0s 863us/step - loss: 0.2408 - accuracy: 0.9093\n",
      "Epoch 258/1500\n",
      "40/40 [==============================] - 0s 857us/step - loss: 0.2263 - accuracy: 0.9085\n",
      "Epoch 259/1500\n",
      "40/40 [==============================] - 0s 840us/step - loss: 0.2417 - accuracy: 0.9034\n",
      "Epoch 260/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.2325 - accuracy: 0.9125\n",
      "Epoch 261/1500\n",
      "40/40 [==============================] - 0s 870us/step - loss: 0.2369 - accuracy: 0.9062\n",
      "Epoch 262/1500\n",
      "40/40 [==============================] - 0s 842us/step - loss: 0.2291 - accuracy: 0.9117\n",
      "Epoch 263/1500\n",
      "40/40 [==============================] - 0s 860us/step - loss: 0.2287 - accuracy: 0.9137\n",
      "Epoch 264/1500\n",
      "40/40 [==============================] - 0s 888us/step - loss: 0.2287 - accuracy: 0.9109\n",
      "Epoch 265/1500\n",
      "40/40 [==============================] - 0s 888us/step - loss: 0.2160 - accuracy: 0.9101\n",
      "Epoch 266/1500\n",
      "40/40 [==============================] - 0s 862us/step - loss: 0.2257 - accuracy: 0.9077\n",
      "Epoch 267/1500\n",
      "40/40 [==============================] - 0s 951us/step - loss: 0.2274 - accuracy: 0.9105\n",
      "Epoch 268/1500\n",
      "40/40 [==============================] - 0s 840us/step - loss: 0.2377 - accuracy: 0.9065\n",
      "Epoch 269/1500\n",
      "40/40 [==============================] - 0s 929us/step - loss: 0.2228 - accuracy: 0.9081\n",
      "Epoch 270/1500\n",
      "40/40 [==============================] - 0s 938us/step - loss: 0.2234 - accuracy: 0.9125\n",
      "Epoch 271/1500\n",
      "40/40 [==============================] - 0s 980us/step - loss: 0.2275 - accuracy: 0.9069\n",
      "Epoch 272/1500\n",
      "40/40 [==============================] - 0s 930us/step - loss: 0.2308 - accuracy: 0.9062\n",
      "Epoch 273/1500\n",
      "40/40 [==============================] - 0s 986us/step - loss: 0.2201 - accuracy: 0.9141\n",
      "Epoch 274/1500\n",
      "40/40 [==============================] - 0s 963us/step - loss: 0.2394 - accuracy: 0.9002\n",
      "Epoch 275/1500\n",
      "40/40 [==============================] - 0s 989us/step - loss: 0.2283 - accuracy: 0.9101\n",
      "Epoch 276/1500\n",
      "40/40 [==============================] - 0s 946us/step - loss: 0.2309 - accuracy: 0.9085\n",
      "Epoch 277/1500\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.2329 - accuracy: 0.9026\n",
      "Epoch 278/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2267 - accuracy: 0.9077\n",
      "Epoch 279/1500\n",
      "40/40 [==============================] - 0s 964us/step - loss: 0.2128 - accuracy: 0.9185\n",
      "Epoch 280/1500\n",
      "40/40 [==============================] - 0s 961us/step - loss: 0.2322 - accuracy: 0.9097\n",
      "Epoch 281/1500\n",
      "40/40 [==============================] - 0s 955us/step - loss: 0.2129 - accuracy: 0.9117\n",
      "Epoch 282/1500\n",
      "40/40 [==============================] - 0s 968us/step - loss: 0.2275 - accuracy: 0.9101\n",
      "Epoch 283/1500\n",
      "40/40 [==============================] - 0s 922us/step - loss: 0.2189 - accuracy: 0.9105\n",
      "Epoch 284/1500\n",
      "40/40 [==============================] - 0s 833us/step - loss: 0.2119 - accuracy: 0.9125\n",
      "Epoch 285/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.9105\n",
      "Epoch 286/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.2220 - accuracy: 0.9081\n",
      "Epoch 287/1500\n",
      "40/40 [==============================] - 0s 896us/step - loss: 0.2303 - accuracy: 0.9058\n",
      "Epoch 288/1500\n",
      "40/40 [==============================] - 0s 826us/step - loss: 0.2064 - accuracy: 0.9253\n",
      "Epoch 289/1500\n",
      "40/40 [==============================] - 0s 833us/step - loss: 0.2130 - accuracy: 0.9097\n",
      "Epoch 290/1500\n",
      "40/40 [==============================] - 0s 854us/step - loss: 0.2225 - accuracy: 0.9181\n",
      "Epoch 291/1500\n",
      "40/40 [==============================] - 0s 840us/step - loss: 0.1943 - accuracy: 0.9261\n",
      "Epoch 292/1500\n",
      "40/40 [==============================] - 0s 817us/step - loss: 0.2178 - accuracy: 0.9121\n",
      "Epoch 293/1500\n",
      "40/40 [==============================] - 0s 828us/step - loss: 0.2235 - accuracy: 0.9157\n",
      "Epoch 294/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.2178 - accuracy: 0.9081\n",
      "Epoch 295/1500\n",
      "40/40 [==============================] - 0s 823us/step - loss: 0.2179 - accuracy: 0.9093\n",
      "Epoch 296/1500\n",
      "40/40 [==============================] - 0s 813us/step - loss: 0.2136 - accuracy: 0.9185\n",
      "Epoch 297/1500\n",
      "40/40 [==============================] - 0s 848us/step - loss: 0.2245 - accuracy: 0.9077\n",
      "Epoch 298/1500\n",
      "40/40 [==============================] - 0s 842us/step - loss: 0.2080 - accuracy: 0.9133\n",
      "Epoch 299/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.2204 - accuracy: 0.9113\n",
      "Epoch 300/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.2135 - accuracy: 0.9161\n",
      "Epoch 301/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.2150 - accuracy: 0.9121\n",
      "Epoch 302/1500\n",
      "40/40 [==============================] - 0s 800us/step - loss: 0.2111 - accuracy: 0.9153\n",
      "Epoch 303/1500\n",
      "40/40 [==============================] - 0s 800us/step - loss: 0.2116 - accuracy: 0.9181\n",
      "Epoch 304/1500\n",
      "40/40 [==============================] - 0s 881us/step - loss: 0.2272 - accuracy: 0.9089\n",
      "Epoch 305/1500\n",
      "40/40 [==============================] - 0s 866us/step - loss: 0.2016 - accuracy: 0.9201\n",
      "Epoch 306/1500\n",
      "40/40 [==============================] - 0s 881us/step - loss: 0.2031 - accuracy: 0.9221\n",
      "Epoch 307/1500\n",
      "40/40 [==============================] - 0s 874us/step - loss: 0.2096 - accuracy: 0.9221\n",
      "Epoch 308/1500\n",
      "40/40 [==============================] - 0s 889us/step - loss: 0.2103 - accuracy: 0.9153\n",
      "Epoch 309/1500\n",
      "40/40 [==============================] - 0s 848us/step - loss: 0.2071 - accuracy: 0.9141\n",
      "Epoch 310/1500\n",
      "40/40 [==============================] - 0s 835us/step - loss: 0.2015 - accuracy: 0.9193\n",
      "Epoch 311/1500\n",
      "40/40 [==============================] - 0s 841us/step - loss: 0.2033 - accuracy: 0.9249\n",
      "Epoch 312/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.2134 - accuracy: 0.9145\n",
      "Epoch 313/1500\n",
      "40/40 [==============================] - 0s 823us/step - loss: 0.2017 - accuracy: 0.9205\n",
      "Epoch 314/1500\n",
      "40/40 [==============================] - 0s 943us/step - loss: 0.2042 - accuracy: 0.9193\n",
      "Epoch 315/1500\n",
      "40/40 [==============================] - 0s 914us/step - loss: 0.2080 - accuracy: 0.9153\n",
      "Epoch 316/1500\n",
      "40/40 [==============================] - 0s 948us/step - loss: 0.1999 - accuracy: 0.9221\n",
      "Epoch 317/1500\n",
      "40/40 [==============================] - 0s 957us/step - loss: 0.2069 - accuracy: 0.9141\n",
      "Epoch 318/1500\n",
      "40/40 [==============================] - 0s 965us/step - loss: 0.2053 - accuracy: 0.9233\n",
      "Epoch 319/1500\n",
      "40/40 [==============================] - 0s 932us/step - loss: 0.2007 - accuracy: 0.9177\n",
      "Epoch 320/1500\n",
      "40/40 [==============================] - 0s 910us/step - loss: 0.2112 - accuracy: 0.9197\n",
      "Epoch 321/1500\n",
      "40/40 [==============================] - 0s 935us/step - loss: 0.1897 - accuracy: 0.9225\n",
      "Epoch 322/1500\n",
      "40/40 [==============================] - 0s 899us/step - loss: 0.2011 - accuracy: 0.9185\n",
      "Epoch 323/1500\n",
      "40/40 [==============================] - 0s 896us/step - loss: 0.2078 - accuracy: 0.9125\n",
      "Epoch 324/1500\n",
      "40/40 [==============================] - 0s 876us/step - loss: 0.1924 - accuracy: 0.9249\n",
      "Epoch 325/1500\n",
      "40/40 [==============================] - 0s 930us/step - loss: 0.2018 - accuracy: 0.9185\n",
      "Epoch 326/1500\n",
      "40/40 [==============================] - 0s 930us/step - loss: 0.2283 - accuracy: 0.9097\n",
      "Epoch 327/1500\n",
      "40/40 [==============================] - 0s 963us/step - loss: 0.1946 - accuracy: 0.9237\n",
      "Epoch 328/1500\n",
      "40/40 [==============================] - 0s 987us/step - loss: 0.2047 - accuracy: 0.9157\n",
      "Epoch 329/1500\n",
      "40/40 [==============================] - 0s 976us/step - loss: 0.2177 - accuracy: 0.9117\n",
      "Epoch 330/1500\n",
      "40/40 [==============================] - 0s 963us/step - loss: 0.2052 - accuracy: 0.9157\n",
      "Epoch 331/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9205\n",
      "Epoch 332/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.9165\n",
      "Epoch 333/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2129 - accuracy: 0.9169\n",
      "Epoch 334/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9221\n",
      "Epoch 335/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1932 - accuracy: 0.9233\n",
      "Epoch 336/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.2063 - accuracy: 0.9225\n",
      "Epoch 337/1500\n",
      "40/40 [==============================] - 0s 5ms/step - loss: 0.1986 - accuracy: 0.9241\n",
      "Epoch 338/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9173\n",
      "Epoch 339/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1941 - accuracy: 0.9249\n",
      "Epoch 340/1500\n",
      "40/40 [==============================] - 0s 991us/step - loss: 0.2133 - accuracy: 0.9161\n",
      "Epoch 341/1500\n",
      "40/40 [==============================] - 0s 976us/step - loss: 0.1902 - accuracy: 0.9225\n",
      "Epoch 342/1500\n",
      "40/40 [==============================] - 0s 939us/step - loss: 0.1987 - accuracy: 0.9225\n",
      "Epoch 343/1500\n",
      "40/40 [==============================] - 0s 868us/step - loss: 0.1935 - accuracy: 0.9241\n",
      "Epoch 344/1500\n",
      "40/40 [==============================] - 0s 853us/step - loss: 0.1982 - accuracy: 0.9205\n",
      "Epoch 345/1500\n",
      "40/40 [==============================] - 0s 874us/step - loss: 0.2128 - accuracy: 0.9137\n",
      "Epoch 346/1500\n",
      "40/40 [==============================] - 0s 929us/step - loss: 0.2005 - accuracy: 0.9165\n",
      "Epoch 347/1500\n",
      "40/40 [==============================] - 0s 964us/step - loss: 0.1973 - accuracy: 0.9233\n",
      "Epoch 348/1500\n",
      "40/40 [==============================] - 0s 956us/step - loss: 0.1834 - accuracy: 0.9249\n",
      "Epoch 349/1500\n",
      "40/40 [==============================] - 0s 890us/step - loss: 0.2035 - accuracy: 0.9169\n",
      "Epoch 350/1500\n",
      "40/40 [==============================] - 0s 844us/step - loss: 0.1961 - accuracy: 0.9217\n",
      "Epoch 351/1500\n",
      "40/40 [==============================] - 0s 824us/step - loss: 0.2066 - accuracy: 0.9205\n",
      "Epoch 352/1500\n",
      "40/40 [==============================] - 0s 894us/step - loss: 0.1909 - accuracy: 0.9285\n",
      "Epoch 353/1500\n",
      "40/40 [==============================] - 0s 832us/step - loss: 0.1960 - accuracy: 0.9277\n",
      "Epoch 354/1500\n",
      "40/40 [==============================] - 0s 834us/step - loss: 0.2107 - accuracy: 0.9141\n",
      "Epoch 355/1500\n",
      "40/40 [==============================] - 0s 842us/step - loss: 0.1873 - accuracy: 0.9237\n",
      "Epoch 356/1500\n",
      "40/40 [==============================] - 0s 788us/step - loss: 0.1967 - accuracy: 0.9269\n",
      "Epoch 357/1500\n",
      "40/40 [==============================] - 0s 835us/step - loss: 0.1964 - accuracy: 0.9193\n",
      "Epoch 358/1500\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.1947 - accuracy: 0.9249\n",
      "Epoch 359/1500\n",
      "40/40 [==============================] - 0s 827us/step - loss: 0.1832 - accuracy: 0.9253\n",
      "Epoch 360/1500\n",
      "40/40 [==============================] - 0s 830us/step - loss: 0.2035 - accuracy: 0.9181\n",
      "Epoch 361/1500\n",
      "40/40 [==============================] - 0s 870us/step - loss: 0.1868 - accuracy: 0.9261\n",
      "Epoch 362/1500\n",
      "40/40 [==============================] - 0s 806us/step - loss: 0.1898 - accuracy: 0.9313\n",
      "Epoch 363/1500\n",
      "40/40 [==============================] - 0s 859us/step - loss: 0.1892 - accuracy: 0.9245\n",
      "Epoch 364/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1891 - accuracy: 0.9261\n",
      "Epoch 365/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.9201\n",
      "Epoch 366/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1959 - accuracy: 0.9213\n",
      "Epoch 367/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 0.9237\n",
      "Epoch 368/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.1806 - accuracy: 0.9249\n",
      "Epoch 369/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9197\n",
      "Epoch 370/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.1824 - accuracy: 0.9285\n",
      "Epoch 371/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9269\n",
      "Epoch 372/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.9273\n",
      "Epoch 373/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9321\n",
      "Epoch 374/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1992 - accuracy: 0.9237\n",
      "Epoch 375/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.1847 - accuracy: 0.9281\n",
      "Epoch 376/1500\n",
      "40/40 [==============================] - 0s 958us/step - loss: 0.1933 - accuracy: 0.9181\n",
      "Epoch 377/1500\n",
      "40/40 [==============================] - 0s 902us/step - loss: 0.1949 - accuracy: 0.9245\n",
      "Epoch 378/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9181\n",
      "Epoch 379/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9169\n",
      "Epoch 380/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.1791 - accuracy: 0.9325\n",
      "Epoch 381/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.9313\n",
      "Epoch 382/1500\n",
      "40/40 [==============================] - 0s 980us/step - loss: 0.1791 - accuracy: 0.9229\n",
      "Epoch 383/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9329\n",
      "Epoch 384/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9293\n",
      "Epoch 385/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1941 - accuracy: 0.9181\n",
      "Epoch 386/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1943 - accuracy: 0.9257\n",
      "Epoch 387/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.9277\n",
      "Epoch 388/1500\n",
      "40/40 [==============================] - 0s 978us/step - loss: 0.1793 - accuracy: 0.9285\n",
      "Epoch 389/1500\n",
      "40/40 [==============================] - 0s 972us/step - loss: 0.2104 - accuracy: 0.9197\n",
      "Epoch 390/1500\n",
      "40/40 [==============================] - 0s 950us/step - loss: 0.1795 - accuracy: 0.9305\n",
      "Epoch 391/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9321\n",
      "Epoch 392/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1929 - accuracy: 0.9229\n",
      "Epoch 393/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1995 - accuracy: 0.9209\n",
      "Epoch 394/1500\n",
      "40/40 [==============================] - 0s 906us/step - loss: 0.1870 - accuracy: 0.9277\n",
      "Epoch 395/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.9309\n",
      "Epoch 396/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.1869 - accuracy: 0.9257\n",
      "Epoch 397/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.1821 - accuracy: 0.9221\n",
      "Epoch 398/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9269\n",
      "Epoch 399/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.1742 - accuracy: 0.9293\n",
      "Epoch 400/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.1778 - accuracy: 0.9265\n",
      "Epoch 401/1500\n",
      "40/40 [==============================] - 0s 4ms/step - loss: 0.1715 - accuracy: 0.9345\n",
      "Epoch 402/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.1842 - accuracy: 0.9281\n",
      "Epoch 403/1500\n",
      "40/40 [==============================] - 0s 7ms/step - loss: 0.1811 - accuracy: 0.9281\n",
      "Epoch 404/1500\n",
      "40/40 [==============================] - 0s 3ms/step - loss: 0.1882 - accuracy: 0.9309\n",
      "Epoch 405/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.9333\n",
      "Epoch 406/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9261\n",
      "Epoch 407/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1722 - accuracy: 0.9357\n",
      "Epoch 408/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9305\n",
      "Epoch 409/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.9257\n",
      "Epoch 410/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9317\n",
      "Epoch 411/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.9313\n",
      "Epoch 412/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.9273\n",
      "Epoch 413/1500\n",
      "40/40 [==============================] - 0s 934us/step - loss: 0.1739 - accuracy: 0.9373\n",
      "Epoch 414/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9377\n",
      "Epoch 415/1500\n",
      "40/40 [==============================] - 0s 930us/step - loss: 0.1837 - accuracy: 0.9293\n",
      "Epoch 416/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.9365\n",
      "Epoch 417/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.2009 - accuracy: 0.9241\n",
      "Epoch 418/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1881 - accuracy: 0.9313\n",
      "Epoch 419/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1929 - accuracy: 0.9233\n",
      "Epoch 420/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.9373\n",
      "Epoch 421/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9353\n",
      "Epoch 422/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9305\n",
      "Epoch 423/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9285\n",
      "Epoch 424/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9341\n",
      "Epoch 425/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1799 - accuracy: 0.9305\n",
      "Epoch 426/1500\n",
      "40/40 [==============================] - 0s 991us/step - loss: 0.1743 - accuracy: 0.9317\n",
      "Epoch 427/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9285\n",
      "Epoch 428/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9249\n",
      "Epoch 429/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1748 - accuracy: 0.9305\n",
      "Epoch 430/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9301\n",
      "Epoch 431/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.9365\n",
      "Epoch 432/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.9329\n",
      "Epoch 433/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.9277\n",
      "Epoch 434/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.9297\n",
      "Epoch 435/1500\n",
      "40/40 [==============================] - 0s 995us/step - loss: 0.1630 - accuracy: 0.9361\n",
      "Epoch 436/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1787 - accuracy: 0.9361\n",
      "Epoch 437/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9293\n",
      "Epoch 438/1500\n",
      "40/40 [==============================] - 0s 952us/step - loss: 0.1818 - accuracy: 0.9273\n",
      "Epoch 439/1500\n",
      "40/40 [==============================] - 0s 872us/step - loss: 0.1703 - accuracy: 0.9309\n",
      "Epoch 440/1500\n",
      "40/40 [==============================] - 0s 936us/step - loss: 0.1731 - accuracy: 0.9389\n",
      "Epoch 441/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9281\n",
      "Epoch 442/1500\n",
      "40/40 [==============================] - 0s 977us/step - loss: 0.1761 - accuracy: 0.9341\n",
      "Epoch 443/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9409\n",
      "Epoch 444/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1771 - accuracy: 0.9305\n",
      "Epoch 445/1500\n",
      "40/40 [==============================] - 0s 6ms/step - loss: 0.1792 - accuracy: 0.9237\n",
      "Epoch 446/1500\n",
      "40/40 [==============================] - 0s 2ms/step - loss: 0.1865 - accuracy: 0.9261\n",
      "Epoch 447/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9313\n",
      "Epoch 448/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9397\n",
      "Epoch 449/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.9381\n",
      "Epoch 450/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.9281\n",
      "Epoch 451/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9397\n",
      "Epoch 452/1500\n",
      "40/40 [==============================] - 0s 887us/step - loss: 0.1741 - accuracy: 0.9313\n",
      "Epoch 453/1500\n",
      "40/40 [==============================] - 0s 912us/step - loss: 0.1665 - accuracy: 0.9333\n",
      "Epoch 454/1500\n",
      "40/40 [==============================] - 0s 826us/step - loss: 0.1650 - accuracy: 0.9349\n",
      "Epoch 455/1500\n",
      "40/40 [==============================] - 0s 808us/step - loss: 0.1732 - accuracy: 0.9289\n",
      "Epoch 456/1500\n",
      "40/40 [==============================] - 0s 831us/step - loss: 0.1645 - accuracy: 0.9381\n",
      "Epoch 457/1500\n",
      "40/40 [==============================] - 0s 809us/step - loss: 0.1606 - accuracy: 0.9357\n",
      "Epoch 458/1500\n",
      "40/40 [==============================] - 0s 810us/step - loss: 0.1777 - accuracy: 0.9369\n",
      "Epoch 459/1500\n",
      "40/40 [==============================] - 0s 874us/step - loss: 0.1746 - accuracy: 0.9341\n",
      "Epoch 460/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.9317\n",
      "Epoch 461/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1719 - accuracy: 0.9293\n",
      "Epoch 462/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9341\n",
      "Epoch 463/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1799 - accuracy: 0.9261\n",
      "Epoch 464/1500\n",
      "40/40 [==============================] - 0s 994us/step - loss: 0.1691 - accuracy: 0.9277\n",
      "Epoch 465/1500\n",
      "40/40 [==============================] - 0s 896us/step - loss: 0.1836 - accuracy: 0.9269\n",
      "Epoch 466/1500\n",
      "40/40 [==============================] - 0s 829us/step - loss: 0.1842 - accuracy: 0.9301\n",
      "Epoch 467/1500\n",
      "40/40 [==============================] - 0s 852us/step - loss: 0.1689 - accuracy: 0.9317\n",
      "Epoch 468/1500\n",
      "40/40 [==============================] - 0s 862us/step - loss: 0.1712 - accuracy: 0.9269\n",
      "Epoch 469/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1643 - accuracy: 0.9377\n",
      "Epoch 470/1500\n",
      "40/40 [==============================] - 0s 962us/step - loss: 0.1826 - accuracy: 0.9321\n",
      "Epoch 471/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9341\n",
      "Epoch 472/1500\n",
      "40/40 [==============================] - 0s 864us/step - loss: 0.1615 - accuracy: 0.9381\n",
      "Epoch 473/1500\n",
      "40/40 [==============================] - 0s 845us/step - loss: 0.1681 - accuracy: 0.9349\n",
      "Epoch 474/1500\n",
      "40/40 [==============================] - 0s 841us/step - loss: 0.1610 - accuracy: 0.9401\n",
      "Epoch 475/1500\n",
      "40/40 [==============================] - 0s 867us/step - loss: 0.1568 - accuracy: 0.9413\n",
      "Epoch 476/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1719 - accuracy: 0.9329\n",
      "Epoch 477/1500\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9337\n",
      "Epoch 478/1500\n",
      " 1/40 [..............................] - ETA: 0s - loss: 0.2311 - accuracy: 0.9219Restoring model weights from the end of the best epoch: 448.\n",
      "40/40 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9401\n",
      "Epoch 478: early stopping\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.9091 - accuracy: 0.6928\n",
      "5/5 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.64 (16/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "Final Test Results - Loss: 0.9090795516967773, Accuracy: 0.6928104758262634, Precision: 0.5799484466151132, Recall: 0.6857500736811083, F1 Score: 0.5950198920147769\n",
      "Confusion Matrix:\n",
      " [[81 11 24]\n",
      " [ 6 16  2]\n",
      " [ 4  0  9]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "005A    10\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "027A     7\n",
      "008A     6\n",
      "108A     6\n",
      "109A     6\n",
      "053A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "003A     4\n",
      "105A     4\n",
      "009A     4\n",
      "026A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "012A     3\n",
      "113A     3\n",
      "014A     3\n",
      "060A     3\n",
      "011A     2\n",
      "102A     2\n",
      "032A     2\n",
      "069A     2\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "038A     2\n",
      "019B     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "092A     1\n",
      "004A     1\n",
      "041A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "073A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "067A    19\n",
      "042A    14\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "063A    11\n",
      "016A    10\n",
      "033A     9\n",
      "051B     9\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "117A     7\n",
      "050A     7\n",
      "007A     6\n",
      "075A     5\n",
      "025C     5\n",
      "087A     2\n",
      "061A     2\n",
      "025B     2\n",
      "043A     1\n",
      "066A     1\n",
      "048A     1\n",
      "088A     1\n",
      "091A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    263\n",
      "M    226\n",
      "F    177\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    111\n",
      "X     85\n",
      "F     75\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 015A, 001A, 103A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 109A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 113...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 067A, 002B, 091A, 039A, 063A, 013...\n",
      "kitten                       [047A, 042A, 050A, 043A, 048A]\n",
      "senior                 [116A, 051B, 117A, 016A, 061A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 11, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 5, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A' '011A'\n",
      " '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A' '045A'\n",
      " '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '062A' '064A' '065A' '069A' '070A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A'\n",
      " '039A' '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A'\n",
      " '067A' '068A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'070A'}\n",
      "Moved to Test Set:\n",
      "{'070A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A'\n",
      " '011A' '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A'\n",
      " '031A' '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A'\n",
      " '045A' '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '059A' '060A' '062A' '064A' '065A' '069A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A' '039A'\n",
      " '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A' '067A'\n",
      " '068A' '070A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "Length of X_train_val:\n",
      "700\n",
      "Length of y_train_val:\n",
      "700\n",
      "Length of groups_train_val:\n",
      "700\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     409\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     179\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     443\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     145\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 886, 2: 685, 1: 600})\n",
      "Epoch 1/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.9995 - accuracy: 0.5302\n",
      "Epoch 2/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8507 - accuracy: 0.6149\n",
      "Epoch 3/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.8076 - accuracy: 0.6527\n",
      "Epoch 4/1500\n",
      "34/34 [==============================] - 0s 928us/step - loss: 0.7647 - accuracy: 0.6707\n",
      "Epoch 5/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.7592 - accuracy: 0.6697\n",
      "Epoch 6/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.7398 - accuracy: 0.6753\n",
      "Epoch 7/1500\n",
      "34/34 [==============================] - 0s 875us/step - loss: 0.7162 - accuracy: 0.6817\n",
      "Epoch 8/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.7012 - accuracy: 0.6891\n",
      "Epoch 9/1500\n",
      "34/34 [==============================] - 0s 955us/step - loss: 0.6970 - accuracy: 0.6932\n",
      "Epoch 10/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6852 - accuracy: 0.7029\n",
      "Epoch 11/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6561 - accuracy: 0.7117\n",
      "Epoch 12/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6440 - accuracy: 0.7255\n",
      "Epoch 13/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6472 - accuracy: 0.7144\n",
      "Epoch 14/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6312 - accuracy: 0.7310\n",
      "Epoch 15/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6602 - accuracy: 0.7149\n",
      "Epoch 16/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6163 - accuracy: 0.7324\n",
      "Epoch 17/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.6035 - accuracy: 0.7402\n",
      "Epoch 18/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5942 - accuracy: 0.7296\n",
      "Epoch 19/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5724 - accuracy: 0.7471\n",
      "Epoch 20/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.5770 - accuracy: 0.7462\n",
      "Epoch 21/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5927 - accuracy: 0.7361\n",
      "Epoch 22/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.5890 - accuracy: 0.7480\n",
      "Epoch 23/1500\n",
      "34/34 [==============================] - 0s 936us/step - loss: 0.5619 - accuracy: 0.7462\n",
      "Epoch 24/1500\n",
      "34/34 [==============================] - 0s 963us/step - loss: 0.5705 - accuracy: 0.7480\n",
      "Epoch 25/1500\n",
      "34/34 [==============================] - 0s 880us/step - loss: 0.5621 - accuracy: 0.7513\n",
      "Epoch 26/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.5520 - accuracy: 0.7600\n",
      "Epoch 27/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.5620 - accuracy: 0.7457\n",
      "Epoch 28/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.5503 - accuracy: 0.7632\n",
      "Epoch 29/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.5319 - accuracy: 0.7766\n",
      "Epoch 30/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.5327 - accuracy: 0.7660\n",
      "Epoch 31/1500\n",
      "34/34 [==============================] - 0s 845us/step - loss: 0.5321 - accuracy: 0.7711\n",
      "Epoch 32/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.5034 - accuracy: 0.7738\n",
      "Epoch 33/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.5296 - accuracy: 0.7573\n",
      "Epoch 34/1500\n",
      "34/34 [==============================] - 0s 837us/step - loss: 0.5292 - accuracy: 0.7748\n",
      "Epoch 35/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.5008 - accuracy: 0.7748\n",
      "Epoch 36/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.5000 - accuracy: 0.7798\n",
      "Epoch 37/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.5001 - accuracy: 0.7826\n",
      "Epoch 38/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.5070 - accuracy: 0.7775\n",
      "Epoch 39/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.4975 - accuracy: 0.7969\n",
      "Epoch 40/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.5058 - accuracy: 0.7729\n",
      "Epoch 41/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.4788 - accuracy: 0.7895\n",
      "Epoch 42/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.4917 - accuracy: 0.7835\n",
      "Epoch 43/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.4828 - accuracy: 0.7835\n",
      "Epoch 44/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.4774 - accuracy: 0.7895\n",
      "Epoch 45/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.4820 - accuracy: 0.7900\n",
      "Epoch 46/1500\n",
      "34/34 [==============================] - 0s 852us/step - loss: 0.4677 - accuracy: 0.7923\n",
      "Epoch 47/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.4807 - accuracy: 0.7964\n",
      "Epoch 48/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.4797 - accuracy: 0.7913\n",
      "Epoch 49/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.4679 - accuracy: 0.8065\n",
      "Epoch 50/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.4589 - accuracy: 0.7909\n",
      "Epoch 51/1500\n",
      "34/34 [==============================] - 0s 850us/step - loss: 0.4558 - accuracy: 0.8038\n",
      "Epoch 52/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.4388 - accuracy: 0.8042\n",
      "Epoch 53/1500\n",
      "34/34 [==============================] - 0s 870us/step - loss: 0.4536 - accuracy: 0.8098\n",
      "Epoch 54/1500\n",
      "34/34 [==============================] - 0s 844us/step - loss: 0.4416 - accuracy: 0.8171\n",
      "Epoch 55/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.4402 - accuracy: 0.8102\n",
      "Epoch 56/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.4426 - accuracy: 0.8102\n",
      "Epoch 57/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.4440 - accuracy: 0.8024\n",
      "Epoch 58/1500\n",
      "34/34 [==============================] - 0s 835us/step - loss: 0.4153 - accuracy: 0.8254\n",
      "Epoch 59/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.4593 - accuracy: 0.7959\n",
      "Epoch 60/1500\n",
      "34/34 [==============================] - 0s 836us/step - loss: 0.4491 - accuracy: 0.8079\n",
      "Epoch 61/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.4271 - accuracy: 0.8167\n",
      "Epoch 62/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.4319 - accuracy: 0.8162\n",
      "Epoch 63/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4277 - accuracy: 0.8102\n",
      "Epoch 64/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4303 - accuracy: 0.8190\n",
      "Epoch 65/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4216 - accuracy: 0.8231\n",
      "Epoch 66/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8176\n",
      "Epoch 67/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4244 - accuracy: 0.8231\n",
      "Epoch 68/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4041 - accuracy: 0.8277\n",
      "Epoch 69/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4028 - accuracy: 0.8245\n",
      "Epoch 70/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4160 - accuracy: 0.8208\n",
      "Epoch 71/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4175 - accuracy: 0.8135\n",
      "Epoch 72/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4134 - accuracy: 0.8273\n",
      "Epoch 73/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3998 - accuracy: 0.8263\n",
      "Epoch 74/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.4117 - accuracy: 0.8171\n",
      "Epoch 75/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4245 - accuracy: 0.8061\n",
      "Epoch 76/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4064 - accuracy: 0.8268\n",
      "Epoch 77/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.4095 - accuracy: 0.8190\n",
      "Epoch 78/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3931 - accuracy: 0.8305\n",
      "Epoch 79/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3871 - accuracy: 0.8259\n",
      "Epoch 80/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3773 - accuracy: 0.8360\n",
      "Epoch 81/1500\n",
      "34/34 [==============================] - 0s 924us/step - loss: 0.3985 - accuracy: 0.8319\n",
      "Epoch 82/1500\n",
      "34/34 [==============================] - 0s 912us/step - loss: 0.3788 - accuracy: 0.8314\n",
      "Epoch 83/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8314\n",
      "Epoch 84/1500\n",
      "34/34 [==============================] - 0s 957us/step - loss: 0.3890 - accuracy: 0.8305\n",
      "Epoch 85/1500\n",
      "34/34 [==============================] - 0s 908us/step - loss: 0.3761 - accuracy: 0.8356\n",
      "Epoch 86/1500\n",
      "34/34 [==============================] - 0s 961us/step - loss: 0.3900 - accuracy: 0.8305\n",
      "Epoch 87/1500\n",
      "34/34 [==============================] - 0s 892us/step - loss: 0.3776 - accuracy: 0.8379\n",
      "Epoch 88/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.3790 - accuracy: 0.8439\n",
      "Epoch 89/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.3656 - accuracy: 0.8448\n",
      "Epoch 90/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.3732 - accuracy: 0.8406\n",
      "Epoch 91/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3767 - accuracy: 0.8462\n",
      "Epoch 92/1500\n",
      "34/34 [==============================] - 0s 924us/step - loss: 0.3862 - accuracy: 0.8300\n",
      "Epoch 93/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.3733 - accuracy: 0.8374\n",
      "Epoch 94/1500\n",
      "34/34 [==============================] - 0s 928us/step - loss: 0.3843 - accuracy: 0.8411\n",
      "Epoch 95/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.3759 - accuracy: 0.8356\n",
      "Epoch 96/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.3683 - accuracy: 0.8462\n",
      "Epoch 97/1500\n",
      "34/34 [==============================] - 0s 843us/step - loss: 0.3508 - accuracy: 0.8471\n",
      "Epoch 98/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.3703 - accuracy: 0.8494\n",
      "Epoch 99/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.3645 - accuracy: 0.8420\n",
      "Epoch 100/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.3536 - accuracy: 0.8457\n",
      "Epoch 101/1500\n",
      "34/34 [==============================] - 0s 872us/step - loss: 0.3657 - accuracy: 0.8420\n",
      "Epoch 102/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.3709 - accuracy: 0.8406\n",
      "Epoch 103/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.3535 - accuracy: 0.8526\n",
      "Epoch 104/1500\n",
      "34/34 [==============================] - 0s 887us/step - loss: 0.3498 - accuracy: 0.8646\n",
      "Epoch 105/1500\n",
      "34/34 [==============================] - 0s 889us/step - loss: 0.3610 - accuracy: 0.8508\n",
      "Epoch 106/1500\n",
      "34/34 [==============================] - 0s 891us/step - loss: 0.3465 - accuracy: 0.8494\n",
      "Epoch 107/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.3431 - accuracy: 0.8466\n",
      "Epoch 108/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.8609\n",
      "Epoch 109/1500\n",
      "34/34 [==============================] - 0s 966us/step - loss: 0.3634 - accuracy: 0.8462\n",
      "Epoch 110/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3393 - accuracy: 0.8521\n",
      "Epoch 111/1500\n",
      "34/34 [==============================] - 0s 984us/step - loss: 0.3437 - accuracy: 0.8544\n",
      "Epoch 112/1500\n",
      "34/34 [==============================] - 0s 946us/step - loss: 0.3575 - accuracy: 0.8383\n",
      "Epoch 113/1500\n",
      "34/34 [==============================] - 0s 849us/step - loss: 0.3444 - accuracy: 0.8526\n",
      "Epoch 114/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.3377 - accuracy: 0.8535\n",
      "Epoch 115/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.3385 - accuracy: 0.8535\n",
      "Epoch 116/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.3373 - accuracy: 0.8554\n",
      "Epoch 117/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3299 - accuracy: 0.8655\n",
      "Epoch 118/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.8434\n",
      "Epoch 119/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.3377 - accuracy: 0.8554\n",
      "Epoch 120/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.3225 - accuracy: 0.8637\n",
      "Epoch 121/1500\n",
      "34/34 [==============================] - 0s 871us/step - loss: 0.3238 - accuracy: 0.8637\n",
      "Epoch 122/1500\n",
      "34/34 [==============================] - 0s 977us/step - loss: 0.3348 - accuracy: 0.8535\n",
      "Epoch 123/1500\n",
      "34/34 [==============================] - 0s 971us/step - loss: 0.3285 - accuracy: 0.8614\n",
      "Epoch 124/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.3393 - accuracy: 0.8549\n",
      "Epoch 125/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.3242 - accuracy: 0.8567\n",
      "Epoch 126/1500\n",
      "34/34 [==============================] - 0s 888us/step - loss: 0.3272 - accuracy: 0.8577\n",
      "Epoch 127/1500\n",
      "34/34 [==============================] - 0s 948us/step - loss: 0.3262 - accuracy: 0.8678\n",
      "Epoch 128/1500\n",
      "34/34 [==============================] - 0s 841us/step - loss: 0.3170 - accuracy: 0.8655\n",
      "Epoch 129/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.3256 - accuracy: 0.8655\n",
      "Epoch 130/1500\n",
      "34/34 [==============================] - 0s 911us/step - loss: 0.3146 - accuracy: 0.8618\n",
      "Epoch 131/1500\n",
      "34/34 [==============================] - 0s 928us/step - loss: 0.3282 - accuracy: 0.8637\n",
      "Epoch 132/1500\n",
      "34/34 [==============================] - 0s 982us/step - loss: 0.3233 - accuracy: 0.8673\n",
      "Epoch 133/1500\n",
      "34/34 [==============================] - 0s 966us/step - loss: 0.3099 - accuracy: 0.8664\n",
      "Epoch 134/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.3165 - accuracy: 0.8655\n",
      "Epoch 135/1500\n",
      "34/34 [==============================] - 0s 935us/step - loss: 0.3171 - accuracy: 0.8733\n",
      "Epoch 136/1500\n",
      "34/34 [==============================] - 0s 983us/step - loss: 0.3092 - accuracy: 0.8775\n",
      "Epoch 137/1500\n",
      "34/34 [==============================] - 0s 855us/step - loss: 0.3118 - accuracy: 0.8733\n",
      "Epoch 138/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.2987 - accuracy: 0.8793\n",
      "Epoch 139/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.3097 - accuracy: 0.8743\n",
      "Epoch 140/1500\n",
      "34/34 [==============================] - 0s 866us/step - loss: 0.2994 - accuracy: 0.8775\n",
      "Epoch 141/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.3003 - accuracy: 0.8715\n",
      "Epoch 142/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.3086 - accuracy: 0.8673\n",
      "Epoch 143/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.3033 - accuracy: 0.8719\n",
      "Epoch 144/1500\n",
      "34/34 [==============================] - 0s 772us/step - loss: 0.3011 - accuracy: 0.8719\n",
      "Epoch 145/1500\n",
      "34/34 [==============================] - 0s 782us/step - loss: 0.2925 - accuracy: 0.8789\n",
      "Epoch 146/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.2992 - accuracy: 0.8779\n",
      "Epoch 147/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.3074 - accuracy: 0.8743\n",
      "Epoch 148/1500\n",
      "34/34 [==============================] - 0s 904us/step - loss: 0.3140 - accuracy: 0.8701\n",
      "Epoch 149/1500\n",
      "34/34 [==============================] - 0s 921us/step - loss: 0.3165 - accuracy: 0.8669\n",
      "Epoch 150/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.3045 - accuracy: 0.8692\n",
      "Epoch 151/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.2968 - accuracy: 0.8729\n",
      "Epoch 152/1500\n",
      "34/34 [==============================] - 0s 739us/step - loss: 0.3125 - accuracy: 0.8655\n",
      "Epoch 153/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.2852 - accuracy: 0.8779\n",
      "Epoch 154/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.3017 - accuracy: 0.8687\n",
      "Epoch 155/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2913 - accuracy: 0.8848\n",
      "Epoch 156/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2916 - accuracy: 0.8844\n",
      "Epoch 157/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2830 - accuracy: 0.8922\n",
      "Epoch 158/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2836 - accuracy: 0.8784\n",
      "Epoch 159/1500\n",
      "34/34 [==============================] - 0s 842us/step - loss: 0.2866 - accuracy: 0.8770\n",
      "Epoch 160/1500\n",
      "34/34 [==============================] - 0s 869us/step - loss: 0.2715 - accuracy: 0.8899\n",
      "Epoch 161/1500\n",
      "34/34 [==============================] - 0s 833us/step - loss: 0.2816 - accuracy: 0.8871\n",
      "Epoch 162/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.2935 - accuracy: 0.8761\n",
      "Epoch 163/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2865 - accuracy: 0.8825\n",
      "Epoch 164/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.2868 - accuracy: 0.8761\n",
      "Epoch 165/1500\n",
      "34/34 [==============================] - 0s 802us/step - loss: 0.2942 - accuracy: 0.8733\n",
      "Epoch 166/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.2788 - accuracy: 0.8858\n",
      "Epoch 167/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.2814 - accuracy: 0.8816\n",
      "Epoch 168/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2753 - accuracy: 0.8918\n",
      "Epoch 169/1500\n",
      "34/34 [==============================] - 0s 740us/step - loss: 0.2907 - accuracy: 0.8895\n",
      "Epoch 170/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.2781 - accuracy: 0.8775\n",
      "Epoch 171/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.2617 - accuracy: 0.8853\n",
      "Epoch 172/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.2686 - accuracy: 0.8830\n",
      "Epoch 173/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.2850 - accuracy: 0.8812\n",
      "Epoch 174/1500\n",
      "34/34 [==============================] - 0s 803us/step - loss: 0.2735 - accuracy: 0.8848\n",
      "Epoch 175/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2696 - accuracy: 0.8931\n",
      "Epoch 176/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.2802 - accuracy: 0.8835\n",
      "Epoch 177/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.2622 - accuracy: 0.8913\n",
      "Epoch 178/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.2675 - accuracy: 0.8904\n",
      "Epoch 179/1500\n",
      "34/34 [==============================] - 0s 913us/step - loss: 0.2659 - accuracy: 0.8918\n",
      "Epoch 180/1500\n",
      "34/34 [==============================] - 0s 961us/step - loss: 0.2672 - accuracy: 0.8881\n",
      "Epoch 181/1500\n",
      "34/34 [==============================] - 0s 974us/step - loss: 0.2647 - accuracy: 0.8835\n",
      "Epoch 182/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2630 - accuracy: 0.8913\n",
      "Epoch 183/1500\n",
      "34/34 [==============================] - 0s 968us/step - loss: 0.2616 - accuracy: 0.8954\n",
      "Epoch 184/1500\n",
      "34/34 [==============================] - 0s 969us/step - loss: 0.2561 - accuracy: 0.8950\n",
      "Epoch 185/1500\n",
      "34/34 [==============================] - 0s 943us/step - loss: 0.2606 - accuracy: 0.8941\n",
      "Epoch 186/1500\n",
      "34/34 [==============================] - 0s 852us/step - loss: 0.2621 - accuracy: 0.8904\n",
      "Epoch 187/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.2501 - accuracy: 0.9037\n",
      "Epoch 188/1500\n",
      "34/34 [==============================] - 0s 795us/step - loss: 0.2672 - accuracy: 0.8839\n",
      "Epoch 189/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.2543 - accuracy: 0.8959\n",
      "Epoch 190/1500\n",
      "34/34 [==============================] - 0s 780us/step - loss: 0.2545 - accuracy: 0.9005\n",
      "Epoch 191/1500\n",
      "34/34 [==============================] - 0s 764us/step - loss: 0.2698 - accuracy: 0.8885\n",
      "Epoch 192/1500\n",
      "34/34 [==============================] - 0s 874us/step - loss: 0.2602 - accuracy: 0.8954\n",
      "Epoch 193/1500\n",
      "34/34 [==============================] - 0s 934us/step - loss: 0.2448 - accuracy: 0.8968\n",
      "Epoch 194/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.2509 - accuracy: 0.8964\n",
      "Epoch 195/1500\n",
      "34/34 [==============================] - 0s 919us/step - loss: 0.2549 - accuracy: 0.8991\n",
      "Epoch 196/1500\n",
      "34/34 [==============================] - 0s 931us/step - loss: 0.2544 - accuracy: 0.8954\n",
      "Epoch 197/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2401 - accuracy: 0.9000\n",
      "Epoch 198/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2450 - accuracy: 0.9023\n",
      "Epoch 199/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.8945\n",
      "Epoch 200/1500\n",
      "34/34 [==============================] - 0s 976us/step - loss: 0.2337 - accuracy: 0.9056\n",
      "Epoch 201/1500\n",
      "34/34 [==============================] - 0s 864us/step - loss: 0.2583 - accuracy: 0.8927\n",
      "Epoch 202/1500\n",
      "34/34 [==============================] - 0s 967us/step - loss: 0.2445 - accuracy: 0.9033\n",
      "Epoch 203/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.2372 - accuracy: 0.9014\n",
      "Epoch 204/1500\n",
      "34/34 [==============================] - 0s 927us/step - loss: 0.2387 - accuracy: 0.9088\n",
      "Epoch 205/1500\n",
      "34/34 [==============================] - 0s 817us/step - loss: 0.2516 - accuracy: 0.8844\n",
      "Epoch 206/1500\n",
      "34/34 [==============================] - 0s 805us/step - loss: 0.2491 - accuracy: 0.8996\n",
      "Epoch 207/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.2485 - accuracy: 0.8991\n",
      "Epoch 208/1500\n",
      "34/34 [==============================] - 0s 775us/step - loss: 0.2564 - accuracy: 0.8941\n",
      "Epoch 209/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.2389 - accuracy: 0.9042\n",
      "Epoch 210/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.2410 - accuracy: 0.9070\n",
      "Epoch 211/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.2382 - accuracy: 0.9060\n",
      "Epoch 212/1500\n",
      "34/34 [==============================] - 0s 908us/step - loss: 0.2342 - accuracy: 0.9028\n",
      "Epoch 213/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.2256 - accuracy: 0.9143\n",
      "Epoch 214/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.2373 - accuracy: 0.9047\n",
      "Epoch 215/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2310 - accuracy: 0.9060\n",
      "Epoch 216/1500\n",
      "34/34 [==============================] - 0s 786us/step - loss: 0.2486 - accuracy: 0.8977\n",
      "Epoch 217/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.2488 - accuracy: 0.9014\n",
      "Epoch 218/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.2289 - accuracy: 0.9083\n",
      "Epoch 219/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.2266 - accuracy: 0.9093\n",
      "Epoch 220/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2390 - accuracy: 0.9083\n",
      "Epoch 221/1500\n",
      "34/34 [==============================] - 0s 893us/step - loss: 0.2310 - accuracy: 0.9102\n",
      "Epoch 222/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.2485 - accuracy: 0.9047\n",
      "Epoch 223/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.2276 - accuracy: 0.9037\n",
      "Epoch 224/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2306 - accuracy: 0.9019\n",
      "Epoch 225/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.2327 - accuracy: 0.9070\n",
      "Epoch 226/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2434 - accuracy: 0.9056\n",
      "Epoch 227/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.2283 - accuracy: 0.9102\n",
      "Epoch 228/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2169 - accuracy: 0.9171\n",
      "Epoch 229/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2207 - accuracy: 0.9171\n",
      "Epoch 230/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2251 - accuracy: 0.9116\n",
      "Epoch 231/1500\n",
      "34/34 [==============================] - 0s 958us/step - loss: 0.2228 - accuracy: 0.9171\n",
      "Epoch 232/1500\n",
      "34/34 [==============================] - 0s 993us/step - loss: 0.2295 - accuracy: 0.9106\n",
      "Epoch 233/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2358 - accuracy: 0.9106\n",
      "Epoch 234/1500\n",
      "34/34 [==============================] - 0s 785us/step - loss: 0.2297 - accuracy: 0.9060\n",
      "Epoch 235/1500\n",
      "34/34 [==============================] - 0s 771us/step - loss: 0.2178 - accuracy: 0.9093\n",
      "Epoch 236/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2136 - accuracy: 0.9111\n",
      "Epoch 237/1500\n",
      "34/34 [==============================] - 0s 716us/step - loss: 0.2176 - accuracy: 0.9194\n",
      "Epoch 238/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.2285 - accuracy: 0.9023\n",
      "Epoch 239/1500\n",
      "34/34 [==============================] - 0s 808us/step - loss: 0.2357 - accuracy: 0.9065\n",
      "Epoch 240/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.2048 - accuracy: 0.9175\n",
      "Epoch 241/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.2057 - accuracy: 0.9143\n",
      "Epoch 242/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.2240 - accuracy: 0.9102\n",
      "Epoch 243/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.2230 - accuracy: 0.9097\n",
      "Epoch 244/1500\n",
      "34/34 [==============================] - 0s 804us/step - loss: 0.2232 - accuracy: 0.9070\n",
      "Epoch 245/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.2207 - accuracy: 0.9116\n",
      "Epoch 246/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1996 - accuracy: 0.9258\n",
      "Epoch 247/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.2014 - accuracy: 0.9203\n",
      "Epoch 248/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.2079 - accuracy: 0.9217\n",
      "Epoch 249/1500\n",
      "34/34 [==============================] - 0s 717us/step - loss: 0.2304 - accuracy: 0.9047\n",
      "Epoch 250/1500\n",
      "34/34 [==============================] - 0s 717us/step - loss: 0.2107 - accuracy: 0.9152\n",
      "Epoch 251/1500\n",
      "34/34 [==============================] - 0s 776us/step - loss: 0.2123 - accuracy: 0.9166\n",
      "Epoch 252/1500\n",
      "34/34 [==============================] - 0s 725us/step - loss: 0.2218 - accuracy: 0.9189\n",
      "Epoch 253/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.2043 - accuracy: 0.9199\n",
      "Epoch 254/1500\n",
      "34/34 [==============================] - 0s 766us/step - loss: 0.1995 - accuracy: 0.9212\n",
      "Epoch 255/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.2003 - accuracy: 0.9272\n",
      "Epoch 256/1500\n",
      "34/34 [==============================] - 0s 6ms/step - loss: 0.2031 - accuracy: 0.9245\n",
      "Epoch 257/1500\n",
      "34/34 [==============================] - 0s 2ms/step - loss: 0.2126 - accuracy: 0.9148\n",
      "Epoch 258/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1942 - accuracy: 0.9217\n",
      "Epoch 259/1500\n",
      "34/34 [==============================] - 0s 901us/step - loss: 0.2117 - accuracy: 0.9162\n",
      "Epoch 260/1500\n",
      "34/34 [==============================] - 0s 917us/step - loss: 0.2001 - accuracy: 0.9203\n",
      "Epoch 261/1500\n",
      "34/34 [==============================] - 0s 838us/step - loss: 0.2158 - accuracy: 0.9180\n",
      "Epoch 262/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.2014 - accuracy: 0.9240\n",
      "Epoch 263/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.2085 - accuracy: 0.9171\n",
      "Epoch 264/1500\n",
      "34/34 [==============================] - 0s 927us/step - loss: 0.2113 - accuracy: 0.9134\n",
      "Epoch 265/1500\n",
      "34/34 [==============================] - 0s 846us/step - loss: 0.2129 - accuracy: 0.9143\n",
      "Epoch 266/1500\n",
      "34/34 [==============================] - 0s 813us/step - loss: 0.2008 - accuracy: 0.9231\n",
      "Epoch 267/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.2148 - accuracy: 0.9157\n",
      "Epoch 268/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.1963 - accuracy: 0.9254\n",
      "Epoch 269/1500\n",
      "34/34 [==============================] - 0s 797us/step - loss: 0.2141 - accuracy: 0.9157\n",
      "Epoch 270/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.1959 - accuracy: 0.9258\n",
      "Epoch 271/1500\n",
      "34/34 [==============================] - 0s 760us/step - loss: 0.2133 - accuracy: 0.9166\n",
      "Epoch 272/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.2044 - accuracy: 0.9203\n",
      "Epoch 273/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.1937 - accuracy: 0.9304\n",
      "Epoch 274/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1895 - accuracy: 0.9245\n",
      "Epoch 275/1500\n",
      "34/34 [==============================] - 0s 878us/step - loss: 0.1972 - accuracy: 0.9208\n",
      "Epoch 276/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.2087 - accuracy: 0.9162\n",
      "Epoch 277/1500\n",
      "34/34 [==============================] - 0s 977us/step - loss: 0.1905 - accuracy: 0.9327\n",
      "Epoch 278/1500\n",
      "34/34 [==============================] - 0s 926us/step - loss: 0.1891 - accuracy: 0.9272\n",
      "Epoch 279/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.9309\n",
      "Epoch 280/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1929 - accuracy: 0.9231\n",
      "Epoch 281/1500\n",
      "34/34 [==============================] - 0s 938us/step - loss: 0.2031 - accuracy: 0.9125\n",
      "Epoch 282/1500\n",
      "34/34 [==============================] - 0s 905us/step - loss: 0.1846 - accuracy: 0.9226\n",
      "Epoch 283/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.1879 - accuracy: 0.9309\n",
      "Epoch 284/1500\n",
      "34/34 [==============================] - 0s 908us/step - loss: 0.1860 - accuracy: 0.9304\n",
      "Epoch 285/1500\n",
      "34/34 [==============================] - 0s 949us/step - loss: 0.1740 - accuracy: 0.9277\n",
      "Epoch 286/1500\n",
      "34/34 [==============================] - 0s 830us/step - loss: 0.1744 - accuracy: 0.9295\n",
      "Epoch 287/1500\n",
      "34/34 [==============================] - 0s 862us/step - loss: 0.1879 - accuracy: 0.9258\n",
      "Epoch 288/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.1984 - accuracy: 0.9263\n",
      "Epoch 289/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.1907 - accuracy: 0.9272\n",
      "Epoch 290/1500\n",
      "34/34 [==============================] - 0s 755us/step - loss: 0.1929 - accuracy: 0.9231\n",
      "Epoch 291/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.2071 - accuracy: 0.9199\n",
      "Epoch 292/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.2013 - accuracy: 0.9222\n",
      "Epoch 293/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.1672 - accuracy: 0.9355\n",
      "Epoch 294/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.2065 - accuracy: 0.9175\n",
      "Epoch 295/1500\n",
      "34/34 [==============================] - 0s 781us/step - loss: 0.1959 - accuracy: 0.9185\n",
      "Epoch 296/1500\n",
      "34/34 [==============================] - 0s 770us/step - loss: 0.2017 - accuracy: 0.9199\n",
      "Epoch 297/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.1870 - accuracy: 0.9254\n",
      "Epoch 298/1500\n",
      "34/34 [==============================] - 0s 753us/step - loss: 0.1829 - accuracy: 0.9318\n",
      "Epoch 299/1500\n",
      "34/34 [==============================] - 0s 806us/step - loss: 0.1868 - accuracy: 0.9277\n",
      "Epoch 300/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.1736 - accuracy: 0.9318\n",
      "Epoch 301/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.1946 - accuracy: 0.9240\n",
      "Epoch 302/1500\n",
      "34/34 [==============================] - 0s 794us/step - loss: 0.1913 - accuracy: 0.9309\n",
      "Epoch 303/1500\n",
      "34/34 [==============================] - 0s 819us/step - loss: 0.1801 - accuracy: 0.9351\n",
      "Epoch 304/1500\n",
      "34/34 [==============================] - 0s 827us/step - loss: 0.1659 - accuracy: 0.9309\n",
      "Epoch 305/1500\n",
      "34/34 [==============================] - 0s 822us/step - loss: 0.1822 - accuracy: 0.9281\n",
      "Epoch 306/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.1888 - accuracy: 0.9249\n",
      "Epoch 307/1500\n",
      "34/34 [==============================] - 0s 823us/step - loss: 0.1816 - accuracy: 0.9337\n",
      "Epoch 308/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.1867 - accuracy: 0.9291\n",
      "Epoch 309/1500\n",
      "34/34 [==============================] - 0s 792us/step - loss: 0.1817 - accuracy: 0.9231\n",
      "Epoch 310/1500\n",
      "34/34 [==============================] - 0s 763us/step - loss: 0.1815 - accuracy: 0.9272\n",
      "Epoch 311/1500\n",
      "34/34 [==============================] - 0s 757us/step - loss: 0.1742 - accuracy: 0.9392\n",
      "Epoch 312/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.1820 - accuracy: 0.9309\n",
      "Epoch 313/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1621 - accuracy: 0.9346\n",
      "Epoch 314/1500\n",
      "34/34 [==============================] - 0s 790us/step - loss: 0.1767 - accuracy: 0.9360\n",
      "Epoch 315/1500\n",
      "34/34 [==============================] - 0s 727us/step - loss: 0.1761 - accuracy: 0.9309\n",
      "Epoch 316/1500\n",
      "34/34 [==============================] - 0s 859us/step - loss: 0.1792 - accuracy: 0.9351\n",
      "Epoch 317/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9304\n",
      "Epoch 318/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.9268\n",
      "Epoch 319/1500\n",
      "34/34 [==============================] - 0s 958us/step - loss: 0.1846 - accuracy: 0.9295\n",
      "Epoch 320/1500\n",
      "34/34 [==============================] - 0s 953us/step - loss: 0.1753 - accuracy: 0.9346\n",
      "Epoch 321/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1677 - accuracy: 0.9332\n",
      "Epoch 322/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1672 - accuracy: 0.9401\n",
      "Epoch 323/1500\n",
      "34/34 [==============================] - 0s 831us/step - loss: 0.1836 - accuracy: 0.9332\n",
      "Epoch 324/1500\n",
      "34/34 [==============================] - 0s 791us/step - loss: 0.1708 - accuracy: 0.9277\n",
      "Epoch 325/1500\n",
      "34/34 [==============================] - 0s 737us/step - loss: 0.1705 - accuracy: 0.9337\n",
      "Epoch 326/1500\n",
      "34/34 [==============================] - 0s 759us/step - loss: 0.1802 - accuracy: 0.9337\n",
      "Epoch 327/1500\n",
      "34/34 [==============================] - 0s 765us/step - loss: 0.1749 - accuracy: 0.9341\n",
      "Epoch 328/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.1735 - accuracy: 0.9277\n",
      "Epoch 329/1500\n",
      "34/34 [==============================] - 0s 828us/step - loss: 0.1634 - accuracy: 0.9360\n",
      "Epoch 330/1500\n",
      "34/34 [==============================] - 0s 923us/step - loss: 0.1729 - accuracy: 0.9360\n",
      "Epoch 331/1500\n",
      "34/34 [==============================] - 0s 867us/step - loss: 0.1792 - accuracy: 0.9314\n",
      "Epoch 332/1500\n",
      "34/34 [==============================] - 0s 885us/step - loss: 0.1692 - accuracy: 0.9360\n",
      "Epoch 333/1500\n",
      "34/34 [==============================] - 0s 883us/step - loss: 0.1800 - accuracy: 0.9300\n",
      "Epoch 334/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.1644 - accuracy: 0.9364\n",
      "Epoch 335/1500\n",
      "34/34 [==============================] - 0s 861us/step - loss: 0.1769 - accuracy: 0.9295\n",
      "Epoch 336/1500\n",
      "34/34 [==============================] - 0s 716us/step - loss: 0.1836 - accuracy: 0.9318\n",
      "Epoch 337/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.1788 - accuracy: 0.9300\n",
      "Epoch 338/1500\n",
      "34/34 [==============================] - 0s 839us/step - loss: 0.1782 - accuracy: 0.9281\n",
      "Epoch 339/1500\n",
      "34/34 [==============================] - 0s 847us/step - loss: 0.1787 - accuracy: 0.9351\n",
      "Epoch 340/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.1590 - accuracy: 0.9433\n",
      "Epoch 341/1500\n",
      "34/34 [==============================] - 0s 798us/step - loss: 0.1716 - accuracy: 0.9318\n",
      "Epoch 342/1500\n",
      "34/34 [==============================] - 0s 779us/step - loss: 0.1565 - accuracy: 0.9383\n",
      "Epoch 343/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.1673 - accuracy: 0.9332\n",
      "Epoch 344/1500\n",
      "34/34 [==============================] - 0s 818us/step - loss: 0.1740 - accuracy: 0.9277\n",
      "Epoch 345/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.1553 - accuracy: 0.9424\n",
      "Epoch 346/1500\n",
      "34/34 [==============================] - 0s 868us/step - loss: 0.1715 - accuracy: 0.9314\n",
      "Epoch 347/1500\n",
      "34/34 [==============================] - 0s 933us/step - loss: 0.1663 - accuracy: 0.9332\n",
      "Epoch 348/1500\n",
      "34/34 [==============================] - 0s 925us/step - loss: 0.1644 - accuracy: 0.9323\n",
      "Epoch 349/1500\n",
      "34/34 [==============================] - 0s 954us/step - loss: 0.1683 - accuracy: 0.9387\n",
      "Epoch 350/1500\n",
      "34/34 [==============================] - 0s 894us/step - loss: 0.1647 - accuracy: 0.9360\n",
      "Epoch 351/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.1668 - accuracy: 0.9341\n",
      "Epoch 352/1500\n",
      "34/34 [==============================] - 0s 816us/step - loss: 0.1460 - accuracy: 0.9484\n",
      "Epoch 353/1500\n",
      "34/34 [==============================] - 0s 815us/step - loss: 0.1690 - accuracy: 0.9304\n",
      "Epoch 354/1500\n",
      "34/34 [==============================] - 0s 807us/step - loss: 0.1551 - accuracy: 0.9397\n",
      "Epoch 355/1500\n",
      "34/34 [==============================] - 0s 983us/step - loss: 0.1687 - accuracy: 0.9327\n",
      "Epoch 356/1500\n",
      "34/34 [==============================] - 0s 991us/step - loss: 0.1691 - accuracy: 0.9387\n",
      "Epoch 357/1500\n",
      "34/34 [==============================] - 0s 998us/step - loss: 0.1477 - accuracy: 0.9429\n",
      "Epoch 358/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.1607 - accuracy: 0.9443\n",
      "Epoch 359/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.1594 - accuracy: 0.9369\n",
      "Epoch 360/1500\n",
      "34/34 [==============================] - 0s 884us/step - loss: 0.1536 - accuracy: 0.9424\n",
      "Epoch 361/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.1690 - accuracy: 0.9364\n",
      "Epoch 362/1500\n",
      "34/34 [==============================] - 0s 873us/step - loss: 0.1594 - accuracy: 0.9401\n",
      "Epoch 363/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.1552 - accuracy: 0.9401\n",
      "Epoch 364/1500\n",
      "34/34 [==============================] - 0s 877us/step - loss: 0.1666 - accuracy: 0.9337\n",
      "Epoch 365/1500\n",
      "34/34 [==============================] - 0s 863us/step - loss: 0.1509 - accuracy: 0.9378\n",
      "Epoch 366/1500\n",
      "34/34 [==============================] - 0s 857us/step - loss: 0.1655 - accuracy: 0.9346\n",
      "Epoch 367/1500\n",
      "34/34 [==============================] - 0s 966us/step - loss: 0.1569 - accuracy: 0.9401\n",
      "Epoch 368/1500\n",
      "34/34 [==============================] - 0s 978us/step - loss: 0.1553 - accuracy: 0.9420\n",
      "Epoch 369/1500\n",
      "34/34 [==============================] - 0s 968us/step - loss: 0.1685 - accuracy: 0.9314\n",
      "Epoch 370/1500\n",
      "34/34 [==============================] - 0s 996us/step - loss: 0.1568 - accuracy: 0.9397\n",
      "Epoch 371/1500\n",
      "34/34 [==============================] - 0s 900us/step - loss: 0.1588 - accuracy: 0.9374\n",
      "Epoch 372/1500\n",
      "34/34 [==============================] - 0s 829us/step - loss: 0.1635 - accuracy: 0.9383\n",
      "Epoch 373/1500\n",
      "34/34 [==============================] - 0s 768us/step - loss: 0.1664 - accuracy: 0.9387\n",
      "Epoch 374/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.1607 - accuracy: 0.9397\n",
      "Epoch 375/1500\n",
      "34/34 [==============================] - 0s 747us/step - loss: 0.1710 - accuracy: 0.9314\n",
      "Epoch 376/1500\n",
      "34/34 [==============================] - 0s 761us/step - loss: 0.1667 - accuracy: 0.9341\n",
      "Epoch 377/1500\n",
      "34/34 [==============================] - 0s 732us/step - loss: 0.1552 - accuracy: 0.9429\n",
      "Epoch 378/1500\n",
      "34/34 [==============================] - 0s 777us/step - loss: 0.1740 - accuracy: 0.9295\n",
      "Epoch 379/1500\n",
      "34/34 [==============================] - 0s 882us/step - loss: 0.1291 - accuracy: 0.9521\n",
      "Epoch 380/1500\n",
      "34/34 [==============================] - 0s 897us/step - loss: 0.1641 - accuracy: 0.9401\n",
      "Epoch 381/1500\n",
      "34/34 [==============================] - 0s 930us/step - loss: 0.1586 - accuracy: 0.9424\n",
      "Epoch 382/1500\n",
      "34/34 [==============================] - 0s 949us/step - loss: 0.1472 - accuracy: 0.9424\n",
      "Epoch 383/1500\n",
      "34/34 [==============================] - 0s 937us/step - loss: 0.1676 - accuracy: 0.9383\n",
      "Epoch 384/1500\n",
      "34/34 [==============================] - 0s 972us/step - loss: 0.1483 - accuracy: 0.9420\n",
      "Epoch 385/1500\n",
      "34/34 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9383\n",
      "Epoch 386/1500\n",
      "34/34 [==============================] - 0s 907us/step - loss: 0.1472 - accuracy: 0.9452\n",
      "Epoch 387/1500\n",
      "34/34 [==============================] - 0s 824us/step - loss: 0.1631 - accuracy: 0.9332\n",
      "Epoch 388/1500\n",
      "34/34 [==============================] - 0s 784us/step - loss: 0.1470 - accuracy: 0.9438\n",
      "Epoch 389/1500\n",
      "34/34 [==============================] - 0s 832us/step - loss: 0.1527 - accuracy: 0.9420\n",
      "Epoch 390/1500\n",
      "34/34 [==============================] - 0s 840us/step - loss: 0.1559 - accuracy: 0.9383\n",
      "Epoch 391/1500\n",
      "34/34 [==============================] - 0s 801us/step - loss: 0.1481 - accuracy: 0.9424\n",
      "Epoch 392/1500\n",
      "34/34 [==============================] - 0s 810us/step - loss: 0.1469 - accuracy: 0.9406\n",
      "Epoch 393/1500\n",
      "34/34 [==============================] - 0s 762us/step - loss: 0.1528 - accuracy: 0.9424\n",
      "Epoch 394/1500\n",
      "34/34 [==============================] - 0s 787us/step - loss: 0.1590 - accuracy: 0.9351\n",
      "Epoch 395/1500\n",
      "34/34 [==============================] - 0s 778us/step - loss: 0.1648 - accuracy: 0.9392\n",
      "Epoch 396/1500\n",
      "34/34 [==============================] - 0s 811us/step - loss: 0.1522 - accuracy: 0.9397\n",
      "Epoch 397/1500\n",
      "34/34 [==============================] - 0s 769us/step - loss: 0.1367 - accuracy: 0.9530\n",
      "Epoch 398/1500\n",
      "34/34 [==============================] - 0s 722us/step - loss: 0.1511 - accuracy: 0.9438\n",
      "Epoch 399/1500\n",
      "34/34 [==============================] - 0s 754us/step - loss: 0.1367 - accuracy: 0.9503\n",
      "Epoch 400/1500\n",
      "34/34 [==============================] - 0s 789us/step - loss: 0.1502 - accuracy: 0.9438\n",
      "Epoch 401/1500\n",
      "34/34 [==============================] - 0s 774us/step - loss: 0.1587 - accuracy: 0.9364\n",
      "Epoch 402/1500\n",
      "34/34 [==============================] - 0s 834us/step - loss: 0.1319 - accuracy: 0.9493\n",
      "Epoch 403/1500\n",
      "34/34 [==============================] - 0s 800us/step - loss: 0.1517 - accuracy: 0.9369\n",
      "Epoch 404/1500\n",
      "34/34 [==============================] - 0s 865us/step - loss: 0.1393 - accuracy: 0.9493\n",
      "Epoch 405/1500\n",
      "34/34 [==============================] - 0s 825us/step - loss: 0.1450 - accuracy: 0.9475\n",
      "Epoch 406/1500\n",
      "34/34 [==============================] - 0s 799us/step - loss: 0.1732 - accuracy: 0.9337\n",
      "Epoch 407/1500\n",
      "34/34 [==============================] - 0s 748us/step - loss: 0.1596 - accuracy: 0.9406\n",
      "Epoch 408/1500\n",
      "34/34 [==============================] - 0s 756us/step - loss: 0.1406 - accuracy: 0.9503\n",
      "Epoch 409/1500\n",
      " 1/34 [..............................] - ETA: 0s - loss: 0.1528 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 379.\n",
      "34/34 [==============================] - 0s 879us/step - loss: 0.1497 - accuracy: 0.9466\n",
      "Epoch 409: early stopping\n",
      "8/8 [==============================] - 0s 782us/step - loss: 0.7734 - accuracy: 0.6835\n",
      "8/8 [==============================] - 0s 590us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.62 (18/29)\n",
      "Before appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "After appending - Cat IDs: 390, Predictions: 390, Actuals: 390, Gender: 390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Results - Loss: 0.7733930945396423, Accuracy: 0.6835442781448364, Precision: 0.7014336917562725, Recall: 0.5419812331997559, F1 Score: 0.5749668650821793\n",
      "Confusion Matrix:\n",
      " [[128   2  15]\n",
      " [ 33  18   0]\n",
      " [ 25   0  16]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "001A    14\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "071A    10\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "007A     6\n",
      "109A     6\n",
      "108A     6\n",
      "037A     6\n",
      "008A     6\n",
      "044A     5\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "034A     5\n",
      "023B     5\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "060A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "054A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "018A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "091A     1\n",
      "110A     1\n",
      "115A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "057A    27\n",
      "074A    25\n",
      "029A    17\n",
      "097A    16\n",
      "106A    14\n",
      "028A    13\n",
      "025A    11\n",
      "005A    10\n",
      "015A     9\n",
      "045A     9\n",
      "072A     9\n",
      "094A     8\n",
      "023A     6\n",
      "053A     6\n",
      "021A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "003A     4\n",
      "058A     3\n",
      "056A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "038A     2\n",
      "073A     1\n",
      "076A     1\n",
      "026C     1\n",
      "100A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    266\n",
      "M    237\n",
      "F    211\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    100\n",
      "X     82\n",
      "F     41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 103A, 071A, 097B, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [055A, 059A, 113A, 116A, 051B, 054A, 117A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 028A, 074A, 062A, 029A, 005A, 072A, 009...\n",
      "kitten                                               [045A]\n",
      "senior     [093A, 097A, 057A, 106A, 104A, 056A, 058A, 094A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 15, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 1, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "Length of X_train_val:\n",
      "714\n",
      "Length of y_train_val:\n",
      "714\n",
      "Length of groups_train_val:\n",
      "714\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 902, 1: 810, 2: 505})\n",
      "Epoch 1/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 1.0946 - accuracy: 0.5101\n",
      "Epoch 2/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.9119 - accuracy: 0.5986\n",
      "Epoch 3/1500\n",
      "35/35 [==============================] - 0s 971us/step - loss: 0.7645 - accuracy: 0.6640\n",
      "Epoch 4/1500\n",
      "35/35 [==============================] - 0s 926us/step - loss: 0.7273 - accuracy: 0.6951\n",
      "Epoch 5/1500\n",
      "35/35 [==============================] - 0s 908us/step - loss: 0.6844 - accuracy: 0.7185\n",
      "Epoch 6/1500\n",
      "35/35 [==============================] - 0s 947us/step - loss: 0.6636 - accuracy: 0.7091\n",
      "Epoch 7/1500\n",
      "35/35 [==============================] - 0s 919us/step - loss: 0.6571 - accuracy: 0.7334\n",
      "Epoch 8/1500\n",
      "35/35 [==============================] - 0s 841us/step - loss: 0.6223 - accuracy: 0.7393\n",
      "Epoch 9/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.5963 - accuracy: 0.7420\n",
      "Epoch 10/1500\n",
      "35/35 [==============================] - 0s 843us/step - loss: 0.6051 - accuracy: 0.7537\n",
      "Epoch 11/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.5676 - accuracy: 0.7555\n",
      "Epoch 12/1500\n",
      "35/35 [==============================] - 0s 807us/step - loss: 0.5587 - accuracy: 0.7749\n",
      "Epoch 13/1500\n",
      "35/35 [==============================] - 0s 787us/step - loss: 0.5507 - accuracy: 0.7582\n",
      "Epoch 14/1500\n",
      "35/35 [==============================] - 0s 847us/step - loss: 0.5451 - accuracy: 0.7731\n",
      "Epoch 15/1500\n",
      "35/35 [==============================] - 0s 927us/step - loss: 0.5303 - accuracy: 0.7740\n",
      "Epoch 16/1500\n",
      "35/35 [==============================] - 0s 846us/step - loss: 0.5275 - accuracy: 0.7835\n",
      "Epoch 17/1500\n",
      "35/35 [==============================] - 0s 925us/step - loss: 0.5119 - accuracy: 0.7876\n",
      "Epoch 18/1500\n",
      "35/35 [==============================] - 0s 883us/step - loss: 0.5113 - accuracy: 0.7857\n",
      "Epoch 19/1500\n",
      "35/35 [==============================] - 0s 877us/step - loss: 0.5148 - accuracy: 0.7876\n",
      "Epoch 20/1500\n",
      "35/35 [==============================] - 0s 883us/step - loss: 0.4924 - accuracy: 0.7885\n",
      "Epoch 21/1500\n",
      "35/35 [==============================] - 0s 900us/step - loss: 0.4984 - accuracy: 0.7862\n",
      "Epoch 22/1500\n",
      "35/35 [==============================] - 0s 906us/step - loss: 0.4954 - accuracy: 0.7808\n",
      "Epoch 23/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.4708 - accuracy: 0.8133\n",
      "Epoch 24/1500\n",
      "35/35 [==============================] - 0s 870us/step - loss: 0.4777 - accuracy: 0.7970\n",
      "Epoch 25/1500\n",
      "35/35 [==============================] - 0s 880us/step - loss: 0.4746 - accuracy: 0.7961\n",
      "Epoch 26/1500\n",
      "35/35 [==============================] - 0s 772us/step - loss: 0.4737 - accuracy: 0.7943\n",
      "Epoch 27/1500\n",
      "35/35 [==============================] - 0s 930us/step - loss: 0.4592 - accuracy: 0.8042\n",
      "Epoch 28/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.4569 - accuracy: 0.8051\n",
      "Epoch 29/1500\n",
      "35/35 [==============================] - 0s 921us/step - loss: 0.4469 - accuracy: 0.8182\n",
      "Epoch 30/1500\n",
      "35/35 [==============================] - 0s 963us/step - loss: 0.4384 - accuracy: 0.8151\n",
      "Epoch 31/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.4452 - accuracy: 0.8205\n",
      "Epoch 32/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.4416 - accuracy: 0.8128\n",
      "Epoch 33/1500\n",
      "35/35 [==============================] - 0s 943us/step - loss: 0.4428 - accuracy: 0.8169\n",
      "Epoch 34/1500\n",
      "35/35 [==============================] - 0s 962us/step - loss: 0.4239 - accuracy: 0.8187\n",
      "Epoch 35/1500\n",
      "35/35 [==============================] - 0s 947us/step - loss: 0.4118 - accuracy: 0.8290\n",
      "Epoch 36/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.4108 - accuracy: 0.8304\n",
      "Epoch 37/1500\n",
      "35/35 [==============================] - 0s 778us/step - loss: 0.4308 - accuracy: 0.8250\n",
      "Epoch 38/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.4286 - accuracy: 0.8214\n",
      "Epoch 39/1500\n",
      "35/35 [==============================] - 0s 998us/step - loss: 0.3985 - accuracy: 0.8340\n",
      "Epoch 40/1500\n",
      "35/35 [==============================] - 0s 982us/step - loss: 0.4047 - accuracy: 0.8345\n",
      "Epoch 41/1500\n",
      "35/35 [==============================] - 0s 899us/step - loss: 0.4042 - accuracy: 0.8336\n",
      "Epoch 42/1500\n",
      "35/35 [==============================] - 0s 887us/step - loss: 0.3993 - accuracy: 0.8372\n",
      "Epoch 43/1500\n",
      "35/35 [==============================] - 0s 940us/step - loss: 0.4035 - accuracy: 0.8336\n",
      "Epoch 44/1500\n",
      "35/35 [==============================] - 0s 978us/step - loss: 0.4010 - accuracy: 0.8322\n",
      "Epoch 45/1500\n",
      "35/35 [==============================] - 0s 959us/step - loss: 0.3981 - accuracy: 0.8268\n",
      "Epoch 46/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3849 - accuracy: 0.8466\n",
      "Epoch 47/1500\n",
      "35/35 [==============================] - 0s 888us/step - loss: 0.3789 - accuracy: 0.8466\n",
      "Epoch 48/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.3896 - accuracy: 0.8417\n",
      "Epoch 49/1500\n",
      "35/35 [==============================] - 0s 854us/step - loss: 0.3747 - accuracy: 0.8426\n",
      "Epoch 50/1500\n",
      "35/35 [==============================] - 0s 867us/step - loss: 0.3724 - accuracy: 0.8453\n",
      "Epoch 51/1500\n",
      "35/35 [==============================] - 0s 797us/step - loss: 0.3797 - accuracy: 0.8421\n",
      "Epoch 52/1500\n",
      "35/35 [==============================] - 0s 825us/step - loss: 0.3817 - accuracy: 0.8408\n",
      "Epoch 53/1500\n",
      "35/35 [==============================] - 0s 811us/step - loss: 0.3696 - accuracy: 0.8444\n",
      "Epoch 54/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.3889 - accuracy: 0.8340\n",
      "Epoch 55/1500\n",
      "35/35 [==============================] - 0s 798us/step - loss: 0.3691 - accuracy: 0.8471\n",
      "Epoch 56/1500\n",
      "35/35 [==============================] - 0s 839us/step - loss: 0.3650 - accuracy: 0.8539\n",
      "Epoch 57/1500\n",
      "35/35 [==============================] - 0s 874us/step - loss: 0.3599 - accuracy: 0.8566\n",
      "Epoch 58/1500\n",
      "35/35 [==============================] - 0s 825us/step - loss: 0.3660 - accuracy: 0.8534\n",
      "Epoch 59/1500\n",
      "35/35 [==============================] - 0s 840us/step - loss: 0.3591 - accuracy: 0.8453\n",
      "Epoch 60/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.3534 - accuracy: 0.8638\n",
      "Epoch 61/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.3574 - accuracy: 0.8566\n",
      "Epoch 62/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.3531 - accuracy: 0.8566\n",
      "Epoch 63/1500\n",
      "35/35 [==============================] - 0s 799us/step - loss: 0.3673 - accuracy: 0.8489\n",
      "Epoch 64/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.3623 - accuracy: 0.8557\n",
      "Epoch 65/1500\n",
      "35/35 [==============================] - 0s 801us/step - loss: 0.3335 - accuracy: 0.8575\n",
      "Epoch 66/1500\n",
      "35/35 [==============================] - 0s 787us/step - loss: 0.3508 - accuracy: 0.8597\n",
      "Epoch 67/1500\n",
      "35/35 [==============================] - 0s 829us/step - loss: 0.3324 - accuracy: 0.8692\n",
      "Epoch 68/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.3361 - accuracy: 0.8656\n",
      "Epoch 69/1500\n",
      "35/35 [==============================] - 0s 866us/step - loss: 0.3393 - accuracy: 0.8588\n",
      "Epoch 70/1500\n",
      "35/35 [==============================] - 0s 868us/step - loss: 0.3457 - accuracy: 0.8588\n",
      "Epoch 71/1500\n",
      "35/35 [==============================] - 0s 823us/step - loss: 0.3419 - accuracy: 0.8471\n",
      "Epoch 72/1500\n",
      "35/35 [==============================] - 0s 970us/step - loss: 0.3471 - accuracy: 0.8557\n",
      "Epoch 73/1500\n",
      "35/35 [==============================] - 0s 957us/step - loss: 0.3283 - accuracy: 0.8678\n",
      "Epoch 74/1500\n",
      "35/35 [==============================] - 0s 965us/step - loss: 0.3276 - accuracy: 0.8642\n",
      "Epoch 75/1500\n",
      "35/35 [==============================] - 0s 948us/step - loss: 0.3403 - accuracy: 0.8552\n",
      "Epoch 76/1500\n",
      "35/35 [==============================] - 0s 909us/step - loss: 0.3362 - accuracy: 0.8606\n",
      "Epoch 77/1500\n",
      "35/35 [==============================] - 0s 965us/step - loss: 0.3302 - accuracy: 0.8701\n",
      "Epoch 78/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.3357 - accuracy: 0.8687\n",
      "Epoch 79/1500\n",
      "35/35 [==============================] - 0s 845us/step - loss: 0.3399 - accuracy: 0.8588\n",
      "Epoch 80/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.3239 - accuracy: 0.8656\n",
      "Epoch 81/1500\n",
      "35/35 [==============================] - 0s 921us/step - loss: 0.3361 - accuracy: 0.8552\n",
      "Epoch 82/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3350 - accuracy: 0.8633\n",
      "Epoch 83/1500\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 0.3244 - accuracy: 0.8629\n",
      "Epoch 84/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3075 - accuracy: 0.8773\n",
      "Epoch 85/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3202 - accuracy: 0.8714\n",
      "Epoch 86/1500\n",
      "35/35 [==============================] - 0s 849us/step - loss: 0.3072 - accuracy: 0.8705\n",
      "Epoch 87/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.2991 - accuracy: 0.8868\n",
      "Epoch 88/1500\n",
      "35/35 [==============================] - 0s 838us/step - loss: 0.3013 - accuracy: 0.8755\n",
      "Epoch 89/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3181 - accuracy: 0.8651\n",
      "Epoch 90/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3264 - accuracy: 0.8647\n",
      "Epoch 91/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3207 - accuracy: 0.8728\n",
      "Epoch 92/1500\n",
      "35/35 [==============================] - 0s 991us/step - loss: 0.3022 - accuracy: 0.8710\n",
      "Epoch 93/1500\n",
      "35/35 [==============================] - 0s 936us/step - loss: 0.3186 - accuracy: 0.8719\n",
      "Epoch 94/1500\n",
      "35/35 [==============================] - 0s 956us/step - loss: 0.3094 - accuracy: 0.8773\n",
      "Epoch 95/1500\n",
      "35/35 [==============================] - 0s 951us/step - loss: 0.3046 - accuracy: 0.8742\n",
      "Epoch 96/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3189 - accuracy: 0.8728\n",
      "Epoch 97/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3192 - accuracy: 0.8696\n",
      "Epoch 98/1500\n",
      "35/35 [==============================] - 0s 922us/step - loss: 0.3038 - accuracy: 0.8809\n",
      "Epoch 99/1500\n",
      "35/35 [==============================] - 0s 811us/step - loss: 0.3025 - accuracy: 0.8796\n",
      "Epoch 100/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.2982 - accuracy: 0.8859\n",
      "Epoch 101/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2826 - accuracy: 0.8895\n",
      "Epoch 102/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3053 - accuracy: 0.8863\n",
      "Epoch 103/1500\n",
      "35/35 [==============================] - 0s 994us/step - loss: 0.2950 - accuracy: 0.8760\n",
      "Epoch 104/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.3036 - accuracy: 0.8701\n",
      "Epoch 105/1500\n",
      "35/35 [==============================] - 0s 995us/step - loss: 0.2906 - accuracy: 0.8854\n",
      "Epoch 106/1500\n",
      "35/35 [==============================] - 0s 898us/step - loss: 0.3009 - accuracy: 0.8764\n",
      "Epoch 107/1500\n",
      "35/35 [==============================] - 0s 905us/step - loss: 0.3007 - accuracy: 0.8805\n",
      "Epoch 108/1500\n",
      "35/35 [==============================] - 0s 926us/step - loss: 0.2859 - accuracy: 0.8818\n",
      "Epoch 109/1500\n",
      "35/35 [==============================] - 0s 993us/step - loss: 0.2824 - accuracy: 0.8881\n",
      "Epoch 110/1500\n",
      "35/35 [==============================] - 0s 967us/step - loss: 0.2883 - accuracy: 0.8859\n",
      "Epoch 111/1500\n",
      "35/35 [==============================] - 0s 988us/step - loss: 0.2892 - accuracy: 0.8818\n",
      "Epoch 112/1500\n",
      "35/35 [==============================] - 0s 925us/step - loss: 0.3000 - accuracy: 0.8805\n",
      "Epoch 113/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2931 - accuracy: 0.8863\n",
      "Epoch 114/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2764 - accuracy: 0.8845\n",
      "Epoch 115/1500\n",
      "35/35 [==============================] - 0s 934us/step - loss: 0.2952 - accuracy: 0.8863\n",
      "Epoch 116/1500\n",
      "35/35 [==============================] - 0s 852us/step - loss: 0.2699 - accuracy: 0.8949\n",
      "Epoch 117/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.2802 - accuracy: 0.8895\n",
      "Epoch 118/1500\n",
      "35/35 [==============================] - 0s 781us/step - loss: 0.2972 - accuracy: 0.8827\n",
      "Epoch 119/1500\n",
      "35/35 [==============================] - 0s 812us/step - loss: 0.2843 - accuracy: 0.8791\n",
      "Epoch 120/1500\n",
      "35/35 [==============================] - 0s 908us/step - loss: 0.2786 - accuracy: 0.8845\n",
      "Epoch 121/1500\n",
      "35/35 [==============================] - 0s 900us/step - loss: 0.2875 - accuracy: 0.8841\n",
      "Epoch 122/1500\n",
      "35/35 [==============================] - 0s 816us/step - loss: 0.2637 - accuracy: 0.8922\n",
      "Epoch 123/1500\n",
      "35/35 [==============================] - 0s 778us/step - loss: 0.2721 - accuracy: 0.8872\n",
      "Epoch 124/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.2720 - accuracy: 0.8899\n",
      "Epoch 125/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.2704 - accuracy: 0.8931\n",
      "Epoch 126/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.2620 - accuracy: 0.8972\n",
      "Epoch 127/1500\n",
      "35/35 [==============================] - 0s 811us/step - loss: 0.2593 - accuracy: 0.8917\n",
      "Epoch 128/1500\n",
      "35/35 [==============================] - 0s 862us/step - loss: 0.2633 - accuracy: 0.8981\n",
      "Epoch 129/1500\n",
      "35/35 [==============================] - 0s 972us/step - loss: 0.2608 - accuracy: 0.8945\n",
      "Epoch 130/1500\n",
      "35/35 [==============================] - 0s 934us/step - loss: 0.2684 - accuracy: 0.8935\n",
      "Epoch 131/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2621 - accuracy: 0.8895\n",
      "Epoch 132/1500\n",
      "35/35 [==============================] - 0s 6ms/step - loss: 0.2663 - accuracy: 0.8954\n",
      "Epoch 133/1500\n",
      "35/35 [==============================] - 0s 2ms/step - loss: 0.2613 - accuracy: 0.8935\n",
      "Epoch 134/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2480 - accuracy: 0.8990\n",
      "Epoch 135/1500\n",
      "35/35 [==============================] - 0s 969us/step - loss: 0.2753 - accuracy: 0.8845\n",
      "Epoch 136/1500\n",
      "35/35 [==============================] - 0s 889us/step - loss: 0.2544 - accuracy: 0.8940\n",
      "Epoch 137/1500\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.2601 - accuracy: 0.8999\n",
      "Epoch 138/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2465 - accuracy: 0.8981\n",
      "Epoch 139/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.2652 - accuracy: 0.8895\n",
      "Epoch 140/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.2470 - accuracy: 0.8990\n",
      "Epoch 141/1500\n",
      "35/35 [==============================] - 0s 879us/step - loss: 0.2621 - accuracy: 0.8935\n",
      "Epoch 142/1500\n",
      "35/35 [==============================] - 0s 791us/step - loss: 0.2493 - accuracy: 0.8949\n",
      "Epoch 143/1500\n",
      "35/35 [==============================] - 0s 758us/step - loss: 0.2401 - accuracy: 0.9039\n",
      "Epoch 144/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.2624 - accuracy: 0.8972\n",
      "Epoch 145/1500\n",
      "35/35 [==============================] - 0s 778us/step - loss: 0.2361 - accuracy: 0.9089\n",
      "Epoch 146/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.2462 - accuracy: 0.8999\n",
      "Epoch 147/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.2585 - accuracy: 0.8922\n",
      "Epoch 148/1500\n",
      "35/35 [==============================] - 0s 825us/step - loss: 0.2500 - accuracy: 0.9071\n",
      "Epoch 149/1500\n",
      "35/35 [==============================] - 0s 842us/step - loss: 0.2544 - accuracy: 0.8976\n",
      "Epoch 150/1500\n",
      "35/35 [==============================] - 0s 753us/step - loss: 0.2471 - accuracy: 0.9012\n",
      "Epoch 151/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.2519 - accuracy: 0.9017\n",
      "Epoch 152/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.2372 - accuracy: 0.9044\n",
      "Epoch 153/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.2564 - accuracy: 0.8985\n",
      "Epoch 154/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.2550 - accuracy: 0.8972\n",
      "Epoch 155/1500\n",
      "35/35 [==============================] - 0s 756us/step - loss: 0.2436 - accuracy: 0.9071\n",
      "Epoch 156/1500\n",
      "35/35 [==============================] - 0s 766us/step - loss: 0.2351 - accuracy: 0.9021\n",
      "Epoch 157/1500\n",
      "35/35 [==============================] - 0s 773us/step - loss: 0.2586 - accuracy: 0.8999\n",
      "Epoch 158/1500\n",
      "35/35 [==============================] - 0s 773us/step - loss: 0.2477 - accuracy: 0.8999\n",
      "Epoch 159/1500\n",
      "35/35 [==============================] - 0s 772us/step - loss: 0.2514 - accuracy: 0.8976\n",
      "Epoch 160/1500\n",
      "35/35 [==============================] - 0s 809us/step - loss: 0.2420 - accuracy: 0.9075\n",
      "Epoch 161/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.2388 - accuracy: 0.8981\n",
      "Epoch 162/1500\n",
      "35/35 [==============================] - 0s 777us/step - loss: 0.2397 - accuracy: 0.9089\n",
      "Epoch 163/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.2366 - accuracy: 0.9071\n",
      "Epoch 164/1500\n",
      "35/35 [==============================] - 0s 812us/step - loss: 0.2440 - accuracy: 0.9053\n",
      "Epoch 165/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.2370 - accuracy: 0.9066\n",
      "Epoch 166/1500\n",
      "35/35 [==============================] - 0s 761us/step - loss: 0.2367 - accuracy: 0.9120\n",
      "Epoch 167/1500\n",
      "35/35 [==============================] - 0s 759us/step - loss: 0.2369 - accuracy: 0.9035\n",
      "Epoch 168/1500\n",
      "35/35 [==============================] - 0s 734us/step - loss: 0.2417 - accuracy: 0.9026\n",
      "Epoch 169/1500\n",
      "35/35 [==============================] - 0s 797us/step - loss: 0.2222 - accuracy: 0.9107\n",
      "Epoch 170/1500\n",
      "35/35 [==============================] - 0s 834us/step - loss: 0.2337 - accuracy: 0.9048\n",
      "Epoch 171/1500\n",
      "35/35 [==============================] - 0s 797us/step - loss: 0.2356 - accuracy: 0.9039\n",
      "Epoch 172/1500\n",
      "35/35 [==============================] - 0s 783us/step - loss: 0.2318 - accuracy: 0.9030\n",
      "Epoch 173/1500\n",
      "35/35 [==============================] - 0s 824us/step - loss: 0.2314 - accuracy: 0.9066\n",
      "Epoch 174/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.2376 - accuracy: 0.9008\n",
      "Epoch 175/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.2206 - accuracy: 0.9129\n",
      "Epoch 176/1500\n",
      "35/35 [==============================] - 0s 825us/step - loss: 0.2384 - accuracy: 0.9044\n",
      "Epoch 177/1500\n",
      "35/35 [==============================] - 0s 844us/step - loss: 0.2411 - accuracy: 0.9035\n",
      "Epoch 178/1500\n",
      "35/35 [==============================] - 0s 817us/step - loss: 0.2387 - accuracy: 0.9053\n",
      "Epoch 179/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.2409 - accuracy: 0.9075\n",
      "Epoch 180/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.2371 - accuracy: 0.9071\n",
      "Epoch 181/1500\n",
      "35/35 [==============================] - 0s 887us/step - loss: 0.2317 - accuracy: 0.9134\n",
      "Epoch 182/1500\n",
      "35/35 [==============================] - 0s 936us/step - loss: 0.2406 - accuracy: 0.9003\n",
      "Epoch 183/1500\n",
      "35/35 [==============================] - 0s 996us/step - loss: 0.2202 - accuracy: 0.9161\n",
      "Epoch 184/1500\n",
      "35/35 [==============================] - 0s 966us/step - loss: 0.2362 - accuracy: 0.9084\n",
      "Epoch 185/1500\n",
      "35/35 [==============================] - 0s 795us/step - loss: 0.2319 - accuracy: 0.9093\n",
      "Epoch 186/1500\n",
      "35/35 [==============================] - 0s 770us/step - loss: 0.2289 - accuracy: 0.9075\n",
      "Epoch 187/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.2227 - accuracy: 0.9093\n",
      "Epoch 188/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.2197 - accuracy: 0.9147\n",
      "Epoch 189/1500\n",
      "35/35 [==============================] - 0s 808us/step - loss: 0.2264 - accuracy: 0.9089\n",
      "Epoch 190/1500\n",
      "35/35 [==============================] - 0s 802us/step - loss: 0.2322 - accuracy: 0.9044\n",
      "Epoch 191/1500\n",
      "35/35 [==============================] - 0s 868us/step - loss: 0.2210 - accuracy: 0.9098\n",
      "Epoch 192/1500\n",
      "35/35 [==============================] - 0s 820us/step - loss: 0.2289 - accuracy: 0.9057\n",
      "Epoch 193/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.2184 - accuracy: 0.9138\n",
      "Epoch 194/1500\n",
      "35/35 [==============================] - 0s 789us/step - loss: 0.2236 - accuracy: 0.9175\n",
      "Epoch 195/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.2228 - accuracy: 0.9161\n",
      "Epoch 196/1500\n",
      "35/35 [==============================] - 0s 816us/step - loss: 0.2183 - accuracy: 0.9147\n",
      "Epoch 197/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.2092 - accuracy: 0.9134\n",
      "Epoch 198/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.2193 - accuracy: 0.9166\n",
      "Epoch 199/1500\n",
      "35/35 [==============================] - 0s 878us/step - loss: 0.2230 - accuracy: 0.9098\n",
      "Epoch 200/1500\n",
      "35/35 [==============================] - 0s 926us/step - loss: 0.2044 - accuracy: 0.9193\n",
      "Epoch 201/1500\n",
      "35/35 [==============================] - 0s 915us/step - loss: 0.2204 - accuracy: 0.9116\n",
      "Epoch 202/1500\n",
      "35/35 [==============================] - 0s 923us/step - loss: 0.2231 - accuracy: 0.9138\n",
      "Epoch 203/1500\n",
      "35/35 [==============================] - 0s 943us/step - loss: 0.2005 - accuracy: 0.9206\n",
      "Epoch 204/1500\n",
      "35/35 [==============================] - 0s 804us/step - loss: 0.2260 - accuracy: 0.9116\n",
      "Epoch 205/1500\n",
      "35/35 [==============================] - 0s 798us/step - loss: 0.2064 - accuracy: 0.9157\n",
      "Epoch 206/1500\n",
      "35/35 [==============================] - 0s 797us/step - loss: 0.2149 - accuracy: 0.9157\n",
      "Epoch 207/1500\n",
      "35/35 [==============================] - 0s 801us/step - loss: 0.2186 - accuracy: 0.9120\n",
      "Epoch 208/1500\n",
      "35/35 [==============================] - 0s 807us/step - loss: 0.1987 - accuracy: 0.9224\n",
      "Epoch 209/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.2049 - accuracy: 0.9197\n",
      "Epoch 210/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.2080 - accuracy: 0.9157\n",
      "Epoch 211/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.2111 - accuracy: 0.9143\n",
      "Epoch 212/1500\n",
      "35/35 [==============================] - 0s 890us/step - loss: 0.1865 - accuracy: 0.9278\n",
      "Epoch 213/1500\n",
      "35/35 [==============================] - 0s 802us/step - loss: 0.2099 - accuracy: 0.9170\n",
      "Epoch 214/1500\n",
      "35/35 [==============================] - 0s 837us/step - loss: 0.1911 - accuracy: 0.9260\n",
      "Epoch 215/1500\n",
      "35/35 [==============================] - 0s 853us/step - loss: 0.2023 - accuracy: 0.9247\n",
      "Epoch 216/1500\n",
      "35/35 [==============================] - 0s 834us/step - loss: 0.2093 - accuracy: 0.9179\n",
      "Epoch 217/1500\n",
      "35/35 [==============================] - 0s 950us/step - loss: 0.2065 - accuracy: 0.9129\n",
      "Epoch 218/1500\n",
      "35/35 [==============================] - 0s 944us/step - loss: 0.2031 - accuracy: 0.9157\n",
      "Epoch 219/1500\n",
      "35/35 [==============================] - 0s 932us/step - loss: 0.2091 - accuracy: 0.9134\n",
      "Epoch 220/1500\n",
      "35/35 [==============================] - 0s 859us/step - loss: 0.2188 - accuracy: 0.9157\n",
      "Epoch 221/1500\n",
      "35/35 [==============================] - 0s 771us/step - loss: 0.2008 - accuracy: 0.9179\n",
      "Epoch 222/1500\n",
      "35/35 [==============================] - 0s 727us/step - loss: 0.2069 - accuracy: 0.9152\n",
      "Epoch 223/1500\n",
      "35/35 [==============================] - 0s 777us/step - loss: 0.2017 - accuracy: 0.9260\n",
      "Epoch 224/1500\n",
      "35/35 [==============================] - 0s 761us/step - loss: 0.2114 - accuracy: 0.9152\n",
      "Epoch 225/1500\n",
      "35/35 [==============================] - 0s 888us/step - loss: 0.1996 - accuracy: 0.9251\n",
      "Epoch 226/1500\n",
      "35/35 [==============================] - 0s 927us/step - loss: 0.2081 - accuracy: 0.9102\n",
      "Epoch 227/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2137 - accuracy: 0.9188\n",
      "Epoch 228/1500\n",
      "35/35 [==============================] - 0s 964us/step - loss: 0.1984 - accuracy: 0.9251\n",
      "Epoch 229/1500\n",
      "35/35 [==============================] - 0s 956us/step - loss: 0.1998 - accuracy: 0.9242\n",
      "Epoch 230/1500\n",
      "35/35 [==============================] - 0s 935us/step - loss: 0.1957 - accuracy: 0.9193\n",
      "Epoch 231/1500\n",
      "35/35 [==============================] - 0s 856us/step - loss: 0.2056 - accuracy: 0.9211\n",
      "Epoch 232/1500\n",
      "35/35 [==============================] - 0s 785us/step - loss: 0.1887 - accuracy: 0.9278\n",
      "Epoch 233/1500\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.1932 - accuracy: 0.9193\n",
      "Epoch 234/1500\n",
      "35/35 [==============================] - 0s 798us/step - loss: 0.1979 - accuracy: 0.9202\n",
      "Epoch 235/1500\n",
      "35/35 [==============================] - 0s 955us/step - loss: 0.1898 - accuracy: 0.9229\n",
      "Epoch 236/1500\n",
      "35/35 [==============================] - 0s 976us/step - loss: 0.1888 - accuracy: 0.9310\n",
      "Epoch 237/1500\n",
      "35/35 [==============================] - 0s 976us/step - loss: 0.1923 - accuracy: 0.9251\n",
      "Epoch 238/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1971 - accuracy: 0.9247\n",
      "Epoch 239/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9229\n",
      "Epoch 240/1500\n",
      "35/35 [==============================] - 0s 935us/step - loss: 0.1860 - accuracy: 0.9269\n",
      "Epoch 241/1500\n",
      "35/35 [==============================] - 0s 857us/step - loss: 0.2011 - accuracy: 0.9197\n",
      "Epoch 242/1500\n",
      "35/35 [==============================] - 0s 858us/step - loss: 0.1793 - accuracy: 0.9256\n",
      "Epoch 243/1500\n",
      "35/35 [==============================] - 0s 828us/step - loss: 0.1825 - accuracy: 0.9247\n",
      "Epoch 244/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.2028 - accuracy: 0.9206\n",
      "Epoch 245/1500\n",
      "35/35 [==============================] - 0s 852us/step - loss: 0.1769 - accuracy: 0.9296\n",
      "Epoch 246/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.1948 - accuracy: 0.9220\n",
      "Epoch 247/1500\n",
      "35/35 [==============================] - 0s 777us/step - loss: 0.1881 - accuracy: 0.9260\n",
      "Epoch 248/1500\n",
      "35/35 [==============================] - 0s 758us/step - loss: 0.1833 - accuracy: 0.9269\n",
      "Epoch 249/1500\n",
      "35/35 [==============================] - 0s 791us/step - loss: 0.1918 - accuracy: 0.9251\n",
      "Epoch 250/1500\n",
      "35/35 [==============================] - 0s 849us/step - loss: 0.1879 - accuracy: 0.9269\n",
      "Epoch 251/1500\n",
      "35/35 [==============================] - 0s 817us/step - loss: 0.1854 - accuracy: 0.9283\n",
      "Epoch 252/1500\n",
      "35/35 [==============================] - 0s 781us/step - loss: 0.1960 - accuracy: 0.9233\n",
      "Epoch 253/1500\n",
      "35/35 [==============================] - 0s 834us/step - loss: 0.1790 - accuracy: 0.9274\n",
      "Epoch 254/1500\n",
      "35/35 [==============================] - 0s 814us/step - loss: 0.1831 - accuracy: 0.9274\n",
      "Epoch 255/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.1782 - accuracy: 0.9274\n",
      "Epoch 256/1500\n",
      "35/35 [==============================] - 0s 858us/step - loss: 0.1832 - accuracy: 0.9332\n",
      "Epoch 257/1500\n",
      "35/35 [==============================] - 0s 773us/step - loss: 0.1828 - accuracy: 0.9233\n",
      "Epoch 258/1500\n",
      "35/35 [==============================] - 0s 764us/step - loss: 0.1882 - accuracy: 0.9269\n",
      "Epoch 259/1500\n",
      "35/35 [==============================] - 0s 821us/step - loss: 0.1790 - accuracy: 0.9350\n",
      "Epoch 260/1500\n",
      "35/35 [==============================] - 0s 801us/step - loss: 0.1818 - accuracy: 0.9310\n",
      "Epoch 261/1500\n",
      "35/35 [==============================] - 0s 737us/step - loss: 0.1917 - accuracy: 0.9256\n",
      "Epoch 262/1500\n",
      "35/35 [==============================] - 0s 734us/step - loss: 0.1732 - accuracy: 0.9355\n",
      "Epoch 263/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.1845 - accuracy: 0.9292\n",
      "Epoch 264/1500\n",
      "35/35 [==============================] - 0s 784us/step - loss: 0.1822 - accuracy: 0.9247\n",
      "Epoch 265/1500\n",
      "35/35 [==============================] - 0s 785us/step - loss: 0.1949 - accuracy: 0.9242\n",
      "Epoch 266/1500\n",
      "35/35 [==============================] - 0s 822us/step - loss: 0.1755 - accuracy: 0.9211\n",
      "Epoch 267/1500\n",
      "35/35 [==============================] - 0s 807us/step - loss: 0.1758 - accuracy: 0.9265\n",
      "Epoch 268/1500\n",
      "35/35 [==============================] - 0s 925us/step - loss: 0.1696 - accuracy: 0.9346\n",
      "Epoch 269/1500\n",
      "35/35 [==============================] - 0s 848us/step - loss: 0.1854 - accuracy: 0.9301\n",
      "Epoch 270/1500\n",
      "35/35 [==============================] - 0s 801us/step - loss: 0.1725 - accuracy: 0.9278\n",
      "Epoch 271/1500\n",
      "35/35 [==============================] - 0s 813us/step - loss: 0.1789 - accuracy: 0.9378\n",
      "Epoch 272/1500\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.1643 - accuracy: 0.9337\n",
      "Epoch 273/1500\n",
      "35/35 [==============================] - 0s 793us/step - loss: 0.1798 - accuracy: 0.9278\n",
      "Epoch 274/1500\n",
      "35/35 [==============================] - 0s 757us/step - loss: 0.1801 - accuracy: 0.9305\n",
      "Epoch 275/1500\n",
      "35/35 [==============================] - 0s 730us/step - loss: 0.1734 - accuracy: 0.9364\n",
      "Epoch 276/1500\n",
      "35/35 [==============================] - 0s 715us/step - loss: 0.1756 - accuracy: 0.9265\n",
      "Epoch 277/1500\n",
      "35/35 [==============================] - 0s 860us/step - loss: 0.1849 - accuracy: 0.9287\n",
      "Epoch 278/1500\n",
      "35/35 [==============================] - 0s 916us/step - loss: 0.1753 - accuracy: 0.9305\n",
      "Epoch 279/1500\n",
      "35/35 [==============================] - 0s 892us/step - loss: 0.1794 - accuracy: 0.9265\n",
      "Epoch 280/1500\n",
      "35/35 [==============================] - 0s 926us/step - loss: 0.1704 - accuracy: 0.9409\n",
      "Epoch 281/1500\n",
      "35/35 [==============================] - 0s 863us/step - loss: 0.1713 - accuracy: 0.9373\n",
      "Epoch 282/1500\n",
      "35/35 [==============================] - 0s 811us/step - loss: 0.1596 - accuracy: 0.9409\n",
      "Epoch 283/1500\n",
      "35/35 [==============================] - 0s 770us/step - loss: 0.1737 - accuracy: 0.9350\n",
      "Epoch 284/1500\n",
      "35/35 [==============================] - 0s 780us/step - loss: 0.1703 - accuracy: 0.9350\n",
      "Epoch 285/1500\n",
      "35/35 [==============================] - 0s 802us/step - loss: 0.1738 - accuracy: 0.9332\n",
      "Epoch 286/1500\n",
      "35/35 [==============================] - 0s 868us/step - loss: 0.1739 - accuracy: 0.9292\n",
      "Epoch 287/1500\n",
      "35/35 [==============================] - 0s 991us/step - loss: 0.1662 - accuracy: 0.9323\n",
      "Epoch 288/1500\n",
      "35/35 [==============================] - 0s 1ms/step - loss: 0.1657 - accuracy: 0.9364\n",
      "Epoch 289/1500\n",
      "35/35 [==============================] - 0s 952us/step - loss: 0.1753 - accuracy: 0.9328\n",
      "Epoch 290/1500\n",
      "35/35 [==============================] - 0s 917us/step - loss: 0.1807 - accuracy: 0.9238\n",
      "Epoch 291/1500\n",
      "35/35 [==============================] - 0s 851us/step - loss: 0.1798 - accuracy: 0.9292\n",
      "Epoch 292/1500\n",
      "35/35 [==============================] - 0s 887us/step - loss: 0.1589 - accuracy: 0.9400\n",
      "Epoch 293/1500\n",
      "35/35 [==============================] - 0s 906us/step - loss: 0.1629 - accuracy: 0.9350\n",
      "Epoch 294/1500\n",
      "35/35 [==============================] - 0s 889us/step - loss: 0.1689 - accuracy: 0.9310\n",
      "Epoch 295/1500\n",
      "35/35 [==============================] - 0s 881us/step - loss: 0.1649 - accuracy: 0.9391\n",
      "Epoch 296/1500\n",
      "35/35 [==============================] - 0s 886us/step - loss: 0.1707 - accuracy: 0.9332\n",
      "Epoch 297/1500\n",
      "35/35 [==============================] - 0s 917us/step - loss: 0.1659 - accuracy: 0.9378\n",
      "Epoch 298/1500\n",
      "35/35 [==============================] - 0s 920us/step - loss: 0.1662 - accuracy: 0.9387\n",
      "Epoch 299/1500\n",
      "35/35 [==============================] - 0s 932us/step - loss: 0.1580 - accuracy: 0.9337\n",
      "Epoch 300/1500\n",
      "35/35 [==============================] - 0s 927us/step - loss: 0.1687 - accuracy: 0.9301\n",
      "Epoch 301/1500\n",
      "35/35 [==============================] - 0s 810us/step - loss: 0.1628 - accuracy: 0.9310\n",
      "Epoch 302/1500\n",
      "35/35 [==============================] - 0s 769us/step - loss: 0.1576 - accuracy: 0.9387\n",
      "Epoch 303/1500\n",
      "35/35 [==============================] - 0s 819us/step - loss: 0.1695 - accuracy: 0.9283\n",
      "Epoch 304/1500\n",
      "35/35 [==============================] - 0s 970us/step - loss: 0.1579 - accuracy: 0.9414\n",
      "Epoch 305/1500\n",
      "35/35 [==============================] - 0s 932us/step - loss: 0.1656 - accuracy: 0.9396\n",
      "Epoch 306/1500\n",
      "35/35 [==============================] - 0s 836us/step - loss: 0.1742 - accuracy: 0.9292\n",
      "Epoch 307/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.1563 - accuracy: 0.9463\n",
      "Epoch 308/1500\n",
      "35/35 [==============================] - 0s 761us/step - loss: 0.1598 - accuracy: 0.9373\n",
      "Epoch 309/1500\n",
      "35/35 [==============================] - 0s 777us/step - loss: 0.1704 - accuracy: 0.9323\n",
      "Epoch 310/1500\n",
      "35/35 [==============================] - 0s 832us/step - loss: 0.1478 - accuracy: 0.9423\n",
      "Epoch 311/1500\n",
      "35/35 [==============================] - 0s 807us/step - loss: 0.1737 - accuracy: 0.9332\n",
      "Epoch 312/1500\n",
      "35/35 [==============================] - 0s 802us/step - loss: 0.1511 - accuracy: 0.9409\n",
      "Epoch 313/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.1670 - accuracy: 0.9292\n",
      "Epoch 314/1500\n",
      "35/35 [==============================] - 0s 748us/step - loss: 0.1569 - accuracy: 0.9391\n",
      "Epoch 315/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.1747 - accuracy: 0.9341\n",
      "Epoch 316/1500\n",
      "35/35 [==============================] - 0s 749us/step - loss: 0.1757 - accuracy: 0.9224\n",
      "Epoch 317/1500\n",
      "35/35 [==============================] - 0s 765us/step - loss: 0.1616 - accuracy: 0.9382\n",
      "Epoch 318/1500\n",
      "35/35 [==============================] - 0s 788us/step - loss: 0.1531 - accuracy: 0.9477\n",
      "Epoch 319/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.1654 - accuracy: 0.9364\n",
      "Epoch 320/1500\n",
      "35/35 [==============================] - 0s 804us/step - loss: 0.1632 - accuracy: 0.9359\n",
      "Epoch 321/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.1590 - accuracy: 0.9396\n",
      "Epoch 322/1500\n",
      "35/35 [==============================] - 0s 740us/step - loss: 0.1565 - accuracy: 0.9450\n",
      "Epoch 323/1500\n",
      "35/35 [==============================] - 0s 792us/step - loss: 0.1601 - accuracy: 0.9378\n",
      "Epoch 324/1500\n",
      "35/35 [==============================] - 0s 764us/step - loss: 0.1461 - accuracy: 0.9472\n",
      "Epoch 325/1500\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.1655 - accuracy: 0.9350\n",
      "Epoch 326/1500\n",
      "35/35 [==============================] - 0s 762us/step - loss: 0.1564 - accuracy: 0.9387\n",
      "Epoch 327/1500\n",
      "35/35 [==============================] - 0s 713us/step - loss: 0.1545 - accuracy: 0.9391\n",
      "Epoch 328/1500\n",
      "35/35 [==============================] - 0s 818us/step - loss: 0.1583 - accuracy: 0.9414\n",
      "Epoch 329/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.1625 - accuracy: 0.9400\n",
      "Epoch 330/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.1575 - accuracy: 0.9441\n",
      "Epoch 331/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.1607 - accuracy: 0.9391\n",
      "Epoch 332/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.1643 - accuracy: 0.9364\n",
      "Epoch 333/1500\n",
      "35/35 [==============================] - 0s 754us/step - loss: 0.1474 - accuracy: 0.9396\n",
      "Epoch 334/1500\n",
      "35/35 [==============================] - 0s 743us/step - loss: 0.1417 - accuracy: 0.9441\n",
      "Epoch 335/1500\n",
      "35/35 [==============================] - 0s 730us/step - loss: 0.1454 - accuracy: 0.9445\n",
      "Epoch 336/1500\n",
      "35/35 [==============================] - 0s 758us/step - loss: 0.1570 - accuracy: 0.9355\n",
      "Epoch 337/1500\n",
      "35/35 [==============================] - 0s 891us/step - loss: 0.1494 - accuracy: 0.9364\n",
      "Epoch 338/1500\n",
      "35/35 [==============================] - 0s 794us/step - loss: 0.1537 - accuracy: 0.9418\n",
      "Epoch 339/1500\n",
      "35/35 [==============================] - 0s 993us/step - loss: 0.1422 - accuracy: 0.9396\n",
      "Epoch 340/1500\n",
      "35/35 [==============================] - 0s 947us/step - loss: 0.1499 - accuracy: 0.9427\n",
      "Epoch 341/1500\n",
      "35/35 [==============================] - 0s 869us/step - loss: 0.1567 - accuracy: 0.9382\n",
      "Epoch 342/1500\n",
      "35/35 [==============================] - 0s 899us/step - loss: 0.1527 - accuracy: 0.9418\n",
      "Epoch 343/1500\n",
      "35/35 [==============================] - 0s 915us/step - loss: 0.1399 - accuracy: 0.9468\n",
      "Epoch 344/1500\n",
      "35/35 [==============================] - 0s 890us/step - loss: 0.1590 - accuracy: 0.9391\n",
      "Epoch 345/1500\n",
      "35/35 [==============================] - 0s 914us/step - loss: 0.1462 - accuracy: 0.9427\n",
      "Epoch 346/1500\n",
      "35/35 [==============================] - 0s 984us/step - loss: 0.1509 - accuracy: 0.9391\n",
      "Epoch 347/1500\n",
      "35/35 [==============================] - 0s 942us/step - loss: 0.1608 - accuracy: 0.9405\n",
      "Epoch 348/1500\n",
      "35/35 [==============================] - 0s 908us/step - loss: 0.1525 - accuracy: 0.9418\n",
      "Epoch 349/1500\n",
      "35/35 [==============================] - 0s 932us/step - loss: 0.1613 - accuracy: 0.9319\n",
      "Epoch 350/1500\n",
      "35/35 [==============================] - 0s 887us/step - loss: 0.1527 - accuracy: 0.9323\n",
      "Epoch 351/1500\n",
      "35/35 [==============================] - 0s 953us/step - loss: 0.1449 - accuracy: 0.9495\n",
      "Epoch 352/1500\n",
      "35/35 [==============================] - 0s 951us/step - loss: 0.1313 - accuracy: 0.9486\n",
      "Epoch 353/1500\n",
      "35/35 [==============================] - 0s 964us/step - loss: 0.1494 - accuracy: 0.9454\n",
      "Epoch 354/1500\n",
      "35/35 [==============================] - 0s 812us/step - loss: 0.1417 - accuracy: 0.9490\n",
      "Epoch 355/1500\n",
      "35/35 [==============================] - 0s 735us/step - loss: 0.1626 - accuracy: 0.9314\n",
      "Epoch 356/1500\n",
      "35/35 [==============================] - 0s 774us/step - loss: 0.1501 - accuracy: 0.9414\n",
      "Epoch 357/1500\n",
      "35/35 [==============================] - 0s 789us/step - loss: 0.1459 - accuracy: 0.9481\n",
      "Epoch 358/1500\n",
      "35/35 [==============================] - 0s 826us/step - loss: 0.1568 - accuracy: 0.9423\n",
      "Epoch 359/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.1468 - accuracy: 0.9405\n",
      "Epoch 360/1500\n",
      "35/35 [==============================] - 0s 760us/step - loss: 0.1342 - accuracy: 0.9495\n",
      "Epoch 361/1500\n",
      "35/35 [==============================] - 0s 746us/step - loss: 0.1504 - accuracy: 0.9400\n",
      "Epoch 362/1500\n",
      "35/35 [==============================] - 0s 779us/step - loss: 0.1406 - accuracy: 0.9526\n",
      "Epoch 363/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.1414 - accuracy: 0.9450\n",
      "Epoch 364/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.1408 - accuracy: 0.9472\n",
      "Epoch 365/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.1393 - accuracy: 0.9477\n",
      "Epoch 366/1500\n",
      "35/35 [==============================] - 0s 777us/step - loss: 0.1537 - accuracy: 0.9355\n",
      "Epoch 367/1500\n",
      "35/35 [==============================] - 0s 720us/step - loss: 0.1450 - accuracy: 0.9463\n",
      "Epoch 368/1500\n",
      "35/35 [==============================] - 0s 775us/step - loss: 0.1624 - accuracy: 0.9369\n",
      "Epoch 369/1500\n",
      "35/35 [==============================] - 0s 747us/step - loss: 0.1481 - accuracy: 0.9400\n",
      "Epoch 370/1500\n",
      "35/35 [==============================] - 0s 750us/step - loss: 0.1444 - accuracy: 0.9450\n",
      "Epoch 371/1500\n",
      "35/35 [==============================] - 0s 732us/step - loss: 0.1356 - accuracy: 0.9445\n",
      "Epoch 372/1500\n",
      "35/35 [==============================] - 0s 715us/step - loss: 0.1515 - accuracy: 0.9405\n",
      "Epoch 373/1500\n",
      "35/35 [==============================] - 0s 767us/step - loss: 0.1454 - accuracy: 0.9432\n",
      "Epoch 374/1500\n",
      "35/35 [==============================] - 0s 733us/step - loss: 0.1397 - accuracy: 0.9432\n",
      "Epoch 375/1500\n",
      "35/35 [==============================] - 0s 763us/step - loss: 0.1422 - accuracy: 0.9490\n",
      "Epoch 376/1500\n",
      "35/35 [==============================] - 0s 752us/step - loss: 0.1456 - accuracy: 0.9423\n",
      "Epoch 377/1500\n",
      "35/35 [==============================] - 0s 765us/step - loss: 0.1390 - accuracy: 0.9450\n",
      "Epoch 378/1500\n",
      "35/35 [==============================] - 0s 786us/step - loss: 0.1389 - accuracy: 0.9472\n",
      "Epoch 379/1500\n",
      "35/35 [==============================] - 0s 790us/step - loss: 0.1379 - accuracy: 0.9432\n",
      "Epoch 380/1500\n",
      "35/35 [==============================] - 0s 776us/step - loss: 0.1425 - accuracy: 0.9427\n",
      "Epoch 381/1500\n",
      "35/35 [==============================] - 0s 782us/step - loss: 0.1399 - accuracy: 0.9481\n",
      "Epoch 382/1500\n",
      " 1/35 [..............................] - ETA: 0s - loss: 0.1866 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 352.\n",
      "35/35 [==============================] - 0s 805us/step - loss: 0.1496 - accuracy: 0.9391\n",
      "Epoch 382: early stopping\n",
      "7/7 [==============================] - 0s 888us/step - loss: 1.0284 - accuracy: 0.6771\n",
      "7/7 [==============================] - 0s 666us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.70 (21/30)\n",
      "Before appending - Cat IDs: 390, Predictions: 390, Actuals: 390, Gender: 390\n",
      "After appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "Final Test Results - Loss: 1.0284087657928467, Accuracy: 0.6771300435066223, Precision: 0.646433754924321, Recall: 0.7376054602331975, F1 Score: 0.6724036511542076\n",
      "Confusion Matrix:\n",
      " [[111   4  22]\n",
      " [  0   9   0]\n",
      " [ 45   1  31]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "063A    11\n",
      "068A    11\n",
      "040A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "022A     9\n",
      "045A     9\n",
      "065A     9\n",
      "072A     9\n",
      "033A     9\n",
      "015A     9\n",
      "094A     8\n",
      "010A     8\n",
      "013B     8\n",
      "117A     7\n",
      "099A     7\n",
      "050A     7\n",
      "007A     6\n",
      "053A     6\n",
      "108A     6\n",
      "109A     6\n",
      "023A     6\n",
      "021A     5\n",
      "025C     5\n",
      "044A     5\n",
      "075A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "056A     3\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "113A     3\n",
      "069A     2\n",
      "025B     2\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "087A     2\n",
      "011A     2\n",
      "093A     2\n",
      "032A     2\n",
      "054A     2\n",
      "088A     1\n",
      "115A     1\n",
      "100A     1\n",
      "024A     1\n",
      "019B     1\n",
      "043A     1\n",
      "091A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "055A    20\n",
      "019A    17\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "111A    13\n",
      "051A    12\n",
      "036A    11\n",
      "014B    10\n",
      "071A    10\n",
      "095A     8\n",
      "027A     7\n",
      "031A     7\n",
      "037A     6\n",
      "008A     6\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "105A     4\n",
      "052A     4\n",
      "014A     3\n",
      "060A     3\n",
      "102A     2\n",
      "110A     1\n",
      "096A     1\n",
      "041A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    268\n",
      "X    259\n",
      "F    182\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    89\n",
      "F    70\n",
      "M    69\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 028A, 074...\n",
      "kitten    [044A, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 097B, 019A, 020A, 101A, 095A, 034A, 027...\n",
      "kitten                             [014B, 111A, 041A, 110A]\n",
      "senior                             [055A, 059A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "Length of X_train_val:\n",
      "709\n",
      "Length of y_train_val:\n",
      "709\n",
      "Length of groups_train_val:\n",
      "709\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 864, 1: 730, 2: 655})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "36/36 [==============================] - 0s 989us/step - loss: 1.4293 - accuracy: 0.4051\n",
      "Epoch 2/1500\n",
      "36/36 [==============================] - 0s 968us/step - loss: 1.0336 - accuracy: 0.5429\n",
      "Epoch 3/1500\n",
      "36/36 [==============================] - 0s 841us/step - loss: 0.9271 - accuracy: 0.5945\n",
      "Epoch 4/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.8620 - accuracy: 0.6287\n",
      "Epoch 5/1500\n",
      "36/36 [==============================] - 0s 786us/step - loss: 0.8296 - accuracy: 0.6447\n",
      "Epoch 6/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.7754 - accuracy: 0.6665\n",
      "Epoch 7/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.7677 - accuracy: 0.6719\n",
      "Epoch 8/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.7392 - accuracy: 0.6799\n",
      "Epoch 9/1500\n",
      "36/36 [==============================] - 0s 864us/step - loss: 0.7444 - accuracy: 0.6821\n",
      "Epoch 10/1500\n",
      "36/36 [==============================] - 0s 3ms/step - loss: 0.6978 - accuracy: 0.6968\n",
      "Epoch 11/1500\n",
      "36/36 [==============================] - 0s 2ms/step - loss: 0.7080 - accuracy: 0.6968\n",
      "Epoch 12/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6890 - accuracy: 0.7065\n",
      "Epoch 13/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6710 - accuracy: 0.7239\n",
      "Epoch 14/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6775 - accuracy: 0.6990\n",
      "Epoch 15/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6475 - accuracy: 0.7243\n",
      "Epoch 16/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6394 - accuracy: 0.7217\n",
      "Epoch 17/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.6297 - accuracy: 0.7159\n",
      "Epoch 18/1500\n",
      "36/36 [==============================] - 0s 994us/step - loss: 0.6203 - accuracy: 0.7252\n",
      "Epoch 19/1500\n",
      "36/36 [==============================] - 0s 958us/step - loss: 0.6164 - accuracy: 0.7301\n",
      "Epoch 20/1500\n",
      "36/36 [==============================] - 0s 894us/step - loss: 0.6061 - accuracy: 0.7368\n",
      "Epoch 21/1500\n",
      "36/36 [==============================] - 0s 896us/step - loss: 0.6153 - accuracy: 0.7363\n",
      "Epoch 22/1500\n",
      "36/36 [==============================] - 0s 919us/step - loss: 0.5982 - accuracy: 0.7359\n",
      "Epoch 23/1500\n",
      "36/36 [==============================] - 0s 953us/step - loss: 0.5910 - accuracy: 0.7328\n",
      "Epoch 24/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.5554 - accuracy: 0.7643\n",
      "Epoch 25/1500\n",
      "36/36 [==============================] - 0s 826us/step - loss: 0.5879 - accuracy: 0.7594\n",
      "Epoch 26/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.5536 - accuracy: 0.7550\n",
      "Epoch 27/1500\n",
      "36/36 [==============================] - 0s 812us/step - loss: 0.5425 - accuracy: 0.7719\n",
      "Epoch 28/1500\n",
      "36/36 [==============================] - 0s 908us/step - loss: 0.5702 - accuracy: 0.7523\n",
      "Epoch 29/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.5486 - accuracy: 0.7581\n",
      "Epoch 30/1500\n",
      "36/36 [==============================] - 0s 779us/step - loss: 0.5442 - accuracy: 0.7617\n",
      "Epoch 31/1500\n",
      "36/36 [==============================] - 0s 825us/step - loss: 0.5339 - accuracy: 0.7701\n",
      "Epoch 32/1500\n",
      "36/36 [==============================] - 0s 908us/step - loss: 0.5170 - accuracy: 0.7741\n",
      "Epoch 33/1500\n",
      "36/36 [==============================] - 0s 770us/step - loss: 0.5364 - accuracy: 0.7577\n",
      "Epoch 34/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.5258 - accuracy: 0.7679\n",
      "Epoch 35/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.5231 - accuracy: 0.7786\n",
      "Epoch 36/1500\n",
      "36/36 [==============================] - 0s 791us/step - loss: 0.5113 - accuracy: 0.7892\n",
      "Epoch 37/1500\n",
      "36/36 [==============================] - 0s 839us/step - loss: 0.5169 - accuracy: 0.7781\n",
      "Epoch 38/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.4966 - accuracy: 0.7835\n",
      "Epoch 39/1500\n",
      "36/36 [==============================] - 0s 810us/step - loss: 0.5067 - accuracy: 0.7857\n",
      "Epoch 40/1500\n",
      "36/36 [==============================] - 0s 777us/step - loss: 0.5161 - accuracy: 0.7715\n",
      "Epoch 41/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.4975 - accuracy: 0.7857\n",
      "Epoch 42/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 0.4967 - accuracy: 0.7826\n",
      "Epoch 43/1500\n",
      "36/36 [==============================] - 0s 768us/step - loss: 0.4901 - accuracy: 0.7799\n",
      "Epoch 44/1500\n",
      "36/36 [==============================] - 0s 758us/step - loss: 0.4921 - accuracy: 0.7795\n",
      "Epoch 45/1500\n",
      "36/36 [==============================] - 0s 803us/step - loss: 0.4953 - accuracy: 0.7892\n",
      "Epoch 46/1500\n",
      "36/36 [==============================] - 0s 752us/step - loss: 0.4962 - accuracy: 0.7857\n",
      "Epoch 47/1500\n",
      "36/36 [==============================] - 0s 754us/step - loss: 0.4723 - accuracy: 0.7986\n",
      "Epoch 48/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 0.4992 - accuracy: 0.7875\n",
      "Epoch 49/1500\n",
      "36/36 [==============================] - 0s 745us/step - loss: 0.4673 - accuracy: 0.7946\n",
      "Epoch 50/1500\n",
      "36/36 [==============================] - 0s 751us/step - loss: 0.4690 - accuracy: 0.7884\n",
      "Epoch 51/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 0.4424 - accuracy: 0.8084\n",
      "Epoch 52/1500\n",
      "36/36 [==============================] - 0s 754us/step - loss: 0.4613 - accuracy: 0.8052\n",
      "Epoch 53/1500\n",
      "36/36 [==============================] - 0s 750us/step - loss: 0.4714 - accuracy: 0.8017\n",
      "Epoch 54/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 0.4510 - accuracy: 0.8106\n",
      "Epoch 55/1500\n",
      "36/36 [==============================] - 0s 739us/step - loss: 0.4706 - accuracy: 0.8004\n",
      "Epoch 56/1500\n",
      "36/36 [==============================] - 0s 740us/step - loss: 0.4445 - accuracy: 0.8110\n",
      "Epoch 57/1500\n",
      "36/36 [==============================] - 0s 816us/step - loss: 0.4332 - accuracy: 0.8217\n",
      "Epoch 58/1500\n",
      "36/36 [==============================] - 0s 751us/step - loss: 0.4418 - accuracy: 0.8106\n",
      "Epoch 59/1500\n",
      "36/36 [==============================] - 0s 754us/step - loss: 0.4305 - accuracy: 0.8119\n",
      "Epoch 60/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.4417 - accuracy: 0.7986\n",
      "Epoch 61/1500\n",
      "36/36 [==============================] - 0s 827us/step - loss: 0.4271 - accuracy: 0.8168\n",
      "Epoch 62/1500\n",
      "36/36 [==============================] - 0s 752us/step - loss: 0.4382 - accuracy: 0.8173\n",
      "Epoch 63/1500\n",
      "36/36 [==============================] - 0s 792us/step - loss: 0.4254 - accuracy: 0.8164\n",
      "Epoch 64/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 0.4299 - accuracy: 0.8155\n",
      "Epoch 65/1500\n",
      "36/36 [==============================] - 0s 753us/step - loss: 0.4328 - accuracy: 0.8186\n",
      "Epoch 66/1500\n",
      "36/36 [==============================] - 0s 781us/step - loss: 0.4233 - accuracy: 0.8297\n",
      "Epoch 67/1500\n",
      "36/36 [==============================] - 0s 740us/step - loss: 0.4409 - accuracy: 0.8186\n",
      "Epoch 68/1500\n",
      "36/36 [==============================] - 0s 799us/step - loss: 0.4261 - accuracy: 0.8181\n",
      "Epoch 69/1500\n",
      "36/36 [==============================] - 0s 752us/step - loss: 0.4274 - accuracy: 0.8173\n",
      "Epoch 70/1500\n",
      "36/36 [==============================] - 0s 735us/step - loss: 0.4248 - accuracy: 0.8230\n",
      "Epoch 71/1500\n",
      "36/36 [==============================] - 0s 761us/step - loss: 0.3943 - accuracy: 0.8324\n",
      "Epoch 72/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.4115 - accuracy: 0.8288\n",
      "Epoch 73/1500\n",
      "36/36 [==============================] - 0s 755us/step - loss: 0.4068 - accuracy: 0.8257\n",
      "Epoch 74/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.4047 - accuracy: 0.8310\n",
      "Epoch 75/1500\n",
      "36/36 [==============================] - 0s 759us/step - loss: 0.4211 - accuracy: 0.8186\n",
      "Epoch 76/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.3919 - accuracy: 0.8359\n",
      "Epoch 77/1500\n",
      "36/36 [==============================] - 0s 689us/step - loss: 0.4130 - accuracy: 0.8173\n",
      "Epoch 78/1500\n",
      "36/36 [==============================] - 0s 754us/step - loss: 0.3934 - accuracy: 0.8386\n",
      "Epoch 79/1500\n",
      "36/36 [==============================] - 0s 751us/step - loss: 0.3985 - accuracy: 0.8301\n",
      "Epoch 80/1500\n",
      "36/36 [==============================] - 0s 743us/step - loss: 0.3952 - accuracy: 0.8293\n",
      "Epoch 81/1500\n",
      "36/36 [==============================] - 0s 779us/step - loss: 0.3936 - accuracy: 0.8395\n",
      "Epoch 82/1500\n",
      "36/36 [==============================] - 0s 990us/step - loss: 0.3934 - accuracy: 0.8373\n",
      "Epoch 83/1500\n",
      "36/36 [==============================] - 0s 871us/step - loss: 0.3942 - accuracy: 0.8266\n",
      "Epoch 84/1500\n",
      "36/36 [==============================] - 0s 862us/step - loss: 0.3873 - accuracy: 0.8297\n",
      "Epoch 85/1500\n",
      "36/36 [==============================] - 0s 904us/step - loss: 0.3986 - accuracy: 0.8404\n",
      "Epoch 86/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.3753 - accuracy: 0.8484\n",
      "Epoch 87/1500\n",
      "36/36 [==============================] - 0s 882us/step - loss: 0.3922 - accuracy: 0.8484\n",
      "Epoch 88/1500\n",
      "36/36 [==============================] - 0s 876us/step - loss: 0.3795 - accuracy: 0.8364\n",
      "Epoch 89/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.4121 - accuracy: 0.8208\n",
      "Epoch 90/1500\n",
      "36/36 [==============================] - 0s 842us/step - loss: 0.3908 - accuracy: 0.8368\n",
      "Epoch 91/1500\n",
      "36/36 [==============================] - 0s 905us/step - loss: 0.3695 - accuracy: 0.8453\n",
      "Epoch 92/1500\n",
      "36/36 [==============================] - 0s 835us/step - loss: 0.3754 - accuracy: 0.8422\n",
      "Epoch 93/1500\n",
      "36/36 [==============================] - 0s 820us/step - loss: 0.3709 - accuracy: 0.8364\n",
      "Epoch 94/1500\n",
      "36/36 [==============================] - 0s 802us/step - loss: 0.3752 - accuracy: 0.8493\n",
      "Epoch 95/1500\n",
      "36/36 [==============================] - 0s 901us/step - loss: 0.3635 - accuracy: 0.8466\n",
      "Epoch 96/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.3802 - accuracy: 0.8404\n",
      "Epoch 97/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.3532 - accuracy: 0.8577\n",
      "Epoch 98/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.3682 - accuracy: 0.8457\n",
      "Epoch 99/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.3829 - accuracy: 0.8417\n",
      "Epoch 100/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.3623 - accuracy: 0.8457\n",
      "Epoch 101/1500\n",
      "36/36 [==============================] - 0s 870us/step - loss: 0.3522 - accuracy: 0.8542\n",
      "Epoch 102/1500\n",
      "36/36 [==============================] - 0s 942us/step - loss: 0.3706 - accuracy: 0.8377\n",
      "Epoch 103/1500\n",
      "36/36 [==============================] - 0s 943us/step - loss: 0.3616 - accuracy: 0.8510\n",
      "Epoch 104/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.3569 - accuracy: 0.8550\n",
      "Epoch 105/1500\n",
      "36/36 [==============================] - 0s 964us/step - loss: 0.3513 - accuracy: 0.8542\n",
      "Epoch 106/1500\n",
      "36/36 [==============================] - 0s 926us/step - loss: 0.3347 - accuracy: 0.8635\n",
      "Epoch 107/1500\n",
      "36/36 [==============================] - 0s 935us/step - loss: 0.3425 - accuracy: 0.8671\n",
      "Epoch 108/1500\n",
      "36/36 [==============================] - 0s 954us/step - loss: 0.3544 - accuracy: 0.8510\n",
      "Epoch 109/1500\n",
      "36/36 [==============================] - 0s 950us/step - loss: 0.3540 - accuracy: 0.8555\n",
      "Epoch 110/1500\n",
      "36/36 [==============================] - 0s 929us/step - loss: 0.3552 - accuracy: 0.8519\n",
      "Epoch 111/1500\n",
      "36/36 [==============================] - 0s 928us/step - loss: 0.3421 - accuracy: 0.8537\n",
      "Epoch 112/1500\n",
      "36/36 [==============================] - 0s 931us/step - loss: 0.3526 - accuracy: 0.8586\n",
      "Epoch 113/1500\n",
      "36/36 [==============================] - 0s 907us/step - loss: 0.3402 - accuracy: 0.8559\n",
      "Epoch 114/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.3249 - accuracy: 0.8608\n",
      "Epoch 115/1500\n",
      "36/36 [==============================] - 0s 916us/step - loss: 0.3563 - accuracy: 0.8528\n",
      "Epoch 116/1500\n",
      "36/36 [==============================] - 0s 908us/step - loss: 0.3536 - accuracy: 0.8582\n",
      "Epoch 117/1500\n",
      "36/36 [==============================] - 0s 927us/step - loss: 0.3536 - accuracy: 0.8515\n",
      "Epoch 118/1500\n",
      "36/36 [==============================] - 0s 925us/step - loss: 0.3380 - accuracy: 0.8644\n",
      "Epoch 119/1500\n",
      "36/36 [==============================] - 0s 929us/step - loss: 0.3493 - accuracy: 0.8590\n",
      "Epoch 120/1500\n",
      "36/36 [==============================] - 0s 880us/step - loss: 0.3293 - accuracy: 0.8622\n",
      "Epoch 121/1500\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.3347 - accuracy: 0.8662\n",
      "Epoch 122/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.3522 - accuracy: 0.8502\n",
      "Epoch 123/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.3313 - accuracy: 0.8582\n",
      "Epoch 124/1500\n",
      "36/36 [==============================] - 0s 861us/step - loss: 0.3306 - accuracy: 0.8644\n",
      "Epoch 125/1500\n",
      "36/36 [==============================] - 0s 914us/step - loss: 0.3248 - accuracy: 0.8675\n",
      "Epoch 126/1500\n",
      "36/36 [==============================] - 0s 900us/step - loss: 0.3388 - accuracy: 0.8613\n",
      "Epoch 127/1500\n",
      "36/36 [==============================] - 0s 929us/step - loss: 0.3323 - accuracy: 0.8622\n",
      "Epoch 128/1500\n",
      "36/36 [==============================] - 0s 925us/step - loss: 0.3333 - accuracy: 0.8599\n",
      "Epoch 129/1500\n",
      "36/36 [==============================] - 0s 957us/step - loss: 0.3351 - accuracy: 0.8639\n",
      "Epoch 130/1500\n",
      "36/36 [==============================] - 0s 839us/step - loss: 0.3303 - accuracy: 0.8635\n",
      "Epoch 131/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.3265 - accuracy: 0.8671\n",
      "Epoch 132/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.3356 - accuracy: 0.8631\n",
      "Epoch 133/1500\n",
      "36/36 [==============================] - 0s 780us/step - loss: 0.3334 - accuracy: 0.8693\n",
      "Epoch 134/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.3254 - accuracy: 0.8666\n",
      "Epoch 135/1500\n",
      "36/36 [==============================] - 0s 835us/step - loss: 0.3217 - accuracy: 0.8724\n",
      "Epoch 136/1500\n",
      "36/36 [==============================] - 0s 803us/step - loss: 0.3175 - accuracy: 0.8724\n",
      "Epoch 137/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.3059 - accuracy: 0.8711\n",
      "Epoch 138/1500\n",
      "36/36 [==============================] - 0s 786us/step - loss: 0.3363 - accuracy: 0.8524\n",
      "Epoch 139/1500\n",
      "36/36 [==============================] - 0s 879us/step - loss: 0.3228 - accuracy: 0.8724\n",
      "Epoch 140/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.3163 - accuracy: 0.8755\n",
      "Epoch 141/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.3215 - accuracy: 0.8662\n",
      "Epoch 142/1500\n",
      "36/36 [==============================] - 0s 904us/step - loss: 0.3150 - accuracy: 0.8724\n",
      "Epoch 143/1500\n",
      "36/36 [==============================] - 0s 907us/step - loss: 0.3242 - accuracy: 0.8733\n",
      "Epoch 144/1500\n",
      "36/36 [==============================] - 0s 940us/step - loss: 0.3114 - accuracy: 0.8719\n",
      "Epoch 145/1500\n",
      "36/36 [==============================] - 0s 810us/step - loss: 0.3205 - accuracy: 0.8697\n",
      "Epoch 146/1500\n",
      "36/36 [==============================] - 0s 832us/step - loss: 0.2986 - accuracy: 0.8768\n",
      "Epoch 147/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 0.3028 - accuracy: 0.8799\n",
      "Epoch 148/1500\n",
      "36/36 [==============================] - 0s 797us/step - loss: 0.3191 - accuracy: 0.8693\n",
      "Epoch 149/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.3124 - accuracy: 0.8728\n",
      "Epoch 150/1500\n",
      "36/36 [==============================] - 0s 837us/step - loss: 0.3071 - accuracy: 0.8733\n",
      "Epoch 151/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.3186 - accuracy: 0.8684\n",
      "Epoch 152/1500\n",
      "36/36 [==============================] - 0s 952us/step - loss: 0.3135 - accuracy: 0.8719\n",
      "Epoch 153/1500\n",
      "36/36 [==============================] - 0s 969us/step - loss: 0.3041 - accuracy: 0.8724\n",
      "Epoch 154/1500\n",
      "36/36 [==============================] - 0s 939us/step - loss: 0.3146 - accuracy: 0.8666\n",
      "Epoch 155/1500\n",
      "36/36 [==============================] - 0s 900us/step - loss: 0.2990 - accuracy: 0.8835\n",
      "Epoch 156/1500\n",
      "36/36 [==============================] - 0s 784us/step - loss: 0.3261 - accuracy: 0.8657\n",
      "Epoch 157/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 0.2982 - accuracy: 0.8755\n",
      "Epoch 158/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.3183 - accuracy: 0.8697\n",
      "Epoch 159/1500\n",
      "36/36 [==============================] - 0s 959us/step - loss: 0.2959 - accuracy: 0.8853\n",
      "Epoch 160/1500\n",
      "36/36 [==============================] - 0s 903us/step - loss: 0.2838 - accuracy: 0.8888\n",
      "Epoch 161/1500\n",
      "36/36 [==============================] - 0s 953us/step - loss: 0.3021 - accuracy: 0.8773\n",
      "Epoch 162/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8688\n",
      "Epoch 163/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2901 - accuracy: 0.8871\n",
      "Epoch 164/1500\n",
      "36/36 [==============================] - 0s 998us/step - loss: 0.3100 - accuracy: 0.8675\n",
      "Epoch 165/1500\n",
      "36/36 [==============================] - 0s 972us/step - loss: 0.3155 - accuracy: 0.8688\n",
      "Epoch 166/1500\n",
      "36/36 [==============================] - 0s 989us/step - loss: 0.2906 - accuracy: 0.8844\n",
      "Epoch 167/1500\n",
      "36/36 [==============================] - 0s 991us/step - loss: 0.3036 - accuracy: 0.8759\n",
      "Epoch 168/1500\n",
      "36/36 [==============================] - 0s 995us/step - loss: 0.3002 - accuracy: 0.8777\n",
      "Epoch 169/1500\n",
      "36/36 [==============================] - 0s 817us/step - loss: 0.2945 - accuracy: 0.8857\n",
      "Epoch 170/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.2931 - accuracy: 0.8804\n",
      "Epoch 171/1500\n",
      "36/36 [==============================] - 0s 757us/step - loss: 0.2850 - accuracy: 0.8880\n",
      "Epoch 172/1500\n",
      "36/36 [==============================] - 0s 746us/step - loss: 0.2990 - accuracy: 0.8839\n",
      "Epoch 173/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.2847 - accuracy: 0.8844\n",
      "Epoch 174/1500\n",
      "36/36 [==============================] - 0s 868us/step - loss: 0.2777 - accuracy: 0.8848\n",
      "Epoch 175/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.2891 - accuracy: 0.8791\n",
      "Epoch 176/1500\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.2782 - accuracy: 0.8942\n",
      "Epoch 177/1500\n",
      "36/36 [==============================] - 0s 758us/step - loss: 0.2804 - accuracy: 0.8871\n",
      "Epoch 178/1500\n",
      "36/36 [==============================] - 0s 748us/step - loss: 0.2887 - accuracy: 0.8813\n",
      "Epoch 179/1500\n",
      "36/36 [==============================] - 0s 749us/step - loss: 0.2848 - accuracy: 0.8862\n",
      "Epoch 180/1500\n",
      "36/36 [==============================] - 0s 752us/step - loss: 0.2913 - accuracy: 0.8888\n",
      "Epoch 181/1500\n",
      "36/36 [==============================] - 0s 813us/step - loss: 0.2818 - accuracy: 0.8880\n",
      "Epoch 182/1500\n",
      "36/36 [==============================] - 0s 768us/step - loss: 0.2970 - accuracy: 0.8782\n",
      "Epoch 183/1500\n",
      "36/36 [==============================] - 0s 937us/step - loss: 0.2770 - accuracy: 0.8928\n",
      "Epoch 184/1500\n",
      "36/36 [==============================] - 0s 946us/step - loss: 0.2927 - accuracy: 0.8853\n",
      "Epoch 185/1500\n",
      "36/36 [==============================] - 0s 982us/step - loss: 0.2924 - accuracy: 0.8839\n",
      "Epoch 186/1500\n",
      "36/36 [==============================] - 0s 950us/step - loss: 0.2792 - accuracy: 0.8880\n",
      "Epoch 187/1500\n",
      "36/36 [==============================] - 0s 945us/step - loss: 0.2790 - accuracy: 0.8808\n",
      "Epoch 188/1500\n",
      "36/36 [==============================] - 0s 926us/step - loss: 0.2724 - accuracy: 0.8920\n",
      "Epoch 189/1500\n",
      "36/36 [==============================] - 0s 937us/step - loss: 0.2782 - accuracy: 0.8893\n",
      "Epoch 190/1500\n",
      "36/36 [==============================] - 0s 1ms/step - loss: 0.2754 - accuracy: 0.8888\n",
      "Epoch 191/1500\n",
      "36/36 [==============================] - 0s 938us/step - loss: 0.2790 - accuracy: 0.8888\n",
      "Epoch 192/1500\n",
      "36/36 [==============================] - 0s 997us/step - loss: 0.2642 - accuracy: 0.8968\n",
      "Epoch 193/1500\n",
      "36/36 [==============================] - 0s 887us/step - loss: 0.2637 - accuracy: 0.8933\n",
      "Epoch 194/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.2701 - accuracy: 0.8960\n",
      "Epoch 195/1500\n",
      "36/36 [==============================] - 0s 789us/step - loss: 0.2796 - accuracy: 0.8893\n",
      "Epoch 196/1500\n",
      "36/36 [==============================] - 0s 764us/step - loss: 0.2717 - accuracy: 0.8893\n",
      "Epoch 197/1500\n",
      "36/36 [==============================] - 0s 765us/step - loss: 0.2778 - accuracy: 0.8848\n",
      "Epoch 198/1500\n",
      "36/36 [==============================] - 0s 739us/step - loss: 0.2765 - accuracy: 0.8933\n",
      "Epoch 199/1500\n",
      "36/36 [==============================] - 0s 754us/step - loss: 0.2692 - accuracy: 0.8937\n",
      "Epoch 200/1500\n",
      "36/36 [==============================] - 0s 788us/step - loss: 0.2664 - accuracy: 0.8871\n",
      "Epoch 201/1500\n",
      "36/36 [==============================] - 0s 753us/step - loss: 0.2672 - accuracy: 0.8897\n",
      "Epoch 202/1500\n",
      "36/36 [==============================] - 0s 739us/step - loss: 0.2820 - accuracy: 0.8857\n",
      "Epoch 203/1500\n",
      "36/36 [==============================] - 0s 817us/step - loss: 0.2637 - accuracy: 0.9004\n",
      "Epoch 204/1500\n",
      "36/36 [==============================] - 0s 838us/step - loss: 0.2824 - accuracy: 0.8862\n",
      "Epoch 205/1500\n",
      "36/36 [==============================] - 0s 872us/step - loss: 0.2535 - accuracy: 0.9017\n",
      "Epoch 206/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.2629 - accuracy: 0.8986\n",
      "Epoch 207/1500\n",
      "36/36 [==============================] - 0s 891us/step - loss: 0.2605 - accuracy: 0.9000\n",
      "Epoch 208/1500\n",
      "36/36 [==============================] - 0s 919us/step - loss: 0.2557 - accuracy: 0.9013\n",
      "Epoch 209/1500\n",
      "36/36 [==============================] - 0s 924us/step - loss: 0.2633 - accuracy: 0.9000\n",
      "Epoch 210/1500\n",
      "36/36 [==============================] - 0s 929us/step - loss: 0.2619 - accuracy: 0.8933\n",
      "Epoch 211/1500\n",
      "36/36 [==============================] - 0s 926us/step - loss: 0.2615 - accuracy: 0.8924\n",
      "Epoch 212/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.2525 - accuracy: 0.9062\n",
      "Epoch 213/1500\n",
      "36/36 [==============================] - 0s 846us/step - loss: 0.2471 - accuracy: 0.8982\n",
      "Epoch 214/1500\n",
      "36/36 [==============================] - 0s 809us/step - loss: 0.2646 - accuracy: 0.8911\n",
      "Epoch 215/1500\n",
      "36/36 [==============================] - 0s 805us/step - loss: 0.2692 - accuracy: 0.8942\n",
      "Epoch 216/1500\n",
      "36/36 [==============================] - 0s 921us/step - loss: 0.2629 - accuracy: 0.8951\n",
      "Epoch 217/1500\n",
      "36/36 [==============================] - 0s 839us/step - loss: 0.2608 - accuracy: 0.8951\n",
      "Epoch 218/1500\n",
      "36/36 [==============================] - 0s 950us/step - loss: 0.2418 - accuracy: 0.9066\n",
      "Epoch 219/1500\n",
      "36/36 [==============================] - 0s 948us/step - loss: 0.2632 - accuracy: 0.8973\n",
      "Epoch 220/1500\n",
      "36/36 [==============================] - 0s 855us/step - loss: 0.2740 - accuracy: 0.8893\n",
      "Epoch 221/1500\n",
      "36/36 [==============================] - 0s 816us/step - loss: 0.2505 - accuracy: 0.9040\n",
      "Epoch 222/1500\n",
      "36/36 [==============================] - 0s 845us/step - loss: 0.2575 - accuracy: 0.8977\n",
      "Epoch 223/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.2654 - accuracy: 0.8920\n",
      "Epoch 224/1500\n",
      "36/36 [==============================] - 0s 877us/step - loss: 0.2687 - accuracy: 0.8960\n",
      "Epoch 225/1500\n",
      "36/36 [==============================] - 0s 839us/step - loss: 0.2529 - accuracy: 0.8924\n",
      "Epoch 226/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.2465 - accuracy: 0.9004\n",
      "Epoch 227/1500\n",
      "36/36 [==============================] - 0s 890us/step - loss: 0.2516 - accuracy: 0.8942\n",
      "Epoch 228/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.2517 - accuracy: 0.8991\n",
      "Epoch 229/1500\n",
      "36/36 [==============================] - 0s 851us/step - loss: 0.2633 - accuracy: 0.8968\n",
      "Epoch 230/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.2628 - accuracy: 0.8951\n",
      "Epoch 231/1500\n",
      "36/36 [==============================] - 0s 798us/step - loss: 0.2661 - accuracy: 0.8915\n",
      "Epoch 232/1500\n",
      "36/36 [==============================] - 0s 771us/step - loss: 0.2621 - accuracy: 0.8933\n",
      "Epoch 233/1500\n",
      "36/36 [==============================] - 0s 792us/step - loss: 0.2461 - accuracy: 0.9035\n",
      "Epoch 234/1500\n",
      "36/36 [==============================] - 0s 833us/step - loss: 0.2508 - accuracy: 0.8977\n",
      "Epoch 235/1500\n",
      "36/36 [==============================] - 0s 778us/step - loss: 0.2417 - accuracy: 0.9008\n",
      "Epoch 236/1500\n",
      "36/36 [==============================] - 0s 860us/step - loss: 0.2479 - accuracy: 0.9053\n",
      "Epoch 237/1500\n",
      "36/36 [==============================] - 0s 787us/step - loss: 0.2428 - accuracy: 0.9035\n",
      "Epoch 238/1500\n",
      "36/36 [==============================] - 0s 804us/step - loss: 0.2403 - accuracy: 0.9115\n",
      "Epoch 239/1500\n",
      "36/36 [==============================] - 0s 776us/step - loss: 0.2475 - accuracy: 0.8942\n",
      "Epoch 240/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.2369 - accuracy: 0.9017\n",
      "Epoch 241/1500\n",
      "36/36 [==============================] - 0s 734us/step - loss: 0.2440 - accuracy: 0.9031\n",
      "Epoch 242/1500\n",
      "36/36 [==============================] - 0s 768us/step - loss: 0.2510 - accuracy: 0.8991\n",
      "Epoch 243/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.2515 - accuracy: 0.8951\n",
      "Epoch 244/1500\n",
      "36/36 [==============================] - 0s 884us/step - loss: 0.2532 - accuracy: 0.9053\n",
      "Epoch 245/1500\n",
      "36/36 [==============================] - 0s 932us/step - loss: 0.2376 - accuracy: 0.9017\n",
      "Epoch 246/1500\n",
      "36/36 [==============================] - 0s 928us/step - loss: 0.2431 - accuracy: 0.9035\n",
      "Epoch 247/1500\n",
      "36/36 [==============================] - 0s 961us/step - loss: 0.2174 - accuracy: 0.9142\n",
      "Epoch 248/1500\n",
      "36/36 [==============================] - 0s 903us/step - loss: 0.2428 - accuracy: 0.9053\n",
      "Epoch 249/1500\n",
      "36/36 [==============================] - 0s 878us/step - loss: 0.2280 - accuracy: 0.9115\n",
      "Epoch 250/1500\n",
      "36/36 [==============================] - 0s 858us/step - loss: 0.2461 - accuracy: 0.9031\n",
      "Epoch 251/1500\n",
      "36/36 [==============================] - 0s 818us/step - loss: 0.2364 - accuracy: 0.9071\n",
      "Epoch 252/1500\n",
      "36/36 [==============================] - 0s 849us/step - loss: 0.2518 - accuracy: 0.8964\n",
      "Epoch 253/1500\n",
      "36/36 [==============================] - 0s 866us/step - loss: 0.2314 - accuracy: 0.9093\n",
      "Epoch 254/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.2381 - accuracy: 0.9106\n",
      "Epoch 255/1500\n",
      "36/36 [==============================] - 0s 829us/step - loss: 0.2358 - accuracy: 0.9053\n",
      "Epoch 256/1500\n",
      "36/36 [==============================] - 0s 821us/step - loss: 0.2392 - accuracy: 0.9044\n",
      "Epoch 257/1500\n",
      "36/36 [==============================] - 0s 785us/step - loss: 0.2566 - accuracy: 0.9000\n",
      "Epoch 258/1500\n",
      "36/36 [==============================] - 0s 794us/step - loss: 0.2383 - accuracy: 0.9044\n",
      "Epoch 259/1500\n",
      "36/36 [==============================] - 0s 774us/step - loss: 0.2270 - accuracy: 0.9093\n",
      "Epoch 260/1500\n",
      "36/36 [==============================] - 0s 782us/step - loss: 0.2267 - accuracy: 0.9129\n",
      "Epoch 261/1500\n",
      "36/36 [==============================] - 0s 796us/step - loss: 0.2241 - accuracy: 0.9124\n",
      "Epoch 262/1500\n",
      "36/36 [==============================] - 0s 831us/step - loss: 0.2232 - accuracy: 0.9115\n",
      "Epoch 263/1500\n",
      "36/36 [==============================] - 0s 874us/step - loss: 0.2237 - accuracy: 0.9106\n",
      "Epoch 264/1500\n",
      "36/36 [==============================] - 0s 892us/step - loss: 0.2395 - accuracy: 0.9044\n",
      "Epoch 265/1500\n",
      "36/36 [==============================] - 0s 857us/step - loss: 0.2333 - accuracy: 0.9017\n",
      "Epoch 266/1500\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.2320 - accuracy: 0.9066\n",
      "Epoch 267/1500\n",
      "36/36 [==============================] - 0s 881us/step - loss: 0.2324 - accuracy: 0.9044\n",
      "Epoch 268/1500\n",
      "36/36 [==============================] - 0s 966us/step - loss: 0.2328 - accuracy: 0.9142\n",
      "Epoch 269/1500\n",
      "36/36 [==============================] - 0s 899us/step - loss: 0.2256 - accuracy: 0.9137\n",
      "Epoch 270/1500\n",
      "36/36 [==============================] - 0s 924us/step - loss: 0.2330 - accuracy: 0.9151\n",
      "Epoch 271/1500\n",
      "36/36 [==============================] - 0s 905us/step - loss: 0.2385 - accuracy: 0.9053\n",
      "Epoch 272/1500\n",
      "36/36 [==============================] - 0s 930us/step - loss: 0.2191 - accuracy: 0.9129\n",
      "Epoch 273/1500\n",
      "36/36 [==============================] - 0s 923us/step - loss: 0.2354 - accuracy: 0.9057\n",
      "Epoch 274/1500\n",
      "36/36 [==============================] - 0s 906us/step - loss: 0.2167 - accuracy: 0.9146\n",
      "Epoch 275/1500\n",
      "36/36 [==============================] - 0s 930us/step - loss: 0.2207 - accuracy: 0.9146\n",
      "Epoch 276/1500\n",
      "36/36 [==============================] - 0s 836us/step - loss: 0.2281 - accuracy: 0.9124\n",
      "Epoch 277/1500\n",
      " 1/36 [..............................] - ETA: 0s - loss: 0.2440 - accuracy: 0.8906Restoring model weights from the end of the best epoch: 247.\n",
      "36/36 [==============================] - 0s 888us/step - loss: 0.2192 - accuracy: 0.9124\n",
      "Epoch 277: early stopping\n",
      "8/8 [==============================] - 0s 718us/step - loss: 0.6566 - accuracy: 0.7588\n",
      "8/8 [==============================] - 0s 614us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.71 (20/28)\n",
      "Before appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "After appending - Cat IDs: 841, Predictions: 841, Actuals: 841, Gender: 841\n",
      "Final Test Results - Loss: 0.6566341519355774, Accuracy: 0.7587719559669495, Precision: 0.7004020963223999, Recall: 0.8042898708856155, F1 Score: 0.7341594061933044\n",
      "Confusion Matrix:\n",
      " [[113  11  32]\n",
      " [  3  22   0]\n",
      " [  8   1  38]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6441374536111171\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8418788909912109\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7030641883611679\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6570544974045266\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6924066594999193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[4]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'RMSprop': RMSprop(learning_rate=0.00017746563142321905)\n",
    "    }\n",
    "    \n",
    "    # Full model definition \n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(64, activation='elu', input_shape=(X_train_full_scaled.shape[1],))) \n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.17420007618491104))\n",
    "    model_full.add(Dense(64, activation='elu'))\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.1782921674765571))             \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "\n",
    "    optimizer = optimizers['RMSprop']\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=64,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90d80b-198d-4293-a1a0-73a65f6588d0",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "e9dd5399-64d4-435b-9e73-cb31f16d042d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 841, Predictions: 841, Actuals: 841, Gender: 841\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "9d9869f0-e23f-4453-a329-395fa44f1208",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "a7f81185-7b51-497f-98cb-80c4b8f35388",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.66 (73/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "adbd2ca0-3dea-4b42-bfad-93138cb1ae30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "b1ea4fe7-6ee1-4185-a009-b864d9aa6b85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, kitten, adult, kitten, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, senior, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, adult, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, kitten,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, kitten, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[senior, senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, adult, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[senior, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, adult, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[kitten, kitten, adult]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, senior, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, kitten, adult, kitten, adult, adult, a...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, adult, senior, senior, senior, senior...        senior           senior                   True\n",
       "64    058A                            [senior, adult, senior]        senior           senior                   True\n",
       "61    055A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, adult, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "46    040A  [kitten, kitten, senior, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "43    037A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, adult,...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "103   109A    [kitten, kitten, kitten, kitten, adult, kitten]        kitten           kitten                   True\n",
       "102   108A   [senior, senior, senior, senior, senior, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "98    103A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "12    011A                                   [senior, senior]        senior           senior                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "28    025A  [senior, adult, senior, senior, adult, adult, ...         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, kitten, adult, adult, ki...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "106   113A                             [adult, senior, adult]         adult           senior                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "90    095A  [senior, adult, adult, senior, senior, kitten,...        senior            adult                  False\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...         adult           senior                  False\n",
       "93    097B  [senior, adult, kitten, adult, adult, kitten, ...        kitten            adult                  False\n",
       "99    104A                     [adult, senior, adult, senior]         adult           senior                  False\n",
       "101   106A  [adult, adult, senior, senior, senior, adult, ...         adult           senior                  False\n",
       "42    036A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "18    016A  [senior, adult, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "40    034A           [senior, senior, senior, senior, senior]        senior            adult                  False\n",
       "47    041A                                    [adult, kitten]         adult           kitten                  False\n",
       "48    042A  [adult, adult, adult, adult, adult, adult, adu...         adult           kitten                  False\n",
       "38    032A                                   [kitten, kitten]        kitten            adult                  False\n",
       "50    044A              [adult, adult, adult, kitten, kitten]         adult           kitten                  False\n",
       "52    047A  [kitten, adult, adult, adult, adult, adult, ki...         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "54    049A                                            [adult]         adult           kitten                  False\n",
       "57    051B  [senior, adult, adult, adult, senior, adult, s...         adult           senior                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "62    056A                             [adult, senior, adult]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, senior, adult, a...         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "66    060A                           [senior, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "30    025C             [senior, senior, adult, senior, adult]        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "70    064A                            [kitten, kitten, adult]        kitten            adult                  False\n",
       "71    065A  [senior, adult, kitten, adult, adult, senior, ...        kitten            adult                  False\n",
       "109   117A  [senior, senior, adult, adult, adult, adult, a...         adult           senior                  False"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "e03366c5-a807-4159-b0c1-8dc88c04d91e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     56\n",
      "kitten     8\n",
      "senior     9\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "0ae8f86b-0e19-49df-a07c-f64383bce76b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             56  76.712329\n",
      "1           kitten           15              8  53.333333\n",
      "2           senior           22              9  40.909091\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "6350c1b2-050d-4bf3-98f1-5a80b3078d3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ea2c7-0827-4ffa-9777-ad2891e5f49b",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "94fbe612-c62e-4abe-8ec3-d75940a993cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          554            433  78.158845\n",
      "1           kitten          109             65  59.633028\n",
      "2           senior          178             94  52.808989\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "50e84f5a-909e-47ca-93ca-2e92abf7ded1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ac46e-4cc6-4237-925c-524d47e83b46",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "id": "1743599c-0d3f-4b10-a095-02c36218c679",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    218      169     77.52\n",
      "1          M    337      252     74.78\n",
      "2          X    286      171     59.79\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "id": "b8546267-aa7a-4e4b-b60e-810f836ebf8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613636d-22ae-4629-ac86-daf482925194",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
