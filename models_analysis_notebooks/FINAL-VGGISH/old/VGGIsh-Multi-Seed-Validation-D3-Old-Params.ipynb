{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17da2043-9a5b-40fe-b3cb-f755b9a98deb",
   "metadata": {},
   "source": [
    "# VGGIsh Multi Seed Validation\n",
    "#### 5 Random (but reproducible) seeds are selected for 5 different runs of train/test\n",
    "#### Models have been optimised and verified on validation sets thoroughly, running a final train/test evaluation here\n",
    "#### The setup:\n",
    "\n",
    "- 4 fold StratifiedGroupKFold for stratification and ensuring each cat_id group only appears in one set at a time\n",
    "- Final scores averaged over the 4 folds\n",
    "- For each seed run we will explore the cat_id predictions through majority voting\n",
    "- For each run we will explore the potential impact of gender\n",
    "\n",
    "The dataset is highly unbalanced, resources for an unbiased estimate have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ac1e09ae-387c-4347-b6f5-8d09dd1bf3f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, SGD, RMSprop, AdamW\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import shap\n",
    "from keras.regularizers import l1, l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d005cfd2-3eda-44c4-bbb5-af343e979bc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [7270  860 5390 5191 5734]\n"
     ]
    }
   ],
   "source": [
    "# Set an initial seed for reproducibility\n",
    "np.random.seed(42)  \n",
    "\n",
    "# Generate a list of 5 random seeds\n",
    "random_seeds = np.random.randint(0, 10000, size=5)\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae122a-9f9c-47a3-8835-2d46d2f8e2f9",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d5097e68-5153-440c-9294-dfa9e6061652",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_initial_group_split(groups_train, groups_test):\n",
    "    \"\"\"\n",
    "    Check if any group is present in both the train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - groups_train: Array of group identifiers for the train set\n",
    "    - groups_test: Array of group identifiers for the test set\n",
    "\n",
    "    Returns:\n",
    "    - Prints out any groups found in both sets and the count of such groups\n",
    "    \"\"\"\n",
    "    train_groups = set(groups_train)\n",
    "    test_groups = set(groups_test)\n",
    "    common_groups = train_groups.intersection(test_groups)\n",
    "\n",
    "    if common_groups:\n",
    "        print(f\"Warning: Found {len(common_groups)} common groups in both train/validation and test sets: {common_groups}\")\n",
    "    else:\n",
    "        print(\"No common groups found between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "88dfe2de-bb1e-4d49-9260-e056c39627bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to perform the swaps based on cat_id, ensuring swaps within the same age_group\n",
    "def swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids):\n",
    "    for cat_id in specific_cat_ids:\n",
    "        # Check if the specific cat_id is not in the training set\n",
    "        if cat_id not in dataframe.iloc[train_val_idx]['cat_id'].values:\n",
    "            # Get the age_group of this cat_id\n",
    "            age_group = dataframe[dataframe['cat_id'] == cat_id]['age_group'].iloc[0]\n",
    "                \n",
    "            # Find a different cat_id within the same age_group in the train set that is not in the test set\n",
    "            other_cat_ids_in_age_group = dataframe[(dataframe['age_group'] == age_group) & \n",
    "                                                   (dataframe['cat_id'] != cat_id) &\n",
    "                                                   (~dataframe['cat_id'].isin(dataframe.iloc[test_idx]['cat_id']))]['cat_id'].unique()\n",
    "            \n",
    "            # Choose one other cat_id for swapping\n",
    "            if len(other_cat_ids_in_age_group) > 0:\n",
    "                other_cat_id = np.random.choice(other_cat_ids_in_age_group)\n",
    "\n",
    "                # Find all instances of the other_cat_id in the train set\n",
    "                other_cat_id_train_val_indices = train_val_idx[dataframe.iloc[train_val_idx]['cat_id'] == other_cat_id]\n",
    "                \n",
    "                # Find all instances of the specific cat_id in the test set\n",
    "                cat_id_test_indices = test_idx[dataframe.iloc[test_idx]['cat_id'] == cat_id]\n",
    "                \n",
    "                # Swap the indices\n",
    "                train_val_idx = np.setdiff1d(train_val_idx, other_cat_id_train_val_indices, assume_unique=True)\n",
    "                test_idx = np.setdiff1d(test_idx, cat_id_test_indices, assume_unique=True)\n",
    "\n",
    "                train_val_idx = np.concatenate((train_val_idx, cat_id_test_indices))\n",
    "                test_idx = np.concatenate((test_idx, other_cat_id_train_val_indices))\n",
    "            else:\n",
    "                print(f\"No alternative cat_id found in the same age_group as {cat_id} for swapping.\")\n",
    "                \n",
    "    return train_val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2e6c2d87-cfab-493e-bca8-b3b8976a2bda",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify differences in groups\n",
    "def find_group_differences(original, new):\n",
    "    # Convert numpy arrays to sets for easy difference computation\n",
    "    original_set = set(original)\n",
    "    new_set = set(new)\n",
    "    # Find differences\n",
    "    moved_to_new = new_set - original_set\n",
    "    moved_to_original = original_set - new_set\n",
    "    return moved_to_new, moved_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "6164b1c3-75dd-4549-aafd-cb6106297941",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create custom logger function for local logs & stored in a .txt\n",
    "def logger(message, file=None):\n",
    "    print(message)\n",
    "    if file is not None:\n",
    "        with open(file, \"a\") as log_file:\n",
    "            log_file.write(message + \"\\n\")\n",
    "\n",
    "log_file_path = \"multi-seed-val-D13.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "899fb535-0e25-4c7b-b6a5-13231e26c502",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "# Define a custom color palette\n",
    "colors = [\"#6aabd1\", \"#b6e2d3\", \"#dac292\"] \n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "# Function to create bar plots with enhanced style\n",
    "def styled_barplot(data, x, y, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bar_plot = sns.barplot(x=x, y=y, data=data, errorbar=None, width=0.5)  \n",
    "    plt.title(title, fontsize=16, fontweight='bold', color=\"#333333\")\n",
    "    plt.xlabel(xlabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.ylabel(ylabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.xticks(fontsize=12, color=\"#333333\")\n",
    "    plt.yticks(fontsize=12, color=\"#333333\")\n",
    "    plt.ylim(0, 100) \n",
    "\n",
    "    # Adding value labels on top of each bar\n",
    "    for p in bar_plot.patches:\n",
    "        height = p.get_height()\n",
    "        # Annotate the height value on the bar\n",
    "        bar_plot.annotate(f'{height:.1f}', \n",
    "                          (p.get_x() + p.get_width() / 2., height), \n",
    "                          ha='center', va='center', \n",
    "                          xytext=(0, 9), \n",
    "                          textcoords='offset points', fontsize=12, color=\"#333333\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936a24a-13bd-4bc3-be13-0b210a09b5da",
   "metadata": {},
   "source": [
    "# RANDOM SEED 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70136a37-0507-4c2e-8307-c2dd905432e8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "14bb802b-f4bd-4464-a5fd-1977438428b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    1020\n",
      "kitten     992\n",
      "adult      842\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[0])) \n",
    "np.random.seed(int(random_seeds[0]))\n",
    "tf.random.set_seed(int(random_seeds[0]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_3.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6ce97c28-4210-4150-a60b-acdbf0e7716c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c26f17c3-4d21-4537-a090-9ca00f2be51a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91a9c3-3474-4e83-8a5b-c5eb4a9a9223",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f4605c0b-f99d-48c3-8c2a-d87dd22c8946",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "005A    10\n",
      "071A    10\n",
      "040A    10\n",
      "014B    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "022A     9\n",
      "072A     9\n",
      "095A     8\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "031A     7\n",
      "027A     7\n",
      "050A     7\n",
      "109A     6\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "037A     6\n",
      "075A     5\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "026A     4\n",
      "035A     4\n",
      "105A     4\n",
      "052A     4\n",
      "003A     4\n",
      "062A     4\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "090A     1\n",
      "100A     1\n",
      "115A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "049A     1\n",
      "048A     1\n",
      "066A     1\n",
      "096A     1\n",
      "026C     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "055A    20\n",
      "001A    14\n",
      "025A    11\n",
      "016A    10\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "117A     7\n",
      "008A     6\n",
      "053A     6\n",
      "104A     4\n",
      "009A     4\n",
      "060A     3\n",
      "056A     3\n",
      "058A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "073A     1\n",
      "076A     1\n",
      "092A     1\n",
      "091A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    325\n",
      "M    253\n",
      "F    197\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    84\n",
      "F    55\n",
      "X    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [097A, 057A, 106A, 059A, 113A, 116A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 001A, 103A, 091A, 009A, 025A, 069A, 032...\n",
      "kitten                                         [045A, 110A]\n",
      "senior    [093A, 104A, 055A, 117A, 056A, 058A, 016A, 094...\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 14, 'senior': 13}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 2, 'senior': 9}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "Length of X_train_val:\n",
      "775\n",
      "Length of y_train_val:\n",
      "775\n",
      "Length of groups_train_val:\n",
      "775\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1204, 1: 1093, 2: 796})\n",
      "Epoch 1/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.8563 - accuracy: 0.6314\n",
      "Epoch 2/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.6848 - accuracy: 0.7181\n",
      "Epoch 3/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.6188 - accuracy: 0.7485\n",
      "Epoch 4/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5868 - accuracy: 0.7533\n",
      "Epoch 5/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5599 - accuracy: 0.7740\n",
      "Epoch 6/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5207 - accuracy: 0.7941\n",
      "Epoch 7/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4997 - accuracy: 0.7918\n",
      "Epoch 8/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5034 - accuracy: 0.8008\n",
      "Epoch 9/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4686 - accuracy: 0.8015\n",
      "Epoch 10/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4604 - accuracy: 0.8080\n",
      "Epoch 11/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4595 - accuracy: 0.8154\n",
      "Epoch 12/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4423 - accuracy: 0.8186\n",
      "Epoch 13/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4365 - accuracy: 0.8160\n",
      "Epoch 14/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4145 - accuracy: 0.8332\n",
      "Epoch 15/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4244 - accuracy: 0.8225\n",
      "Epoch 16/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3995 - accuracy: 0.8387\n",
      "Epoch 17/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4180 - accuracy: 0.8251\n",
      "Epoch 18/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4028 - accuracy: 0.8438\n",
      "Epoch 19/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3850 - accuracy: 0.8497\n",
      "Epoch 20/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3820 - accuracy: 0.8455\n",
      "Epoch 21/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3839 - accuracy: 0.8477\n",
      "Epoch 22/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3573 - accuracy: 0.8506\n",
      "Epoch 23/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3488 - accuracy: 0.8629\n",
      "Epoch 24/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3444 - accuracy: 0.8639\n",
      "Epoch 25/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3522 - accuracy: 0.8516\n",
      "Epoch 26/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3374 - accuracy: 0.8645\n",
      "Epoch 27/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3401 - accuracy: 0.8607\n",
      "Epoch 28/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3235 - accuracy: 0.8726\n",
      "Epoch 29/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3373 - accuracy: 0.8613\n",
      "Epoch 30/1500\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.3097 - accuracy: 0.8752\n",
      "Epoch 31/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3378 - accuracy: 0.8678\n",
      "Epoch 32/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2996 - accuracy: 0.8823\n",
      "Epoch 33/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3144 - accuracy: 0.8752\n",
      "Epoch 34/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3086 - accuracy: 0.8771\n",
      "Epoch 35/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3058 - accuracy: 0.8775\n",
      "Epoch 36/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3044 - accuracy: 0.8762\n",
      "Epoch 37/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2924 - accuracy: 0.8794\n",
      "Epoch 38/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2891 - accuracy: 0.8875\n",
      "Epoch 39/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.8836\n",
      "Epoch 40/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2979 - accuracy: 0.8878\n",
      "Epoch 41/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2863 - accuracy: 0.8972\n",
      "Epoch 42/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.8907\n",
      "Epoch 43/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3151 - accuracy: 0.8742\n",
      "Epoch 44/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.8881\n",
      "Epoch 45/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2755 - accuracy: 0.8927\n",
      "Epoch 46/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2702 - accuracy: 0.8965\n",
      "Epoch 47/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2726 - accuracy: 0.8923\n",
      "Epoch 48/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.8888\n",
      "Epoch 49/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2713 - accuracy: 0.8917\n",
      "Epoch 50/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2447 - accuracy: 0.9062\n",
      "Epoch 51/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2784 - accuracy: 0.8933\n",
      "Epoch 52/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2575 - accuracy: 0.8998\n",
      "Epoch 53/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2611 - accuracy: 0.8959\n",
      "Epoch 54/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2521 - accuracy: 0.9017\n",
      "Epoch 55/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.8952\n",
      "Epoch 56/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2494 - accuracy: 0.9043\n",
      "Epoch 57/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2582 - accuracy: 0.8952\n",
      "Epoch 58/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2640 - accuracy: 0.8952\n",
      "Epoch 59/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2365 - accuracy: 0.9085\n",
      "Epoch 60/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2454 - accuracy: 0.9059\n",
      "Epoch 61/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2389 - accuracy: 0.9124\n",
      "Epoch 62/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2320 - accuracy: 0.9117\n",
      "Epoch 63/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2308 - accuracy: 0.9150\n",
      "Epoch 64/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2289 - accuracy: 0.9085\n",
      "Epoch 65/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.9082\n",
      "Epoch 66/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2285 - accuracy: 0.9130\n",
      "Epoch 67/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2239 - accuracy: 0.9124\n",
      "Epoch 68/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2319 - accuracy: 0.9134\n",
      "Epoch 69/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2332 - accuracy: 0.9137\n",
      "Epoch 70/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2263 - accuracy: 0.9163\n",
      "Epoch 71/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2056 - accuracy: 0.9315\n",
      "Epoch 72/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2336 - accuracy: 0.9121\n",
      "Epoch 73/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9072\n",
      "Epoch 74/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2248 - accuracy: 0.9143\n",
      "Epoch 75/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2054 - accuracy: 0.9247\n",
      "Epoch 76/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2436 - accuracy: 0.9062\n",
      "Epoch 77/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2249 - accuracy: 0.9121\n",
      "Epoch 78/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2270 - accuracy: 0.9130\n",
      "Epoch 79/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2194 - accuracy: 0.9172\n",
      "Epoch 80/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2129 - accuracy: 0.9114\n",
      "Epoch 81/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2059 - accuracy: 0.9211\n",
      "Epoch 82/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2275 - accuracy: 0.9091\n",
      "Epoch 83/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9211\n",
      "Epoch 84/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2135 - accuracy: 0.9195\n",
      "Epoch 85/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9253\n",
      "Epoch 86/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2214 - accuracy: 0.9159\n",
      "Epoch 87/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2128 - accuracy: 0.9214\n",
      "Epoch 88/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2146 - accuracy: 0.9195\n",
      "Epoch 89/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2113 - accuracy: 0.9211\n",
      "Epoch 90/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.9201\n",
      "Epoch 91/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9276\n",
      "Epoch 92/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2034 - accuracy: 0.9201\n",
      "Epoch 93/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9208\n",
      "Epoch 94/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2054 - accuracy: 0.9234\n",
      "Epoch 95/1500\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1967 - accuracy: 0.9279\n",
      "Epoch 96/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2012 - accuracy: 0.9250\n",
      "Epoch 97/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.9218\n",
      "Epoch 98/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.9298\n",
      "Epoch 99/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1935 - accuracy: 0.9263\n",
      "Epoch 100/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2102 - accuracy: 0.9211\n",
      "Epoch 101/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9243\n",
      "Epoch 102/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1879 - accuracy: 0.9292\n",
      "Epoch 103/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9224\n",
      "Epoch 104/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2076 - accuracy: 0.9159\n",
      "Epoch 105/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9253\n",
      "Epoch 106/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1934 - accuracy: 0.9269\n",
      "Epoch 107/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9256\n",
      "Epoch 108/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1935 - accuracy: 0.9218\n",
      "Epoch 109/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.9302\n",
      "Epoch 110/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9302\n",
      "Epoch 111/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9289\n",
      "Epoch 112/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2063 - accuracy: 0.9182\n",
      "Epoch 113/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.9347\n",
      "Epoch 114/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1954 - accuracy: 0.9218\n",
      "Epoch 115/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2069 - accuracy: 0.9205\n",
      "Epoch 116/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9289\n",
      "Epoch 117/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9295\n",
      "Epoch 118/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9289\n",
      "Epoch 119/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1754 - accuracy: 0.9308\n",
      "Epoch 120/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1799 - accuracy: 0.9347\n",
      "Epoch 121/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1869 - accuracy: 0.9321\n",
      "Epoch 122/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9347\n",
      "Epoch 123/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9344\n",
      "Epoch 124/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1688 - accuracy: 0.9363\n",
      "Epoch 125/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9373\n",
      "Epoch 126/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1896 - accuracy: 0.9256\n",
      "Epoch 127/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9363\n",
      "Epoch 128/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9392\n",
      "Epoch 129/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.9318\n",
      "Epoch 130/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.9328\n",
      "Epoch 131/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9340\n",
      "Epoch 132/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9382\n",
      "Epoch 133/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9344\n",
      "Epoch 134/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1683 - accuracy: 0.9434\n",
      "Epoch 135/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9370\n",
      "Epoch 136/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9276\n",
      "Epoch 137/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.9408\n",
      "Epoch 138/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9318\n",
      "Epoch 139/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.9334\n",
      "Epoch 140/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1695 - accuracy: 0.9370\n",
      "Epoch 141/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9376\n",
      "Epoch 142/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9431\n",
      "Epoch 143/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1710 - accuracy: 0.9363\n",
      "Epoch 144/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.9350\n",
      "Epoch 145/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1659 - accuracy: 0.9347\n",
      "Epoch 146/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.9328\n",
      "Epoch 147/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9282\n",
      "Epoch 148/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9415\n",
      "Epoch 149/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9340\n",
      "Epoch 150/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1793 - accuracy: 0.9292\n",
      "Epoch 151/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1965 - accuracy: 0.9269\n",
      "Epoch 152/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9363\n",
      "Epoch 153/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9366\n",
      "Epoch 154/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9353\n",
      "Epoch 155/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9395\n",
      "Epoch 156/1500\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1582 - accuracy: 0.9405\n",
      "Epoch 157/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9350\n",
      "Epoch 158/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9502\n",
      "Epoch 159/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.9431\n",
      "Epoch 160/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9437\n",
      "Epoch 161/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9402\n",
      "Epoch 162/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9486\n",
      "Epoch 163/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9344\n",
      "Epoch 164/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.9467\n",
      "Epoch 165/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9428\n",
      "Epoch 166/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9431\n",
      "Epoch 167/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9418\n",
      "Epoch 168/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1575 - accuracy: 0.9450\n",
      "Epoch 169/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9496\n",
      "Epoch 170/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9376\n",
      "Epoch 171/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.9360\n",
      "Epoch 172/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9402\n",
      "Epoch 173/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9382\n",
      "Epoch 174/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9454\n",
      "Epoch 175/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9467\n",
      "Epoch 176/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1605 - accuracy: 0.9425\n",
      "Epoch 177/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9473\n",
      "Epoch 178/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9412\n",
      "Epoch 179/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.9428\n",
      "Epoch 180/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9386\n",
      "Epoch 181/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9492\n",
      "Epoch 182/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9447\n",
      "Epoch 183/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1586 - accuracy: 0.9376\n",
      "Epoch 184/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9502\n",
      "Epoch 185/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9444\n",
      "Epoch 186/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.9454\n",
      "Epoch 187/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9483\n",
      "Epoch 188/1500\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1468 - accuracy: 0.9421\n",
      "Epoch 189/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1521 - accuracy: 0.9425\n",
      "Epoch 190/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1561 - accuracy: 0.9437\n",
      "Epoch 191/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1468 - accuracy: 0.9412\n",
      "Epoch 192/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9505\n",
      "Epoch 193/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9483\n",
      "Epoch 194/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9428\n",
      "Epoch 195/1500\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1597 - accuracy: 0.9421\n",
      "Epoch 196/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9486\n",
      "Epoch 197/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9450\n",
      "Epoch 198/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9512\n",
      "Epoch 199/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9476\n",
      "Epoch 200/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9476\n",
      "Epoch 201/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9528\n",
      "Epoch 202/1500\n",
      "97/97 [==============================] - 0s 4ms/step - loss: 0.1444 - accuracy: 0.9463\n",
      "Epoch 203/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9502\n",
      "Epoch 204/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9541\n",
      "Epoch 205/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9483\n",
      "Epoch 206/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9544\n",
      "Epoch 207/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9499\n",
      "Epoch 208/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.9460\n",
      "Epoch 209/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9534\n",
      "Epoch 210/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9463\n",
      "Epoch 211/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9515\n",
      "Epoch 212/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.9473\n",
      "Epoch 213/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9541\n",
      "Epoch 214/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9496\n",
      "Epoch 215/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9525\n",
      "Epoch 216/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9528\n",
      "Epoch 217/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9437\n",
      "Epoch 218/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9454\n",
      "Epoch 219/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9483\n",
      "Epoch 220/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9473\n",
      "Epoch 221/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9544\n",
      "Epoch 222/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9515\n",
      "Epoch 223/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1351 - accuracy: 0.9551\n",
      "Epoch 224/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9534\n",
      "Epoch 225/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9560\n",
      "Epoch 226/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9502\n",
      "Epoch 227/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9586\n",
      "Epoch 228/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9554\n",
      "Epoch 229/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9531\n",
      "Epoch 230/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9528\n",
      "Epoch 231/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9538\n",
      "Epoch 232/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9580\n",
      "Epoch 233/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9531\n",
      "Epoch 234/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9479\n",
      "Epoch 235/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.9534\n",
      "Epoch 236/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1266 - accuracy: 0.9525\n",
      "Epoch 237/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9454\n",
      "Epoch 238/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9564\n",
      "Epoch 239/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9586\n",
      "Epoch 240/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9470\n",
      "Epoch 241/1500\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1266 - accuracy: 0.9525\n",
      "Epoch 242/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9502\n",
      "Epoch 243/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9476\n",
      "Epoch 244/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9589\n",
      "Epoch 245/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9586\n",
      "Epoch 246/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9602\n",
      "Epoch 247/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9560\n",
      "Epoch 248/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9518\n",
      "Epoch 249/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9538\n",
      "Epoch 250/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9586\n",
      "Epoch 251/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9583\n",
      "Epoch 252/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9551\n",
      "Epoch 253/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.9502\n",
      "Epoch 254/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9538\n",
      "Epoch 255/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9567\n",
      "Epoch 256/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9521\n",
      "Epoch 257/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1320 - accuracy: 0.9547\n",
      "Epoch 258/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9625\n",
      "Epoch 259/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9531\n",
      "Epoch 260/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9576\n",
      "Epoch 261/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9557\n",
      "Epoch 262/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9434\n",
      "Epoch 263/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9612\n",
      "Epoch 264/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9564\n",
      "Epoch 265/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.9551\n",
      "Epoch 266/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9560\n",
      "Epoch 267/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9606\n",
      "Epoch 268/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9618\n",
      "Epoch 269/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9560\n",
      "Epoch 270/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9547\n",
      "Epoch 271/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9586\n",
      "Epoch 272/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9580\n",
      "Epoch 273/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9573\n",
      "Epoch 274/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9618\n",
      "Epoch 275/1500\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1130 - accuracy: 0.9596\n",
      "Epoch 276/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9570\n",
      "Epoch 277/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9515\n",
      "Epoch 278/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9618\n",
      "Epoch 279/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9547\n",
      "Epoch 280/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9570\n",
      "Epoch 281/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9576\n",
      "Epoch 282/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9596\n",
      "Epoch 283/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9551\n",
      "Epoch 284/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9570\n",
      "Epoch 285/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9596\n",
      "Epoch 286/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1204 - accuracy: 0.9554\n",
      "Epoch 287/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9602\n",
      "Epoch 288/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.9492\n",
      "Epoch 289/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9518\n",
      "Epoch 290/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9512\n",
      "Epoch 291/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9560\n",
      "Epoch 292/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9509\n",
      "Epoch 293/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9551\n",
      "Epoch 294/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9622\n",
      "Epoch 295/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9544\n",
      "Epoch 296/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9570\n",
      "Epoch 297/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9625\n",
      "Epoch 298/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9547\n",
      "Epoch 299/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9631\n",
      "Epoch 300/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9486\n",
      "Epoch 301/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9509\n",
      "Epoch 302/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9470\n",
      "Epoch 303/1500\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1144 - accuracy: 0.9564\n",
      "Epoch 304/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9583\n",
      "Epoch 305/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9544\n",
      "Epoch 306/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9593\n",
      "Epoch 307/1500\n",
      "97/97 [==============================] - 0s 3ms/step - loss: 0.1087 - accuracy: 0.9609\n",
      "Epoch 308/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9534\n",
      "Epoch 309/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9560\n",
      "Epoch 310/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9602\n",
      "Epoch 311/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9573\n",
      "Epoch 312/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9589\n",
      "Epoch 313/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9638\n",
      "Epoch 314/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9544\n",
      "Epoch 315/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9564\n",
      "Epoch 316/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9644\n",
      "Epoch 317/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9606\n",
      "Epoch 318/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9586\n",
      "Epoch 319/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9599\n",
      "Epoch 320/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9622\n",
      "Epoch 321/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1320 - accuracy: 0.9521\n",
      "Epoch 322/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9479\n",
      "Epoch 323/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9583\n",
      "Epoch 324/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9589\n",
      "Epoch 325/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9612\n",
      "Epoch 326/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9609\n",
      "Epoch 327/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.9615\n",
      "Epoch 328/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9664\n",
      "Epoch 329/1500\n",
      "97/97 [==============================] - ETA: 0s - loss: 0.1060 - accuracy: 0.9615Restoring model weights from the end of the best epoch: 299.\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9615\n",
      "Epoch 329: early stopping\n",
      "6/6 [==============================] - 0s 960us/step - loss: 0.9283 - accuracy: 0.6605\n",
      "6/6 [==============================] - 0s 688us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (19/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "Final Test Results - Loss: 0.9283187389373779, Accuracy: 0.6604938507080078, Precision: 0.6453634085213033, Recall: 0.7201516263145024, F1 Score: 0.6755385820603212\n",
      "Confusion Matrix:\n",
      " [[65  5 24]\n",
      " [ 1  9  0]\n",
      " [25  0 33]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "016A    10\n",
      "033A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "108A     6\n",
      "007A     6\n",
      "021A     5\n",
      "034A     5\n",
      "025C     5\n",
      "023B     5\n",
      "075A     5\n",
      "052A     4\n",
      "104A     4\n",
      "026A     4\n",
      "035A     4\n",
      "009A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "056A     3\n",
      "058A     3\n",
      "113A     3\n",
      "038A     2\n",
      "069A     2\n",
      "093A     2\n",
      "087A     2\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "032A     2\n",
      "018A     2\n",
      "115A     1\n",
      "110A     1\n",
      "019B     1\n",
      "090A     1\n",
      "004A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "076A     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "057A    27\n",
      "000B    19\n",
      "029A    17\n",
      "106A    14\n",
      "028A    13\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "071A    10\n",
      "005A    10\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "037A     6\n",
      "044A     5\n",
      "070A     5\n",
      "105A     4\n",
      "014A     3\n",
      "054A     2\n",
      "025B     2\n",
      "096A     1\n",
      "049A     1\n",
      "048A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    264\n",
      "X    198\n",
      "F    193\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    150\n",
      "M     73\n",
      "F     59\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 019...\n",
      "kitten    [014B, 111A, 040A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 116A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 028A, 022A, 029A, 095A, 005A, 039A, 013...\n",
      "kitten                             [044A, 046A, 049A, 048A]\n",
      "senior                             [057A, 106A, 051B, 054A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '047A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '046A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'110A'}\n",
      "Moved to Test Set:\n",
      "{'110A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '046A'\n",
      " '047A' '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A'\n",
      " '061A' '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A'\n",
      " '097B' '099A' '101A' '102A' '103A' '104A' '108A' '109A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '048A' '049A' '051B' '054A' '057A' '068A'\n",
      " '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A' '110A']\n",
      "Length of X_train_val:\n",
      "717\n",
      "Length of y_train_val:\n",
      "717\n",
      "Length of groups_train_val:\n",
      "717\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    126\n",
      "kitten    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     70\n",
      "senior     52\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "kitten    163\n",
      "senior    126\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "senior     52\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({1: 1107, 0: 1023, 2: 870})\n",
      "Epoch 1/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.8380 - accuracy: 0.6383\n",
      "Epoch 2/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.6444 - accuracy: 0.7370\n",
      "Epoch 3/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.5386 - accuracy: 0.7790\n",
      "Epoch 4/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.5296 - accuracy: 0.7877\n",
      "Epoch 5/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4874 - accuracy: 0.8033\n",
      "Epoch 6/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4617 - accuracy: 0.8210\n",
      "Epoch 7/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4591 - accuracy: 0.8150\n",
      "Epoch 8/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4471 - accuracy: 0.8330\n",
      "Epoch 9/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4229 - accuracy: 0.8313\n",
      "Epoch 10/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4086 - accuracy: 0.8397\n",
      "Epoch 11/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3960 - accuracy: 0.8370\n",
      "Epoch 12/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3962 - accuracy: 0.8413\n",
      "Epoch 13/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3743 - accuracy: 0.8500\n",
      "Epoch 14/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3743 - accuracy: 0.8513\n",
      "Epoch 15/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3798 - accuracy: 0.8487\n",
      "Epoch 16/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3494 - accuracy: 0.8570\n",
      "Epoch 17/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3508 - accuracy: 0.8603\n",
      "Epoch 18/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3447 - accuracy: 0.8643\n",
      "Epoch 19/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8700\n",
      "Epoch 20/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3287 - accuracy: 0.8670\n",
      "Epoch 21/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3125 - accuracy: 0.8753\n",
      "Epoch 22/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3393 - accuracy: 0.8677\n",
      "Epoch 23/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3237 - accuracy: 0.8740\n",
      "Epoch 24/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8813\n",
      "Epoch 25/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2991 - accuracy: 0.8870\n",
      "Epoch 26/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3058 - accuracy: 0.8753\n",
      "Epoch 27/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2837 - accuracy: 0.8907\n",
      "Epoch 28/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.8927\n",
      "Epoch 29/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2944 - accuracy: 0.8773\n",
      "Epoch 30/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2851 - accuracy: 0.8880\n",
      "Epoch 31/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.8803\n",
      "Epoch 32/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.8950\n",
      "Epoch 33/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2801 - accuracy: 0.8947\n",
      "Epoch 34/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2835 - accuracy: 0.8900\n",
      "Epoch 35/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2796 - accuracy: 0.8900\n",
      "Epoch 36/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2667 - accuracy: 0.8990\n",
      "Epoch 37/1500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.2767 - accuracy: 0.8957\n",
      "Epoch 38/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2534 - accuracy: 0.9047\n",
      "Epoch 39/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2518 - accuracy: 0.9007\n",
      "Epoch 40/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2528 - accuracy: 0.9017\n",
      "Epoch 41/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2689 - accuracy: 0.8927\n",
      "Epoch 42/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2616 - accuracy: 0.8943\n",
      "Epoch 43/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2487 - accuracy: 0.8983\n",
      "Epoch 44/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2493 - accuracy: 0.9043\n",
      "Epoch 45/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2502 - accuracy: 0.9057\n",
      "Epoch 46/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2425 - accuracy: 0.9043\n",
      "Epoch 47/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2345 - accuracy: 0.9077\n",
      "Epoch 48/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2402 - accuracy: 0.9087\n",
      "Epoch 49/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2604 - accuracy: 0.9050\n",
      "Epoch 50/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.8997\n",
      "Epoch 51/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9090\n",
      "Epoch 52/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2220 - accuracy: 0.9117\n",
      "Epoch 53/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2253 - accuracy: 0.9117\n",
      "Epoch 54/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2370 - accuracy: 0.9147\n",
      "Epoch 55/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2311 - accuracy: 0.9070\n",
      "Epoch 56/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9140\n",
      "Epoch 57/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.9087\n",
      "Epoch 58/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2318 - accuracy: 0.9050\n",
      "Epoch 59/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2233 - accuracy: 0.9127\n",
      "Epoch 60/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2230 - accuracy: 0.9150\n",
      "Epoch 61/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2057 - accuracy: 0.9243\n",
      "Epoch 62/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 0.9177\n",
      "Epoch 63/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9240\n",
      "Epoch 64/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9213\n",
      "Epoch 65/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2105 - accuracy: 0.9193\n",
      "Epoch 66/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9250\n",
      "Epoch 67/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2150 - accuracy: 0.9190\n",
      "Epoch 68/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2005 - accuracy: 0.9290\n",
      "Epoch 69/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9207\n",
      "Epoch 70/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2075 - accuracy: 0.9210\n",
      "Epoch 71/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.9310\n",
      "Epoch 72/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9300\n",
      "Epoch 73/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9330\n",
      "Epoch 74/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.9290\n",
      "Epoch 75/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9353\n",
      "Epoch 76/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9267\n",
      "Epoch 77/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9257\n",
      "Epoch 78/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9220\n",
      "Epoch 79/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1773 - accuracy: 0.9323\n",
      "Epoch 80/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1729 - accuracy: 0.9357\n",
      "Epoch 81/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 0.9220\n",
      "Epoch 82/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9343\n",
      "Epoch 83/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9300\n",
      "Epoch 84/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9240\n",
      "Epoch 85/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1927 - accuracy: 0.9283\n",
      "Epoch 86/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1972 - accuracy: 0.9253\n",
      "Epoch 87/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9330\n",
      "Epoch 88/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.9250\n",
      "Epoch 89/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1793 - accuracy: 0.9297\n",
      "Epoch 90/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9380\n",
      "Epoch 91/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1947 - accuracy: 0.9257\n",
      "Epoch 92/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9340\n",
      "Epoch 93/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9340\n",
      "Epoch 94/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9410\n",
      "Epoch 95/1500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1721 - accuracy: 0.9337\n",
      "Epoch 96/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.9313\n",
      "Epoch 97/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2009 - accuracy: 0.9287\n",
      "Epoch 98/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.9373\n",
      "Epoch 99/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.9430\n",
      "Epoch 100/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9380\n",
      "Epoch 101/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9303\n",
      "Epoch 102/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1746 - accuracy: 0.9343\n",
      "Epoch 103/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9420\n",
      "Epoch 104/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.9350\n",
      "Epoch 105/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1689 - accuracy: 0.9383\n",
      "Epoch 106/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1631 - accuracy: 0.9450\n",
      "Epoch 107/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9427\n",
      "Epoch 108/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9393\n",
      "Epoch 109/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9363\n",
      "Epoch 110/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9400\n",
      "Epoch 111/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.9350\n",
      "Epoch 112/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.9340\n",
      "Epoch 113/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9427\n",
      "Epoch 114/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1590 - accuracy: 0.9450\n",
      "Epoch 115/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1742 - accuracy: 0.9323\n",
      "Epoch 116/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9360\n",
      "Epoch 117/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9410\n",
      "Epoch 118/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1628 - accuracy: 0.9427\n",
      "Epoch 119/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9380\n",
      "Epoch 120/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9413\n",
      "Epoch 121/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9410\n",
      "Epoch 122/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.9513\n",
      "Epoch 123/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9460\n",
      "Epoch 124/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9403\n",
      "Epoch 125/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9463\n",
      "Epoch 126/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9400\n",
      "Epoch 127/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9463\n",
      "Epoch 128/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9453\n",
      "Epoch 129/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9507\n",
      "Epoch 130/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1404 - accuracy: 0.9497\n",
      "Epoch 131/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9440\n",
      "Epoch 132/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9410\n",
      "Epoch 133/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9500\n",
      "Epoch 134/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9420\n",
      "Epoch 135/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9510\n",
      "Epoch 136/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9530\n",
      "Epoch 137/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9477\n",
      "Epoch 138/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9530\n",
      "Epoch 139/1500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1488 - accuracy: 0.9450\n",
      "Epoch 140/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9513\n",
      "Epoch 141/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9470\n",
      "Epoch 142/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9490\n",
      "Epoch 143/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9463\n",
      "Epoch 144/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9547\n",
      "Epoch 145/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1414 - accuracy: 0.9497\n",
      "Epoch 146/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9437\n",
      "Epoch 147/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9443\n",
      "Epoch 148/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9497\n",
      "Epoch 149/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9553\n",
      "Epoch 150/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9463\n",
      "Epoch 151/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9470\n",
      "Epoch 152/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9470\n",
      "Epoch 153/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9593\n",
      "Epoch 154/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1330 - accuracy: 0.9533\n",
      "Epoch 155/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9423\n",
      "Epoch 156/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9483\n",
      "Epoch 157/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9513\n",
      "Epoch 158/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9503\n",
      "Epoch 159/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9413\n",
      "Epoch 160/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9530\n",
      "Epoch 161/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9453\n",
      "Epoch 162/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.9440\n",
      "Epoch 163/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9490\n",
      "Epoch 164/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9520\n",
      "Epoch 165/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9533\n",
      "Epoch 166/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9550\n",
      "Epoch 167/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9493\n",
      "Epoch 168/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9513\n",
      "Epoch 169/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9510\n",
      "Epoch 170/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9463\n",
      "Epoch 171/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9537\n",
      "Epoch 172/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9470\n",
      "Epoch 173/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9603\n",
      "Epoch 174/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9570\n",
      "Epoch 175/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9593\n",
      "Epoch 176/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9453\n",
      "Epoch 177/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9547\n",
      "Epoch 178/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9547\n",
      "Epoch 179/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9517\n",
      "Epoch 180/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9540\n",
      "Epoch 181/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9617\n",
      "Epoch 182/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9520\n",
      "Epoch 183/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9510\n",
      "Epoch 184/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9557\n",
      "Epoch 185/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9580\n",
      "Epoch 186/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9513\n",
      "Epoch 187/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9560\n",
      "Epoch 188/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9510\n",
      "Epoch 189/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9633\n",
      "Epoch 190/1500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1162 - accuracy: 0.9600\n",
      "Epoch 191/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9527\n",
      "Epoch 192/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9567\n",
      "Epoch 193/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9560\n",
      "Epoch 194/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9613\n",
      "Epoch 195/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9583\n",
      "Epoch 196/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9537\n",
      "Epoch 197/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9563\n",
      "Epoch 198/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9633\n",
      "Epoch 199/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9530\n",
      "Epoch 200/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9580\n",
      "Epoch 201/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9543\n",
      "Epoch 202/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9567\n",
      "Epoch 203/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9603\n",
      "Epoch 204/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9547\n",
      "Epoch 205/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9520\n",
      "Epoch 206/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9533\n",
      "Epoch 207/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1204 - accuracy: 0.9577\n",
      "Epoch 208/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9527\n",
      "Epoch 209/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9540\n",
      "Epoch 210/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.9500\n",
      "Epoch 211/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9590\n",
      "Epoch 212/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9570\n",
      "Epoch 213/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9660\n",
      "Epoch 214/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9573\n",
      "Epoch 215/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9460\n",
      "Epoch 216/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9537\n",
      "Epoch 217/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9647\n",
      "Epoch 218/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9583\n",
      "Epoch 219/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9553\n",
      "Epoch 220/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9630\n",
      "Epoch 221/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9597\n",
      "Epoch 222/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9573\n",
      "Epoch 223/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.9593\n",
      "Epoch 224/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9593\n",
      "Epoch 225/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9580\n",
      "Epoch 226/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9573\n",
      "Epoch 227/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9517\n",
      "Epoch 228/1500\n",
      "49/94 [==============>...............] - ETA: 0s - loss: 0.1309 - accuracy: 0.9573Restoring model weights from the end of the best epoch: 198.\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9563\n",
      "Epoch 228: early stopping\n",
      "7/7 [==============================] - 0s 922us/step - loss: 0.9841 - accuracy: 0.6909\n",
      "7/7 [==============================] - 0s 628us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.69 (20/29)\n",
      "Before appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "After appending - Cat IDs: 382, Predictions: 382, Actuals: 382, Gender: 382\n",
      "Final Test Results - Loss: 0.9841047525405884, Accuracy: 0.6909090876579285, Precision: 0.6604030501089325, Recall: 0.6753205128205129, F1 Score: 0.664327485380117\n",
      "Confusion Matrix:\n",
      " [[118   2  40]\n",
      " [  2   6   0]\n",
      " [ 24   0  28]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "106A    14\n",
      "042A    14\n",
      "028A    13\n",
      "111A    13\n",
      "039A    12\n",
      "068A    11\n",
      "063A    11\n",
      "036A    11\n",
      "025A    11\n",
      "005A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "051B     9\n",
      "022A     9\n",
      "065A     9\n",
      "045A     9\n",
      "015A     9\n",
      "010A     8\n",
      "095A     8\n",
      "094A     8\n",
      "013B     8\n",
      "031A     7\n",
      "117A     7\n",
      "053A     6\n",
      "108A     6\n",
      "008A     6\n",
      "109A     6\n",
      "007A     6\n",
      "037A     6\n",
      "070A     5\n",
      "021A     5\n",
      "044A     5\n",
      "023B     5\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "062A     4\n",
      "009A     4\n",
      "104A     4\n",
      "058A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "056A     3\n",
      "113A     3\n",
      "025B     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "054A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "032A     2\n",
      "069A     2\n",
      "049A     1\n",
      "088A     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "004A     1\n",
      "092A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "073A     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "059A    14\n",
      "097B    14\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "072A     9\n",
      "033A     9\n",
      "027A     7\n",
      "099A     7\n",
      "050A     7\n",
      "023A     6\n",
      "034A     5\n",
      "025C     5\n",
      "075A     5\n",
      "052A     4\n",
      "003A     4\n",
      "012A     3\n",
      "006A     3\n",
      "018A     2\n",
      "026C     1\n",
      "019B     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    250\n",
      "F    202\n",
      "M    180\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    157\n",
      "X     98\n",
      "F     50\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 103A, 071A, 028A, 062A, 101...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 043...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 097B, 019A, 074A, 067A, 020A, 002...\n",
      "kitten                                   [047A, 050A, 115A]\n",
      "senior                       [097A, 059A, 116A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 52, 'kitten': 13, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 22, 'kitten': 3, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "632\n",
      "Length of y_train_val:\n",
      "632\n",
      "Length of groups_train_val:\n",
      "632\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 909, 1: 891, 2: 819})\n",
      "Epoch 1/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.9080 - accuracy: 0.6140\n",
      "Epoch 2/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.7161 - accuracy: 0.6930\n",
      "Epoch 3/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.6562 - accuracy: 0.7289\n",
      "Epoch 4/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.6196 - accuracy: 0.7423\n",
      "Epoch 5/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.6029 - accuracy: 0.7438\n",
      "Epoch 6/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.5767 - accuracy: 0.7587\n",
      "Epoch 7/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.5635 - accuracy: 0.7591\n",
      "Epoch 8/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.5599 - accuracy: 0.7698\n",
      "Epoch 9/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.5245 - accuracy: 0.7778\n",
      "Epoch 10/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.5233 - accuracy: 0.7789\n",
      "Epoch 11/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.5073 - accuracy: 0.7896\n",
      "Epoch 12/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4871 - accuracy: 0.7927\n",
      "Epoch 13/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4754 - accuracy: 0.7953\n",
      "Epoch 14/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4687 - accuracy: 0.8099\n",
      "Epoch 15/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4571 - accuracy: 0.8064\n",
      "Epoch 16/1500\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.4398 - accuracy: 0.8202\n",
      "Epoch 17/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4530 - accuracy: 0.8102\n",
      "Epoch 18/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4406 - accuracy: 0.8121\n",
      "Epoch 19/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4295 - accuracy: 0.8205\n",
      "Epoch 20/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4364 - accuracy: 0.8133\n",
      "Epoch 21/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4355 - accuracy: 0.8217\n",
      "Epoch 22/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4185 - accuracy: 0.8282\n",
      "Epoch 23/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4065 - accuracy: 0.8267\n",
      "Epoch 24/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.4021 - accuracy: 0.8381\n",
      "Epoch 25/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3998 - accuracy: 0.8404\n",
      "Epoch 26/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3960 - accuracy: 0.8396\n",
      "Epoch 27/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3784 - accuracy: 0.8480\n",
      "Epoch 28/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3731 - accuracy: 0.8400\n",
      "Epoch 29/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3883 - accuracy: 0.8431\n",
      "Epoch 30/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3779 - accuracy: 0.8469\n",
      "Epoch 31/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3626 - accuracy: 0.8530\n",
      "Epoch 32/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3654 - accuracy: 0.8541\n",
      "Epoch 33/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3543 - accuracy: 0.8538\n",
      "Epoch 34/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3708 - accuracy: 0.8503\n",
      "Epoch 35/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3574 - accuracy: 0.8564\n",
      "Epoch 36/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3362 - accuracy: 0.8633\n",
      "Epoch 37/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3669 - accuracy: 0.8549\n",
      "Epoch 38/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3527 - accuracy: 0.8591\n",
      "Epoch 39/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3555 - accuracy: 0.8557\n",
      "Epoch 40/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3502 - accuracy: 0.8561\n",
      "Epoch 41/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3235 - accuracy: 0.8645\n",
      "Epoch 42/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3196 - accuracy: 0.8759\n",
      "Epoch 43/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3363 - accuracy: 0.8637\n",
      "Epoch 44/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3129 - accuracy: 0.8740\n",
      "Epoch 45/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8595\n",
      "Epoch 46/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3253 - accuracy: 0.8748\n",
      "Epoch 47/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3143 - accuracy: 0.8824\n",
      "Epoch 48/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8664\n",
      "Epoch 49/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2969 - accuracy: 0.8805\n",
      "Epoch 50/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3222 - accuracy: 0.8667\n",
      "Epoch 51/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2970 - accuracy: 0.8813\n",
      "Epoch 52/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2884 - accuracy: 0.8843\n",
      "Epoch 53/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2953 - accuracy: 0.8885\n",
      "Epoch 54/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2960 - accuracy: 0.8862\n",
      "Epoch 55/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2938 - accuracy: 0.8843\n",
      "Epoch 56/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2926 - accuracy: 0.8801\n",
      "Epoch 57/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2786 - accuracy: 0.8862\n",
      "Epoch 58/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2809 - accuracy: 0.9030\n",
      "Epoch 59/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.3029 - accuracy: 0.8820\n",
      "Epoch 60/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2747 - accuracy: 0.8923\n",
      "Epoch 61/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2932 - accuracy: 0.8923\n",
      "Epoch 62/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2772 - accuracy: 0.8904\n",
      "Epoch 63/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2845 - accuracy: 0.8877\n",
      "Epoch 64/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2859 - accuracy: 0.8855\n",
      "Epoch 65/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2830 - accuracy: 0.8889\n",
      "Epoch 66/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2769 - accuracy: 0.8893\n",
      "Epoch 67/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2733 - accuracy: 0.8969\n",
      "Epoch 68/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2701 - accuracy: 0.8862\n",
      "Epoch 69/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2527 - accuracy: 0.9068\n",
      "Epoch 70/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2603 - accuracy: 0.8981\n",
      "Epoch 71/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2662 - accuracy: 0.9030\n",
      "Epoch 72/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2614 - accuracy: 0.8935\n",
      "Epoch 73/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2682 - accuracy: 0.8881\n",
      "Epoch 74/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2684 - accuracy: 0.8988\n",
      "Epoch 75/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.9034\n",
      "Epoch 76/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2664 - accuracy: 0.8981\n",
      "Epoch 77/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2576 - accuracy: 0.9023\n",
      "Epoch 78/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2406 - accuracy: 0.9038\n",
      "Epoch 79/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.9057\n",
      "Epoch 80/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2459 - accuracy: 0.9038\n",
      "Epoch 81/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2568 - accuracy: 0.9038\n",
      "Epoch 82/1500\n",
      "82/82 [==============================] - 0s 3ms/step - loss: 0.2448 - accuracy: 0.9061\n",
      "Epoch 83/1500\n",
      "82/82 [==============================] - 0s 2ms/step - loss: 0.2307 - accuracy: 0.9145\n",
      "Epoch 84/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.9087\n",
      "Epoch 85/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2480 - accuracy: 0.9087\n",
      "Epoch 86/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2441 - accuracy: 0.9080\n",
      "Epoch 87/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2374 - accuracy: 0.9072\n",
      "Epoch 88/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2312 - accuracy: 0.9030\n",
      "Epoch 89/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2553 - accuracy: 0.9042\n",
      "Epoch 90/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9206\n",
      "Epoch 91/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2274 - accuracy: 0.9175\n",
      "Epoch 92/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2657 - accuracy: 0.8908\n",
      "Epoch 93/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2100 - accuracy: 0.9263\n",
      "Epoch 94/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2208 - accuracy: 0.9168\n",
      "Epoch 95/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9103\n",
      "Epoch 96/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.9107\n",
      "Epoch 97/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2287 - accuracy: 0.9133\n",
      "Epoch 98/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2281 - accuracy: 0.9061\n",
      "Epoch 99/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2235 - accuracy: 0.9126\n",
      "Epoch 100/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2328 - accuracy: 0.9110\n",
      "Epoch 101/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2256 - accuracy: 0.9160\n",
      "Epoch 102/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2284 - accuracy: 0.9175\n",
      "Epoch 103/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2262 - accuracy: 0.9187\n",
      "Epoch 104/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.9099\n",
      "Epoch 105/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2315 - accuracy: 0.9099\n",
      "Epoch 106/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.9187\n",
      "Epoch 107/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.9103\n",
      "Epoch 108/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2123 - accuracy: 0.9179\n",
      "Epoch 109/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9206\n",
      "Epoch 110/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9233\n",
      "Epoch 111/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9255\n",
      "Epoch 112/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2157 - accuracy: 0.9183\n",
      "Epoch 113/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2240 - accuracy: 0.9114\n",
      "Epoch 114/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2144 - accuracy: 0.9183\n",
      "Epoch 115/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9198\n",
      "Epoch 116/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.9133\n",
      "Epoch 117/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2149 - accuracy: 0.9160\n",
      "Epoch 118/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.9236\n",
      "Epoch 119/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.9126\n",
      "Epoch 120/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1965 - accuracy: 0.9233\n",
      "Epoch 121/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2010 - accuracy: 0.9233\n",
      "Epoch 122/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9255\n",
      "Epoch 123/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2034 - accuracy: 0.9206\n",
      "Epoch 124/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9236\n",
      "Epoch 125/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9210\n",
      "Epoch 126/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2045 - accuracy: 0.9217\n",
      "Epoch 127/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1971 - accuracy: 0.9233\n",
      "Epoch 128/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.9183\n",
      "Epoch 129/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9339\n",
      "Epoch 130/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9294\n",
      "Epoch 131/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9244\n",
      "Epoch 132/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9309\n",
      "Epoch 133/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1970 - accuracy: 0.9217\n",
      "Epoch 134/1500\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.1816 - accuracy: 0.9317\n",
      "Epoch 135/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.9259\n",
      "Epoch 136/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2059 - accuracy: 0.9179\n",
      "Epoch 137/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9252\n",
      "Epoch 138/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.9301\n",
      "Epoch 139/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9317\n",
      "Epoch 140/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9305\n",
      "Epoch 141/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9332\n",
      "Epoch 142/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.9282\n",
      "Epoch 143/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.9282\n",
      "Epoch 144/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1936 - accuracy: 0.9320\n",
      "Epoch 145/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9179\n",
      "Epoch 146/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9294\n",
      "Epoch 147/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.9351\n",
      "Epoch 148/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1933 - accuracy: 0.9240\n",
      "Epoch 149/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1719 - accuracy: 0.9347\n",
      "Epoch 150/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9355\n",
      "Epoch 151/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1722 - accuracy: 0.9347\n",
      "Epoch 152/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9278\n",
      "Epoch 153/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1930 - accuracy: 0.9255\n",
      "Epoch 154/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9202\n",
      "Epoch 155/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9294\n",
      "Epoch 156/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.9324\n",
      "Epoch 157/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1910 - accuracy: 0.9229\n",
      "Epoch 158/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.9294\n",
      "Epoch 159/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9309\n",
      "Epoch 160/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9347\n",
      "Epoch 161/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9355\n",
      "Epoch 162/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9290\n",
      "Epoch 163/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9305\n",
      "Epoch 164/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9286\n",
      "Epoch 165/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9454\n",
      "Epoch 166/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9339\n",
      "Epoch 167/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.9309\n",
      "Epoch 168/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9294\n",
      "Epoch 169/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.9374\n",
      "Epoch 170/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1683 - accuracy: 0.9366\n",
      "Epoch 171/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.9309\n",
      "Epoch 172/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9381\n",
      "Epoch 173/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9401\n",
      "Epoch 174/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9446\n",
      "Epoch 175/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.9401\n",
      "Epoch 176/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9255\n",
      "Epoch 177/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9381\n",
      "Epoch 178/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.9458\n",
      "Epoch 179/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9454\n",
      "Epoch 180/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9278\n",
      "Epoch 181/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1677 - accuracy: 0.9370\n",
      "Epoch 182/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1683 - accuracy: 0.9378\n",
      "Epoch 183/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9420\n",
      "Epoch 184/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1548 - accuracy: 0.9404\n",
      "Epoch 185/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9385\n",
      "Epoch 186/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9439\n",
      "Epoch 187/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9332\n",
      "Epoch 188/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1773 - accuracy: 0.9301\n",
      "Epoch 189/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9427\n",
      "Epoch 190/1500\n",
      "82/82 [==============================] - 0s 4ms/step - loss: 0.1607 - accuracy: 0.9423\n",
      "Epoch 191/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9385\n",
      "Epoch 192/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9439\n",
      "Epoch 193/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1897 - accuracy: 0.9255\n",
      "Epoch 194/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.9393\n",
      "Epoch 195/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9488\n",
      "Epoch 196/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9469\n",
      "Epoch 197/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9420\n",
      "Epoch 198/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9488\n",
      "Epoch 199/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1607 - accuracy: 0.9385\n",
      "Epoch 200/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1700 - accuracy: 0.9362\n",
      "Epoch 201/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9431\n",
      "Epoch 202/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9492\n",
      "Epoch 203/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9603\n",
      "Epoch 204/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1605 - accuracy: 0.9404\n",
      "Epoch 205/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1584 - accuracy: 0.9401\n",
      "Epoch 206/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9389\n",
      "Epoch 207/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.9462\n",
      "Epoch 208/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1700 - accuracy: 0.9378\n",
      "Epoch 209/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9370\n",
      "Epoch 210/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9450\n",
      "Epoch 211/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1648 - accuracy: 0.9347\n",
      "Epoch 212/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9454\n",
      "Epoch 213/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9378\n",
      "Epoch 214/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1535 - accuracy: 0.9481\n",
      "Epoch 215/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9435\n",
      "Epoch 216/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.9473\n",
      "Epoch 217/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9462\n",
      "Epoch 218/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9446\n",
      "Epoch 219/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1605 - accuracy: 0.9362\n",
      "Epoch 220/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9450\n",
      "Epoch 221/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9469\n",
      "Epoch 222/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.9511\n",
      "Epoch 223/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1409 - accuracy: 0.9473\n",
      "Epoch 224/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9420\n",
      "Epoch 225/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1547 - accuracy: 0.9381\n",
      "Epoch 226/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9458\n",
      "Epoch 227/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1551 - accuracy: 0.9412\n",
      "Epoch 228/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9477\n",
      "Epoch 229/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9515\n",
      "Epoch 230/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.9435\n",
      "Epoch 231/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9519\n",
      "Epoch 232/1500\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9454\n",
      "Epoch 233/1500\n",
      "47/82 [================>.............] - ETA: 0s - loss: 0.1327 - accuracy: 0.9461Restoring model weights from the end of the best epoch: 203.\n",
      "82/82 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9523\n",
      "Epoch 233: early stopping\n",
      "10/10 [==============================] - 0s 806us/step - loss: 0.7165 - accuracy: 0.7607\n",
      "10/10 [==============================] - 0s 622us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (24/30)\n",
      "Before appending - Cat IDs: 382, Predictions: 382, Actuals: 382, Gender: 382\n",
      "After appending - Cat IDs: 687, Predictions: 687, Actuals: 687, Gender: 687\n",
      "Final Test Results - Loss: 0.7164954543113708, Accuracy: 0.7606557607650757, Precision: 0.7311481621826449, Recall: 0.7019195065924038, F1 Score: 0.7049233796907316\n",
      "Confusion Matrix:\n",
      " [[172   4  38]\n",
      " [ 14  22   0]\n",
      " [ 17   0  38]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "071A    10\n",
      "016A    10\n",
      "072A     9\n",
      "022A     9\n",
      "045A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "094A     8\n",
      "095A     8\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "037A     6\n",
      "008A     6\n",
      "023A     6\n",
      "053A     6\n",
      "025C     5\n",
      "075A     5\n",
      "044A     5\n",
      "034A     5\n",
      "070A     5\n",
      "009A     4\n",
      "052A     4\n",
      "105A     4\n",
      "104A     4\n",
      "003A     4\n",
      "060A     3\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "025B     2\n",
      "073A     1\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "048A     1\n",
      "019B     1\n",
      "088A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "065A     9\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "021A     5\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "113A     3\n",
      "064A     3\n",
      "087A     2\n",
      "038A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "043A     1\n",
      "041A     1\n",
      "066A     1\n",
      "004A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    314\n",
      "X    271\n",
      "F    164\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    88\n",
      "X    77\n",
      "M    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 097B, 028...\n",
      "kitten    [044A, 046A, 047A, 050A, 049A, 045A, 048A, 115...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 062A, 101A, 065A, 063A, 038A, 007A, 087...\n",
      "kitten           [014B, 111A, 040A, 042A, 109A, 043A, 041A]\n",
      "senior                             [113A, 108A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 9, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 7, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'092A'}\n",
      "Moved to Test Set:\n",
      "{'092A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A'\n",
      " '029A' '032A' '033A' '034A' '036A' '037A' '039A' '044A' '045A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '073A' '074A' '075A' '076A' '088A' '090A' '091A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A' '038A'\n",
      " '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '087A' '092A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "787\n",
      "Length of y_train_val:\n",
      "787\n",
      "Length of groups_train_val:\n",
      "787\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "kitten     55\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     506\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     82\n",
      "kitten    55\n",
      "senior    13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1236, 2: 1109, 1: 812})\n",
      "Epoch 1/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.8755 - accuracy: 0.6256\n",
      "Epoch 2/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.7476 - accuracy: 0.6899\n",
      "Epoch 3/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.6941 - accuracy: 0.7130\n",
      "Epoch 4/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.6652 - accuracy: 0.7270\n",
      "Epoch 5/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.6309 - accuracy: 0.7339\n",
      "Epoch 6/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.5824 - accuracy: 0.7602\n",
      "Epoch 7/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.5729 - accuracy: 0.7583\n",
      "Epoch 8/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.5452 - accuracy: 0.7729\n",
      "Epoch 9/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.5068 - accuracy: 0.7840\n",
      "Epoch 10/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.5097 - accuracy: 0.7868\n",
      "Epoch 11/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.4867 - accuracy: 0.7992\n",
      "Epoch 12/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.5198 - accuracy: 0.7833\n",
      "Epoch 13/1500\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.4900 - accuracy: 0.8008\n",
      "Epoch 14/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.4563 - accuracy: 0.8103\n",
      "Epoch 15/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.4490 - accuracy: 0.8163\n",
      "Epoch 16/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.4604 - accuracy: 0.8087\n",
      "Epoch 17/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.4385 - accuracy: 0.8194\n",
      "Epoch 18/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.4231 - accuracy: 0.8213\n",
      "Epoch 19/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.4200 - accuracy: 0.8207\n",
      "Epoch 20/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.4267 - accuracy: 0.8255\n",
      "Epoch 21/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.4153 - accuracy: 0.8236\n",
      "Epoch 22/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3947 - accuracy: 0.8423\n",
      "Epoch 23/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3987 - accuracy: 0.8347\n",
      "Epoch 24/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3914 - accuracy: 0.8397\n",
      "Epoch 25/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3985 - accuracy: 0.8416\n",
      "Epoch 26/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3861 - accuracy: 0.8429\n",
      "Epoch 27/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3643 - accuracy: 0.8521\n",
      "Epoch 28/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3653 - accuracy: 0.8492\n",
      "Epoch 29/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3578 - accuracy: 0.8613\n",
      "Epoch 30/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3584 - accuracy: 0.8505\n",
      "Epoch 31/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3533 - accuracy: 0.8559\n",
      "Epoch 32/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8628\n",
      "Epoch 33/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3451 - accuracy: 0.8628\n",
      "Epoch 34/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3430 - accuracy: 0.8575\n",
      "Epoch 35/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8635\n",
      "Epoch 36/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3290 - accuracy: 0.8679\n",
      "Epoch 37/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3359 - accuracy: 0.8657\n",
      "Epoch 38/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3289 - accuracy: 0.8685\n",
      "Epoch 39/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3160 - accuracy: 0.8736\n",
      "Epoch 40/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3153 - accuracy: 0.8714\n",
      "Epoch 41/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3114 - accuracy: 0.8736\n",
      "Epoch 42/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3205 - accuracy: 0.8746\n",
      "Epoch 43/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3159 - accuracy: 0.8739\n",
      "Epoch 44/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3037 - accuracy: 0.8765\n",
      "Epoch 45/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2991 - accuracy: 0.8790\n",
      "Epoch 46/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2977 - accuracy: 0.8872\n",
      "Epoch 47/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3012 - accuracy: 0.8831\n",
      "Epoch 48/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2969 - accuracy: 0.8901\n",
      "Epoch 49/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8752\n",
      "Epoch 50/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2970 - accuracy: 0.8822\n",
      "Epoch 51/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2985 - accuracy: 0.8850\n",
      "Epoch 52/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2841 - accuracy: 0.8838\n",
      "Epoch 53/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2754 - accuracy: 0.8929\n",
      "Epoch 54/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.8898\n",
      "Epoch 55/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2955 - accuracy: 0.8831\n",
      "Epoch 56/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2893 - accuracy: 0.8872\n",
      "Epoch 57/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2706 - accuracy: 0.8923\n",
      "Epoch 58/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2682 - accuracy: 0.8933\n",
      "Epoch 59/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2703 - accuracy: 0.8920\n",
      "Epoch 60/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2716 - accuracy: 0.8933\n",
      "Epoch 61/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2867 - accuracy: 0.8825\n",
      "Epoch 62/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2706 - accuracy: 0.8942\n",
      "Epoch 63/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2662 - accuracy: 0.8901\n",
      "Epoch 64/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2657 - accuracy: 0.8958\n",
      "Epoch 65/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2635 - accuracy: 0.8990\n",
      "Epoch 66/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2664 - accuracy: 0.8967\n",
      "Epoch 67/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2654 - accuracy: 0.9012\n",
      "Epoch 68/1500\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.2684 - accuracy: 0.8958\n",
      "Epoch 69/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2478 - accuracy: 0.9072\n",
      "Epoch 70/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2607 - accuracy: 0.8996\n",
      "Epoch 71/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.9037\n",
      "Epoch 72/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2515 - accuracy: 0.9009\n",
      "Epoch 73/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2446 - accuracy: 0.9056\n",
      "Epoch 74/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2483 - accuracy: 0.8967\n",
      "Epoch 75/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2567 - accuracy: 0.9005\n",
      "Epoch 76/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2317 - accuracy: 0.9126\n",
      "Epoch 77/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2282 - accuracy: 0.9142\n",
      "Epoch 78/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2418 - accuracy: 0.9078\n",
      "Epoch 79/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2345 - accuracy: 0.9034\n",
      "Epoch 80/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.9072\n",
      "Epoch 81/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2337 - accuracy: 0.9135\n",
      "Epoch 82/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.9075\n",
      "Epoch 83/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.9173\n",
      "Epoch 84/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2414 - accuracy: 0.9047\n",
      "Epoch 85/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2261 - accuracy: 0.9154\n",
      "Epoch 86/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2317 - accuracy: 0.9050\n",
      "Epoch 87/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9072\n",
      "Epoch 88/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2096 - accuracy: 0.9233\n",
      "Epoch 89/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2348 - accuracy: 0.9104\n",
      "Epoch 90/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2194 - accuracy: 0.9167\n",
      "Epoch 91/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.9116\n",
      "Epoch 92/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2120 - accuracy: 0.9138\n",
      "Epoch 93/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2244 - accuracy: 0.9148\n",
      "Epoch 94/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9148\n",
      "Epoch 95/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9180\n",
      "Epoch 96/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2091 - accuracy: 0.9176\n",
      "Epoch 97/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2180 - accuracy: 0.9135\n",
      "Epoch 98/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1991 - accuracy: 0.9268\n",
      "Epoch 99/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9243\n",
      "Epoch 100/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2083 - accuracy: 0.9202\n",
      "Epoch 101/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2131 - accuracy: 0.9211\n",
      "Epoch 102/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2171 - accuracy: 0.9199\n",
      "Epoch 103/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2234 - accuracy: 0.9142\n",
      "Epoch 104/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9176\n",
      "Epoch 105/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9297\n",
      "Epoch 106/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2178 - accuracy: 0.9161\n",
      "Epoch 107/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9237\n",
      "Epoch 108/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9224\n",
      "Epoch 109/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9237\n",
      "Epoch 110/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9240\n",
      "Epoch 111/1500\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.2150 - accuracy: 0.9189\n",
      "Epoch 112/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9161\n",
      "Epoch 113/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2026 - accuracy: 0.9230\n",
      "Epoch 114/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2100 - accuracy: 0.9214\n",
      "Epoch 115/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9221\n",
      "Epoch 116/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1971 - accuracy: 0.9259\n",
      "Epoch 117/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9322\n",
      "Epoch 118/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2062 - accuracy: 0.9195\n",
      "Epoch 119/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1932 - accuracy: 0.9252\n",
      "Epoch 120/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1947 - accuracy: 0.9243\n",
      "Epoch 121/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9271\n",
      "Epoch 122/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.9316\n",
      "Epoch 123/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.9300\n",
      "Epoch 124/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1921 - accuracy: 0.9240\n",
      "Epoch 125/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9300\n",
      "Epoch 126/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1955 - accuracy: 0.9183\n",
      "Epoch 127/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9208\n",
      "Epoch 128/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.9335\n",
      "Epoch 129/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2085 - accuracy: 0.9148\n",
      "Epoch 130/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.9379\n",
      "Epoch 131/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1940 - accuracy: 0.9265\n",
      "Epoch 132/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1909 - accuracy: 0.9268\n",
      "Epoch 133/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9224\n",
      "Epoch 134/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1917 - accuracy: 0.9259\n",
      "Epoch 135/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1826 - accuracy: 0.9319\n",
      "Epoch 136/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.9287\n",
      "Epoch 137/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9347\n",
      "Epoch 138/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9382\n",
      "Epoch 139/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9357\n",
      "Epoch 140/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9309\n",
      "Epoch 141/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9221\n",
      "Epoch 142/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9370\n",
      "Epoch 143/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9306\n",
      "Epoch 144/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9275\n",
      "Epoch 145/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9351\n",
      "Epoch 146/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.9322\n",
      "Epoch 147/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.9328\n",
      "Epoch 148/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9351\n",
      "Epoch 149/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9389\n",
      "Epoch 150/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9379\n",
      "Epoch 151/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.9385\n",
      "Epoch 152/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1685 - accuracy: 0.9404\n",
      "Epoch 153/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9344\n",
      "Epoch 154/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9392\n",
      "Epoch 155/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9376\n",
      "Epoch 156/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9389\n",
      "Epoch 157/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.9319\n",
      "Epoch 158/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.9360\n",
      "Epoch 159/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.9294\n",
      "Epoch 160/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9313\n",
      "Epoch 161/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9373\n",
      "Epoch 162/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9344\n",
      "Epoch 163/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9436\n",
      "Epoch 164/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1605 - accuracy: 0.9379\n",
      "Epoch 165/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9354\n",
      "Epoch 166/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.9335\n",
      "Epoch 167/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9414\n",
      "Epoch 168/1500\n",
      "99/99 [==============================] - 0s 3ms/step - loss: 0.1626 - accuracy: 0.9436\n",
      "Epoch 169/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9319\n",
      "Epoch 170/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.9477\n",
      "Epoch 171/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9411\n",
      "Epoch 172/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9392\n",
      "Epoch 173/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.9357\n",
      "Epoch 174/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9328\n",
      "Epoch 175/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9313\n",
      "Epoch 176/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9366\n",
      "Epoch 177/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9376\n",
      "Epoch 178/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.9338\n",
      "Epoch 179/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9471\n",
      "Epoch 180/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.9420\n",
      "Epoch 181/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9325\n",
      "Epoch 182/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9433\n",
      "Epoch 183/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9401\n",
      "Epoch 184/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1651 - accuracy: 0.9363\n",
      "Epoch 185/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9414\n",
      "Epoch 186/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.9401\n",
      "Epoch 187/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9436\n",
      "Epoch 188/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9401\n",
      "Epoch 189/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.9436\n",
      "Epoch 190/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9411\n",
      "Epoch 191/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9420\n",
      "Epoch 192/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9493\n",
      "Epoch 193/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1618 - accuracy: 0.9351\n",
      "Epoch 194/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1557 - accuracy: 0.9376\n",
      "Epoch 195/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9316\n",
      "Epoch 196/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9376\n",
      "Epoch 197/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9455\n",
      "Epoch 198/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9417\n",
      "Epoch 199/1500\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9544\n",
      "Epoch 200/1500\n",
      "93/99 [===========================>..] - ETA: 0s - loss: 0.1576 - accuracy: 0.9368Restoring model weights from the end of the best epoch: 170.\n",
      "99/99 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9382\n",
      "Epoch 200: early stopping\n",
      "5/5 [==============================] - 0s 979us/step - loss: 0.5769 - accuracy: 0.7600\n",
      "5/5 [==============================] - 0s 818us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 687, Predictions: 687, Actuals: 687, Gender: 687\n",
      "After appending - Cat IDs: 837, Predictions: 837, Actuals: 837, Gender: 837\n",
      "Final Test Results - Loss: 0.5768625736236572, Accuracy: 0.7599999904632568, Precision: 0.6378561253561253, Recall: 0.6435101483881972, F1 Score: 0.6381156556013354\n",
      "Confusion Matrix:\n",
      " [[63  5 14]\n",
      " [ 8 47  0]\n",
      " [ 9  0  4]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6707262756831264\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8014453798532486\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7180146723985672\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6686926865422514\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6852254485289041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[0]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79e793-c694-4bc7-8cc6-05e124ddf71f",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2c3c239d-6215-4589-a583-b37a18ca22cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 837, Predictions: 837, Actuals: 837, Gender: 837\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "99978976-e4a9-4c04-a6b2-6b14a3111277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "189b812b-d37f-462f-a99f-6de8e4a7899f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (83/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "907d957f-7340-4a76-b3fa-7cf166f43049",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7016fa20-094f-409a-9a2f-b6d6793aed03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, kitten, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, adult, adult, kitten, kitten, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, kitten,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "76    070A              [adult, adult, senior, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, kitten, adult, adult, senior, adult, a...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "64    058A                           [senior, senior, senior]        senior           senior                   True\n",
       "63    057A  [senior, adult, adult, senior, senior, adult, ...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [adult, senior, senior, senior, senior, senior...        senior           senior                   True\n",
       "59    053A        [adult, adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [kitten, adult, adult, kitten, kitten, kitten,...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, senior, adult, senior, a...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "98    103A  [adult, adult, senior, adult, senior, senior, ...         adult            adult                   True\n",
       "97    102A                                    [senior, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                     [adult, adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "81    075A               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "50    044A           [kitten, kitten, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "48    042A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A        [adult, kitten, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "22    020A  [adult, senior, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, adult, s...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A       [adult, senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, senior, adult, senior, adult, a...         adult            adult                   True\n",
       "30    025C              [senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "42    036A  [adult, senior, senior, adult, adult, adult, a...         adult            adult                   True\n",
       "41    035A                     [senior, adult, senior, adult]         adult            adult                   True\n",
       "40    034A               [adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "43    037A       [adult, senior, adult, adult, adult, senior]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, senior, adult, se...         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                     [adult, adult]         adult           kitten                  False\n",
       "103   109A        [adult, adult, kitten, adult, adult, adult]         adult           kitten                  False\n",
       "4     003A                    [senior, senior, adult, senior]        senior            adult                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "102   108A      [senior, adult, adult, adult, senior, senior]         adult           senior                  False\n",
       "101   106A  [adult, senior, senior, senior, adult, adult, ...         adult           senior                  False\n",
       "106   113A                             [adult, senior, adult]         adult           senior                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "99    104A                     [senior, adult, senior, adult]         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, adult, senior, senior...        senior            adult                  False\n",
       "7     006A                            [senior, senior, adult]        senior            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "90    095A  [senior, adult, adult, senior, senior, kitten,...        senior            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "36    029A  [adult, adult, adult, adult, senior, senior, s...        senior            adult                  False\n",
       "65    059A  [adult, adult, adult, adult, senior, senior, s...         adult           senior                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, se...         adult           senior                  False"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "994531fd-e6fb-4491-9eab-86e41ac1ed3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     60\n",
      "kitten    12\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "229af3e2-6a64-4f3d-a594-e04108fce87f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             60  82.191781\n",
      "1           kitten           15             12  80.000000\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3960450a-db52-4464-a965-00f1e600bda5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbf768-64c2-48ec-80e3-3a961b0b12a6",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fe16003e-4015-4d28-ae37-ca0c94952758",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          550            418  76.000000\n",
      "1           kitten          109             84  77.064220\n",
      "2           senior          178            103  57.865169\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5b5b8766-b4bd-4dd2-92e2-6513741b03ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efa447-1e32-42df-8664-a3f1f0e0f812",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5eecd6a6-4094-4747-8029-f795a87f8ff0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      162     76.06\n",
      "1          M    338      225     66.57\n",
      "2          X    286      218     76.22\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "115592e9-47eb-42fe-8cb5-26beeb7328ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae066af0-659b-43f0-924c-2c50be0e6c40",
   "metadata": {},
   "source": [
    "# RANDOM SEED 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cc76296a-4029-447e-b12c-41520160251a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    1020\n",
      "kitten     992\n",
      "adult      842\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[1]))\n",
    "np.random.seed(int(random_seeds[1]))\n",
    "tf.random.set_seed(int(random_seeds[1]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_3.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "093977a2-1813-4a02-9573-f757b3d2a567",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0c89dbfa-4c9b-4e46-a0d9-f659db30532f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a62fdf-86b5-45e7-9249-73a55985936a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ee1b36c7-4a91-4071-a713-c904da913c3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "106A    14\n",
      "097B    14\n",
      "001A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "005A    10\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "022A     9\n",
      "051B     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "033A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "027A     7\n",
      "117A     7\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "053A     6\n",
      "008A     6\n",
      "023A     6\n",
      "037A     6\n",
      "007A     6\n",
      "034A     5\n",
      "025C     5\n",
      "070A     5\n",
      "023B     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "062A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "052A     4\n",
      "009A     4\n",
      "060A     3\n",
      "012A     3\n",
      "006A     3\n",
      "064A     3\n",
      "058A     3\n",
      "054A     2\n",
      "061A     2\n",
      "087A     2\n",
      "069A     2\n",
      "032A     2\n",
      "011A     2\n",
      "018A     2\n",
      "025B     2\n",
      "093A     2\n",
      "088A     1\n",
      "091A     1\n",
      "100A     1\n",
      "090A     1\n",
      "019B     1\n",
      "115A     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "073A     1\n",
      "026C     1\n",
      "076A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "014B    10\n",
      "015A     9\n",
      "099A     7\n",
      "050A     7\n",
      "021A     5\n",
      "003A     4\n",
      "056A     3\n",
      "014A     3\n",
      "113A     3\n",
      "038A     2\n",
      "102A     2\n",
      "096A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    233\n",
      "M    226\n",
      "F    210\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    115\n",
      "M    111\n",
      "F     42\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 040A, 046A, 109A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 059A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 103A, 074A, 002B, 101A, 038A, 099A, 014...\n",
      "kitten                 [014B, 111A, 047A, 042A, 050A, 110A]\n",
      "senior                       [097A, 113A, 056A, 051A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "669\n",
      "Length of y_train_val:\n",
      "669\n",
      "Length of groups_train_val:\n",
      "669\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1030, 2: 975, 1: 658})\n",
      "Epoch 1/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.9442 - accuracy: 0.5753\n",
      "Epoch 2/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.7658 - accuracy: 0.6849\n",
      "Epoch 3/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.6989 - accuracy: 0.6973\n",
      "Epoch 4/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.6270 - accuracy: 0.7270\n",
      "Epoch 5/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.6264 - accuracy: 0.7326\n",
      "Epoch 6/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.6143 - accuracy: 0.7431\n",
      "Epoch 7/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.5575 - accuracy: 0.7642\n",
      "Epoch 8/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.5636 - accuracy: 0.7687\n",
      "Epoch 9/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.5444 - accuracy: 0.7796\n",
      "Epoch 10/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.5102 - accuracy: 0.7942\n",
      "Epoch 11/1500\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.5131 - accuracy: 0.7826\n",
      "Epoch 12/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4892 - accuracy: 0.7893\n",
      "Epoch 13/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4886 - accuracy: 0.7946\n",
      "Epoch 14/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4819 - accuracy: 0.8040\n",
      "Epoch 15/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4878 - accuracy: 0.7908\n",
      "Epoch 16/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4540 - accuracy: 0.8179\n",
      "Epoch 17/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4451 - accuracy: 0.8216\n",
      "Epoch 18/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4318 - accuracy: 0.8284\n",
      "Epoch 19/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.8134\n",
      "Epoch 20/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4318 - accuracy: 0.8186\n",
      "Epoch 21/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4184 - accuracy: 0.8239\n",
      "Epoch 22/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4277 - accuracy: 0.8167\n",
      "Epoch 23/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4093 - accuracy: 0.8374\n",
      "Epoch 24/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.4122 - accuracy: 0.8261\n",
      "Epoch 25/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3918 - accuracy: 0.8464\n",
      "Epoch 26/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3915 - accuracy: 0.8434\n",
      "Epoch 27/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3801 - accuracy: 0.8423\n",
      "Epoch 28/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3864 - accuracy: 0.8415\n",
      "Epoch 29/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3811 - accuracy: 0.8464\n",
      "Epoch 30/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3697 - accuracy: 0.8475\n",
      "Epoch 31/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3476 - accuracy: 0.8581\n",
      "Epoch 32/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8475\n",
      "Epoch 33/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3761 - accuracy: 0.8423\n",
      "Epoch 34/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3584 - accuracy: 0.8644\n",
      "Epoch 35/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3425 - accuracy: 0.8644\n",
      "Epoch 36/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3412 - accuracy: 0.8566\n",
      "Epoch 37/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3384 - accuracy: 0.8682\n",
      "Epoch 38/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3471 - accuracy: 0.8584\n",
      "Epoch 39/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3359 - accuracy: 0.8682\n",
      "Epoch 40/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3292 - accuracy: 0.8735\n",
      "Epoch 41/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3171 - accuracy: 0.8750\n",
      "Epoch 42/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8641\n",
      "Epoch 43/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3455 - accuracy: 0.8588\n",
      "Epoch 44/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3157 - accuracy: 0.8765\n",
      "Epoch 45/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8765\n",
      "Epoch 46/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3050 - accuracy: 0.8783\n",
      "Epoch 47/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3132 - accuracy: 0.8802\n",
      "Epoch 48/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3127 - accuracy: 0.8712\n",
      "Epoch 49/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3065 - accuracy: 0.8738\n",
      "Epoch 50/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3244 - accuracy: 0.8750\n",
      "Epoch 51/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3159 - accuracy: 0.8738\n",
      "Epoch 52/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3093 - accuracy: 0.8682\n",
      "Epoch 53/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3020 - accuracy: 0.8765\n",
      "Epoch 54/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.8776\n",
      "Epoch 55/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3041 - accuracy: 0.8798\n",
      "Epoch 56/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.3055 - accuracy: 0.8768\n",
      "Epoch 57/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2972 - accuracy: 0.8810\n",
      "Epoch 58/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2991 - accuracy: 0.8828\n",
      "Epoch 59/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2893 - accuracy: 0.8855\n",
      "Epoch 60/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2727 - accuracy: 0.9001\n",
      "Epoch 61/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2961 - accuracy: 0.8862\n",
      "Epoch 62/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2854 - accuracy: 0.8870\n",
      "Epoch 63/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2715 - accuracy: 0.8907\n",
      "Epoch 64/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2965 - accuracy: 0.8787\n",
      "Epoch 65/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2766 - accuracy: 0.8866\n",
      "Epoch 66/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8941\n",
      "Epoch 67/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2614 - accuracy: 0.8997\n",
      "Epoch 68/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.9016\n",
      "Epoch 69/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2608 - accuracy: 0.8952\n",
      "Epoch 70/1500\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.2637 - accuracy: 0.8956\n",
      "Epoch 71/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2673 - accuracy: 0.8952\n",
      "Epoch 72/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2720 - accuracy: 0.8915\n",
      "Epoch 73/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2655 - accuracy: 0.9001\n",
      "Epoch 74/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2694 - accuracy: 0.8922\n",
      "Epoch 75/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2681 - accuracy: 0.8911\n",
      "Epoch 76/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9016\n",
      "Epoch 77/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2429 - accuracy: 0.9061\n",
      "Epoch 78/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2520 - accuracy: 0.9031\n",
      "Epoch 79/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2472 - accuracy: 0.9009\n",
      "Epoch 80/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2534 - accuracy: 0.9020\n",
      "Epoch 81/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2515 - accuracy: 0.9035\n",
      "Epoch 82/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2466 - accuracy: 0.9065\n",
      "Epoch 83/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2462 - accuracy: 0.9061\n",
      "Epoch 84/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2394 - accuracy: 0.9065\n",
      "Epoch 85/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2290 - accuracy: 0.9118\n",
      "Epoch 86/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2242 - accuracy: 0.9144\n",
      "Epoch 87/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2344 - accuracy: 0.9151\n",
      "Epoch 88/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2295 - accuracy: 0.9110\n",
      "Epoch 89/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2332 - accuracy: 0.9095\n",
      "Epoch 90/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2374 - accuracy: 0.9069\n",
      "Epoch 91/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2479 - accuracy: 0.9016\n",
      "Epoch 92/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2436 - accuracy: 0.9012\n",
      "Epoch 93/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2257 - accuracy: 0.9121\n",
      "Epoch 94/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2274 - accuracy: 0.9110\n",
      "Epoch 95/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 0.9215\n",
      "Epoch 96/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2334 - accuracy: 0.9091\n",
      "Epoch 97/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2316 - accuracy: 0.9133\n",
      "Epoch 98/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2348 - accuracy: 0.9065\n",
      "Epoch 99/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2330 - accuracy: 0.9118\n",
      "Epoch 100/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2497 - accuracy: 0.9031\n",
      "Epoch 101/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.9069\n",
      "Epoch 102/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2286 - accuracy: 0.9080\n",
      "Epoch 103/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2369 - accuracy: 0.9061\n",
      "Epoch 104/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2367 - accuracy: 0.9110\n",
      "Epoch 105/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2250 - accuracy: 0.9155\n",
      "Epoch 106/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2113 - accuracy: 0.9226\n",
      "Epoch 107/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2209 - accuracy: 0.9178\n",
      "Epoch 108/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2099 - accuracy: 0.9181\n",
      "Epoch 109/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9170\n",
      "Epoch 110/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9260\n",
      "Epoch 111/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2129 - accuracy: 0.9185\n",
      "Epoch 112/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9208\n",
      "Epoch 113/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9170\n",
      "Epoch 114/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2056 - accuracy: 0.9219\n",
      "Epoch 115/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2098 - accuracy: 0.9185\n",
      "Epoch 116/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9260\n",
      "Epoch 117/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2307 - accuracy: 0.9151\n",
      "Epoch 118/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2010 - accuracy: 0.9260\n",
      "Epoch 119/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2098 - accuracy: 0.9234\n",
      "Epoch 120/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2324 - accuracy: 0.9095\n",
      "Epoch 121/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1998 - accuracy: 0.9215\n",
      "Epoch 122/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2039 - accuracy: 0.9193\n",
      "Epoch 123/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.9230\n",
      "Epoch 124/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2278 - accuracy: 0.9106\n",
      "Epoch 125/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2236 - accuracy: 0.9118\n",
      "Epoch 126/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1913 - accuracy: 0.9245\n",
      "Epoch 127/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.9226\n",
      "Epoch 128/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9302\n",
      "Epoch 129/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1935 - accuracy: 0.9241\n",
      "Epoch 130/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.9313\n",
      "Epoch 131/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9260\n",
      "Epoch 132/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9287\n",
      "Epoch 133/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1928 - accuracy: 0.9283\n",
      "Epoch 134/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9332\n",
      "Epoch 135/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2036 - accuracy: 0.9211\n",
      "Epoch 136/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2112 - accuracy: 0.9196\n",
      "Epoch 137/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9283\n",
      "Epoch 138/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.2045 - accuracy: 0.9200\n",
      "Epoch 139/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9354\n",
      "Epoch 140/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9343\n",
      "Epoch 141/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1923 - accuracy: 0.9313\n",
      "Epoch 142/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9309\n",
      "Epoch 143/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9302\n",
      "Epoch 144/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1908 - accuracy: 0.9279\n",
      "Epoch 145/1500\n",
      "84/84 [==============================] - 0s 4ms/step - loss: 0.1928 - accuracy: 0.9264\n",
      "Epoch 146/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9343\n",
      "Epoch 147/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9260\n",
      "Epoch 148/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.9362\n",
      "Epoch 149/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9287\n",
      "Epoch 150/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1795 - accuracy: 0.9317\n",
      "Epoch 151/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1907 - accuracy: 0.9294\n",
      "Epoch 152/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9365\n",
      "Epoch 153/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1751 - accuracy: 0.9332\n",
      "Epoch 154/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9294\n",
      "Epoch 155/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9369\n",
      "Epoch 156/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9350\n",
      "Epoch 157/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1778 - accuracy: 0.9287\n",
      "Epoch 158/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1896 - accuracy: 0.9271\n",
      "Epoch 159/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9317\n",
      "Epoch 160/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9309\n",
      "Epoch 161/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1892 - accuracy: 0.9241\n",
      "Epoch 162/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9369\n",
      "Epoch 163/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.9392\n",
      "Epoch 164/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9324\n",
      "Epoch 165/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9365\n",
      "Epoch 166/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9399\n",
      "Epoch 167/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.9305\n",
      "Epoch 168/1500\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9279\n",
      "Epoch 169/1500\n",
      "47/84 [===============>..............] - ETA: 0s - loss: 0.1866 - accuracy: 0.9302Restoring model weights from the end of the best epoch: 139.\n",
      "84/84 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.9354\n",
      "Epoch 169: early stopping\n",
      "9/9 [==============================] - 0s 741us/step - loss: 0.6860 - accuracy: 0.7127\n",
      "9/9 [==============================] - 0s 617us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.68 (17/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "Final Test Results - Loss: 0.6860322952270508, Accuracy: 0.7126865386962891, Precision: 0.6882005294682908, Recall: 0.6882420091324201, F1 Score: 0.6715226609931845\n",
      "Confusion Matrix:\n",
      " [[116   3  41]\n",
      " [ 19  54   0]\n",
      " [ 14   0  21]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "053A     6\n",
      "108A     6\n",
      "023A     6\n",
      "007A     6\n",
      "025C     5\n",
      "044A     5\n",
      "023B     5\n",
      "021A     5\n",
      "070A     5\n",
      "034A     5\n",
      "003A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "104A     4\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "113A     3\n",
      "056A     3\n",
      "018A     2\n",
      "038A     2\n",
      "025B     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "102A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "073A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "063A    11\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "095A     8\n",
      "037A     6\n",
      "008A     6\n",
      "109A     6\n",
      "075A     5\n",
      "105A     4\n",
      "064A     3\n",
      "060A     3\n",
      "093A     2\n",
      "087A     2\n",
      "011A     2\n",
      "061A     2\n",
      "076A     1\n",
      "043A     1\n",
      "091A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    238\n",
      "F    229\n",
      "X    221\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    127\n",
      "M     99\n",
      "F     23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 015A, 103A, 071A, 097B, 028A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 050A, 049...\n",
      "senior    [097A, 057A, 106A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 001A, 019A, 067A, 022A, 029A, 095A, 072...\n",
      "kitten                             [046A, 109A, 043A, 045A]\n",
      "senior                             [093A, 051B, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '044A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '043A'\n",
      " '045A' '046A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'044A'}\n",
      "Moved to Test Set:\n",
      "{'044A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '043A'\n",
      " '044A' '045A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "Length of X_train_val:\n",
      "746\n",
      "Length of y_train_val:\n",
      "746\n",
      "Length of groups_train_val:\n",
      "746\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     155\n",
      "kitten     79\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten    150\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     155\n",
      "kitten     21\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({2: 1111, 0: 1050, 1: 1014})\n",
      "Epoch 1/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.8303 - accuracy: 0.6431\n",
      "Epoch 2/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6384 - accuracy: 0.7345\n",
      "Epoch 3/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.6016 - accuracy: 0.7490\n",
      "Epoch 4/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.5690 - accuracy: 0.7657\n",
      "Epoch 5/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.5463 - accuracy: 0.7789\n",
      "Epoch 6/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.5156 - accuracy: 0.7934\n",
      "Epoch 7/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4859 - accuracy: 0.8057\n",
      "Epoch 8/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4850 - accuracy: 0.8050\n",
      "Epoch 9/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4681 - accuracy: 0.8104\n",
      "Epoch 10/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4550 - accuracy: 0.8176\n",
      "Epoch 11/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4382 - accuracy: 0.8252\n",
      "Epoch 12/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4366 - accuracy: 0.8261\n",
      "Epoch 13/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4030 - accuracy: 0.8387\n",
      "Epoch 14/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.8309\n",
      "Epoch 15/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4117 - accuracy: 0.8391\n",
      "Epoch 16/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.4099 - accuracy: 0.8450\n",
      "Epoch 17/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.8466\n",
      "Epoch 18/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3843 - accuracy: 0.8463\n",
      "Epoch 19/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3755 - accuracy: 0.8501\n",
      "Epoch 20/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8466\n",
      "Epoch 21/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8491\n",
      "Epoch 22/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3562 - accuracy: 0.8633\n",
      "Epoch 23/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3485 - accuracy: 0.8557\n",
      "Epoch 24/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3432 - accuracy: 0.8649\n",
      "Epoch 25/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3516 - accuracy: 0.8592\n",
      "Epoch 26/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3412 - accuracy: 0.8646\n",
      "Epoch 27/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3371 - accuracy: 0.8683\n",
      "Epoch 28/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3168 - accuracy: 0.8769\n",
      "Epoch 29/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3307 - accuracy: 0.8668\n",
      "Epoch 30/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3241 - accuracy: 0.8709\n",
      "Epoch 31/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3267 - accuracy: 0.8699\n",
      "Epoch 32/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3170 - accuracy: 0.8791\n",
      "Epoch 33/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8794\n",
      "Epoch 34/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.8806\n",
      "Epoch 35/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3086 - accuracy: 0.8784\n",
      "Epoch 36/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.8816\n",
      "Epoch 37/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2909 - accuracy: 0.8850\n",
      "Epoch 38/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.3071 - accuracy: 0.8746\n",
      "Epoch 39/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2795 - accuracy: 0.8901\n",
      "Epoch 40/1500\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.3043 - accuracy: 0.8797\n",
      "Epoch 41/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2821 - accuracy: 0.8835\n",
      "Epoch 42/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2910 - accuracy: 0.8879\n",
      "Epoch 43/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2895 - accuracy: 0.8825\n",
      "Epoch 44/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2939 - accuracy: 0.8835\n",
      "Epoch 45/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2738 - accuracy: 0.8917\n",
      "Epoch 46/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2868 - accuracy: 0.8860\n",
      "Epoch 47/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2740 - accuracy: 0.8926\n",
      "Epoch 48/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.8920\n",
      "Epoch 49/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2402 - accuracy: 0.9068\n",
      "Epoch 50/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2635 - accuracy: 0.9017\n",
      "Epoch 51/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2658 - accuracy: 0.8980\n",
      "Epoch 52/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.8847\n",
      "Epoch 53/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2498 - accuracy: 0.9011\n",
      "Epoch 54/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2447 - accuracy: 0.9024\n",
      "Epoch 55/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.9052\n",
      "Epoch 56/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2577 - accuracy: 0.8957\n",
      "Epoch 57/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2492 - accuracy: 0.9083\n",
      "Epoch 58/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2609 - accuracy: 0.9011\n",
      "Epoch 59/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2476 - accuracy: 0.9017\n",
      "Epoch 60/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2476 - accuracy: 0.9055\n",
      "Epoch 61/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2554 - accuracy: 0.8998\n",
      "Epoch 62/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2356 - accuracy: 0.9146\n",
      "Epoch 63/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2489 - accuracy: 0.9011\n",
      "Epoch 64/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2325 - accuracy: 0.9093\n",
      "Epoch 65/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2429 - accuracy: 0.9046\n",
      "Epoch 66/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.9093\n",
      "Epoch 67/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2320 - accuracy: 0.9118\n",
      "Epoch 68/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2394 - accuracy: 0.9099\n",
      "Epoch 69/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.9074\n",
      "Epoch 70/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2341 - accuracy: 0.9058\n",
      "Epoch 71/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2190 - accuracy: 0.9178\n",
      "Epoch 72/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2193 - accuracy: 0.9169\n",
      "Epoch 73/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2229 - accuracy: 0.9159\n",
      "Epoch 74/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2250 - accuracy: 0.9118\n",
      "Epoch 75/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9099\n",
      "Epoch 76/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2278 - accuracy: 0.9058\n",
      "Epoch 77/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2193 - accuracy: 0.9115\n",
      "Epoch 78/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2135 - accuracy: 0.9159\n",
      "Epoch 79/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9197\n",
      "Epoch 80/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2266 - accuracy: 0.9065\n",
      "Epoch 81/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2123 - accuracy: 0.9162\n",
      "Epoch 82/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9181\n",
      "Epoch 83/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2168 - accuracy: 0.9165\n",
      "Epoch 84/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2178 - accuracy: 0.9169\n",
      "Epoch 85/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9219\n",
      "Epoch 86/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2148 - accuracy: 0.9172\n",
      "Epoch 87/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2064 - accuracy: 0.9181\n",
      "Epoch 88/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2006 - accuracy: 0.9260\n",
      "Epoch 89/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2034 - accuracy: 0.9203\n",
      "Epoch 90/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9291\n",
      "Epoch 91/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9254\n",
      "Epoch 92/1500\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.2018 - accuracy: 0.9307\n",
      "Epoch 93/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.9320\n",
      "Epoch 94/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9228\n",
      "Epoch 95/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1989 - accuracy: 0.9216\n",
      "Epoch 96/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9266\n",
      "Epoch 97/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1869 - accuracy: 0.9317\n",
      "Epoch 98/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2075 - accuracy: 0.9244\n",
      "Epoch 99/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1880 - accuracy: 0.9310\n",
      "Epoch 100/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.9282\n",
      "Epoch 101/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9225\n",
      "Epoch 102/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9231\n",
      "Epoch 103/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9191\n",
      "Epoch 104/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.9279\n",
      "Epoch 105/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9310\n",
      "Epoch 106/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9313\n",
      "Epoch 107/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1809 - accuracy: 0.9357\n",
      "Epoch 108/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.9335\n",
      "Epoch 109/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9313\n",
      "Epoch 110/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.9272\n",
      "Epoch 111/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1778 - accuracy: 0.9354\n",
      "Epoch 112/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1936 - accuracy: 0.9244\n",
      "Epoch 113/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9310\n",
      "Epoch 114/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.9345\n",
      "Epoch 115/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9335\n",
      "Epoch 116/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1695 - accuracy: 0.9313\n",
      "Epoch 117/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9342\n",
      "Epoch 118/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1717 - accuracy: 0.9345\n",
      "Epoch 119/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.9294\n",
      "Epoch 120/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1792 - accuracy: 0.9332\n",
      "Epoch 121/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9213\n",
      "Epoch 122/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1950 - accuracy: 0.9254\n",
      "Epoch 123/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9276\n",
      "Epoch 124/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.9329\n",
      "Epoch 125/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1747 - accuracy: 0.9354\n",
      "Epoch 126/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1799 - accuracy: 0.9285\n",
      "Epoch 127/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.9357\n",
      "Epoch 128/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1751 - accuracy: 0.9254\n",
      "Epoch 129/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.9376\n",
      "Epoch 130/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9370\n",
      "Epoch 131/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9361\n",
      "Epoch 132/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9332\n",
      "Epoch 133/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9373\n",
      "Epoch 134/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9313\n",
      "Epoch 135/1500\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1672 - accuracy: 0.9342\n",
      "Epoch 136/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9420\n",
      "Epoch 137/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.9304\n",
      "Epoch 138/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9301\n",
      "Epoch 139/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1592 - accuracy: 0.9408\n",
      "Epoch 140/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.9367\n",
      "Epoch 141/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9417\n",
      "Epoch 142/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1651 - accuracy: 0.9411\n",
      "Epoch 143/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9420\n",
      "Epoch 144/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9395\n",
      "Epoch 145/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9276\n",
      "Epoch 146/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9370\n",
      "Epoch 147/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.9257\n",
      "Epoch 148/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9332\n",
      "Epoch 149/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9351\n",
      "Epoch 150/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1721 - accuracy: 0.9313\n",
      "Epoch 151/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9427\n",
      "Epoch 152/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9490\n",
      "Epoch 153/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9496\n",
      "Epoch 154/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9465\n",
      "Epoch 155/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.9383\n",
      "Epoch 156/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9411\n",
      "Epoch 157/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.9408\n",
      "Epoch 158/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9465\n",
      "Epoch 159/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9417\n",
      "Epoch 160/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9417\n",
      "Epoch 161/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1648 - accuracy: 0.9380\n",
      "Epoch 162/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9370\n",
      "Epoch 163/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.9502\n",
      "Epoch 164/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9471\n",
      "Epoch 165/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9427\n",
      "Epoch 166/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9414\n",
      "Epoch 167/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9452\n",
      "Epoch 168/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9474\n",
      "Epoch 169/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9408\n",
      "Epoch 170/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9509\n",
      "Epoch 171/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9436\n",
      "Epoch 172/1500\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1419 - accuracy: 0.9452\n",
      "Epoch 173/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9417\n",
      "Epoch 174/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.9329\n",
      "Epoch 175/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9452\n",
      "Epoch 176/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.9471\n",
      "Epoch 177/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9449\n",
      "Epoch 178/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9490\n",
      "Epoch 179/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9436\n",
      "Epoch 180/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9509\n",
      "Epoch 181/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9465\n",
      "Epoch 182/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9420\n",
      "Epoch 183/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9452\n",
      "Epoch 184/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9449\n",
      "Epoch 185/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9424\n",
      "Epoch 186/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9537\n",
      "Epoch 187/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9493\n",
      "Epoch 188/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9528\n",
      "Epoch 189/1500\n",
      "100/100 [==============================] - 0s 981us/step - loss: 0.1553 - accuracy: 0.9411\n",
      "Epoch 190/1500\n",
      "100/100 [==============================] - 0s 985us/step - loss: 0.1481 - accuracy: 0.9477\n",
      "Epoch 191/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9461\n",
      "Epoch 192/1500\n",
      "100/100 [==============================] - 0s 963us/step - loss: 0.1348 - accuracy: 0.9496\n",
      "Epoch 193/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1617 - accuracy: 0.9443\n",
      "Epoch 194/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1365 - accuracy: 0.9528\n",
      "Epoch 195/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9474\n",
      "Epoch 196/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9569\n",
      "Epoch 197/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9471\n",
      "Epoch 198/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9543\n",
      "Epoch 199/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9487\n",
      "Epoch 200/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9483\n",
      "Epoch 201/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9559\n",
      "Epoch 202/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9487\n",
      "Epoch 203/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9534\n",
      "Epoch 204/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9452\n",
      "Epoch 205/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9490\n",
      "Epoch 206/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9531\n",
      "Epoch 207/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9537\n",
      "Epoch 208/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1414 - accuracy: 0.9477\n",
      "Epoch 209/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9613\n",
      "Epoch 210/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9515\n",
      "Epoch 211/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9543\n",
      "Epoch 212/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9449\n",
      "Epoch 213/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9509\n",
      "Epoch 214/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9455\n",
      "Epoch 215/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9528\n",
      "Epoch 216/1500\n",
      "100/100 [==============================] - 0s 3ms/step - loss: 0.1306 - accuracy: 0.9540\n",
      "Epoch 217/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9487\n",
      "Epoch 218/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9559\n",
      "Epoch 219/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9512\n",
      "Epoch 220/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9578\n",
      "Epoch 221/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9556\n",
      "Epoch 222/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9534\n",
      "Epoch 223/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9506\n",
      "Epoch 224/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1330 - accuracy: 0.9512\n",
      "Epoch 225/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9553\n",
      "Epoch 226/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9477\n",
      "Epoch 227/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9537\n",
      "Epoch 228/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9506\n",
      "Epoch 229/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9572\n",
      "Epoch 230/1500\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9584\n",
      "Epoch 231/1500\n",
      " 98/100 [============================>.] - ETA: 0s - loss: 0.1288 - accuracy: 0.9503Restoring model weights from the end of the best epoch: 201.\n",
      "100/100 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9502\n",
      "Epoch 231: early stopping\n",
      "6/6 [==============================] - 0s 887us/step - loss: 0.8699 - accuracy: 0.6963\n",
      "6/6 [==============================] - 0s 724us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.62 (18/29)\n",
      "Before appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "After appending - Cat IDs: 459, Predictions: 459, Actuals: 459, Gender: 459\n",
      "Final Test Results - Loss: 0.8698641061782837, Accuracy: 0.6963350772857666, Precision: 0.4945775729646697, Recall: 0.6133128520225294, F1 Score: 0.5223758320532513\n",
      "Confusion Matrix:\n",
      " [[111  23  21]\n",
      " [  3  18   0]\n",
      " [ 10   1   4]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "042A    14\n",
      "097B    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "095A     8\n",
      "117A     7\n",
      "050A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "008A     6\n",
      "109A     6\n",
      "053A     6\n",
      "037A     6\n",
      "108A     6\n",
      "023A     6\n",
      "023B     5\n",
      "075A     5\n",
      "021A     5\n",
      "105A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "003A     4\n",
      "056A     3\n",
      "113A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "012A     3\n",
      "058A     3\n",
      "061A     2\n",
      "011A     2\n",
      "102A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "069A     2\n",
      "041A     1\n",
      "019B     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "096A     1\n",
      "076A     1\n",
      "088A     1\n",
      "092A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "073A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "000B    19\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "025A    11\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "010A     8\n",
      "013B     8\n",
      "007A     6\n",
      "070A     5\n",
      "044A     5\n",
      "034A     5\n",
      "025C     5\n",
      "104A     4\n",
      "009A     4\n",
      "006A     3\n",
      "018A     2\n",
      "054A     2\n",
      "025B     2\n",
      "032A     2\n",
      "049A     1\n",
      "026C     1\n",
      "066A     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    322\n",
      "M    241\n",
      "F    159\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "F    93\n",
      "X    26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 001A, 103A, 097B, 019A, 074...\n",
      "kitten    [014B, 111A, 040A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 057A, 055A, 113A, 051B, 117A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 071A, 028A, 020A, 034A, 005A, 002A, 009...\n",
      "kitten                                   [044A, 049A, 115A]\n",
      "senior           [106A, 104A, 059A, 116A, 054A, 016A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 13, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 3, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "722\n",
      "Length of y_train_val:\n",
      "722\n",
      "Length of groups_train_val:\n",
      "722\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({1: 1116, 0: 1063, 2: 797})\n",
      "Epoch 1/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.8905 - accuracy: 0.6280\n",
      "Epoch 2/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.6926 - accuracy: 0.7043\n",
      "Epoch 3/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.6276 - accuracy: 0.7416\n",
      "Epoch 4/1500\n",
      "93/93 [==============================] - 0s 985us/step - loss: 0.5746 - accuracy: 0.7658\n",
      "Epoch 5/1500\n",
      "93/93 [==============================] - 0s 980us/step - loss: 0.5573 - accuracy: 0.7634\n",
      "Epoch 6/1500\n",
      "93/93 [==============================] - 0s 994us/step - loss: 0.5147 - accuracy: 0.7913\n",
      "Epoch 7/1500\n",
      "93/93 [==============================] - 0s 999us/step - loss: 0.5016 - accuracy: 0.7970\n",
      "Epoch 8/1500\n",
      "93/93 [==============================] - 0s 980us/step - loss: 0.4738 - accuracy: 0.8051\n",
      "Epoch 9/1500\n",
      "93/93 [==============================] - 0s 990us/step - loss: 0.4665 - accuracy: 0.8031\n",
      "Epoch 10/1500\n",
      "93/93 [==============================] - 0s 990us/step - loss: 0.4603 - accuracy: 0.8175\n",
      "Epoch 11/1500\n",
      "93/93 [==============================] - 0s 983us/step - loss: 0.4256 - accuracy: 0.8239\n",
      "Epoch 12/1500\n",
      "93/93 [==============================] - 0s 995us/step - loss: 0.4219 - accuracy: 0.8269\n",
      "Epoch 13/1500\n",
      "93/93 [==============================] - 0s 984us/step - loss: 0.4112 - accuracy: 0.8404\n",
      "Epoch 14/1500\n",
      "93/93 [==============================] - 0s 998us/step - loss: 0.4087 - accuracy: 0.8296\n",
      "Epoch 15/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3915 - accuracy: 0.8431\n",
      "Epoch 16/1500\n",
      "93/93 [==============================] - 0s 983us/step - loss: 0.4025 - accuracy: 0.8431\n",
      "Epoch 17/1500\n",
      "93/93 [==============================] - 0s 1000us/step - loss: 0.3823 - accuracy: 0.8485\n",
      "Epoch 18/1500\n",
      "93/93 [==============================] - 0s 985us/step - loss: 0.3563 - accuracy: 0.8565\n",
      "Epoch 19/1500\n",
      "93/93 [==============================] - 0s 984us/step - loss: 0.3775 - accuracy: 0.8481\n",
      "Epoch 20/1500\n",
      "93/93 [==============================] - 0s 976us/step - loss: 0.3575 - accuracy: 0.8609\n",
      "Epoch 21/1500\n",
      "93/93 [==============================] - 0s 993us/step - loss: 0.3584 - accuracy: 0.8612\n",
      "Epoch 22/1500\n",
      "93/93 [==============================] - 0s 983us/step - loss: 0.3427 - accuracy: 0.8575\n",
      "Epoch 23/1500\n",
      "93/93 [==============================] - 0s 983us/step - loss: 0.3726 - accuracy: 0.8495\n",
      "Epoch 24/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3383 - accuracy: 0.8639\n",
      "Epoch 25/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8582\n",
      "Epoch 26/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3389 - accuracy: 0.8609\n",
      "Epoch 27/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3087 - accuracy: 0.8730\n",
      "Epoch 28/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3158 - accuracy: 0.8723\n",
      "Epoch 29/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3135 - accuracy: 0.8743\n",
      "Epoch 30/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3170 - accuracy: 0.8686\n",
      "Epoch 31/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3135 - accuracy: 0.8757\n",
      "Epoch 32/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3124 - accuracy: 0.8743\n",
      "Epoch 33/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3006 - accuracy: 0.8807\n",
      "Epoch 34/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2915 - accuracy: 0.8881\n",
      "Epoch 35/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2850 - accuracy: 0.8898\n",
      "Epoch 36/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3234 - accuracy: 0.8733\n",
      "Epoch 37/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2834 - accuracy: 0.8891\n",
      "Epoch 38/1500\n",
      "93/93 [==============================] - 0s 978us/step - loss: 0.2946 - accuracy: 0.8837\n",
      "Epoch 39/1500\n",
      "93/93 [==============================] - 0s 989us/step - loss: 0.2757 - accuracy: 0.8915\n",
      "Epoch 40/1500\n",
      "93/93 [==============================] - 0s 972us/step - loss: 0.2826 - accuracy: 0.8858\n",
      "Epoch 41/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2787 - accuracy: 0.8942\n",
      "Epoch 42/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2715 - accuracy: 0.8928\n",
      "Epoch 43/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2661 - accuracy: 0.8945\n",
      "Epoch 44/1500\n",
      "93/93 [==============================] - 0s 997us/step - loss: 0.2616 - accuracy: 0.8985\n",
      "Epoch 45/1500\n",
      "93/93 [==============================] - 0s 993us/step - loss: 0.2676 - accuracy: 0.8982\n",
      "Epoch 46/1500\n",
      "93/93 [==============================] - 0s 994us/step - loss: 0.2705 - accuracy: 0.8989\n",
      "Epoch 47/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2493 - accuracy: 0.9029\n",
      "Epoch 48/1500\n",
      "93/93 [==============================] - 0s 983us/step - loss: 0.2600 - accuracy: 0.9026\n",
      "Epoch 49/1500\n",
      "93/93 [==============================] - 0s 996us/step - loss: 0.2803 - accuracy: 0.8871\n",
      "Epoch 50/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2706 - accuracy: 0.8982\n",
      "Epoch 51/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2533 - accuracy: 0.9046\n",
      "Epoch 52/1500\n",
      "93/93 [==============================] - 0s 980us/step - loss: 0.2657 - accuracy: 0.8928\n",
      "Epoch 53/1500\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.2543 - accuracy: 0.9015\n",
      "Epoch 54/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2554 - accuracy: 0.8978\n",
      "Epoch 55/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2461 - accuracy: 0.9123\n",
      "Epoch 56/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2458 - accuracy: 0.9049\n",
      "Epoch 57/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2375 - accuracy: 0.9069\n",
      "Epoch 58/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.9126\n",
      "Epoch 59/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2304 - accuracy: 0.9086\n",
      "Epoch 60/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2184 - accuracy: 0.9163\n",
      "Epoch 61/1500\n",
      "93/93 [==============================] - 0s 999us/step - loss: 0.2368 - accuracy: 0.9153\n",
      "Epoch 62/1500\n",
      "93/93 [==============================] - 0s 990us/step - loss: 0.2210 - accuracy: 0.9150\n",
      "Epoch 63/1500\n",
      "93/93 [==============================] - 0s 984us/step - loss: 0.2366 - accuracy: 0.9086\n",
      "Epoch 64/1500\n",
      "93/93 [==============================] - 0s 982us/step - loss: 0.2290 - accuracy: 0.9150\n",
      "Epoch 65/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9190\n",
      "Epoch 66/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2150 - accuracy: 0.9217\n",
      "Epoch 67/1500\n",
      "93/93 [==============================] - 0s 991us/step - loss: 0.2192 - accuracy: 0.9187\n",
      "Epoch 68/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2245 - accuracy: 0.9103\n",
      "Epoch 69/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2240 - accuracy: 0.9083\n",
      "Epoch 70/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2029 - accuracy: 0.9231\n",
      "Epoch 71/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2171 - accuracy: 0.9207\n",
      "Epoch 72/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2303 - accuracy: 0.9096\n",
      "Epoch 73/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2191 - accuracy: 0.9143\n",
      "Epoch 74/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2137 - accuracy: 0.9163\n",
      "Epoch 75/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2096 - accuracy: 0.9244\n",
      "Epoch 76/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2062 - accuracy: 0.9247\n",
      "Epoch 77/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2054 - accuracy: 0.9247\n",
      "Epoch 78/1500\n",
      "93/93 [==============================] - 0s 993us/step - loss: 0.2041 - accuracy: 0.9267\n",
      "Epoch 79/1500\n",
      "93/93 [==============================] - 0s 989us/step - loss: 0.1989 - accuracy: 0.9214\n",
      "Epoch 80/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9190\n",
      "Epoch 81/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1995 - accuracy: 0.9261\n",
      "Epoch 82/1500\n",
      "93/93 [==============================] - 0s 977us/step - loss: 0.2046 - accuracy: 0.9197\n",
      "Epoch 83/1500\n",
      "93/93 [==============================] - 0s 996us/step - loss: 0.1976 - accuracy: 0.9278\n",
      "Epoch 84/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9298\n",
      "Epoch 85/1500\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1995 - accuracy: 0.9244\n",
      "Epoch 86/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1938 - accuracy: 0.9281\n",
      "Epoch 87/1500\n",
      "93/93 [==============================] - 0s 995us/step - loss: 0.2087 - accuracy: 0.9187\n",
      "Epoch 88/1500\n",
      "93/93 [==============================] - 0s 980us/step - loss: 0.1960 - accuracy: 0.9301\n",
      "Epoch 89/1500\n",
      "93/93 [==============================] - 0s 993us/step - loss: 0.1913 - accuracy: 0.9291\n",
      "Epoch 90/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1970 - accuracy: 0.9227\n",
      "Epoch 91/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9338\n",
      "Epoch 92/1500\n",
      "93/93 [==============================] - 0s 988us/step - loss: 0.1834 - accuracy: 0.9331\n",
      "Epoch 93/1500\n",
      "93/93 [==============================] - 0s 991us/step - loss: 0.1921 - accuracy: 0.9267\n",
      "Epoch 94/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.9301\n",
      "Epoch 95/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1907 - accuracy: 0.9298\n",
      "Epoch 96/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.9261\n",
      "Epoch 97/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1910 - accuracy: 0.9301\n",
      "Epoch 98/1500\n",
      "93/93 [==============================] - 0s 991us/step - loss: 0.1937 - accuracy: 0.9274\n",
      "Epoch 99/1500\n",
      "93/93 [==============================] - 0s 992us/step - loss: 0.1796 - accuracy: 0.9301\n",
      "Epoch 100/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1926 - accuracy: 0.9281\n",
      "Epoch 101/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1717 - accuracy: 0.9392\n",
      "Epoch 102/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9392\n",
      "Epoch 103/1500\n",
      "93/93 [==============================] - 0s 984us/step - loss: 0.1696 - accuracy: 0.9429\n",
      "Epoch 104/1500\n",
      "93/93 [==============================] - 0s 998us/step - loss: 0.1787 - accuracy: 0.9308\n",
      "Epoch 105/1500\n",
      "93/93 [==============================] - 0s 998us/step - loss: 0.1680 - accuracy: 0.9382\n",
      "Epoch 106/1500\n",
      "93/93 [==============================] - 0s 976us/step - loss: 0.1784 - accuracy: 0.9315\n",
      "Epoch 107/1500\n",
      "93/93 [==============================] - 0s 989us/step - loss: 0.1770 - accuracy: 0.9351\n",
      "Epoch 108/1500\n",
      "93/93 [==============================] - 0s 981us/step - loss: 0.1714 - accuracy: 0.9382\n",
      "Epoch 109/1500\n",
      "93/93 [==============================] - 0s 971us/step - loss: 0.1658 - accuracy: 0.9362\n",
      "Epoch 110/1500\n",
      "93/93 [==============================] - 0s 959us/step - loss: 0.1744 - accuracy: 0.9345\n",
      "Epoch 111/1500\n",
      "93/93 [==============================] - 0s 999us/step - loss: 0.1768 - accuracy: 0.9308\n",
      "Epoch 112/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9311\n",
      "Epoch 113/1500\n",
      "93/93 [==============================] - 0s 991us/step - loss: 0.1702 - accuracy: 0.9348\n",
      "Epoch 114/1500\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.1595 - accuracy: 0.9415\n",
      "Epoch 115/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.9409\n",
      "Epoch 116/1500\n",
      "93/93 [==============================] - 0s 998us/step - loss: 0.1605 - accuracy: 0.9392\n",
      "Epoch 117/1500\n",
      "93/93 [==============================] - 0s 996us/step - loss: 0.1634 - accuracy: 0.9395\n",
      "Epoch 118/1500\n",
      "93/93 [==============================] - 0s 972us/step - loss: 0.1874 - accuracy: 0.9308\n",
      "Epoch 119/1500\n",
      "93/93 [==============================] - 0s 979us/step - loss: 0.1614 - accuracy: 0.9415\n",
      "Epoch 120/1500\n",
      "93/93 [==============================] - 0s 977us/step - loss: 0.1614 - accuracy: 0.9452\n",
      "Epoch 121/1500\n",
      "93/93 [==============================] - 0s 988us/step - loss: 0.1698 - accuracy: 0.9358\n",
      "Epoch 122/1500\n",
      "93/93 [==============================] - 0s 963us/step - loss: 0.1610 - accuracy: 0.9449\n",
      "Epoch 123/1500\n",
      "93/93 [==============================] - 0s 982us/step - loss: 0.1684 - accuracy: 0.9385\n",
      "Epoch 124/1500\n",
      "93/93 [==============================] - 0s 996us/step - loss: 0.1744 - accuracy: 0.9328\n",
      "Epoch 125/1500\n",
      "93/93 [==============================] - 0s 988us/step - loss: 0.1586 - accuracy: 0.9439\n",
      "Epoch 126/1500\n",
      "93/93 [==============================] - 0s 987us/step - loss: 0.1577 - accuracy: 0.9395\n",
      "Epoch 127/1500\n",
      "93/93 [==============================] - 0s 981us/step - loss: 0.1687 - accuracy: 0.9395\n",
      "Epoch 128/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1692 - accuracy: 0.9368\n",
      "Epoch 129/1500\n",
      "93/93 [==============================] - 0s 992us/step - loss: 0.1554 - accuracy: 0.9402\n",
      "Epoch 130/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9422\n",
      "Epoch 131/1500\n",
      "93/93 [==============================] - 0s 997us/step - loss: 0.1675 - accuracy: 0.9368\n",
      "Epoch 132/1500\n",
      "93/93 [==============================] - 0s 995us/step - loss: 0.1499 - accuracy: 0.9483\n",
      "Epoch 133/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9335\n",
      "Epoch 134/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9449\n",
      "Epoch 135/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9422\n",
      "Epoch 136/1500\n",
      "93/93 [==============================] - 0s 992us/step - loss: 0.1612 - accuracy: 0.9385\n",
      "Epoch 137/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9422\n",
      "Epoch 138/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9422\n",
      "Epoch 139/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9469\n",
      "Epoch 140/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1561 - accuracy: 0.9432\n",
      "Epoch 141/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9399\n",
      "Epoch 142/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9429\n",
      "Epoch 143/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9432\n",
      "Epoch 144/1500\n",
      "93/93 [==============================] - 0s 998us/step - loss: 0.1461 - accuracy: 0.9452\n",
      "Epoch 145/1500\n",
      "93/93 [==============================] - 0s 983us/step - loss: 0.1519 - accuracy: 0.9459\n",
      "Epoch 146/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1632 - accuracy: 0.9392\n",
      "Epoch 147/1500\n",
      "93/93 [==============================] - 0s 996us/step - loss: 0.1461 - accuracy: 0.9446\n",
      "Epoch 148/1500\n",
      "93/93 [==============================] - 0s 983us/step - loss: 0.1594 - accuracy: 0.9409\n",
      "Epoch 149/1500\n",
      "93/93 [==============================] - 0s 976us/step - loss: 0.1460 - accuracy: 0.9466\n",
      "Epoch 150/1500\n",
      "93/93 [==============================] - 0s 986us/step - loss: 0.1576 - accuracy: 0.9412\n",
      "Epoch 151/1500\n",
      "93/93 [==============================] - 0s 985us/step - loss: 0.1535 - accuracy: 0.9425\n",
      "Epoch 152/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.9459\n",
      "Epoch 153/1500\n",
      "93/93 [==============================] - 0s 993us/step - loss: 0.1438 - accuracy: 0.9469\n",
      "Epoch 154/1500\n",
      "93/93 [==============================] - 0s 992us/step - loss: 0.1378 - accuracy: 0.9486\n",
      "Epoch 155/1500\n",
      "93/93 [==============================] - 0s 979us/step - loss: 0.1456 - accuracy: 0.9456\n",
      "Epoch 156/1500\n",
      "93/93 [==============================] - 0s 976us/step - loss: 0.1601 - accuracy: 0.9469\n",
      "Epoch 157/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9519\n",
      "Epoch 158/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9486\n",
      "Epoch 159/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9462\n",
      "Epoch 160/1500\n",
      "93/93 [==============================] - 0s 973us/step - loss: 0.1325 - accuracy: 0.9543\n",
      "Epoch 161/1500\n",
      "93/93 [==============================] - 0s 981us/step - loss: 0.1435 - accuracy: 0.9456\n",
      "Epoch 162/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9462\n",
      "Epoch 163/1500\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.1456 - accuracy: 0.9472\n",
      "Epoch 164/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9546\n",
      "Epoch 165/1500\n",
      "93/93 [==============================] - 0s 979us/step - loss: 0.1349 - accuracy: 0.9550\n",
      "Epoch 166/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.9442\n",
      "Epoch 167/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9419\n",
      "Epoch 168/1500\n",
      "93/93 [==============================] - 0s 983us/step - loss: 0.1282 - accuracy: 0.9496\n",
      "Epoch 169/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9560\n",
      "Epoch 170/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9506\n",
      "Epoch 171/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9429\n",
      "Epoch 172/1500\n",
      "93/93 [==============================] - 0s 983us/step - loss: 0.1424 - accuracy: 0.9479\n",
      "Epoch 173/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9556\n",
      "Epoch 174/1500\n",
      "93/93 [==============================] - 0s 981us/step - loss: 0.1520 - accuracy: 0.9452\n",
      "Epoch 175/1500\n",
      "93/93 [==============================] - 0s 988us/step - loss: 0.1282 - accuracy: 0.9550\n",
      "Epoch 176/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.9506\n",
      "Epoch 177/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9503\n",
      "Epoch 178/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9506\n",
      "Epoch 179/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9580\n",
      "Epoch 180/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9523\n",
      "Epoch 181/1500\n",
      "93/93 [==============================] - 0s 985us/step - loss: 0.1290 - accuracy: 0.9530\n",
      "Epoch 182/1500\n",
      "93/93 [==============================] - 0s 986us/step - loss: 0.1340 - accuracy: 0.9483\n",
      "Epoch 183/1500\n",
      "93/93 [==============================] - 0s 993us/step - loss: 0.1342 - accuracy: 0.9493\n",
      "Epoch 184/1500\n",
      "93/93 [==============================] - 0s 961us/step - loss: 0.1394 - accuracy: 0.9456\n",
      "Epoch 185/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1446 - accuracy: 0.9496\n",
      "Epoch 186/1500\n",
      "93/93 [==============================] - 0s 978us/step - loss: 0.1361 - accuracy: 0.9506\n",
      "Epoch 187/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9526\n",
      "Epoch 188/1500\n",
      "93/93 [==============================] - 0s 999us/step - loss: 0.1370 - accuracy: 0.9493\n",
      "Epoch 189/1500\n",
      "93/93 [==============================] - 0s 985us/step - loss: 0.1297 - accuracy: 0.9546\n",
      "Epoch 190/1500\n",
      "93/93 [==============================] - 0s 970us/step - loss: 0.1284 - accuracy: 0.9546\n",
      "Epoch 191/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9540\n",
      "Epoch 192/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9496\n",
      "Epoch 193/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9506\n",
      "Epoch 194/1500\n",
      "93/93 [==============================] - 0s 996us/step - loss: 0.1363 - accuracy: 0.9489\n",
      "Epoch 195/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9546\n",
      "Epoch 196/1500\n",
      "93/93 [==============================] - 0s 980us/step - loss: 0.1248 - accuracy: 0.9550\n",
      "Epoch 197/1500\n",
      "93/93 [==============================] - 0s 974us/step - loss: 0.1242 - accuracy: 0.9546\n",
      "Epoch 198/1500\n",
      "93/93 [==============================] - 0s 996us/step - loss: 0.1214 - accuracy: 0.9560\n",
      "Epoch 199/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9509\n",
      "Epoch 200/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9587\n",
      "Epoch 201/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9553\n",
      "Epoch 202/1500\n",
      "93/93 [==============================] - 0s 997us/step - loss: 0.1290 - accuracy: 0.9560\n",
      "Epoch 203/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9486\n",
      "Epoch 204/1500\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.1363 - accuracy: 0.9506\n",
      "Epoch 205/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.9593\n",
      "Epoch 206/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9546\n",
      "Epoch 207/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9513\n",
      "Epoch 208/1500\n",
      "93/93 [==============================] - 0s 999us/step - loss: 0.1288 - accuracy: 0.9540\n",
      "Epoch 209/1500\n",
      "93/93 [==============================] - 0s 979us/step - loss: 0.1203 - accuracy: 0.9570\n",
      "Epoch 210/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9603\n",
      "Epoch 211/1500\n",
      "93/93 [==============================] - 0s 992us/step - loss: 0.1209 - accuracy: 0.9546\n",
      "Epoch 212/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9637\n",
      "Epoch 213/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9610\n",
      "Epoch 214/1500\n",
      "93/93 [==============================] - 0s 988us/step - loss: 0.1200 - accuracy: 0.9590\n",
      "Epoch 215/1500\n",
      "93/93 [==============================] - 0s 952us/step - loss: 0.1120 - accuracy: 0.9610\n",
      "Epoch 216/1500\n",
      "93/93 [==============================] - 0s 921us/step - loss: 0.1232 - accuracy: 0.9553\n",
      "Epoch 217/1500\n",
      "93/93 [==============================] - 0s 905us/step - loss: 0.1229 - accuracy: 0.9533\n",
      "Epoch 218/1500\n",
      "93/93 [==============================] - 0s 943us/step - loss: 0.1064 - accuracy: 0.9627\n",
      "Epoch 219/1500\n",
      "93/93 [==============================] - 0s 945us/step - loss: 0.1332 - accuracy: 0.9519\n",
      "Epoch 220/1500\n",
      "93/93 [==============================] - 0s 891us/step - loss: 0.1132 - accuracy: 0.9573\n",
      "Epoch 221/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9563\n",
      "Epoch 222/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9630\n",
      "Epoch 223/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9560\n",
      "Epoch 224/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9536\n",
      "Epoch 225/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.9483\n",
      "Epoch 226/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9637\n",
      "Epoch 227/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9523\n",
      "Epoch 228/1500\n",
      "93/93 [==============================] - 0s 980us/step - loss: 0.1096 - accuracy: 0.9647\n",
      "Epoch 229/1500\n",
      "93/93 [==============================] - 0s 995us/step - loss: 0.1172 - accuracy: 0.9597\n",
      "Epoch 230/1500\n",
      "93/93 [==============================] - 0s 997us/step - loss: 0.1286 - accuracy: 0.9516\n",
      "Epoch 231/1500\n",
      "93/93 [==============================] - 0s 995us/step - loss: 0.1150 - accuracy: 0.9580\n",
      "Epoch 232/1500\n",
      "93/93 [==============================] - 0s 995us/step - loss: 0.1174 - accuracy: 0.9526\n",
      "Epoch 233/1500\n",
      "93/93 [==============================] - 0s 998us/step - loss: 0.1126 - accuracy: 0.9587\n",
      "Epoch 234/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9590\n",
      "Epoch 235/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9614\n",
      "Epoch 236/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9600\n",
      "Epoch 237/1500\n",
      "93/93 [==============================] - 0s 988us/step - loss: 0.1303 - accuracy: 0.9503\n",
      "Epoch 238/1500\n",
      "93/93 [==============================] - 0s 996us/step - loss: 0.1119 - accuracy: 0.9583\n",
      "Epoch 239/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9583\n",
      "Epoch 240/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9617\n",
      "Epoch 241/1500\n",
      "93/93 [==============================] - 0s 985us/step - loss: 0.1137 - accuracy: 0.9573\n",
      "Epoch 242/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9624\n",
      "Epoch 243/1500\n",
      "93/93 [==============================] - 0s 990us/step - loss: 0.1065 - accuracy: 0.9617\n",
      "Epoch 244/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9614\n",
      "Epoch 245/1500\n",
      "93/93 [==============================] - 0s 993us/step - loss: 0.1309 - accuracy: 0.9513\n",
      "Epoch 246/1500\n",
      "93/93 [==============================] - 0s 999us/step - loss: 0.1193 - accuracy: 0.9580\n",
      "Epoch 247/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9644\n",
      "Epoch 248/1500\n",
      "93/93 [==============================] - 0s 990us/step - loss: 0.1108 - accuracy: 0.9620\n",
      "Epoch 249/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9600\n",
      "Epoch 250/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9627\n",
      "Epoch 251/1500\n",
      "93/93 [==============================] - 0s 980us/step - loss: 0.0998 - accuracy: 0.9603\n",
      "Epoch 252/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9657\n",
      "Epoch 253/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9627\n",
      "Epoch 254/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9560\n",
      "Epoch 255/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9657\n",
      "Epoch 256/1500\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.1024 - accuracy: 0.9630\n",
      "Epoch 257/1500\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1116 - accuracy: 0.9590\n",
      "Epoch 258/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9563\n",
      "Epoch 259/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9610\n",
      "Epoch 260/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9590\n",
      "Epoch 261/1500\n",
      "93/93 [==============================] - 0s 997us/step - loss: 0.1064 - accuracy: 0.9637\n",
      "Epoch 262/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9607\n",
      "Epoch 263/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9640\n",
      "Epoch 264/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9610\n",
      "Epoch 265/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9634\n",
      "Epoch 266/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9523\n",
      "Epoch 267/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9553\n",
      "Epoch 268/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9540\n",
      "Epoch 269/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9533\n",
      "Epoch 270/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9600\n",
      "Epoch 271/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9637\n",
      "Epoch 272/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9674\n",
      "Epoch 273/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9553\n",
      "Epoch 274/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9570\n",
      "Epoch 275/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9640\n",
      "Epoch 276/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9600\n",
      "Epoch 277/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9640\n",
      "Epoch 278/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9607\n",
      "Epoch 279/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9634\n",
      "Epoch 280/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9651\n",
      "Epoch 281/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.9620\n",
      "Epoch 282/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9610\n",
      "Epoch 283/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9556\n",
      "Epoch 284/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9607\n",
      "Epoch 285/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9583\n",
      "Epoch 286/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9644\n",
      "Epoch 287/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.9704\n",
      "Epoch 288/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9647\n",
      "Epoch 289/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9634\n",
      "Epoch 290/1500\n",
      "93/93 [==============================] - 0s 997us/step - loss: 0.1123 - accuracy: 0.9560\n",
      "Epoch 291/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9627\n",
      "Epoch 292/1500\n",
      "93/93 [==============================] - 0s 999us/step - loss: 0.0888 - accuracy: 0.9671\n",
      "Epoch 293/1500\n",
      "93/93 [==============================] - 0s 972us/step - loss: 0.1166 - accuracy: 0.9560\n",
      "Epoch 294/1500\n",
      "93/93 [==============================] - 0s 992us/step - loss: 0.0997 - accuracy: 0.9637\n",
      "Epoch 295/1500\n",
      "93/93 [==============================] - 0s 985us/step - loss: 0.0938 - accuracy: 0.9684\n",
      "Epoch 296/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9674\n",
      "Epoch 297/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9553\n",
      "Epoch 298/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9644\n",
      "Epoch 299/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.9590\n",
      "Epoch 300/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9610\n",
      "Epoch 301/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9624\n",
      "Epoch 302/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.9637\n",
      "Epoch 303/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9637\n",
      "Epoch 304/1500\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.0937 - accuracy: 0.9677\n",
      "Epoch 305/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9698\n",
      "Epoch 306/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9607\n",
      "Epoch 307/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9617\n",
      "Epoch 308/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.9610\n",
      "Epoch 309/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9620\n",
      "Epoch 310/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9567\n",
      "Epoch 311/1500\n",
      "93/93 [==============================] - 0s 990us/step - loss: 0.1000 - accuracy: 0.9651\n",
      "Epoch 312/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9600\n",
      "Epoch 313/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9614\n",
      "Epoch 314/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9607\n",
      "Epoch 315/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9698\n",
      "Epoch 316/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9684\n",
      "Epoch 317/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9688\n",
      "Epoch 318/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9684\n",
      "Epoch 319/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.9761\n",
      "Epoch 320/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9674\n",
      "Epoch 321/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9620\n",
      "Epoch 322/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9600\n",
      "Epoch 323/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9614\n",
      "Epoch 324/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9634\n",
      "Epoch 325/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9654\n",
      "Epoch 326/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.9644\n",
      "Epoch 327/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9674\n",
      "Epoch 328/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9647\n",
      "Epoch 329/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9610\n",
      "Epoch 330/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9644\n",
      "Epoch 331/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9704\n",
      "Epoch 332/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9704\n",
      "Epoch 333/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9728\n",
      "Epoch 334/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9671\n",
      "Epoch 335/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9607\n",
      "Epoch 336/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9704\n",
      "Epoch 337/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9731\n",
      "Epoch 338/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9694\n",
      "Epoch 339/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9640\n",
      "Epoch 340/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9671\n",
      "Epoch 341/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9661\n",
      "Epoch 342/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9654\n",
      "Epoch 343/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9677\n",
      "Epoch 344/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9708\n",
      "Epoch 345/1500\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.0937 - accuracy: 0.9688\n",
      "Epoch 346/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9644\n",
      "Epoch 347/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9691\n",
      "Epoch 348/1500\n",
      "93/93 [==============================] - 0s 999us/step - loss: 0.0951 - accuracy: 0.9640\n",
      "Epoch 349/1500\n",
      "51/93 [===============>..............] - ETA: 0s - loss: 0.0691 - accuracy: 0.9786Restoring model weights from the end of the best epoch: 319.\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9701\n",
      "Epoch 349: early stopping\n",
      "7/7 [==============================] - 0s 875us/step - loss: 1.0309 - accuracy: 0.7674\n",
      "7/7 [==============================] - 0s 717us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (25/30)\n",
      "Before appending - Cat IDs: 459, Predictions: 459, Actuals: 459, Gender: 459\n",
      "After appending - Cat IDs: 674, Predictions: 674, Actuals: 674, Gender: 674\n",
      "Final Test Results - Loss: 1.0309141874313354, Accuracy: 0.7674418687820435, Precision: 0.7341177341177341, Recall: 0.8131753224119903, F1 Score: 0.768310657596372\n",
      "Confusion Matrix:\n",
      " [[122   2  27]\n",
      " [  0   7   0]\n",
      " [ 21   0  36]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "001A    14\n",
      "042A    14\n",
      "059A    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "025A    11\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "005A    10\n",
      "016A    10\n",
      "014B    10\n",
      "071A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "015A     9\n",
      "045A     9\n",
      "022A     9\n",
      "065A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "050A     7\n",
      "099A     7\n",
      "109A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "070A     5\n",
      "021A     5\n",
      "075A     5\n",
      "009A     4\n",
      "104A     4\n",
      "105A     4\n",
      "003A     4\n",
      "113A     3\n",
      "056A     3\n",
      "064A     3\n",
      "060A     3\n",
      "014A     3\n",
      "006A     3\n",
      "025B     2\n",
      "102A     2\n",
      "011A     2\n",
      "061A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "018A     2\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "043A     1\n",
      "115A     1\n",
      "076A     1\n",
      "066A     1\n",
      "026C     1\n",
      "096A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "057A    27\n",
      "055A    20\n",
      "097B    14\n",
      "039A    12\n",
      "040A    10\n",
      "094A     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "023A     6\n",
      "053A     6\n",
      "108A     6\n",
      "023B     5\n",
      "026A     4\n",
      "062A     4\n",
      "052A     4\n",
      "035A     4\n",
      "058A     3\n",
      "012A     3\n",
      "069A     2\n",
      "048A     1\n",
      "088A     1\n",
      "004A     1\n",
      "073A     1\n",
      "041A     1\n",
      "092A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    306\n",
      "X    268\n",
      "F    158\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    94\n",
      "X    80\n",
      "M    31\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 106A, 104A, 059A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 097B, 062A, 039A, 023A, 027A, 069A, 026...\n",
      "kitten                                   [040A, 041A, 048A]\n",
      "senior                 [057A, 055A, 117A, 058A, 094A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 13, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 3, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '020A'\n",
      " '021A' '022A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '012A' '019B' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'101A'}\n",
      "Moved to Test Set:\n",
      "{'101A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A'\n",
      " '009A' '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A'\n",
      " '020A' '021A' '022A' '024A' '025A' '025B' '025C' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A'\n",
      " '097A' '099A' '100A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '012A' '019B' '023A' '023B' '026A' '027A' '031A' '035A' '039A'\n",
      " '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A' '069A'\n",
      " '073A' '088A' '092A' '094A' '097B' '101A' '108A' '117A']\n",
      "Length of X_train_val:\n",
      "756\n",
      "Length of y_train_val:\n",
      "756\n",
      "Length of groups_train_val:\n",
      "756\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     466\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     122\n",
      "senior     71\n",
      "kitten     12\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     490\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     98\n",
      "senior    71\n",
      "kitten    12\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1194, 1: 1083, 2: 711})\n",
      "Epoch 1/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.8790 - accuracy: 0.6292\n",
      "Epoch 2/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.6678 - accuracy: 0.7246\n",
      "Epoch 3/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.6190 - accuracy: 0.7477\n",
      "Epoch 4/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.5639 - accuracy: 0.7677\n",
      "Epoch 5/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.5405 - accuracy: 0.7805\n",
      "Epoch 6/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.5241 - accuracy: 0.7925\n",
      "Epoch 7/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4761 - accuracy: 0.8099\n",
      "Epoch 8/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4852 - accuracy: 0.8109\n",
      "Epoch 9/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4431 - accuracy: 0.8183\n",
      "Epoch 10/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4446 - accuracy: 0.8149\n",
      "Epoch 11/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4227 - accuracy: 0.8293\n",
      "Epoch 12/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4254 - accuracy: 0.8337\n",
      "Epoch 13/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4247 - accuracy: 0.8317\n",
      "Epoch 14/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4058 - accuracy: 0.8370\n",
      "Epoch 15/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.4104 - accuracy: 0.8343\n",
      "Epoch 16/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3804 - accuracy: 0.8504\n",
      "Epoch 17/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3944 - accuracy: 0.8414\n",
      "Epoch 18/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3647 - accuracy: 0.8574\n",
      "Epoch 19/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3796 - accuracy: 0.8444\n",
      "Epoch 20/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3814 - accuracy: 0.8444\n",
      "Epoch 21/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3803 - accuracy: 0.8541\n",
      "Epoch 22/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3600 - accuracy: 0.8581\n",
      "Epoch 23/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3497 - accuracy: 0.8591\n",
      "Epoch 24/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3517 - accuracy: 0.8601\n",
      "Epoch 25/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3409 - accuracy: 0.8638\n",
      "Epoch 26/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3457 - accuracy: 0.8614\n",
      "Epoch 27/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3414 - accuracy: 0.8584\n",
      "Epoch 28/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3562 - accuracy: 0.8558\n",
      "Epoch 29/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3281 - accuracy: 0.8641\n",
      "Epoch 30/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8598\n",
      "Epoch 31/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3407 - accuracy: 0.8668\n",
      "Epoch 32/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3323 - accuracy: 0.8655\n",
      "Epoch 33/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3288 - accuracy: 0.8701\n",
      "Epoch 34/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3073 - accuracy: 0.8775\n",
      "Epoch 35/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3064 - accuracy: 0.8725\n",
      "Epoch 36/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3112 - accuracy: 0.8809\n",
      "Epoch 37/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2994 - accuracy: 0.8792\n",
      "Epoch 38/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3146 - accuracy: 0.8722\n",
      "Epoch 39/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3118 - accuracy: 0.8735\n",
      "Epoch 40/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.3061 - accuracy: 0.8775\n",
      "Epoch 41/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2989 - accuracy: 0.8802\n",
      "Epoch 42/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2877 - accuracy: 0.8879\n",
      "Epoch 43/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2845 - accuracy: 0.8889\n",
      "Epoch 44/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2945 - accuracy: 0.8802\n",
      "Epoch 45/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2791 - accuracy: 0.8916\n",
      "Epoch 46/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2747 - accuracy: 0.8865\n",
      "Epoch 47/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2742 - accuracy: 0.8976\n",
      "Epoch 48/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2679 - accuracy: 0.8963\n",
      "Epoch 49/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2808 - accuracy: 0.8872\n",
      "Epoch 50/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2754 - accuracy: 0.8886\n",
      "Epoch 51/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2702 - accuracy: 0.8959\n",
      "Epoch 52/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2699 - accuracy: 0.8979\n",
      "Epoch 53/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2627 - accuracy: 0.8959\n",
      "Epoch 54/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2651 - accuracy: 0.9043\n",
      "Epoch 55/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2645 - accuracy: 0.8876\n",
      "Epoch 56/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2745 - accuracy: 0.8865\n",
      "Epoch 57/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2557 - accuracy: 0.8979\n",
      "Epoch 58/1500\n",
      "94/94 [==============================] - 0s 2ms/step - loss: 0.2512 - accuracy: 0.8966\n",
      "Epoch 59/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2464 - accuracy: 0.9029\n",
      "Epoch 60/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2688 - accuracy: 0.8942\n",
      "Epoch 61/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2586 - accuracy: 0.8989\n",
      "Epoch 62/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.8983\n",
      "Epoch 63/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2453 - accuracy: 0.9029\n",
      "Epoch 64/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2578 - accuracy: 0.8986\n",
      "Epoch 65/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2671 - accuracy: 0.8876\n",
      "Epoch 66/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2453 - accuracy: 0.9046\n",
      "Epoch 67/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2388 - accuracy: 0.9016\n",
      "Epoch 68/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9060\n",
      "Epoch 69/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2275 - accuracy: 0.9083\n",
      "Epoch 70/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.9083\n",
      "Epoch 71/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2417 - accuracy: 0.9019\n",
      "Epoch 72/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2613 - accuracy: 0.8956\n",
      "Epoch 73/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2316 - accuracy: 0.9120\n",
      "Epoch 74/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2316 - accuracy: 0.9096\n",
      "Epoch 75/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.9033\n",
      "Epoch 76/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2325 - accuracy: 0.9106\n",
      "Epoch 77/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2278 - accuracy: 0.9110\n",
      "Epoch 78/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2295 - accuracy: 0.9143\n",
      "Epoch 79/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2224 - accuracy: 0.9133\n",
      "Epoch 80/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2292 - accuracy: 0.9133\n",
      "Epoch 81/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2258 - accuracy: 0.9203\n",
      "Epoch 82/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.9076\n",
      "Epoch 83/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2218 - accuracy: 0.9150\n",
      "Epoch 84/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2178 - accuracy: 0.9127\n",
      "Epoch 85/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2248 - accuracy: 0.9110\n",
      "Epoch 86/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2084 - accuracy: 0.9200\n",
      "Epoch 87/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2096 - accuracy: 0.9227\n",
      "Epoch 88/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2194 - accuracy: 0.9157\n",
      "Epoch 89/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2167 - accuracy: 0.9183\n",
      "Epoch 90/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2052 - accuracy: 0.9234\n",
      "Epoch 91/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2165 - accuracy: 0.9133\n",
      "Epoch 92/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2156 - accuracy: 0.9177\n",
      "Epoch 93/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2112 - accuracy: 0.9190\n",
      "Epoch 94/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.9173\n",
      "Epoch 95/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9183\n",
      "Epoch 96/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9183\n",
      "Epoch 97/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2253 - accuracy: 0.9133\n",
      "Epoch 98/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9190\n",
      "Epoch 99/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2089 - accuracy: 0.9203\n",
      "Epoch 100/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9314\n",
      "Epoch 101/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2135 - accuracy: 0.9187\n",
      "Epoch 102/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2083 - accuracy: 0.9220\n",
      "Epoch 103/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1938 - accuracy: 0.9224\n",
      "Epoch 104/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9214\n",
      "Epoch 105/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2047 - accuracy: 0.9190\n",
      "Epoch 106/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2054 - accuracy: 0.9250\n",
      "Epoch 107/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9147\n",
      "Epoch 108/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1963 - accuracy: 0.9254\n",
      "Epoch 109/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9247\n",
      "Epoch 110/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1900 - accuracy: 0.9247\n",
      "Epoch 111/1500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1872 - accuracy: 0.9290\n",
      "Epoch 112/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9264\n",
      "Epoch 113/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2038 - accuracy: 0.9167\n",
      "Epoch 114/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9304\n",
      "Epoch 115/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1954 - accuracy: 0.9220\n",
      "Epoch 116/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1891 - accuracy: 0.9224\n",
      "Epoch 117/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1778 - accuracy: 0.9344\n",
      "Epoch 118/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9351\n",
      "Epoch 119/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1702 - accuracy: 0.9374\n",
      "Epoch 120/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.9321\n",
      "Epoch 121/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1678 - accuracy: 0.9378\n",
      "Epoch 122/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.9307\n",
      "Epoch 123/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9344\n",
      "Epoch 124/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9334\n",
      "Epoch 125/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.2086 - accuracy: 0.9203\n",
      "Epoch 126/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9260\n",
      "Epoch 127/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9364\n",
      "Epoch 128/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9317\n",
      "Epoch 129/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.9347\n",
      "Epoch 130/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.9334\n",
      "Epoch 131/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1809 - accuracy: 0.9354\n",
      "Epoch 132/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.9294\n",
      "Epoch 133/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9284\n",
      "Epoch 134/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9394\n",
      "Epoch 135/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.9391\n",
      "Epoch 136/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.9394\n",
      "Epoch 137/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1710 - accuracy: 0.9354\n",
      "Epoch 138/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1702 - accuracy: 0.9378\n",
      "Epoch 139/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9294\n",
      "Epoch 140/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.9274\n",
      "Epoch 141/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.9294\n",
      "Epoch 142/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9347\n",
      "Epoch 143/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9404\n",
      "Epoch 144/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9347\n",
      "Epoch 145/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.9347\n",
      "Epoch 146/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9331\n",
      "Epoch 147/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9394\n",
      "Epoch 148/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9357\n",
      "Epoch 149/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9304\n",
      "Epoch 150/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.9311\n",
      "Epoch 151/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9334\n",
      "Epoch 152/1500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1684 - accuracy: 0.9354\n",
      "Epoch 153/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9351\n",
      "Epoch 154/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9364\n",
      "Epoch 155/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.9495\n",
      "Epoch 156/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9364\n",
      "Epoch 157/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9408\n",
      "Epoch 158/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9404\n",
      "Epoch 159/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.9347\n",
      "Epoch 160/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.9394\n",
      "Epoch 161/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9418\n",
      "Epoch 162/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9411\n",
      "Epoch 163/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9371\n",
      "Epoch 164/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1661 - accuracy: 0.9381\n",
      "Epoch 165/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9334\n",
      "Epoch 166/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1655 - accuracy: 0.9341\n",
      "Epoch 167/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9478\n",
      "Epoch 168/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1602 - accuracy: 0.9378\n",
      "Epoch 169/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9327\n",
      "Epoch 170/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1582 - accuracy: 0.9391\n",
      "Epoch 171/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1587 - accuracy: 0.9401\n",
      "Epoch 172/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9444\n",
      "Epoch 173/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9461\n",
      "Epoch 174/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9408\n",
      "Epoch 175/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.9431\n",
      "Epoch 176/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9454\n",
      "Epoch 177/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9394\n",
      "Epoch 178/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9434\n",
      "Epoch 179/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9414\n",
      "Epoch 180/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.9347\n",
      "Epoch 181/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9341\n",
      "Epoch 182/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9505\n",
      "Epoch 183/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1608 - accuracy: 0.9418\n",
      "Epoch 184/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9404\n",
      "Epoch 185/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1678 - accuracy: 0.9378\n",
      "Epoch 186/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9465\n",
      "Epoch 187/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9508\n",
      "Epoch 188/1500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1397 - accuracy: 0.9531\n",
      "Epoch 189/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9438\n",
      "Epoch 190/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9438\n",
      "Epoch 191/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9431\n",
      "Epoch 192/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.9461\n",
      "Epoch 193/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9444\n",
      "Epoch 194/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.9418\n",
      "Epoch 195/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9454\n",
      "Epoch 196/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.9475\n",
      "Epoch 197/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1535 - accuracy: 0.9471\n",
      "Epoch 198/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.9498\n",
      "Epoch 199/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9468\n",
      "Epoch 200/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9518\n",
      "Epoch 201/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9454\n",
      "Epoch 202/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9485\n",
      "Epoch 203/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1561 - accuracy: 0.9388\n",
      "Epoch 204/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9511\n",
      "Epoch 205/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9444\n",
      "Epoch 206/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9521\n",
      "Epoch 207/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9485\n",
      "Epoch 208/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9495\n",
      "Epoch 209/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.9414\n",
      "Epoch 210/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.9424\n",
      "Epoch 211/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9481\n",
      "Epoch 212/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1422 - accuracy: 0.9485\n",
      "Epoch 213/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9454\n",
      "Epoch 214/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9488\n",
      "Epoch 215/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9508\n",
      "Epoch 216/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9508\n",
      "Epoch 217/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1628 - accuracy: 0.9404\n",
      "Epoch 218/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9481\n",
      "Epoch 219/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9518\n",
      "Epoch 220/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9475\n",
      "Epoch 221/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9465\n",
      "Epoch 222/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9485\n",
      "Epoch 223/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9508\n",
      "Epoch 224/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9525\n",
      "Epoch 225/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9404\n",
      "Epoch 226/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9528\n",
      "Epoch 227/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9488\n",
      "Epoch 228/1500\n",
      "94/94 [==============================] - 0s 4ms/step - loss: 0.1439 - accuracy: 0.9468\n",
      "Epoch 229/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9468\n",
      "Epoch 230/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.9461\n",
      "Epoch 231/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9511\n",
      "Epoch 232/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9505\n",
      "Epoch 233/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9608\n",
      "Epoch 234/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9505\n",
      "Epoch 235/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9521\n",
      "Epoch 236/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9518\n",
      "Epoch 237/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9471\n",
      "Epoch 238/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9541\n",
      "Epoch 239/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9545\n",
      "Epoch 240/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9588\n",
      "Epoch 241/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9465\n",
      "Epoch 242/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9528\n",
      "Epoch 243/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9491\n",
      "Epoch 244/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9511\n",
      "Epoch 245/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9598\n",
      "Epoch 246/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9448\n",
      "Epoch 247/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.9518\n",
      "Epoch 248/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9528\n",
      "Epoch 249/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9525\n",
      "Epoch 250/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9555\n",
      "Epoch 251/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9541\n",
      "Epoch 252/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9525\n",
      "Epoch 253/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9531\n",
      "Epoch 254/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9515\n",
      "Epoch 255/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9548\n",
      "Epoch 256/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9562\n",
      "Epoch 257/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9505\n",
      "Epoch 258/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9515\n",
      "Epoch 259/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9495\n",
      "Epoch 260/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.9531\n",
      "Epoch 261/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9501\n",
      "Epoch 262/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9535\n",
      "Epoch 263/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9545\n",
      "Epoch 264/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9552\n",
      "Epoch 265/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9558\n",
      "Epoch 266/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9562\n",
      "Epoch 267/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9501\n",
      "Epoch 268/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9498\n",
      "Epoch 269/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9562\n",
      "Epoch 270/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9592\n",
      "Epoch 271/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9632\n",
      "Epoch 272/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9582\n",
      "Epoch 273/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9498\n",
      "Epoch 274/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.9438\n",
      "Epoch 275/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9505\n",
      "Epoch 276/1500\n",
      "94/94 [==============================] - 0s 3ms/step - loss: 0.1191 - accuracy: 0.9585\n",
      "Epoch 277/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9582\n",
      "Epoch 278/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9531\n",
      "Epoch 279/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9612\n",
      "Epoch 280/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9545\n",
      "Epoch 281/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9568\n",
      "Epoch 282/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9528\n",
      "Epoch 283/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9572\n",
      "Epoch 284/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9548\n",
      "Epoch 285/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9585\n",
      "Epoch 286/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9595\n",
      "Epoch 287/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9488\n",
      "Epoch 288/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.9485\n",
      "Epoch 289/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9508\n",
      "Epoch 290/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9491\n",
      "Epoch 291/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9605\n",
      "Epoch 292/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9575\n",
      "Epoch 293/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9531\n",
      "Epoch 294/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9585\n",
      "Epoch 295/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9582\n",
      "Epoch 296/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9605\n",
      "Epoch 297/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9565\n",
      "Epoch 298/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9498\n",
      "Epoch 299/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9552\n",
      "Epoch 300/1500\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9531\n",
      "Epoch 301/1500\n",
      "91/94 [============================>.] - ETA: 0s - loss: 0.1261 - accuracy: 0.9526Restoring model weights from the end of the best epoch: 271.\n",
      "94/94 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9525\n",
      "Epoch 301: early stopping\n",
      "6/6 [==============================] - 0s 893us/step - loss: 1.3520 - accuracy: 0.6409\n",
      "6/6 [==============================] - 0s 831us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 674, Predictions: 674, Actuals: 674, Gender: 674\n",
      "After appending - Cat IDs: 855, Predictions: 855, Actuals: 855, Gender: 855\n",
      "Final Test Results - Loss: 1.3519936800003052, Accuracy: 0.6408839821815491, Precision: 0.7207015409570153, Recall: 0.6885679154290824, F1 Score: 0.6761401455163075\n",
      "Confusion Matrix:\n",
      " [[85  1 12]\n",
      " [ 1 11  0]\n",
      " [51  0 20]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6595873240397788\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.9847010672092438\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.704336866736412\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6593993443769275\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7008245247490055\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[1]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2845ad-17c1-494a-8bd0-971aefca9f01",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3910db95-6772-4098-bef1-fb5857901e5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 855, Predictions: 855, Actuals: 855, Gender: 855\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d543c7c2-c11b-4511-ba5a-c52969a61a62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6b6f2173-89a8-40ba-afa6-410005c2dcb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.72 (79/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "aa8d9ba5-9d96-4aee-b3e2-ba87553d1899",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0b4ff1f7-d22d-4409-98d4-49e9a82bcb58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, kitten, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, senior, senior, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, adult, adult, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[senior, adult, senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, kitten, senior, senior, adult, ...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "65    059A  [senior, adult, senior, adult, senior, senior,...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "1     001A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "48    042A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "77    071A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "80    074A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [senior, senior, adult]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, senior, senior, adult, senior, ...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "29    025B                                    [senior, adult]         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, adult, adult, adult, se...         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "43    037A       [adult, senior, adult, adult, adult, senior]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "18    016A  [senior, adult, adult, senior, senior, senior,...        senior           senior                   True\n",
       "17    015A  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                     [senior, adult, senior, adult]         adult            adult                   True\n",
       "40    034A               [adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, se...         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "98    103A  [adult, adult, senior, adult, senior, senior, ...        senior            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "102   108A         [adult, adult, adult, adult, adult, adult]         adult           senior                  False\n",
       "101   106A  [adult, senior, senior, senior, adult, adult, ...         adult           senior                  False\n",
       "6     005A  [senior, adult, senior, senior, senior, senior...        senior            adult                  False\n",
       "103   109A      [adult, adult, kitten, kitten, kitten, adult]         adult           kitten                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "88    093A                                    [adult, senior]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "42    036A  [senior, senior, senior, senior, adult, senior...        senior            adult                  False\n",
       "39    033A  [kitten, adult, kitten, kitten, adult, adult, ...        kitten            adult                  False\n",
       "52    047A  [adult, adult, adult, kitten, kitten, adult, k...         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "56    051A  [adult, senior, adult, adult, senior, senior, ...         adult           senior                  False\n",
       "57    051B  [senior, adult, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "61    055A  [adult, adult, senior, adult, senior, adult, a...         adult           senior                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                    [kitten, adult]         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "69    063A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten            adult                  False\n",
       "30    025C            [senior, adult, senior, senior, senior]        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "81    075A             [senior, adult, senior, senior, adult]        senior            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5ec47984-bc63-4f3f-a7a4-d3979dbd4c52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     59\n",
      "kitten    11\n",
      "senior     9\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "952d49d2-180d-4f36-85b0-6e2b96610559",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             59  80.821918\n",
      "1           kitten           15             11  73.333333\n",
      "2           senior           22              9  40.909091\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "6a94de06-f9df-49f6-99e7-abc9024f3dc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885dd133-e156-4b01-8e84-4d94546e0eca",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "dd0127df-e0fb-4862-9ac4-2113bd346e78",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          564            434  76.950355\n",
      "1           kitten          113             90  79.646018\n",
      "2           senior          178             81  45.505618\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "4a0df220-5e5c-4c46-8474-ac4ecf291071",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a7528-860f-45b0-8c00-d0ddf10a0d8f",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2924eee7-fd8b-4cee-9e43-bb0352e4c43b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    228      168     73.68\n",
      "1          M    337      243     72.11\n",
      "2          X    290      194     66.90\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b7b2139e-f100-44e6-aff4-807f96753499",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58f49-cac6-40b1-8422-e3d95576c453",
   "metadata": {},
   "source": [
    "# RANDOM SEED 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "bf9d1b64-575e-47ca-bf37-e8916438d506",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    1020\n",
      "kitten     992\n",
      "adult      842\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[2]))\n",
    "np.random.seed(int(random_seeds[2]))\n",
    "tf.random.set_seed(int(random_seeds[2]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_3.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "91d5db1d-6c9f-4047-97b9-5a0eef4fbaaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "e435598c-cabc-4129-8504-68aac9cc06bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29b526-5098-4ce6-bc77-a93d1e6e1397",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "79a922e6-198c-4c43-a6f2-90dc580ae875",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "071A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "033A     9\n",
      "072A     9\n",
      "015A     9\n",
      "022A     9\n",
      "051B     9\n",
      "065A     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "108A     6\n",
      "109A     6\n",
      "037A     6\n",
      "007A     6\n",
      "008A     6\n",
      "025C     5\n",
      "070A     5\n",
      "021A     5\n",
      "034A     5\n",
      "075A     5\n",
      "023B     5\n",
      "035A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "058A     3\n",
      "014A     3\n",
      "011A     2\n",
      "061A     2\n",
      "032A     2\n",
      "093A     2\n",
      "025B     2\n",
      "087A     2\n",
      "069A     2\n",
      "073A     1\n",
      "115A     1\n",
      "088A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "019B     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Group Distribution:\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "005A    10\n",
      "045A     9\n",
      "117A     7\n",
      "053A     6\n",
      "023A     6\n",
      "044A     5\n",
      "105A     4\n",
      "052A     4\n",
      "104A     4\n",
      "018A     2\n",
      "054A     2\n",
      "038A     2\n",
      "102A     2\n",
      "091A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    265\n",
      "M    256\n",
      "F    198\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    83\n",
      "M    81\n",
      "F    54\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 103A, 071A, 028A, 067...\n",
      "kitten    [014B, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [001A, 097B, 019A, 074A, 029A, 005A, 091A, 023...\n",
      "kitten                             [044A, 111A, 045A, 110A]\n",
      "senior                             [104A, 054A, 117A, 051A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "Length of X_train_val:\n",
      "719\n",
      "Length of y_train_val:\n",
      "719\n",
      "Length of groups_train_val:\n",
      "719\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({2: 1065, 0: 1026, 1: 971})\n",
      "Epoch 1/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.8407 - accuracy: 0.6401\n",
      "Epoch 2/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.6704 - accuracy: 0.7266\n",
      "Epoch 3/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.5952 - accuracy: 0.7698\n",
      "Epoch 4/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.5702 - accuracy: 0.7652\n",
      "Epoch 5/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.5200 - accuracy: 0.7982\n",
      "Epoch 6/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.5104 - accuracy: 0.8027\n",
      "Epoch 7/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4949 - accuracy: 0.8070\n",
      "Epoch 8/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4545 - accuracy: 0.8197\n",
      "Epoch 9/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4712 - accuracy: 0.8174\n",
      "Epoch 10/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4257 - accuracy: 0.8357\n",
      "Epoch 11/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4298 - accuracy: 0.8377\n",
      "Epoch 12/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4389 - accuracy: 0.8259\n",
      "Epoch 13/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3999 - accuracy: 0.8416\n",
      "Epoch 14/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3994 - accuracy: 0.8488\n",
      "Epoch 15/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.4022 - accuracy: 0.8334\n",
      "Epoch 16/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3927 - accuracy: 0.8488\n",
      "Epoch 17/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3808 - accuracy: 0.8521\n",
      "Epoch 18/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3555 - accuracy: 0.8602\n",
      "Epoch 19/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3535 - accuracy: 0.8628\n",
      "Epoch 20/1500\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.3609 - accuracy: 0.8586\n",
      "Epoch 21/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3432 - accuracy: 0.8592\n",
      "Epoch 22/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3343 - accuracy: 0.8713\n",
      "Epoch 23/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3290 - accuracy: 0.8668\n",
      "Epoch 24/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3384 - accuracy: 0.8671\n",
      "Epoch 25/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3329 - accuracy: 0.8694\n",
      "Epoch 26/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3179 - accuracy: 0.8769\n",
      "Epoch 27/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8654\n",
      "Epoch 28/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3139 - accuracy: 0.8785\n",
      "Epoch 29/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3122 - accuracy: 0.8801\n",
      "Epoch 30/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3033 - accuracy: 0.8805\n",
      "Epoch 31/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2950 - accuracy: 0.8867\n",
      "Epoch 32/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3022 - accuracy: 0.8834\n",
      "Epoch 33/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3051 - accuracy: 0.8837\n",
      "Epoch 34/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2962 - accuracy: 0.8808\n",
      "Epoch 35/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.3051 - accuracy: 0.8834\n",
      "Epoch 36/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2841 - accuracy: 0.8932\n",
      "Epoch 37/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2904 - accuracy: 0.8867\n",
      "Epoch 38/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2766 - accuracy: 0.8935\n",
      "Epoch 39/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2906 - accuracy: 0.8873\n",
      "Epoch 40/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2608 - accuracy: 0.8994\n",
      "Epoch 41/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2826 - accuracy: 0.8903\n",
      "Epoch 42/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2818 - accuracy: 0.8890\n",
      "Epoch 43/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2560 - accuracy: 0.9001\n",
      "Epoch 44/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2778 - accuracy: 0.8971\n",
      "Epoch 45/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2648 - accuracy: 0.9001\n",
      "Epoch 46/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2543 - accuracy: 0.9043\n",
      "Epoch 47/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2624 - accuracy: 0.8965\n",
      "Epoch 48/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2495 - accuracy: 0.9056\n",
      "Epoch 49/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.8916\n",
      "Epoch 50/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2735 - accuracy: 0.8968\n",
      "Epoch 51/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2415 - accuracy: 0.9076\n",
      "Epoch 52/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2343 - accuracy: 0.9086\n",
      "Epoch 53/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2408 - accuracy: 0.9040\n",
      "Epoch 54/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2506 - accuracy: 0.9046\n",
      "Epoch 55/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2620 - accuracy: 0.8961\n",
      "Epoch 56/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2481 - accuracy: 0.8994\n",
      "Epoch 57/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2449 - accuracy: 0.9099\n",
      "Epoch 58/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.9027\n",
      "Epoch 59/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2369 - accuracy: 0.9017\n",
      "Epoch 60/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.9095\n",
      "Epoch 61/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2205 - accuracy: 0.9203\n",
      "Epoch 62/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2409 - accuracy: 0.9079\n",
      "Epoch 63/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2272 - accuracy: 0.9105\n",
      "Epoch 64/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2240 - accuracy: 0.9197\n",
      "Epoch 65/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2300 - accuracy: 0.9125\n",
      "Epoch 66/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2275 - accuracy: 0.9082\n",
      "Epoch 67/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2112 - accuracy: 0.9177\n",
      "Epoch 68/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2253 - accuracy: 0.9095\n",
      "Epoch 69/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9206\n",
      "Epoch 70/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2234 - accuracy: 0.9193\n",
      "Epoch 71/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2193 - accuracy: 0.9138\n",
      "Epoch 72/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2158 - accuracy: 0.9197\n",
      "Epoch 73/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9298\n",
      "Epoch 74/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2100 - accuracy: 0.9197\n",
      "Epoch 75/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2160 - accuracy: 0.9154\n",
      "Epoch 76/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2196 - accuracy: 0.9190\n",
      "Epoch 77/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2038 - accuracy: 0.9246\n",
      "Epoch 78/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2151 - accuracy: 0.9141\n",
      "Epoch 79/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2190 - accuracy: 0.9193\n",
      "Epoch 80/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9229\n",
      "Epoch 81/1500\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1986 - accuracy: 0.9255\n",
      "Epoch 82/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9203\n",
      "Epoch 83/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9259\n",
      "Epoch 84/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1972 - accuracy: 0.9249\n",
      "Epoch 85/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1947 - accuracy: 0.9242\n",
      "Epoch 86/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1981 - accuracy: 0.9252\n",
      "Epoch 87/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2063 - accuracy: 0.9223\n",
      "Epoch 88/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.9360\n",
      "Epoch 89/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.9229\n",
      "Epoch 90/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1936 - accuracy: 0.9249\n",
      "Epoch 91/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1950 - accuracy: 0.9311\n",
      "Epoch 92/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.2012 - accuracy: 0.9262\n",
      "Epoch 93/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9268\n",
      "Epoch 94/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1970 - accuracy: 0.9295\n",
      "Epoch 95/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.9275\n",
      "Epoch 96/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.9288\n",
      "Epoch 97/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1779 - accuracy: 0.9357\n",
      "Epoch 98/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1932 - accuracy: 0.9275\n",
      "Epoch 99/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9236\n",
      "Epoch 100/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1731 - accuracy: 0.9376\n",
      "Epoch 101/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.9337\n",
      "Epoch 102/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9285\n",
      "Epoch 103/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9327\n",
      "Epoch 104/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1773 - accuracy: 0.9304\n",
      "Epoch 105/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9304\n",
      "Epoch 106/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9363\n",
      "Epoch 107/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1908 - accuracy: 0.9278\n",
      "Epoch 108/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9376\n",
      "Epoch 109/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1773 - accuracy: 0.9304\n",
      "Epoch 110/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9344\n",
      "Epoch 111/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9334\n",
      "Epoch 112/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.9298\n",
      "Epoch 113/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.9278\n",
      "Epoch 114/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9353\n",
      "Epoch 115/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9428\n",
      "Epoch 116/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9379\n",
      "Epoch 117/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1676 - accuracy: 0.9370\n",
      "Epoch 118/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.9409\n",
      "Epoch 119/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9393\n",
      "Epoch 120/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1721 - accuracy: 0.9357\n",
      "Epoch 121/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9337\n",
      "Epoch 122/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1695 - accuracy: 0.9344\n",
      "Epoch 123/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1726 - accuracy: 0.9389\n",
      "Epoch 124/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9357\n",
      "Epoch 125/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1698 - accuracy: 0.9412\n",
      "Epoch 126/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9363\n",
      "Epoch 127/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9340\n",
      "Epoch 128/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9393\n",
      "Epoch 129/1500\n",
      "96/96 [==============================] - 0s 997us/step - loss: 0.1648 - accuracy: 0.9366\n",
      "Epoch 130/1500\n",
      "96/96 [==============================] - 0s 3ms/step - loss: 0.1615 - accuracy: 0.9389\n",
      "Epoch 131/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9409\n",
      "Epoch 132/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9366\n",
      "Epoch 133/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9360\n",
      "Epoch 134/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1534 - accuracy: 0.9445\n",
      "Epoch 135/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.9347\n",
      "Epoch 136/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9474\n",
      "Epoch 137/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9399\n",
      "Epoch 138/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9448\n",
      "Epoch 139/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9445\n",
      "Epoch 140/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9412\n",
      "Epoch 141/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1972 - accuracy: 0.9285\n",
      "Epoch 142/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.9321\n",
      "Epoch 143/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9461\n",
      "Epoch 144/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9350\n",
      "Epoch 145/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1525 - accuracy: 0.9419\n",
      "Epoch 146/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9428\n",
      "Epoch 147/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9432\n",
      "Epoch 148/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9445\n",
      "Epoch 149/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9406\n",
      "Epoch 150/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.9432\n",
      "Epoch 151/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9419\n",
      "Epoch 152/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9389\n",
      "Epoch 153/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9435\n",
      "Epoch 154/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9438\n",
      "Epoch 155/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9500\n",
      "Epoch 156/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9412\n",
      "Epoch 157/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9438\n",
      "Epoch 158/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9448\n",
      "Epoch 159/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9530\n",
      "Epoch 160/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9373\n",
      "Epoch 161/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9344\n",
      "Epoch 162/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1751 - accuracy: 0.9347\n",
      "Epoch 163/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9487\n",
      "Epoch 164/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9500\n",
      "Epoch 165/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9350\n",
      "Epoch 166/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9402\n",
      "Epoch 167/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9461\n",
      "Epoch 168/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9415\n",
      "Epoch 169/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9517\n",
      "Epoch 170/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9406\n",
      "Epoch 171/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9458\n",
      "Epoch 172/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1388 - accuracy: 0.9487\n",
      "Epoch 173/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9474\n",
      "Epoch 174/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1523 - accuracy: 0.9393\n",
      "Epoch 175/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9494\n",
      "Epoch 176/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9442\n",
      "Epoch 177/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1590 - accuracy: 0.9419\n",
      "Epoch 178/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9357\n",
      "Epoch 179/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9500\n",
      "Epoch 180/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9497\n",
      "Epoch 181/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9481\n",
      "Epoch 182/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9445\n",
      "Epoch 183/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9491\n",
      "Epoch 184/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9520\n",
      "Epoch 185/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9491\n",
      "Epoch 186/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9520\n",
      "Epoch 187/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1558 - accuracy: 0.9451\n",
      "Epoch 188/1500\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9428\n",
      "Epoch 189/1500\n",
      "95/96 [============================>.] - ETA: 0s - loss: 0.1586 - accuracy: 0.9411Restoring model weights from the end of the best epoch: 159.\n",
      "96/96 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9415\n",
      "Epoch 189: early stopping\n",
      "7/7 [==============================] - 0s 805us/step - loss: 1.1298 - accuracy: 0.6560\n",
      "7/7 [==============================] - 0s 660us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (18/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "Final Test Results - Loss: 1.1298319101333618, Accuracy: 0.6559633016586304, Precision: 0.603061475409836, Recall: 0.6491341991341991, F1 Score: 0.6035730196917045\n",
      "Confusion Matrix:\n",
      " [[108   6  51]\n",
      " [  3  25   0]\n",
      " [ 14   1  10]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "072A     9\n",
      "033A     9\n",
      "045A     9\n",
      "022A     9\n",
      "094A     8\n",
      "013B     8\n",
      "010A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "007A     6\n",
      "037A     6\n",
      "053A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "021A     5\n",
      "035A     4\n",
      "003A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "009A     4\n",
      "105A     4\n",
      "058A     3\n",
      "064A     3\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "014A     3\n",
      "093A     2\n",
      "025B     2\n",
      "038A     2\n",
      "087A     2\n",
      "102A     2\n",
      "032A     2\n",
      "054A     2\n",
      "018A     2\n",
      "115A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "110A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "088A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "042A    14\n",
      "106A    14\n",
      "116A    12\n",
      "039A    12\n",
      "063A    11\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "014B    10\n",
      "065A     9\n",
      "015A     9\n",
      "051B     9\n",
      "095A     8\n",
      "070A     5\n",
      "075A     5\n",
      "023B     5\n",
      "062A     4\n",
      "060A     3\n",
      "011A     2\n",
      "069A     2\n",
      "061A     2\n",
      "043A     1\n",
      "092A     1\n",
      "076A     1\n",
      "004A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    241\n",
      "F    181\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "X    75\n",
      "F    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 001A, 103A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 111A, 046A, 047A, 109A, 050A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 104A, 055A, 059A, 113A, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 071A, 062A, 002B, 095A, 065A, 039...\n",
      "kitten                             [014B, 040A, 042A, 043A]\n",
      "senior                 [106A, 116A, 051B, 016A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 12, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 4, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '018A' '019A' '020A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '004A' '011A' '014B' '015A' '016A' '019B' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'007A'}\n",
      "Moved to Test Set:\n",
      "{'007A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '018A' '019A' '020A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '007A' '011A' '014B' '015A' '016A' '019B' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "Length of X_train_val:\n",
      "728\n",
      "Length of y_train_val:\n",
      "728\n",
      "Length of groups_train_val:\n",
      "728\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     430\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     158\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     463\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     125\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1125, 1: 944, 2: 881})\n",
      "Epoch 1/1500\n",
      "93/93 [==============================] - 1s 3ms/step - loss: 0.9233 - accuracy: 0.6149\n",
      "Epoch 2/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.7421 - accuracy: 0.6878\n",
      "Epoch 3/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.6474 - accuracy: 0.7359\n",
      "Epoch 4/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.6048 - accuracy: 0.7505\n",
      "Epoch 5/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.5833 - accuracy: 0.7580\n",
      "Epoch 6/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.5539 - accuracy: 0.7685\n",
      "Epoch 7/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.5363 - accuracy: 0.7912\n",
      "Epoch 8/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.5106 - accuracy: 0.7878\n",
      "Epoch 9/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.5107 - accuracy: 0.7861\n",
      "Epoch 10/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.4660 - accuracy: 0.8119\n",
      "Epoch 11/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.4655 - accuracy: 0.8058\n",
      "Epoch 12/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.4627 - accuracy: 0.8153\n",
      "Epoch 13/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.4455 - accuracy: 0.8190\n",
      "Epoch 14/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.4400 - accuracy: 0.8227\n",
      "Epoch 15/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.4183 - accuracy: 0.8247\n",
      "Epoch 16/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.4302 - accuracy: 0.8207\n",
      "Epoch 17/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.4108 - accuracy: 0.8295\n",
      "Epoch 18/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3826 - accuracy: 0.8481\n",
      "Epoch 19/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3902 - accuracy: 0.8373\n",
      "Epoch 20/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3824 - accuracy: 0.8407\n",
      "Epoch 21/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8441\n",
      "Epoch 22/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8553\n",
      "Epoch 23/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3708 - accuracy: 0.8451\n",
      "Epoch 24/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3413 - accuracy: 0.8658\n",
      "Epoch 25/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3335 - accuracy: 0.8705\n",
      "Epoch 26/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3578 - accuracy: 0.8654\n",
      "Epoch 27/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3389 - accuracy: 0.8583\n",
      "Epoch 28/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3476 - accuracy: 0.8583\n",
      "Epoch 29/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3432 - accuracy: 0.8590\n",
      "Epoch 30/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3156 - accuracy: 0.8708\n",
      "Epoch 31/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3121 - accuracy: 0.8732\n",
      "Epoch 32/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2959 - accuracy: 0.8827\n",
      "Epoch 33/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3161 - accuracy: 0.8766\n",
      "Epoch 34/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2933 - accuracy: 0.8851\n",
      "Epoch 35/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8722\n",
      "Epoch 36/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2964 - accuracy: 0.8841\n",
      "Epoch 37/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2947 - accuracy: 0.8861\n",
      "Epoch 38/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2706 - accuracy: 0.8990\n",
      "Epoch 39/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2980 - accuracy: 0.8854\n",
      "Epoch 40/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2895 - accuracy: 0.8875\n",
      "Epoch 41/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2795 - accuracy: 0.8875\n",
      "Epoch 42/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2825 - accuracy: 0.8898\n",
      "Epoch 43/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2809 - accuracy: 0.8851\n",
      "Epoch 44/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2642 - accuracy: 0.8942\n",
      "Epoch 45/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2899 - accuracy: 0.8844\n",
      "Epoch 46/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2718 - accuracy: 0.8990\n",
      "Epoch 47/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2681 - accuracy: 0.8946\n",
      "Epoch 48/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2689 - accuracy: 0.8936\n",
      "Epoch 49/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.9041\n",
      "Epoch 50/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2657 - accuracy: 0.8976\n",
      "Epoch 51/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2494 - accuracy: 0.9034\n",
      "Epoch 52/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2570 - accuracy: 0.8980\n",
      "Epoch 53/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2425 - accuracy: 0.9047\n",
      "Epoch 54/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2404 - accuracy: 0.9068\n",
      "Epoch 55/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.9027\n",
      "Epoch 56/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2479 - accuracy: 0.9041\n",
      "Epoch 57/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2343 - accuracy: 0.9115\n",
      "Epoch 58/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2212 - accuracy: 0.9210\n",
      "Epoch 59/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2238 - accuracy: 0.9156\n",
      "Epoch 60/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2353 - accuracy: 0.9129\n",
      "Epoch 61/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2336 - accuracy: 0.9153\n",
      "Epoch 62/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2386 - accuracy: 0.9078\n",
      "Epoch 63/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2208 - accuracy: 0.9146\n",
      "Epoch 64/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2260 - accuracy: 0.9081\n",
      "Epoch 65/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2286 - accuracy: 0.9075\n",
      "Epoch 66/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2262 - accuracy: 0.9159\n",
      "Epoch 67/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.9203\n",
      "Epoch 68/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2346 - accuracy: 0.9105\n",
      "Epoch 69/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2224 - accuracy: 0.9183\n",
      "Epoch 70/1500\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.2166 - accuracy: 0.9224\n",
      "Epoch 71/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9132\n",
      "Epoch 72/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.9156\n",
      "Epoch 73/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2160 - accuracy: 0.9207\n",
      "Epoch 74/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2188 - accuracy: 0.9217\n",
      "Epoch 75/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2189 - accuracy: 0.9186\n",
      "Epoch 76/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.9234\n",
      "Epoch 77/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9234\n",
      "Epoch 78/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2079 - accuracy: 0.9227\n",
      "Epoch 79/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2009 - accuracy: 0.9234\n",
      "Epoch 80/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2184 - accuracy: 0.9197\n",
      "Epoch 81/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2097 - accuracy: 0.9210\n",
      "Epoch 82/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2010 - accuracy: 0.9234\n",
      "Epoch 83/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2120 - accuracy: 0.9200\n",
      "Epoch 84/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2052 - accuracy: 0.9193\n",
      "Epoch 85/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9298\n",
      "Epoch 86/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1943 - accuracy: 0.9258\n",
      "Epoch 87/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1965 - accuracy: 0.9275\n",
      "Epoch 88/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9302\n",
      "Epoch 89/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1932 - accuracy: 0.9244\n",
      "Epoch 90/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9349\n",
      "Epoch 91/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9200\n",
      "Epoch 92/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1998 - accuracy: 0.9234\n",
      "Epoch 93/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.9268\n",
      "Epoch 94/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9288\n",
      "Epoch 95/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1871 - accuracy: 0.9312\n",
      "Epoch 96/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.9356\n",
      "Epoch 97/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1960 - accuracy: 0.9247\n",
      "Epoch 98/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9308\n",
      "Epoch 99/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9305\n",
      "Epoch 100/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9285\n",
      "Epoch 101/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.9349\n",
      "Epoch 102/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9325\n",
      "Epoch 103/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9349\n",
      "Epoch 104/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9376\n",
      "Epoch 105/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9386\n",
      "Epoch 106/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9403\n",
      "Epoch 107/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9424\n",
      "Epoch 108/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9380\n",
      "Epoch 109/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9475\n",
      "Epoch 110/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.9298\n",
      "Epoch 111/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1635 - accuracy: 0.9349\n",
      "Epoch 112/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1699 - accuracy: 0.9373\n",
      "Epoch 113/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9319\n",
      "Epoch 114/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9441\n",
      "Epoch 115/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1562 - accuracy: 0.9447\n",
      "Epoch 116/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9458\n",
      "Epoch 117/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9454\n",
      "Epoch 118/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9414\n",
      "Epoch 119/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9424\n",
      "Epoch 120/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9363\n",
      "Epoch 121/1500\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.1724 - accuracy: 0.9376\n",
      "Epoch 122/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9319\n",
      "Epoch 123/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.9373\n",
      "Epoch 124/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.9373\n",
      "Epoch 125/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9346\n",
      "Epoch 126/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9292\n",
      "Epoch 127/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9427\n",
      "Epoch 128/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9400\n",
      "Epoch 129/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9485\n",
      "Epoch 130/1500\n",
      "93/93 [==============================] - 0s 983us/step - loss: 0.1553 - accuracy: 0.9397\n",
      "Epoch 131/1500\n",
      "93/93 [==============================] - 0s 984us/step - loss: 0.1494 - accuracy: 0.9475\n",
      "Epoch 132/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9414\n",
      "Epoch 133/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9376\n",
      "Epoch 134/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9441\n",
      "Epoch 135/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9451\n",
      "Epoch 136/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.9451\n",
      "Epoch 137/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9403\n",
      "Epoch 138/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.9420\n",
      "Epoch 139/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1493 - accuracy: 0.9451\n",
      "Epoch 140/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.9475\n",
      "Epoch 141/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9397\n",
      "Epoch 142/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9447\n",
      "Epoch 143/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9532\n",
      "Epoch 144/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9529\n",
      "Epoch 145/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9519\n",
      "Epoch 146/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9451\n",
      "Epoch 147/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9407\n",
      "Epoch 148/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9515\n",
      "Epoch 149/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1320 - accuracy: 0.9512\n",
      "Epoch 150/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9549\n",
      "Epoch 151/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9481\n",
      "Epoch 152/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9512\n",
      "Epoch 153/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9525\n",
      "Epoch 154/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9495\n",
      "Epoch 155/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9492\n",
      "Epoch 156/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9498\n",
      "Epoch 157/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9508\n",
      "Epoch 158/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9525\n",
      "Epoch 159/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9505\n",
      "Epoch 160/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9512\n",
      "Epoch 161/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9485\n",
      "Epoch 162/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9458\n",
      "Epoch 163/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9542\n",
      "Epoch 164/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9539\n",
      "Epoch 165/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9488\n",
      "Epoch 166/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9492\n",
      "Epoch 167/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9539\n",
      "Epoch 168/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9529\n",
      "Epoch 169/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9488\n",
      "Epoch 170/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9553\n",
      "Epoch 171/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9590\n",
      "Epoch 172/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9525\n",
      "Epoch 173/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1330 - accuracy: 0.9471\n",
      "Epoch 174/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9539\n",
      "Epoch 175/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9488\n",
      "Epoch 176/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9536\n",
      "Epoch 177/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9519\n",
      "Epoch 178/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9563\n",
      "Epoch 179/1500\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.1294 - accuracy: 0.9512\n",
      "Epoch 180/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9532\n",
      "Epoch 181/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9573\n",
      "Epoch 182/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9597\n",
      "Epoch 183/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9569\n",
      "Epoch 184/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9536\n",
      "Epoch 185/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9573\n",
      "Epoch 186/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9522\n",
      "Epoch 187/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9532\n",
      "Epoch 188/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9495\n",
      "Epoch 189/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9614\n",
      "Epoch 190/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9600\n",
      "Epoch 191/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9525\n",
      "Epoch 192/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9576\n",
      "Epoch 193/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1111 - accuracy: 0.9593\n",
      "Epoch 194/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9593\n",
      "Epoch 195/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9614\n",
      "Epoch 196/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9607\n",
      "Epoch 197/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9627\n",
      "Epoch 198/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9580\n",
      "Epoch 199/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9563\n",
      "Epoch 200/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9651\n",
      "Epoch 201/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9590\n",
      "Epoch 202/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9522\n",
      "Epoch 203/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9559\n",
      "Epoch 204/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9586\n",
      "Epoch 205/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9586\n",
      "Epoch 206/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9576\n",
      "Epoch 207/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9675\n",
      "Epoch 208/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9498\n",
      "Epoch 209/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9553\n",
      "Epoch 210/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9546\n",
      "Epoch 211/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9569\n",
      "Epoch 212/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9593\n",
      "Epoch 213/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9593\n",
      "Epoch 214/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9593\n",
      "Epoch 215/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.9637\n",
      "Epoch 216/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9508\n",
      "Epoch 217/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9641\n",
      "Epoch 218/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9566\n",
      "Epoch 219/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9563\n",
      "Epoch 220/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9614\n",
      "Epoch 221/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9583\n",
      "Epoch 222/1500\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1111 - accuracy: 0.9624\n",
      "Epoch 223/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9542\n",
      "Epoch 224/1500\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.1040 - accuracy: 0.9617\n",
      "Epoch 225/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9569\n",
      "Epoch 226/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9607\n",
      "Epoch 227/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9583\n",
      "Epoch 228/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9647\n",
      "Epoch 229/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9614\n",
      "Epoch 230/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9675\n",
      "Epoch 231/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.9661\n",
      "Epoch 232/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9614\n",
      "Epoch 233/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9542\n",
      "Epoch 234/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9620\n",
      "Epoch 235/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9634\n",
      "Epoch 236/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9668\n",
      "Epoch 237/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9644\n",
      "Epoch 238/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9698\n",
      "Epoch 239/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9688\n",
      "Epoch 240/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9692\n",
      "Epoch 241/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9617\n",
      "Epoch 242/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9624\n",
      "Epoch 243/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.9688\n",
      "Epoch 244/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.9654\n",
      "Epoch 245/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9634\n",
      "Epoch 246/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9627\n",
      "Epoch 247/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9610\n",
      "Epoch 248/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9668\n",
      "Epoch 249/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0994 - accuracy: 0.9647\n",
      "Epoch 250/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9576\n",
      "Epoch 251/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9647\n",
      "Epoch 252/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9668\n",
      "Epoch 253/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9678\n",
      "Epoch 254/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9641\n",
      "Epoch 255/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9654\n",
      "Epoch 256/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9641\n",
      "Epoch 257/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1017 - accuracy: 0.9627\n",
      "Epoch 258/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9586\n",
      "Epoch 259/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9637\n",
      "Epoch 260/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9644\n",
      "Epoch 261/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9624\n",
      "Epoch 262/1500\n",
      "93/93 [==============================] - 0s 3ms/step - loss: 0.0986 - accuracy: 0.9654\n",
      "Epoch 263/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9651\n",
      "Epoch 264/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9702\n",
      "Epoch 265/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.9675\n",
      "Epoch 266/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9725\n",
      "Epoch 267/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9695\n",
      "Epoch 268/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.9671\n",
      "Epoch 269/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9634\n",
      "Epoch 270/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9644\n",
      "Epoch 271/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9671\n",
      "Epoch 272/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9624\n",
      "Epoch 273/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9654\n",
      "Epoch 274/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9661\n",
      "Epoch 275/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.9685\n",
      "Epoch 276/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9712\n",
      "Epoch 277/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9634\n",
      "Epoch 278/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9637\n",
      "Epoch 279/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9627\n",
      "Epoch 280/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9624\n",
      "Epoch 281/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9658\n",
      "Epoch 282/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9644\n",
      "Epoch 283/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9569\n",
      "Epoch 284/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9681\n",
      "Epoch 285/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9647\n",
      "Epoch 286/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9681\n",
      "Epoch 287/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9631\n",
      "Epoch 288/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9668\n",
      "Epoch 289/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9661\n",
      "Epoch 290/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.9695\n",
      "Epoch 291/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9644\n",
      "Epoch 292/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9725\n",
      "Epoch 293/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9688\n",
      "Epoch 294/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9678\n",
      "Epoch 295/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.9651\n",
      "Epoch 296/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9658\n",
      "Epoch 297/1500\n",
      "93/93 [==============================] - 0s 4ms/step - loss: 0.0955 - accuracy: 0.9678\n",
      "Epoch 298/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9593\n",
      "Epoch 299/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9647\n",
      "Epoch 300/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9603\n",
      "Epoch 301/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9654\n",
      "Epoch 302/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9712\n",
      "Epoch 303/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9766\n",
      "Epoch 304/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9654\n",
      "Epoch 305/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9671\n",
      "Epoch 306/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9692\n",
      "Epoch 307/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.9783\n",
      "Epoch 308/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9708\n",
      "Epoch 309/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.9692\n",
      "Epoch 310/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9705\n",
      "Epoch 311/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.9627\n",
      "Epoch 312/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.9715\n",
      "Epoch 313/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9729\n",
      "Epoch 314/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9671\n",
      "Epoch 315/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9664\n",
      "Epoch 316/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9624\n",
      "Epoch 317/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9664\n",
      "Epoch 318/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9675\n",
      "Epoch 319/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9668\n",
      "Epoch 320/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.9705\n",
      "Epoch 321/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9637\n",
      "Epoch 322/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9688\n",
      "Epoch 323/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9736\n",
      "Epoch 324/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9719\n",
      "Epoch 325/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9712\n",
      "Epoch 326/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9783\n",
      "Epoch 327/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9637\n",
      "Epoch 328/1500\n",
      "93/93 [==============================] - 0s 5ms/step - loss: 0.0805 - accuracy: 0.9695\n",
      "Epoch 329/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.9766\n",
      "Epoch 330/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9671\n",
      "Epoch 331/1500\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.1003 - accuracy: 0.9610\n",
      "Epoch 332/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.9695\n",
      "Epoch 333/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9698\n",
      "Epoch 334/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9705\n",
      "Epoch 335/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9688\n",
      "Epoch 336/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.9624\n",
      "Epoch 337/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9681\n",
      "Epoch 338/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9600\n",
      "Epoch 339/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9702\n",
      "Epoch 340/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9712\n",
      "Epoch 341/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9688\n",
      "Epoch 342/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.9712\n",
      "Epoch 343/1500\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.0728 - accuracy: 0.9749\n",
      "Epoch 344/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9668\n",
      "Epoch 345/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.9722\n",
      "Epoch 346/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9668\n",
      "Epoch 347/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9590\n",
      "Epoch 348/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9675\n",
      "Epoch 349/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.9702\n",
      "Epoch 350/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9702\n",
      "Epoch 351/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9783\n",
      "Epoch 352/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.9688\n",
      "Epoch 353/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9729\n",
      "Epoch 354/1500\n",
      "93/93 [==============================] - 0s 2ms/step - loss: 0.0816 - accuracy: 0.9719\n",
      "Epoch 355/1500\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9729\n",
      "Epoch 356/1500\n",
      "92/93 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9752Restoring model weights from the end of the best epoch: 326.\n",
      "93/93 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9749\n",
      "Epoch 356: early stopping\n",
      "7/7 [==============================] - 0s 967us/step - loss: 0.8969 - accuracy: 0.7703\n",
      "7/7 [==============================] - 0s 679us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (22/28)\n",
      "Before appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "After appending - Cat IDs: 427, Predictions: 427, Actuals: 427, Gender: 427\n",
      "Final Test Results - Loss: 0.8968534469604492, Accuracy: 0.7703348994255066, Precision: 0.7481481481481481, Recall: 0.7768707482993197, F1 Score: 0.7602319112812919\n",
      "Confusion Matrix:\n",
      " [[100   6  19]\n",
      " [  0  35   0]\n",
      " [ 23   0  26]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "042A    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "040A    10\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "015A     9\n",
      "045A     9\n",
      "095A     8\n",
      "117A     7\n",
      "099A     7\n",
      "031A     7\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "012A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "060A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "038A     2\n",
      "093A     2\n",
      "018A     2\n",
      "054A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "091A     1\n",
      "019B     1\n",
      "092A     1\n",
      "004A     1\n",
      "049A     1\n",
      "076A     1\n",
      "043A     1\n",
      "026C     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "020A    23\n",
      "067A    19\n",
      "097A    16\n",
      "059A    14\n",
      "022A     9\n",
      "072A     9\n",
      "010A     8\n",
      "094A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "037A     6\n",
      "109A     6\n",
      "008A     6\n",
      "009A     4\n",
      "003A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "025B     2\n",
      "087A     2\n",
      "032A     2\n",
      "066A     1\n",
      "048A     1\n",
      "041A     1\n",
      "115A     1\n",
      "096A     1\n",
      "100A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    243\n",
      "X    229\n",
      "F    226\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    119\n",
      "M     94\n",
      "F     26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 043A, 049...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [067A, 020A, 022A, 072A, 009A, 027A, 013B, 014...\n",
      "kitten                 [046A, 109A, 050A, 041A, 048A, 115A]\n",
      "senior                       [097A, 059A, 058A, 094A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '044A' '045A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '046A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'045A'}\n",
      "Moved to Test Set:\n",
      "{'045A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '044A' '046A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '045A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "Length of X_train_val:\n",
      "752\n",
      "Length of y_train_val:\n",
      "752\n",
      "Length of groups_train_val:\n",
      "752\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     470\n",
      "senior    136\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     118\n",
      "kitten     79\n",
      "senior     42\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     470\n",
      "kitten    146\n",
      "senior    136\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     118\n",
      "senior     42\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1136, 1: 994, 2: 896})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "95/95 [==============================] - 1s 2ms/step - loss: 0.8947 - accuracy: 0.6213\n",
      "Epoch 2/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.6809 - accuracy: 0.7174\n",
      "Epoch 3/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.6566 - accuracy: 0.7217\n",
      "Epoch 4/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.6333 - accuracy: 0.7336\n",
      "Epoch 5/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.5906 - accuracy: 0.7528\n",
      "Epoch 6/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.5769 - accuracy: 0.7588\n",
      "Epoch 7/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.5539 - accuracy: 0.7683\n",
      "Epoch 8/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.5332 - accuracy: 0.7710\n",
      "Epoch 9/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.5123 - accuracy: 0.7779\n",
      "Epoch 10/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.5079 - accuracy: 0.7809\n",
      "Epoch 11/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.4678 - accuracy: 0.8014\n",
      "Epoch 12/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.4818 - accuracy: 0.7915\n",
      "Epoch 13/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.4820 - accuracy: 0.7971\n",
      "Epoch 14/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.4520 - accuracy: 0.8100\n",
      "Epoch 15/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.4512 - accuracy: 0.8073\n",
      "Epoch 16/1500\n",
      "95/95 [==============================] - 0s 4ms/step - loss: 0.4582 - accuracy: 0.8044\n",
      "Epoch 17/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.4266 - accuracy: 0.8219\n",
      "Epoch 18/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.4259 - accuracy: 0.8182\n",
      "Epoch 19/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.4325 - accuracy: 0.8189\n",
      "Epoch 20/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.4299 - accuracy: 0.8176\n",
      "Epoch 21/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.4208 - accuracy: 0.8272\n",
      "Epoch 22/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8153\n",
      "Epoch 23/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3987 - accuracy: 0.8361\n",
      "Epoch 24/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3910 - accuracy: 0.8308\n",
      "Epoch 25/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3878 - accuracy: 0.8440\n",
      "Epoch 26/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3892 - accuracy: 0.8407\n",
      "Epoch 27/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3807 - accuracy: 0.8391\n",
      "Epoch 28/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3994 - accuracy: 0.8334\n",
      "Epoch 29/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3472 - accuracy: 0.8549\n",
      "Epoch 30/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3540 - accuracy: 0.8619\n",
      "Epoch 31/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8437\n",
      "Epoch 32/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3654 - accuracy: 0.8572\n",
      "Epoch 33/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3520 - accuracy: 0.8553\n",
      "Epoch 34/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3534 - accuracy: 0.8589\n",
      "Epoch 35/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3291 - accuracy: 0.8721\n",
      "Epoch 36/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3603 - accuracy: 0.8566\n",
      "Epoch 37/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3247 - accuracy: 0.8635\n",
      "Epoch 38/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3360 - accuracy: 0.8612\n",
      "Epoch 39/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8635\n",
      "Epoch 40/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3319 - accuracy: 0.8605\n",
      "Epoch 41/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3128 - accuracy: 0.8794\n",
      "Epoch 42/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3263 - accuracy: 0.8642\n",
      "Epoch 43/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3298 - accuracy: 0.8675\n",
      "Epoch 44/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3170 - accuracy: 0.8685\n",
      "Epoch 45/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3139 - accuracy: 0.8714\n",
      "Epoch 46/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3037 - accuracy: 0.8787\n",
      "Epoch 47/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2942 - accuracy: 0.8833\n",
      "Epoch 48/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3168 - accuracy: 0.8708\n",
      "Epoch 49/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.3061 - accuracy: 0.8771\n",
      "Epoch 50/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2901 - accuracy: 0.8883\n",
      "Epoch 51/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2880 - accuracy: 0.8824\n",
      "Epoch 52/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2826 - accuracy: 0.8896\n",
      "Epoch 53/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2933 - accuracy: 0.8824\n",
      "Epoch 54/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2879 - accuracy: 0.8870\n",
      "Epoch 55/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2987 - accuracy: 0.8738\n",
      "Epoch 56/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2957 - accuracy: 0.8761\n",
      "Epoch 57/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2849 - accuracy: 0.8850\n",
      "Epoch 58/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2880 - accuracy: 0.8866\n",
      "Epoch 59/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2951 - accuracy: 0.8800\n",
      "Epoch 60/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2698 - accuracy: 0.8929\n",
      "Epoch 61/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2652 - accuracy: 0.8992\n",
      "Epoch 62/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2631 - accuracy: 0.8995\n",
      "Epoch 63/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2642 - accuracy: 0.8999\n",
      "Epoch 64/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2605 - accuracy: 0.8966\n",
      "Epoch 65/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2764 - accuracy: 0.8863\n",
      "Epoch 66/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2706 - accuracy: 0.8913\n",
      "Epoch 67/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2798 - accuracy: 0.8863\n",
      "Epoch 68/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2735 - accuracy: 0.8850\n",
      "Epoch 69/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2649 - accuracy: 0.8919\n",
      "Epoch 70/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2682 - accuracy: 0.8942\n",
      "Epoch 71/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2587 - accuracy: 0.8976\n",
      "Epoch 72/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2690 - accuracy: 0.8966\n",
      "Epoch 73/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2503 - accuracy: 0.8926\n",
      "Epoch 74/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2560 - accuracy: 0.9002\n",
      "Epoch 75/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.8995\n",
      "Epoch 76/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.8992\n",
      "Epoch 77/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2367 - accuracy: 0.9068\n",
      "Epoch 78/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2395 - accuracy: 0.9052\n",
      "Epoch 79/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2442 - accuracy: 0.9061\n",
      "Epoch 80/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2317 - accuracy: 0.9151\n",
      "Epoch 81/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2358 - accuracy: 0.9058\n",
      "Epoch 82/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2635 - accuracy: 0.9045\n",
      "Epoch 83/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2601 - accuracy: 0.8909\n",
      "Epoch 84/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2684 - accuracy: 0.8959\n",
      "Epoch 85/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2492 - accuracy: 0.8966\n",
      "Epoch 86/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2202 - accuracy: 0.9151\n",
      "Epoch 87/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2264 - accuracy: 0.9098\n",
      "Epoch 88/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2356 - accuracy: 0.9111\n",
      "Epoch 89/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.9137\n",
      "Epoch 90/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2251 - accuracy: 0.9134\n",
      "Epoch 91/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2309 - accuracy: 0.9071\n",
      "Epoch 92/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2535 - accuracy: 0.9065\n",
      "Epoch 93/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2184 - accuracy: 0.9147\n",
      "Epoch 94/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9157\n",
      "Epoch 95/1500\n",
      "95/95 [==============================] - 0s 4ms/step - loss: 0.2251 - accuracy: 0.9128\n",
      "Epoch 96/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2282 - accuracy: 0.9081\n",
      "Epoch 97/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.9233\n",
      "Epoch 98/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2158 - accuracy: 0.9194\n",
      "Epoch 99/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2410 - accuracy: 0.9052\n",
      "Epoch 100/1500\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.2319 - accuracy: 0.9128\n",
      "Epoch 101/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2250 - accuracy: 0.9161\n",
      "Epoch 102/1500\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.2314 - accuracy: 0.9104\n",
      "Epoch 103/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2268 - accuracy: 0.9108\n",
      "Epoch 104/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2311 - accuracy: 0.9104\n",
      "Epoch 105/1500\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.2338 - accuracy: 0.9095\n",
      "Epoch 106/1500\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.2204 - accuracy: 0.9151\n",
      "Epoch 107/1500\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.2112 - accuracy: 0.9194\n",
      "Epoch 108/1500\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.2188 - accuracy: 0.9128\n",
      "Epoch 109/1500\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9197\n",
      "Epoch 110/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2258 - accuracy: 0.9091\n",
      "Epoch 111/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2265 - accuracy: 0.9118\n",
      "Epoch 112/1500\n",
      "95/95 [==============================] - 0s 4ms/step - loss: 0.2274 - accuracy: 0.9101\n",
      "Epoch 113/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2159 - accuracy: 0.9108\n",
      "Epoch 114/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9293\n",
      "Epoch 115/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2079 - accuracy: 0.9204\n",
      "Epoch 116/1500\n",
      "95/95 [==============================] - 0s 2ms/step - loss: 0.2022 - accuracy: 0.9207\n",
      "Epoch 117/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2105 - accuracy: 0.9174\n",
      "Epoch 118/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9266\n",
      "Epoch 119/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2065 - accuracy: 0.9194\n",
      "Epoch 120/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2211 - accuracy: 0.9128\n",
      "Epoch 121/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1929 - accuracy: 0.9253\n",
      "Epoch 122/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2072 - accuracy: 0.9144\n",
      "Epoch 123/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9240\n",
      "Epoch 124/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9270\n",
      "Epoch 125/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9161\n",
      "Epoch 126/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2019 - accuracy: 0.9250\n",
      "Epoch 127/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1912 - accuracy: 0.9247\n",
      "Epoch 128/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9187\n",
      "Epoch 129/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9213\n",
      "Epoch 130/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1992 - accuracy: 0.9293\n",
      "Epoch 131/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2082 - accuracy: 0.9213\n",
      "Epoch 132/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2023 - accuracy: 0.9223\n",
      "Epoch 133/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1792 - accuracy: 0.9356\n",
      "Epoch 134/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9273\n",
      "Epoch 135/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9194\n",
      "Epoch 136/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1992 - accuracy: 0.9256\n",
      "Epoch 137/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9217\n",
      "Epoch 138/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9223\n",
      "Epoch 139/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9286\n",
      "Epoch 140/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9270\n",
      "Epoch 141/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9230\n",
      "Epoch 142/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2123 - accuracy: 0.9141\n",
      "Epoch 143/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9213\n",
      "Epoch 144/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2000 - accuracy: 0.9210\n",
      "Epoch 145/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.9319\n",
      "Epoch 146/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9247\n",
      "Epoch 147/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9336\n",
      "Epoch 148/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9177\n",
      "Epoch 149/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9200\n",
      "Epoch 150/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.9385\n",
      "Epoch 151/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.9293\n",
      "Epoch 152/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9237\n",
      "Epoch 153/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9200\n",
      "Epoch 154/1500\n",
      "95/95 [==============================] - 0s 987us/step - loss: 0.1760 - accuracy: 0.9313\n",
      "Epoch 155/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9303\n",
      "Epoch 156/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.9309\n",
      "Epoch 157/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9289\n",
      "Epoch 158/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9253\n",
      "Epoch 159/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9230\n",
      "Epoch 160/1500\n",
      "95/95 [==============================] - 0s 4ms/step - loss: 0.1742 - accuracy: 0.9356\n",
      "Epoch 161/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1891 - accuracy: 0.9263\n",
      "Epoch 162/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1881 - accuracy: 0.9283\n",
      "Epoch 163/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1881 - accuracy: 0.9250\n",
      "Epoch 164/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.9263\n",
      "Epoch 165/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.9266\n",
      "Epoch 166/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9286\n",
      "Epoch 167/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9349\n",
      "Epoch 168/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1936 - accuracy: 0.9270\n",
      "Epoch 169/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9280\n",
      "Epoch 170/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9329\n",
      "Epoch 171/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1752 - accuracy: 0.9296\n",
      "Epoch 172/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9319\n",
      "Epoch 173/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9276\n",
      "Epoch 174/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1632 - accuracy: 0.9375\n",
      "Epoch 175/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.9286\n",
      "Epoch 176/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9385\n",
      "Epoch 177/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1634 - accuracy: 0.9365\n",
      "Epoch 178/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9339\n",
      "Epoch 179/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9253\n",
      "Epoch 180/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9339\n",
      "Epoch 181/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9451\n",
      "Epoch 182/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1787 - accuracy: 0.9336\n",
      "Epoch 183/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9359\n",
      "Epoch 184/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9408\n",
      "Epoch 185/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9382\n",
      "Epoch 186/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9365\n",
      "Epoch 187/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.9402\n",
      "Epoch 188/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9395\n",
      "Epoch 189/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9342\n",
      "Epoch 190/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9356\n",
      "Epoch 191/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9362\n",
      "Epoch 192/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1608 - accuracy: 0.9405\n",
      "Epoch 193/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.9346\n",
      "Epoch 194/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9339\n",
      "Epoch 195/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9418\n",
      "Epoch 196/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1661 - accuracy: 0.9392\n",
      "Epoch 197/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9382\n",
      "Epoch 198/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.9296\n",
      "Epoch 199/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9412\n",
      "Epoch 200/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9319\n",
      "Epoch 201/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9319\n",
      "Epoch 202/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1618 - accuracy: 0.9342\n",
      "Epoch 203/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9296\n",
      "Epoch 204/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9375\n",
      "Epoch 205/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.9382\n",
      "Epoch 206/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9276\n",
      "Epoch 207/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.9359\n",
      "Epoch 208/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9418\n",
      "Epoch 209/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1546 - accuracy: 0.9408\n",
      "Epoch 210/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9415\n",
      "Epoch 211/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9442\n",
      "Epoch 212/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9418\n",
      "Epoch 213/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9418\n",
      "Epoch 214/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9385\n",
      "Epoch 215/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9438\n",
      "Epoch 216/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1634 - accuracy: 0.9385\n",
      "Epoch 217/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9458\n",
      "Epoch 218/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1685 - accuracy: 0.9362\n",
      "Epoch 219/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.9362\n",
      "Epoch 220/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9432\n",
      "Epoch 221/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1634 - accuracy: 0.9329\n",
      "Epoch 222/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9481\n",
      "Epoch 223/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9395\n",
      "Epoch 224/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.9289\n",
      "Epoch 225/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9399\n",
      "Epoch 226/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9349\n",
      "Epoch 227/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9389\n",
      "Epoch 228/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9481\n",
      "Epoch 229/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9382\n",
      "Epoch 230/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9448\n",
      "Epoch 231/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9379\n",
      "Epoch 232/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1557 - accuracy: 0.9408\n",
      "Epoch 233/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9448\n",
      "Epoch 234/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9365\n",
      "Epoch 235/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9346\n",
      "Epoch 236/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9352\n",
      "Epoch 237/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1586 - accuracy: 0.9425\n",
      "Epoch 238/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9468\n",
      "Epoch 239/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1475 - accuracy: 0.9455\n",
      "Epoch 240/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1397 - accuracy: 0.9484\n",
      "Epoch 241/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.9438\n",
      "Epoch 242/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1582 - accuracy: 0.9442\n",
      "Epoch 243/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9352\n",
      "Epoch 244/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9445\n",
      "Epoch 245/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9478\n",
      "Epoch 246/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9461\n",
      "Epoch 247/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.9448\n",
      "Epoch 248/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1468 - accuracy: 0.9425\n",
      "Epoch 249/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9547\n",
      "Epoch 250/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9504\n",
      "Epoch 251/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9451\n",
      "Epoch 252/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9494\n",
      "Epoch 253/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1388 - accuracy: 0.9491\n",
      "Epoch 254/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9508\n",
      "Epoch 255/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9498\n",
      "Epoch 256/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9382\n",
      "Epoch 257/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.9445\n",
      "Epoch 258/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9432\n",
      "Epoch 259/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9405\n",
      "Epoch 260/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9442\n",
      "Epoch 261/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9445\n",
      "Epoch 262/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9428\n",
      "Epoch 263/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9478\n",
      "Epoch 264/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9448\n",
      "Epoch 265/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9527\n",
      "Epoch 266/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9382\n",
      "Epoch 267/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9478\n",
      "Epoch 268/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1599 - accuracy: 0.9402\n",
      "Epoch 269/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9461\n",
      "Epoch 270/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9448\n",
      "Epoch 271/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9415\n",
      "Epoch 272/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1523 - accuracy: 0.9425\n",
      "Epoch 273/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1497 - accuracy: 0.9408\n",
      "Epoch 274/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9415\n",
      "Epoch 275/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.9501\n",
      "Epoch 276/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9461\n",
      "Epoch 277/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9521\n",
      "Epoch 278/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9432\n",
      "Epoch 279/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9518\n",
      "Epoch 280/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9504\n",
      "Epoch 281/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9481\n",
      "Epoch 282/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9478\n",
      "Epoch 283/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9432\n",
      "Epoch 284/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9518\n",
      "Epoch 285/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9527\n",
      "Epoch 286/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9481\n",
      "Epoch 287/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9435\n",
      "Epoch 288/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9488\n",
      "Epoch 289/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9537\n",
      "Epoch 290/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.9458\n",
      "Epoch 291/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9478\n",
      "Epoch 292/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1204 - accuracy: 0.9547\n",
      "Epoch 293/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9524\n",
      "Epoch 294/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9422\n",
      "Epoch 295/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9488\n",
      "Epoch 296/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1551 - accuracy: 0.9438\n",
      "Epoch 297/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9527\n",
      "Epoch 298/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9458\n",
      "Epoch 299/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9570\n",
      "Epoch 300/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9570\n",
      "Epoch 301/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9511\n",
      "Epoch 302/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9574\n",
      "Epoch 303/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9537\n",
      "Epoch 304/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9551\n",
      "Epoch 305/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9541\n",
      "Epoch 306/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9504\n",
      "Epoch 307/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9488\n",
      "Epoch 308/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.9481\n",
      "Epoch 309/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9537\n",
      "Epoch 310/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9521\n",
      "Epoch 311/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9484\n",
      "Epoch 312/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9488\n",
      "Epoch 313/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9541\n",
      "Epoch 314/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.9527\n",
      "Epoch 315/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9564\n",
      "Epoch 316/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9455\n",
      "Epoch 317/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9501\n",
      "Epoch 318/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9590\n",
      "Epoch 319/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9511\n",
      "Epoch 320/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9534\n",
      "Epoch 321/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9475\n",
      "Epoch 322/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9511\n",
      "Epoch 323/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9537\n",
      "Epoch 324/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9461\n",
      "Epoch 325/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9564\n",
      "Epoch 326/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9508\n",
      "Epoch 327/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9438\n",
      "Epoch 328/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9560\n",
      "Epoch 329/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9521\n",
      "Epoch 330/1500\n",
      "95/95 [==============================] - 0s 951us/step - loss: 0.1316 - accuracy: 0.9471\n",
      "Epoch 331/1500\n",
      "95/95 [==============================] - 0s 977us/step - loss: 0.1005 - accuracy: 0.9666\n",
      "Epoch 332/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9518\n",
      "Epoch 333/1500\n",
      "95/95 [==============================] - 0s 962us/step - loss: 0.1276 - accuracy: 0.9554\n",
      "Epoch 334/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9471\n",
      "Epoch 335/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9521\n",
      "Epoch 336/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9541\n",
      "Epoch 337/1500\n",
      "95/95 [==============================] - 0s 4ms/step - loss: 0.1403 - accuracy: 0.9491\n",
      "Epoch 338/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9514\n",
      "Epoch 339/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9508\n",
      "Epoch 340/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9627\n",
      "Epoch 341/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9564\n",
      "Epoch 342/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9461\n",
      "Epoch 343/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9527\n",
      "Epoch 344/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9574\n",
      "Epoch 345/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9518\n",
      "Epoch 346/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9537\n",
      "Epoch 347/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9554\n",
      "Epoch 348/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9557\n",
      "Epoch 349/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9544\n",
      "Epoch 350/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9527\n",
      "Epoch 351/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9511\n",
      "Epoch 352/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9560\n",
      "Epoch 353/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9570\n",
      "Epoch 354/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9577\n",
      "Epoch 355/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9541\n",
      "Epoch 356/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9594\n",
      "Epoch 357/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9557\n",
      "Epoch 358/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9607\n",
      "Epoch 359/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.9587\n",
      "Epoch 360/1500\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9610\n",
      "Epoch 361/1500\n",
      "86/95 [==========================>...] - ETA: 0s - loss: 0.1194 - accuracy: 0.9600Restoring model weights from the end of the best epoch: 331.\n",
      "95/95 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9597\n",
      "Epoch 361: early stopping\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3840 - accuracy: 0.8432\n",
      "6/6 [==============================] - 0s 792us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.87 (26/30)\n",
      "Before appending - Cat IDs: 427, Predictions: 427, Actuals: 427, Gender: 427\n",
      "After appending - Cat IDs: 612, Predictions: 612, Actuals: 612, Gender: 612\n",
      "Final Test Results - Loss: 0.38401684165000916, Accuracy: 0.8432432413101196, Precision: 0.8264492753623188, Recall: 0.8300403551251009, F1 Score: 0.8275597792765175\n",
      "Confusion Matrix:\n",
      " [[102   3  13]\n",
      " [  4  21   0]\n",
      " [  9   0  33]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "116A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "016A    10\n",
      "005A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "050A     7\n",
      "117A     7\n",
      "037A     6\n",
      "053A     6\n",
      "008A     6\n",
      "109A     6\n",
      "023A     6\n",
      "044A     5\n",
      "023B     5\n",
      "070A     5\n",
      "075A     5\n",
      "009A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "060A     3\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "025B     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "032A     2\n",
      "018A     2\n",
      "069A     2\n",
      "092A     1\n",
      "100A     1\n",
      "096A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "041A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "066A     1\n",
      "091A     1\n",
      "076A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "055A    20\n",
      "101A    15\n",
      "028A    13\n",
      "002A    13\n",
      "025A    11\n",
      "033A     9\n",
      "031A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "034A     5\n",
      "025C     5\n",
      "021A     5\n",
      "026A     4\n",
      "035A     4\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "093A     2\n",
      "049A     1\n",
      "026C     1\n",
      "073A     1\n",
      "088A     1\n",
      "090A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    277\n",
      "M    271\n",
      "F    151\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    101\n",
      "X     71\n",
      "M     66\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 071A, 097B, 019A, 074A, 067...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 050...\n",
      "senior    [097A, 106A, 104A, 059A, 116A, 051B, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 103A, 028A, 101A, 034A, 002A, 099...\n",
      "kitten                                         [047A, 049A]\n",
      "senior           [093A, 057A, 055A, 113A, 056A, 108A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 14, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 2, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "Length of X_train_val:\n",
      "699\n",
      "Length of y_train_val:\n",
      "699\n",
      "Length of groups_train_val:\n",
      "699\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1075, 1: 934, 2: 752})\n",
      "Epoch 1/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.9277 - accuracy: 0.6030\n",
      "Epoch 2/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.7001 - accuracy: 0.7142\n",
      "Epoch 3/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.6364 - accuracy: 0.7436\n",
      "Epoch 4/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.5965 - accuracy: 0.7544\n",
      "Epoch 5/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.5789 - accuracy: 0.7704\n",
      "Epoch 6/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.5267 - accuracy: 0.7852\n",
      "Epoch 7/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.5398 - accuracy: 0.7794\n",
      "Epoch 8/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.5279 - accuracy: 0.7787\n",
      "Epoch 9/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.4726 - accuracy: 0.8022\n",
      "Epoch 10/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.4722 - accuracy: 0.8088\n",
      "Epoch 11/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.4875 - accuracy: 0.7965\n",
      "Epoch 12/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.4603 - accuracy: 0.8153\n",
      "Epoch 13/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.4384 - accuracy: 0.8127\n",
      "Epoch 14/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.4481 - accuracy: 0.8189\n",
      "Epoch 15/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.4285 - accuracy: 0.8247\n",
      "Epoch 16/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.4303 - accuracy: 0.8214\n",
      "Epoch 17/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3988 - accuracy: 0.8370\n",
      "Epoch 18/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3966 - accuracy: 0.8392\n",
      "Epoch 19/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3851 - accuracy: 0.8410\n",
      "Epoch 20/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3914 - accuracy: 0.8388\n",
      "Epoch 21/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3798 - accuracy: 0.8493\n",
      "Epoch 22/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3839 - accuracy: 0.8435\n",
      "Epoch 23/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8511\n",
      "Epoch 24/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3564 - accuracy: 0.8504\n",
      "Epoch 25/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3690 - accuracy: 0.8479\n",
      "Epoch 26/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3595 - accuracy: 0.8562\n",
      "Epoch 27/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3514 - accuracy: 0.8544\n",
      "Epoch 28/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3658 - accuracy: 0.8555\n",
      "Epoch 29/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3379 - accuracy: 0.8624\n",
      "Epoch 30/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.8667\n",
      "Epoch 31/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3352 - accuracy: 0.8577\n",
      "Epoch 32/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3577 - accuracy: 0.8595\n",
      "Epoch 33/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3406 - accuracy: 0.8627\n",
      "Epoch 34/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3025 - accuracy: 0.8848\n",
      "Epoch 35/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3275 - accuracy: 0.8725\n",
      "Epoch 36/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3239 - accuracy: 0.8689\n",
      "Epoch 37/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3221 - accuracy: 0.8649\n",
      "Epoch 38/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3039 - accuracy: 0.8740\n",
      "Epoch 39/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8769\n",
      "Epoch 40/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2949 - accuracy: 0.8855\n",
      "Epoch 41/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2850 - accuracy: 0.8819\n",
      "Epoch 42/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2986 - accuracy: 0.8794\n",
      "Epoch 43/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2856 - accuracy: 0.8837\n",
      "Epoch 44/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2809 - accuracy: 0.8917\n",
      "Epoch 45/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2769 - accuracy: 0.8884\n",
      "Epoch 46/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.8823\n",
      "Epoch 47/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.3019 - accuracy: 0.8769\n",
      "Epoch 48/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2997 - accuracy: 0.8816\n",
      "Epoch 49/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2618 - accuracy: 0.8975\n",
      "Epoch 50/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2840 - accuracy: 0.8895\n",
      "Epoch 51/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2819 - accuracy: 0.8870\n",
      "Epoch 52/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2898 - accuracy: 0.8841\n",
      "Epoch 53/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2658 - accuracy: 0.8979\n",
      "Epoch 54/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2547 - accuracy: 0.8975\n",
      "Epoch 55/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2491 - accuracy: 0.8968\n",
      "Epoch 56/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2670 - accuracy: 0.8910\n",
      "Epoch 57/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2648 - accuracy: 0.8953\n",
      "Epoch 58/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2601 - accuracy: 0.8968\n",
      "Epoch 59/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2640 - accuracy: 0.8910\n",
      "Epoch 60/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2560 - accuracy: 0.8953\n",
      "Epoch 61/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2480 - accuracy: 0.8989\n",
      "Epoch 62/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2386 - accuracy: 0.9040\n",
      "Epoch 63/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2567 - accuracy: 0.8953\n",
      "Epoch 64/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2611 - accuracy: 0.8982\n",
      "Epoch 65/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2661 - accuracy: 0.8968\n",
      "Epoch 66/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2503 - accuracy: 0.9008\n",
      "Epoch 67/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2371 - accuracy: 0.9080\n",
      "Epoch 68/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.9084\n",
      "Epoch 69/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2383 - accuracy: 0.9062\n",
      "Epoch 70/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2482 - accuracy: 0.9026\n",
      "Epoch 71/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2494 - accuracy: 0.8989\n",
      "Epoch 72/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2477 - accuracy: 0.8993\n",
      "Epoch 73/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2509 - accuracy: 0.8993\n",
      "Epoch 74/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9185\n",
      "Epoch 75/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2291 - accuracy: 0.9069\n",
      "Epoch 76/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2344 - accuracy: 0.9076\n",
      "Epoch 77/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2336 - accuracy: 0.9095\n",
      "Epoch 78/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2259 - accuracy: 0.9134\n",
      "Epoch 79/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9149\n",
      "Epoch 80/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2234 - accuracy: 0.9098\n",
      "Epoch 81/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2339 - accuracy: 0.9105\n",
      "Epoch 82/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2266 - accuracy: 0.9120\n",
      "Epoch 83/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2287 - accuracy: 0.9080\n",
      "Epoch 84/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2153 - accuracy: 0.9145\n",
      "Epoch 85/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2319 - accuracy: 0.9069\n",
      "Epoch 86/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9131\n",
      "Epoch 87/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2280 - accuracy: 0.9055\n",
      "Epoch 88/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 0.9105\n",
      "Epoch 89/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2360 - accuracy: 0.9066\n",
      "Epoch 90/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2125 - accuracy: 0.9207\n",
      "Epoch 91/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2206 - accuracy: 0.9116\n",
      "Epoch 92/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2168 - accuracy: 0.9196\n",
      "Epoch 93/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2177 - accuracy: 0.9138\n",
      "Epoch 94/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2089 - accuracy: 0.9225\n",
      "Epoch 95/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2169 - accuracy: 0.9156\n",
      "Epoch 96/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2125 - accuracy: 0.9207\n",
      "Epoch 97/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2099 - accuracy: 0.9178\n",
      "Epoch 98/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2299 - accuracy: 0.9102\n",
      "Epoch 99/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2239 - accuracy: 0.9105\n",
      "Epoch 100/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.9203\n",
      "Epoch 101/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2185 - accuracy: 0.9116\n",
      "Epoch 102/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2117 - accuracy: 0.9200\n",
      "Epoch 103/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9196\n",
      "Epoch 104/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2091 - accuracy: 0.9174\n",
      "Epoch 105/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2236 - accuracy: 0.9163\n",
      "Epoch 106/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9265\n",
      "Epoch 107/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1896 - accuracy: 0.9254\n",
      "Epoch 108/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9254\n",
      "Epoch 109/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2061 - accuracy: 0.9167\n",
      "Epoch 110/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2153 - accuracy: 0.9116\n",
      "Epoch 111/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9312\n",
      "Epoch 112/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.9326\n",
      "Epoch 113/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1950 - accuracy: 0.9265\n",
      "Epoch 114/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9283\n",
      "Epoch 115/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1870 - accuracy: 0.9294\n",
      "Epoch 116/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2068 - accuracy: 0.9189\n",
      "Epoch 117/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1880 - accuracy: 0.9265\n",
      "Epoch 118/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.9312\n",
      "Epoch 119/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.2157 - accuracy: 0.9178\n",
      "Epoch 120/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9221\n",
      "Epoch 121/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9261\n",
      "Epoch 122/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9207\n",
      "Epoch 123/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9276\n",
      "Epoch 124/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9305\n",
      "Epoch 125/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9413\n",
      "Epoch 126/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1892 - accuracy: 0.9229\n",
      "Epoch 127/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1981 - accuracy: 0.9254\n",
      "Epoch 128/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9286\n",
      "Epoch 129/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9355\n",
      "Epoch 130/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9366\n",
      "Epoch 131/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9373\n",
      "Epoch 132/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1932 - accuracy: 0.9337\n",
      "Epoch 133/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9326\n",
      "Epoch 134/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9352\n",
      "Epoch 135/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9258\n",
      "Epoch 136/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.9305\n",
      "Epoch 137/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.9384\n",
      "Epoch 138/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9373\n",
      "Epoch 139/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9301\n",
      "Epoch 140/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9326\n",
      "Epoch 141/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.9395\n",
      "Epoch 142/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9319\n",
      "Epoch 143/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9301\n",
      "Epoch 144/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1711 - accuracy: 0.9308\n",
      "Epoch 145/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1752 - accuracy: 0.9337\n",
      "Epoch 146/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1754 - accuracy: 0.9377\n",
      "Epoch 147/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1685 - accuracy: 0.9330\n",
      "Epoch 148/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9348\n",
      "Epoch 149/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9326\n",
      "Epoch 150/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9355\n",
      "Epoch 151/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9420\n",
      "Epoch 152/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9305\n",
      "Epoch 153/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9334\n",
      "Epoch 154/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9326\n",
      "Epoch 155/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9366\n",
      "Epoch 156/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9326\n",
      "Epoch 157/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9326\n",
      "Epoch 158/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9294\n",
      "Epoch 159/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9428\n",
      "Epoch 160/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9381\n",
      "Epoch 161/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1548 - accuracy: 0.9442\n",
      "Epoch 162/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9435\n",
      "Epoch 163/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9381\n",
      "Epoch 164/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9373\n",
      "Epoch 165/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9399\n",
      "Epoch 166/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1568 - accuracy: 0.9420\n",
      "Epoch 167/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9460\n",
      "Epoch 168/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9457\n",
      "Epoch 169/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9417\n",
      "Epoch 170/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9435\n",
      "Epoch 171/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9439\n",
      "Epoch 172/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1698 - accuracy: 0.9312\n",
      "Epoch 173/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9428\n",
      "Epoch 174/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9439\n",
      "Epoch 175/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.9381\n",
      "Epoch 176/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1549 - accuracy: 0.9413\n",
      "Epoch 177/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9482\n",
      "Epoch 178/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.9348\n",
      "Epoch 179/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9442\n",
      "Epoch 180/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.9478\n",
      "Epoch 181/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9453\n",
      "Epoch 182/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9439\n",
      "Epoch 183/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1494 - accuracy: 0.9486\n",
      "Epoch 184/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.9475\n",
      "Epoch 185/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9417\n",
      "Epoch 186/1500\n",
      "87/87 [==============================] - 0s 4ms/step - loss: 0.1688 - accuracy: 0.9359\n",
      "Epoch 187/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9388\n",
      "Epoch 188/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9449\n",
      "Epoch 189/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9366\n",
      "Epoch 190/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9388\n",
      "Epoch 191/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9442\n",
      "Epoch 192/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9424\n",
      "Epoch 193/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9442\n",
      "Epoch 194/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9482\n",
      "Epoch 195/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9439\n",
      "Epoch 196/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9544\n",
      "Epoch 197/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.9406\n",
      "Epoch 198/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1397 - accuracy: 0.9482\n",
      "Epoch 199/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9460\n",
      "Epoch 200/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9381\n",
      "Epoch 201/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9420\n",
      "Epoch 202/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9460\n",
      "Epoch 203/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9497\n",
      "Epoch 204/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9428\n",
      "Epoch 205/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9431\n",
      "Epoch 206/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9384\n",
      "Epoch 207/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9544\n",
      "Epoch 208/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9511\n",
      "Epoch 209/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9381\n",
      "Epoch 210/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9428\n",
      "Epoch 211/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9562\n",
      "Epoch 212/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9489\n",
      "Epoch 213/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9497\n",
      "Epoch 214/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1575 - accuracy: 0.9399\n",
      "Epoch 215/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9507\n",
      "Epoch 216/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9406\n",
      "Epoch 217/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1446 - accuracy: 0.9446\n",
      "Epoch 218/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9424\n",
      "Epoch 219/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9471\n",
      "Epoch 220/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9471\n",
      "Epoch 221/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.9478\n",
      "Epoch 222/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9489\n",
      "Epoch 223/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9428\n",
      "Epoch 224/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9428\n",
      "Epoch 225/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9526\n",
      "Epoch 226/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9504\n",
      "Epoch 227/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9558\n",
      "Epoch 228/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1266 - accuracy: 0.9507\n",
      "Epoch 229/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.9515\n",
      "Epoch 230/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9515\n",
      "Epoch 231/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9413\n",
      "Epoch 232/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9515\n",
      "Epoch 233/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9518\n",
      "Epoch 234/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9471\n",
      "Epoch 235/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9515\n",
      "Epoch 236/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9482\n",
      "Epoch 237/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.9511\n",
      "Epoch 238/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9576\n",
      "Epoch 239/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9478\n",
      "Epoch 240/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9558\n",
      "Epoch 241/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9402\n",
      "Epoch 242/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9478\n",
      "Epoch 243/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1494 - accuracy: 0.9446\n",
      "Epoch 244/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9529\n",
      "Epoch 245/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9533\n",
      "Epoch 246/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9497\n",
      "Epoch 247/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9526\n",
      "Epoch 248/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9540\n",
      "Epoch 249/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9536\n",
      "Epoch 250/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9555\n",
      "Epoch 251/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9612\n",
      "Epoch 252/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9435\n",
      "Epoch 253/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9540\n",
      "Epoch 254/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9439\n",
      "Epoch 255/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9468\n",
      "Epoch 256/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9569\n",
      "Epoch 257/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9478\n",
      "Epoch 258/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9573\n",
      "Epoch 259/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9562\n",
      "Epoch 260/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9547\n",
      "Epoch 261/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9544\n",
      "Epoch 262/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9511\n",
      "Epoch 263/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1414 - accuracy: 0.9471\n",
      "Epoch 264/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9594\n",
      "Epoch 265/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9576\n",
      "Epoch 266/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9576\n",
      "Epoch 267/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9536\n",
      "Epoch 268/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9565\n",
      "Epoch 269/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9486\n",
      "Epoch 270/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.9442\n",
      "Epoch 271/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9500\n",
      "Epoch 272/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9526\n",
      "Epoch 273/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9591\n",
      "Epoch 274/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9605\n",
      "Epoch 275/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9670\n",
      "Epoch 276/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9562\n",
      "Epoch 277/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9533\n",
      "Epoch 278/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9555\n",
      "Epoch 279/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9562\n",
      "Epoch 280/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9544\n",
      "Epoch 281/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9565\n",
      "Epoch 282/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9547\n",
      "Epoch 283/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1111 - accuracy: 0.9591\n",
      "Epoch 284/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1330 - accuracy: 0.9511\n",
      "Epoch 285/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9591\n",
      "Epoch 286/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9583\n",
      "Epoch 287/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9544\n",
      "Epoch 288/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9569\n",
      "Epoch 289/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9558\n",
      "Epoch 290/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.9518\n",
      "Epoch 291/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9558\n",
      "Epoch 292/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9486\n",
      "Epoch 293/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9569\n",
      "Epoch 294/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9540\n",
      "Epoch 295/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9562\n",
      "Epoch 296/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9544\n",
      "Epoch 297/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9594\n",
      "Epoch 298/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9565\n",
      "Epoch 299/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9609\n",
      "Epoch 300/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9649\n",
      "Epoch 301/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9605\n",
      "Epoch 302/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9594\n",
      "Epoch 303/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9569\n",
      "Epoch 304/1500\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9522\n",
      "Epoch 305/1500\n",
      "85/87 [============================>.] - ETA: 0s - loss: 0.1173 - accuracy: 0.9555Restoring model weights from the end of the best epoch: 275.\n",
      "87/87 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9558\n",
      "Epoch 305: early stopping\n",
      "8/8 [==============================] - 0s 871us/step - loss: 1.0294 - accuracy: 0.6429\n",
      "8/8 [==============================] - 0s 752us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (22/29)\n",
      "Before appending - Cat IDs: 612, Predictions: 612, Actuals: 612, Gender: 612\n",
      "After appending - Cat IDs: 850, Predictions: 850, Actuals: 850, Gender: 850\n",
      "Final Test Results - Loss: 1.0293524265289307, Accuracy: 0.6428571343421936, Precision: 0.6737874575223972, Recall: 0.5844210876786754, F1 Score: 0.6182739672854737\n",
      "Confusion Matrix:\n",
      " [[114   1  32]\n",
      " [ 10  19   0]\n",
      " [ 42   0  20]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.7024096693837469\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8600136563181877\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7280996441841125\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.7128615891106751\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7101165975593238\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[2]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be692ae4-6d3f-4353-b5bf-554d20da4df3",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "8da9a092-ed2e-4397-a6c8-2c4888735265",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 850, Predictions: 850, Actuals: 850, Gender: 850\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "51cf386a-c49e-4716-ba15-aa3b7930419a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a8d43ac5-d50e-430d-98a1-ff4f45006bae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.78 (86/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ccc9acb7-bb1b-42a6-bb25-cdf5a3356315",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "95e69b27-cae1-4a3a-ba70-5244a11aadf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, adult, kitten, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, kitten, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, kitten, adult, kitten, kitten, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[senior, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, senior, adult, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "48    042A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, a...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, adult, kitten, adult, adult, a...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, adult, senior, senior, senior, adult,...        senior           senior                   True\n",
       "64    058A                           [senior, senior, senior]        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "56    051A  [adult, kitten, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, kitten, adult, kitten, kitten, kitten,...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "50    044A             [adult, adult, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, adult,...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A    [senior, senior, senior, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                     [senior, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                    [senior, senior, adult, senior]        senior           senior                   True\n",
       "97    102A                                    [senior, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, adult, senior, ...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "16    014B  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "24    022A  [adult, kitten, adult, senior, adult, adult, a...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, senior, adult, adult, senior, senior, ...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                     [adult, senior, adult, senior]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "32    026B                                            [adult]         adult            adult                   True\n",
       "45    039A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "43    037A        [adult, adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "42    036A  [senior, senior, adult, senior, adult, adult, ...         adult            adult                   True\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "98    103A  [adult, adult, senior, adult, senior, senior, ...        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "101   106A  [adult, senior, senior, senior, adult, adult, ...         adult           senior                  False\n",
       "103   109A      [adult, kitten, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "61    055A  [adult, adult, senior, adult, adult, adult, ad...         adult           senior                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "62    056A                              [adult, adult, adult]         adult           senior                  False\n",
       "90    095A  [senior, senior, adult, senior, senior, senior...        senior            adult                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "88    093A                                    [adult, senior]         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, adult, senior, senior, a...         adult           senior                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "38    032A                                   [kitten, kitten]        kitten            adult                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "36    029A  [senior, senior, senior, senior, adult, senior...        senior            adult                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d36b3c54-3377-4249-a774-6d31557e36da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     61\n",
      "kitten    13\n",
      "senior    12\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b36eb8a4-57f3-48c0-b92c-4a8e5a52c59e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             61  83.561644\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22             12  54.545455\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "1750e2da-df8c-4f00-b860-539dd822864f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6b34e-2368-4956-8241-8e6ab6e8cf81",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "fdefd9be-6e6d-4903-a0ec-7824da4313d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          555            424  76.396396\n",
      "1           kitten          117            100  85.470085\n",
      "2           senior          178             89  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "57576e25-349c-46e0-b57b-8bf3cee3b5de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b69785-d4c1-49ab-ba75-4a733326cdc1",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "15c5b6e5-d7b0-4089-ae78-3dafe1cead3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      154     72.30\n",
      "1          M    343      244     71.14\n",
      "2          X    294      215     73.13\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b6001f1b-3f01-42e9-8761-4253acbed5e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      154     72.30\n",
      "1          M    343      244     71.14\n",
      "2          X    294      215     73.13\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b5d44-f489-4de0-8785-4bfa52d09797",
   "metadata": {},
   "source": [
    "# RANDOM SEED 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "c4439fc4-670c-4009-8ca9-23c419ee99ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    1020\n",
      "kitten     992\n",
      "adult      842\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[3])) \n",
    "np.random.seed(int(random_seeds[3]))\n",
    "tf.random.set_seed(int(random_seeds[3]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_3.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "5fe04e3d-fd90-43c1-97af-5eac75e35f85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2ed5261d-2aeb-412e-a017-65b461a2cf5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70aa6-1716-400a-be48-cccb3511542e",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "fb9d0248-481e-4807-85dd-407fa88963f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "072A     9\n",
      "015A     9\n",
      "051B     9\n",
      "022A     9\n",
      "033A     9\n",
      "045A     9\n",
      "095A     8\n",
      "013B     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "109A     6\n",
      "023A     6\n",
      "037A     6\n",
      "075A     5\n",
      "070A     5\n",
      "025C     5\n",
      "021A     5\n",
      "034A     5\n",
      "044A     5\n",
      "023B     5\n",
      "003A     4\n",
      "105A     4\n",
      "035A     4\n",
      "026A     4\n",
      "052A     4\n",
      "062A     4\n",
      "012A     3\n",
      "113A     3\n",
      "058A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "025B     2\n",
      "032A     2\n",
      "093A     2\n",
      "054A     2\n",
      "069A     2\n",
      "087A     2\n",
      "038A     2\n",
      "073A     1\n",
      "004A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "091A     1\n",
      "019B     1\n",
      "066A     1\n",
      "048A     1\n",
      "092A     1\n",
      "026C     1\n",
      "076A     1\n",
      "043A     1\n",
      "041A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "019A    17\n",
      "101A    15\n",
      "039A    12\n",
      "063A    11\n",
      "071A    10\n",
      "005A    10\n",
      "065A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "099A     7\n",
      "008A     6\n",
      "009A     4\n",
      "104A     4\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "096A     1\n",
      "049A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    296\n",
      "X    286\n",
      "F    208\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    62\n",
      "F    44\n",
      "M    41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 028...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 019A, 101A, 005A, 065A, 039A, 009A, 063...\n",
      "kitten                                         [050A, 049A]\n",
      "senior                       [104A, 056A, 094A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 14, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 2, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "Length of X_train_val:\n",
      "790\n",
      "Length of y_train_val:\n",
      "790\n",
      "Length of groups_train_val:\n",
      "790\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1139, 1: 1115, 2: 1059})\n",
      "Epoch 1/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.8474 - accuracy: 0.6405\n",
      "Epoch 2/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6755 - accuracy: 0.7129\n",
      "Epoch 3/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.6051 - accuracy: 0.7422\n",
      "Epoch 4/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5652 - accuracy: 0.7751\n",
      "Epoch 5/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5602 - accuracy: 0.7591\n",
      "Epoch 6/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5465 - accuracy: 0.7721\n",
      "Epoch 7/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.5052 - accuracy: 0.7884\n",
      "Epoch 8/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4931 - accuracy: 0.8023\n",
      "Epoch 9/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4880 - accuracy: 0.7960\n",
      "Epoch 10/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4662 - accuracy: 0.8005\n",
      "Epoch 11/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4605 - accuracy: 0.8092\n",
      "Epoch 12/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4431 - accuracy: 0.8147\n",
      "Epoch 13/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4584 - accuracy: 0.8074\n",
      "Epoch 14/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4344 - accuracy: 0.8132\n",
      "Epoch 15/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4148 - accuracy: 0.8258\n",
      "Epoch 16/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3982 - accuracy: 0.8346\n",
      "Epoch 17/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4118 - accuracy: 0.8340\n",
      "Epoch 18/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.4080 - accuracy: 0.8325\n",
      "Epoch 19/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3882 - accuracy: 0.8397\n",
      "Epoch 20/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3905 - accuracy: 0.8364\n",
      "Epoch 21/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3712 - accuracy: 0.8560\n",
      "Epoch 22/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3743 - accuracy: 0.8415\n",
      "Epoch 23/1500\n",
      "104/104 [==============================] - 0s 4ms/step - loss: 0.3770 - accuracy: 0.8485\n",
      "Epoch 24/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3687 - accuracy: 0.8433\n",
      "Epoch 25/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3660 - accuracy: 0.8488\n",
      "Epoch 26/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3620 - accuracy: 0.8554\n",
      "Epoch 27/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3576 - accuracy: 0.8557\n",
      "Epoch 28/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3428 - accuracy: 0.8621\n",
      "Epoch 29/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3264 - accuracy: 0.8639\n",
      "Epoch 30/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3429 - accuracy: 0.8648\n",
      "Epoch 31/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3436 - accuracy: 0.8645\n",
      "Epoch 32/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3215 - accuracy: 0.8720\n",
      "Epoch 33/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3431 - accuracy: 0.8599\n",
      "Epoch 34/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3270 - accuracy: 0.8687\n",
      "Epoch 35/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3289 - accuracy: 0.8657\n",
      "Epoch 36/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3066 - accuracy: 0.8808\n",
      "Epoch 37/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3255 - accuracy: 0.8741\n",
      "Epoch 38/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3050 - accuracy: 0.8744\n",
      "Epoch 39/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3102 - accuracy: 0.8787\n",
      "Epoch 40/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3160 - accuracy: 0.8793\n",
      "Epoch 41/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3176 - accuracy: 0.8708\n",
      "Epoch 42/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2913 - accuracy: 0.8820\n",
      "Epoch 43/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2864 - accuracy: 0.8877\n",
      "Epoch 44/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2919 - accuracy: 0.8868\n",
      "Epoch 45/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2905 - accuracy: 0.8856\n",
      "Epoch 46/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.3031 - accuracy: 0.8778\n",
      "Epoch 47/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2780 - accuracy: 0.8962\n",
      "Epoch 48/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2825 - accuracy: 0.8880\n",
      "Epoch 49/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8808\n",
      "Epoch 50/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2967 - accuracy: 0.8838\n",
      "Epoch 51/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2890 - accuracy: 0.8838\n",
      "Epoch 52/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2771 - accuracy: 0.8916\n",
      "Epoch 53/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2676 - accuracy: 0.8950\n",
      "Epoch 54/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2689 - accuracy: 0.8959\n",
      "Epoch 55/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2751 - accuracy: 0.8889\n",
      "Epoch 56/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2838 - accuracy: 0.8910\n",
      "Epoch 57/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2691 - accuracy: 0.8895\n",
      "Epoch 58/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2604 - accuracy: 0.8925\n",
      "Epoch 59/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.8886\n",
      "Epoch 60/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2593 - accuracy: 0.8941\n",
      "Epoch 61/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2578 - accuracy: 0.9025\n",
      "Epoch 62/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2580 - accuracy: 0.9001\n",
      "Epoch 63/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.9028\n",
      "Epoch 64/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2694 - accuracy: 0.8953\n",
      "Epoch 65/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2578 - accuracy: 0.9016\n",
      "Epoch 66/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2565 - accuracy: 0.8968\n",
      "Epoch 67/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2492 - accuracy: 0.9013\n",
      "Epoch 68/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2532 - accuracy: 0.9016\n",
      "Epoch 69/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2394 - accuracy: 0.9052\n",
      "Epoch 70/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2532 - accuracy: 0.8931\n",
      "Epoch 71/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2521 - accuracy: 0.8938\n",
      "Epoch 72/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2439 - accuracy: 0.9040\n",
      "Epoch 73/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2453 - accuracy: 0.9046\n",
      "Epoch 74/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2604 - accuracy: 0.8998\n",
      "Epoch 75/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2315 - accuracy: 0.9143\n",
      "Epoch 76/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2444 - accuracy: 0.9082\n",
      "Epoch 77/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2266 - accuracy: 0.9131\n",
      "Epoch 78/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2384 - accuracy: 0.9025\n",
      "Epoch 79/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2317 - accuracy: 0.9128\n",
      "Epoch 80/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2318 - accuracy: 0.9110\n",
      "Epoch 81/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2199 - accuracy: 0.9107\n",
      "Epoch 82/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2249 - accuracy: 0.9146\n",
      "Epoch 83/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2507 - accuracy: 0.9034\n",
      "Epoch 84/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9125\n",
      "Epoch 85/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2185 - accuracy: 0.9158\n",
      "Epoch 86/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2278 - accuracy: 0.9073\n",
      "Epoch 87/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2166 - accuracy: 0.9209\n",
      "Epoch 88/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9140\n",
      "Epoch 89/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9167\n",
      "Epoch 90/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2234 - accuracy: 0.9143\n",
      "Epoch 91/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2223 - accuracy: 0.9173\n",
      "Epoch 92/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9233\n",
      "Epoch 93/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9152\n",
      "Epoch 94/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2126 - accuracy: 0.9164\n",
      "Epoch 95/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9233\n",
      "Epoch 96/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2181 - accuracy: 0.9167\n",
      "Epoch 97/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2178 - accuracy: 0.9173\n",
      "Epoch 98/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2064 - accuracy: 0.9215\n",
      "Epoch 99/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2142 - accuracy: 0.9170\n",
      "Epoch 100/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9146\n",
      "Epoch 101/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9194\n",
      "Epoch 102/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1959 - accuracy: 0.9239\n",
      "Epoch 103/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2288 - accuracy: 0.9116\n",
      "Epoch 104/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9200\n",
      "Epoch 105/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2175 - accuracy: 0.9158\n",
      "Epoch 106/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2111 - accuracy: 0.9200\n",
      "Epoch 107/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9221\n",
      "Epoch 108/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2111 - accuracy: 0.9146\n",
      "Epoch 109/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2030 - accuracy: 0.9245\n",
      "Epoch 110/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2091 - accuracy: 0.9164\n",
      "Epoch 111/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9348\n",
      "Epoch 112/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1888 - accuracy: 0.9242\n",
      "Epoch 113/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9206\n",
      "Epoch 114/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1927 - accuracy: 0.9270\n",
      "Epoch 115/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9267\n",
      "Epoch 116/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1926 - accuracy: 0.9288\n",
      "Epoch 117/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2072 - accuracy: 0.9206\n",
      "Epoch 118/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9285\n",
      "Epoch 119/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9209\n",
      "Epoch 120/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.9227\n",
      "Epoch 121/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2066 - accuracy: 0.9200\n",
      "Epoch 122/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1900 - accuracy: 0.9279\n",
      "Epoch 123/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1928 - accuracy: 0.9257\n",
      "Epoch 124/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9273\n",
      "Epoch 125/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2019 - accuracy: 0.9254\n",
      "Epoch 126/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9230\n",
      "Epoch 127/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2103 - accuracy: 0.9194\n",
      "Epoch 128/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.9236\n",
      "Epoch 129/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9239\n",
      "Epoch 130/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2104 - accuracy: 0.9227\n",
      "Epoch 131/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1867 - accuracy: 0.9330\n",
      "Epoch 132/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9312\n",
      "Epoch 133/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.9327\n",
      "Epoch 134/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1698 - accuracy: 0.9345\n",
      "Epoch 135/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9212\n",
      "Epoch 136/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.9242\n",
      "Epoch 137/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.9309\n",
      "Epoch 138/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9227\n",
      "Epoch 139/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9291\n",
      "Epoch 140/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9279\n",
      "Epoch 141/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9357\n",
      "Epoch 142/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1947 - accuracy: 0.9206\n",
      "Epoch 143/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9291\n",
      "Epoch 144/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9333\n",
      "Epoch 145/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9333\n",
      "Epoch 146/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.9312\n",
      "Epoch 147/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9363\n",
      "Epoch 148/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9369\n",
      "Epoch 149/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9327\n",
      "Epoch 150/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9312\n",
      "Epoch 151/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1651 - accuracy: 0.9396\n",
      "Epoch 152/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.9318\n",
      "Epoch 153/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1688 - accuracy: 0.9315\n",
      "Epoch 154/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9360\n",
      "Epoch 155/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1672 - accuracy: 0.9306\n",
      "Epoch 156/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.9369\n",
      "Epoch 157/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9351\n",
      "Epoch 158/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9348\n",
      "Epoch 159/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1648 - accuracy: 0.9351\n",
      "Epoch 160/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9354\n",
      "Epoch 161/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.9342\n",
      "Epoch 162/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9354\n",
      "Epoch 163/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.9378\n",
      "Epoch 164/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.9357\n",
      "Epoch 165/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9387\n",
      "Epoch 166/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9442\n",
      "Epoch 167/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9375\n",
      "Epoch 168/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1582 - accuracy: 0.9387\n",
      "Epoch 169/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9342\n",
      "Epoch 170/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.9339\n",
      "Epoch 171/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.9372\n",
      "Epoch 172/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9420\n",
      "Epoch 173/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9390\n",
      "Epoch 174/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1688 - accuracy: 0.9372\n",
      "Epoch 175/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1642 - accuracy: 0.9354\n",
      "Epoch 176/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9360\n",
      "Epoch 177/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9378\n",
      "Epoch 178/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9436\n",
      "Epoch 179/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.9363\n",
      "Epoch 180/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9387\n",
      "Epoch 181/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9348\n",
      "Epoch 182/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.9390\n",
      "Epoch 183/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1743 - accuracy: 0.9318\n",
      "Epoch 184/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1525 - accuracy: 0.9433\n",
      "Epoch 185/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9475\n",
      "Epoch 186/1500\n",
      "104/104 [==============================] - 0s 3ms/step - loss: 0.1590 - accuracy: 0.9445\n",
      "Epoch 187/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1661 - accuracy: 0.9351\n",
      "Epoch 188/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9457\n",
      "Epoch 189/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1521 - accuracy: 0.9417\n",
      "Epoch 190/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.9469\n",
      "Epoch 191/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1672 - accuracy: 0.9345\n",
      "Epoch 192/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9414\n",
      "Epoch 193/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9499\n",
      "Epoch 194/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9439\n",
      "Epoch 195/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9427\n",
      "Epoch 196/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1590 - accuracy: 0.9408\n",
      "Epoch 197/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9454\n",
      "Epoch 198/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9460\n",
      "Epoch 199/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9463\n",
      "Epoch 200/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9420\n",
      "Epoch 201/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1468 - accuracy: 0.9430\n",
      "Epoch 202/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9445\n",
      "Epoch 203/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9393\n",
      "Epoch 204/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1547 - accuracy: 0.9399\n",
      "Epoch 205/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9454\n",
      "Epoch 206/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9457\n",
      "Epoch 207/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9580\n",
      "Epoch 208/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1494 - accuracy: 0.9442\n",
      "Epoch 209/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9478\n",
      "Epoch 210/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9378\n",
      "Epoch 211/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9478\n",
      "Epoch 212/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9360\n",
      "Epoch 213/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1546 - accuracy: 0.9433\n",
      "Epoch 214/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9493\n",
      "Epoch 215/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9472\n",
      "Epoch 216/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1493 - accuracy: 0.9454\n",
      "Epoch 217/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9439\n",
      "Epoch 218/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9499\n",
      "Epoch 219/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9414\n",
      "Epoch 220/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9466\n",
      "Epoch 221/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9463\n",
      "Epoch 222/1500\n",
      "104/104 [==============================] - 0s 992us/step - loss: 0.1383 - accuracy: 0.9484\n",
      "Epoch 223/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9387\n",
      "Epoch 224/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9448\n",
      "Epoch 225/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9396\n",
      "Epoch 226/1500\n",
      "104/104 [==============================] - 0s 988us/step - loss: 0.1455 - accuracy: 0.9460\n",
      "Epoch 227/1500\n",
      "104/104 [==============================] - 0s 992us/step - loss: 0.1503 - accuracy: 0.9448\n",
      "Epoch 228/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9442\n",
      "Epoch 229/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9433\n",
      "Epoch 230/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9411\n",
      "Epoch 231/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9475\n",
      "Epoch 232/1500\n",
      "104/104 [==============================] - 0s 992us/step - loss: 0.1332 - accuracy: 0.9511\n",
      "Epoch 233/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9502\n",
      "Epoch 234/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9475\n",
      "Epoch 235/1500\n",
      "104/104 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9520\n",
      "Epoch 236/1500\n",
      "104/104 [==============================] - 0s 961us/step - loss: 0.1313 - accuracy: 0.9520\n",
      "Epoch 237/1500\n",
      " 53/104 [==============>...............] - ETA: 0s - loss: 0.1515 - accuracy: 0.9434Restoring model weights from the end of the best epoch: 207.\n",
      "104/104 [==============================] - 0s 998us/step - loss: 0.1412 - accuracy: 0.9448\n",
      "Epoch 237: early stopping\n",
      "5/5 [==============================] - 0s 928us/step - loss: 0.6102 - accuracy: 0.7823\n",
      "5/5 [==============================] - 0s 720us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.84 (21/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "Final Test Results - Loss: 0.6102049350738525, Accuracy: 0.7823129296302795, Precision: 0.6401360544217688, Recall: 0.837280701754386, F1 Score: 0.6996674183218525\n",
      "Confusion Matrix:\n",
      " [[93  6 21]\n",
      " [ 0  8  0]\n",
      " [ 5  0 14]]\n",
      "outer_fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "042A    14\n",
      "059A    14\n",
      "001A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "025A    11\n",
      "036A    11\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "065A     9\n",
      "051B     9\n",
      "072A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "099A     7\n",
      "008A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "026A     4\n",
      "009A     4\n",
      "105A     4\n",
      "035A     4\n",
      "003A     4\n",
      "104A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "014A     3\n",
      "012A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "018A     2\n",
      "032A     2\n",
      "093A     2\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "048A     1\n",
      "096A     1\n",
      "088A     1\n",
      "076A     1\n",
      "091A     1\n",
      "115A     1\n",
      "110A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "097B    14\n",
      "068A    11\n",
      "015A     9\n",
      "045A     9\n",
      "033A     9\n",
      "109A     6\n",
      "053A     6\n",
      "075A     5\n",
      "025C     5\n",
      "062A     4\n",
      "060A     3\n",
      "025B     2\n",
      "087A     2\n",
      "038A     2\n",
      "054A     2\n",
      "041A     1\n",
      "043A     1\n",
      "026C     1\n",
      "066A     1\n",
      "004A     1\n",
      "019B     1\n",
      "090A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    299\n",
      "F    216\n",
      "M    214\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    123\n",
      "X     49\n",
      "F     36\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 071A, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 050...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 097B, 067A, 020A, 062A, 002B, 029...\n",
      "kitten                             [109A, 043A, 041A, 045A]\n",
      "senior                             [055A, 054A, 090A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "Length of X_train_val:\n",
      "729\n",
      "Length of y_train_val:\n",
      "729\n",
      "Length of groups_train_val:\n",
      "729\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({1: 1046, 2: 1022, 0: 1014})\n",
      "Epoch 1/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.8521 - accuracy: 0.6363\n",
      "Epoch 2/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.7314 - accuracy: 0.6944\n",
      "Epoch 3/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.6422 - accuracy: 0.7274\n",
      "Epoch 4/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5950 - accuracy: 0.7550\n",
      "Epoch 5/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.5698 - accuracy: 0.7625\n",
      "Epoch 6/1500\n",
      "97/97 [==============================] - 0s 981us/step - loss: 0.5594 - accuracy: 0.7677\n",
      "Epoch 7/1500\n",
      "97/97 [==============================] - 0s 964us/step - loss: 0.5259 - accuracy: 0.7729\n",
      "Epoch 8/1500\n",
      "97/97 [==============================] - 0s 936us/step - loss: 0.5140 - accuracy: 0.7917\n",
      "Epoch 9/1500\n",
      "97/97 [==============================] - 0s 958us/step - loss: 0.5105 - accuracy: 0.7914\n",
      "Epoch 10/1500\n",
      "97/97 [==============================] - 0s 933us/step - loss: 0.4957 - accuracy: 0.7933\n",
      "Epoch 11/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4764 - accuracy: 0.8138\n",
      "Epoch 12/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4774 - accuracy: 0.8014\n",
      "Epoch 13/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4657 - accuracy: 0.8151\n",
      "Epoch 14/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4533 - accuracy: 0.8125\n",
      "Epoch 15/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4559 - accuracy: 0.8108\n",
      "Epoch 16/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4219 - accuracy: 0.8235\n",
      "Epoch 17/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4109 - accuracy: 0.8332\n",
      "Epoch 18/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4085 - accuracy: 0.8348\n",
      "Epoch 19/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4116 - accuracy: 0.8384\n",
      "Epoch 20/1500\n",
      "97/97 [==============================] - 0s 958us/step - loss: 0.3974 - accuracy: 0.8348\n",
      "Epoch 21/1500\n",
      "97/97 [==============================] - 0s 973us/step - loss: 0.4088 - accuracy: 0.8297\n",
      "Epoch 22/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.4000 - accuracy: 0.8374\n",
      "Epoch 23/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3688 - accuracy: 0.8475\n",
      "Epoch 24/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3723 - accuracy: 0.8494\n",
      "Epoch 25/1500\n",
      "97/97 [==============================] - 0s 998us/step - loss: 0.3685 - accuracy: 0.8452\n",
      "Epoch 26/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3680 - accuracy: 0.8501\n",
      "Epoch 27/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3512 - accuracy: 0.8592\n",
      "Epoch 28/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3619 - accuracy: 0.8572\n",
      "Epoch 29/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3380 - accuracy: 0.8650\n",
      "Epoch 30/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3452 - accuracy: 0.8556\n",
      "Epoch 31/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3334 - accuracy: 0.8615\n",
      "Epoch 32/1500\n",
      "97/97 [==============================] - 0s 973us/step - loss: 0.3423 - accuracy: 0.8660\n",
      "Epoch 33/1500\n",
      "97/97 [==============================] - 0s 986us/step - loss: 0.3291 - accuracy: 0.8722\n",
      "Epoch 34/1500\n",
      "97/97 [==============================] - 0s 963us/step - loss: 0.3382 - accuracy: 0.8556\n",
      "Epoch 35/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8615\n",
      "Epoch 36/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3228 - accuracy: 0.8679\n",
      "Epoch 37/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3282 - accuracy: 0.8615\n",
      "Epoch 38/1500\n",
      "97/97 [==============================] - 0s 977us/step - loss: 0.3157 - accuracy: 0.8689\n",
      "Epoch 39/1500\n",
      "97/97 [==============================] - 0s 981us/step - loss: 0.3140 - accuracy: 0.8725\n",
      "Epoch 40/1500\n",
      "97/97 [==============================] - 0s 947us/step - loss: 0.3304 - accuracy: 0.8696\n",
      "Epoch 41/1500\n",
      "97/97 [==============================] - 0s 930us/step - loss: 0.3137 - accuracy: 0.8709\n",
      "Epoch 42/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3099 - accuracy: 0.8715\n",
      "Epoch 43/1500\n",
      "97/97 [==============================] - 0s 986us/step - loss: 0.3172 - accuracy: 0.8712\n",
      "Epoch 44/1500\n",
      "97/97 [==============================] - 0s 912us/step - loss: 0.3074 - accuracy: 0.8751\n",
      "Epoch 45/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3070 - accuracy: 0.8725\n",
      "Epoch 46/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2901 - accuracy: 0.8829\n",
      "Epoch 47/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2865 - accuracy: 0.8842\n",
      "Epoch 48/1500\n",
      "97/97 [==============================] - 0s 958us/step - loss: 0.3007 - accuracy: 0.8783\n",
      "Epoch 49/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2764 - accuracy: 0.8913\n",
      "Epoch 50/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3020 - accuracy: 0.8780\n",
      "Epoch 51/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.3014 - accuracy: 0.8754\n",
      "Epoch 52/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2813 - accuracy: 0.8855\n",
      "Epoch 53/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2921 - accuracy: 0.8835\n",
      "Epoch 54/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2845 - accuracy: 0.8874\n",
      "Epoch 55/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2842 - accuracy: 0.8868\n",
      "Epoch 56/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2840 - accuracy: 0.8900\n",
      "Epoch 57/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.8894\n",
      "Epoch 58/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2596 - accuracy: 0.8991\n",
      "Epoch 59/1500\n",
      "97/97 [==============================] - 0s 923us/step - loss: 0.2669 - accuracy: 0.9023\n",
      "Epoch 60/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2776 - accuracy: 0.8894\n",
      "Epoch 61/1500\n",
      "97/97 [==============================] - 0s 996us/step - loss: 0.2675 - accuracy: 0.8942\n",
      "Epoch 62/1500\n",
      "97/97 [==============================] - 0s 967us/step - loss: 0.2625 - accuracy: 0.9004\n",
      "Epoch 63/1500\n",
      "97/97 [==============================] - 0s 994us/step - loss: 0.2699 - accuracy: 0.8903\n",
      "Epoch 64/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.8958\n",
      "Epoch 65/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2566 - accuracy: 0.9040\n",
      "Epoch 66/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2747 - accuracy: 0.8897\n",
      "Epoch 67/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.8991\n",
      "Epoch 68/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2566 - accuracy: 0.8945\n",
      "Epoch 69/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2393 - accuracy: 0.9127\n",
      "Epoch 70/1500\n",
      "97/97 [==============================] - 0s 980us/step - loss: 0.2503 - accuracy: 0.8981\n",
      "Epoch 71/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2464 - accuracy: 0.9056\n",
      "Epoch 72/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2508 - accuracy: 0.9014\n",
      "Epoch 73/1500\n",
      "97/97 [==============================] - 0s 965us/step - loss: 0.2314 - accuracy: 0.9111\n",
      "Epoch 74/1500\n",
      "97/97 [==============================] - 0s 973us/step - loss: 0.2539 - accuracy: 0.8994\n",
      "Epoch 75/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2322 - accuracy: 0.9079\n",
      "Epoch 76/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.9049\n",
      "Epoch 77/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.9030\n",
      "Epoch 78/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2409 - accuracy: 0.9056\n",
      "Epoch 79/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2427 - accuracy: 0.8975\n",
      "Epoch 80/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2350 - accuracy: 0.9046\n",
      "Epoch 81/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.9069\n",
      "Epoch 82/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2268 - accuracy: 0.9189\n",
      "Epoch 83/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2363 - accuracy: 0.9056\n",
      "Epoch 84/1500\n",
      "97/97 [==============================] - 0s 932us/step - loss: 0.2331 - accuracy: 0.9069\n",
      "Epoch 85/1500\n",
      "97/97 [==============================] - 0s 947us/step - loss: 0.2313 - accuracy: 0.9095\n",
      "Epoch 86/1500\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9137\n",
      "Epoch 87/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2343 - accuracy: 0.9062\n",
      "Epoch 88/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2208 - accuracy: 0.9195\n",
      "Epoch 89/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2335 - accuracy: 0.9036\n",
      "Epoch 90/1500\n",
      "97/97 [==============================] - 0s 994us/step - loss: 0.2433 - accuracy: 0.9085\n",
      "Epoch 91/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2174 - accuracy: 0.9130\n",
      "Epoch 92/1500\n",
      "97/97 [==============================] - 0s 934us/step - loss: 0.2109 - accuracy: 0.9179\n",
      "Epoch 93/1500\n",
      "97/97 [==============================] - 0s 962us/step - loss: 0.2069 - accuracy: 0.9202\n",
      "Epoch 94/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9130\n",
      "Epoch 95/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9140\n",
      "Epoch 96/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2239 - accuracy: 0.9082\n",
      "Epoch 97/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2146 - accuracy: 0.9134\n",
      "Epoch 98/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2192 - accuracy: 0.9147\n",
      "Epoch 99/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2191 - accuracy: 0.9082\n",
      "Epoch 100/1500\n",
      "97/97 [==============================] - 0s 969us/step - loss: 0.1959 - accuracy: 0.9335\n",
      "Epoch 101/1500\n",
      "97/97 [==============================] - 0s 940us/step - loss: 0.2071 - accuracy: 0.9189\n",
      "Epoch 102/1500\n",
      "97/97 [==============================] - 0s 944us/step - loss: 0.2277 - accuracy: 0.9117\n",
      "Epoch 103/1500\n",
      "97/97 [==============================] - 0s 919us/step - loss: 0.2191 - accuracy: 0.9150\n",
      "Epoch 104/1500\n",
      "97/97 [==============================] - 0s 987us/step - loss: 0.2195 - accuracy: 0.9121\n",
      "Epoch 105/1500\n",
      "97/97 [==============================] - 0s 962us/step - loss: 0.2180 - accuracy: 0.9140\n",
      "Epoch 106/1500\n",
      "97/97 [==============================] - 0s 1000us/step - loss: 0.2250 - accuracy: 0.9104\n",
      "Epoch 107/1500\n",
      "97/97 [==============================] - 0s 962us/step - loss: 0.2265 - accuracy: 0.9088\n",
      "Epoch 108/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9263\n",
      "Epoch 109/1500\n",
      "97/97 [==============================] - 0s 989us/step - loss: 0.2234 - accuracy: 0.9127\n",
      "Epoch 110/1500\n",
      "97/97 [==============================] - 0s 989us/step - loss: 0.1957 - accuracy: 0.9257\n",
      "Epoch 111/1500\n",
      "97/97 [==============================] - 0s 940us/step - loss: 0.2011 - accuracy: 0.9192\n",
      "Epoch 112/1500\n",
      "97/97 [==============================] - 0s 961us/step - loss: 0.2124 - accuracy: 0.9186\n",
      "Epoch 113/1500\n",
      "97/97 [==============================] - 0s 972us/step - loss: 0.1920 - accuracy: 0.9319\n",
      "Epoch 114/1500\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2152 - accuracy: 0.9192\n",
      "Epoch 115/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2132 - accuracy: 0.9186\n",
      "Epoch 116/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9189\n",
      "Epoch 117/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2151 - accuracy: 0.9225\n",
      "Epoch 118/1500\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2005 - accuracy: 0.9254\n",
      "Epoch 119/1500\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.1865 - accuracy: 0.9341\n",
      "Epoch 120/1500\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2072 - accuracy: 0.9176\n",
      "Epoch 121/1500\n",
      "97/97 [==============================] - 0s 5ms/step - loss: 0.1840 - accuracy: 0.9286\n",
      "Epoch 122/1500\n",
      "97/97 [==============================] - 0s 2ms/step - loss: 0.2025 - accuracy: 0.9218\n",
      "Epoch 123/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2037 - accuracy: 0.9231\n",
      "Epoch 124/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2068 - accuracy: 0.9205\n",
      "Epoch 125/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2009 - accuracy: 0.9267\n",
      "Epoch 126/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9338\n",
      "Epoch 127/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1923 - accuracy: 0.9283\n",
      "Epoch 128/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9296\n",
      "Epoch 129/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1913 - accuracy: 0.9244\n",
      "Epoch 130/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.2192 - accuracy: 0.9189\n",
      "Epoch 131/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9302\n",
      "Epoch 132/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.9354\n",
      "Epoch 133/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9260\n",
      "Epoch 134/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9254\n",
      "Epoch 135/1500\n",
      "97/97 [==============================] - 0s 987us/step - loss: 0.1936 - accuracy: 0.9280\n",
      "Epoch 136/1500\n",
      "97/97 [==============================] - 0s 989us/step - loss: 0.2049 - accuracy: 0.9238\n",
      "Epoch 137/1500\n",
      "97/97 [==============================] - 0s 994us/step - loss: 0.1882 - accuracy: 0.9276\n",
      "Epoch 138/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9225\n",
      "Epoch 139/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9293\n",
      "Epoch 140/1500\n",
      "97/97 [==============================] - 0s 962us/step - loss: 0.2082 - accuracy: 0.9163\n",
      "Epoch 141/1500\n",
      "97/97 [==============================] - 0s 965us/step - loss: 0.1830 - accuracy: 0.9286\n",
      "Epoch 142/1500\n",
      "97/97 [==============================] - 0s 934us/step - loss: 0.1801 - accuracy: 0.9325\n",
      "Epoch 143/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.9280\n",
      "Epoch 144/1500\n",
      "97/97 [==============================] - 0s 931us/step - loss: 0.1782 - accuracy: 0.9289\n",
      "Epoch 145/1500\n",
      "97/97 [==============================] - 0s 941us/step - loss: 0.1892 - accuracy: 0.9254\n",
      "Epoch 146/1500\n",
      "97/97 [==============================] - 0s 978us/step - loss: 0.1817 - accuracy: 0.9267\n",
      "Epoch 147/1500\n",
      "97/97 [==============================] - 0s 937us/step - loss: 0.1762 - accuracy: 0.9302\n",
      "Epoch 148/1500\n",
      "97/97 [==============================] - 0s 951us/step - loss: 0.1810 - accuracy: 0.9306\n",
      "Epoch 149/1500\n",
      "97/97 [==============================] - 0s 966us/step - loss: 0.1865 - accuracy: 0.9221\n",
      "Epoch 150/1500\n",
      "97/97 [==============================] - 0s 973us/step - loss: 0.1786 - accuracy: 0.9341\n",
      "Epoch 151/1500\n",
      "97/97 [==============================] - 0s 954us/step - loss: 0.1892 - accuracy: 0.9299\n",
      "Epoch 152/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.9387\n",
      "Epoch 153/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9322\n",
      "Epoch 154/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9267\n",
      "Epoch 155/1500\n",
      "97/97 [==============================] - 0s 978us/step - loss: 0.1782 - accuracy: 0.9312\n",
      "Epoch 156/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.9299\n",
      "Epoch 157/1500\n",
      "97/97 [==============================] - 0s 966us/step - loss: 0.1921 - accuracy: 0.9263\n",
      "Epoch 158/1500\n",
      "97/97 [==============================] - 0s 940us/step - loss: 0.1718 - accuracy: 0.9364\n",
      "Epoch 159/1500\n",
      "97/97 [==============================] - 0s 991us/step - loss: 0.1626 - accuracy: 0.9380\n",
      "Epoch 160/1500\n",
      "97/97 [==============================] - 0s 941us/step - loss: 0.1660 - accuracy: 0.9371\n",
      "Epoch 161/1500\n",
      "97/97 [==============================] - 0s 951us/step - loss: 0.1665 - accuracy: 0.9377\n",
      "Epoch 162/1500\n",
      "97/97 [==============================] - 0s 935us/step - loss: 0.1662 - accuracy: 0.9409\n",
      "Epoch 163/1500\n",
      "97/97 [==============================] - 0s 933us/step - loss: 0.1817 - accuracy: 0.9296\n",
      "Epoch 164/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9374\n",
      "Epoch 165/1500\n",
      "97/97 [==============================] - 0s 979us/step - loss: 0.1753 - accuracy: 0.9358\n",
      "Epoch 166/1500\n",
      "97/97 [==============================] - 0s 957us/step - loss: 0.1731 - accuracy: 0.9348\n",
      "Epoch 167/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.9296\n",
      "Epoch 168/1500\n",
      "97/97 [==============================] - 0s 962us/step - loss: 0.1769 - accuracy: 0.9273\n",
      "Epoch 169/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.9361\n",
      "Epoch 170/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1677 - accuracy: 0.9332\n",
      "Epoch 171/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1706 - accuracy: 0.9348\n",
      "Epoch 172/1500\n",
      "97/97 [==============================] - 0s 965us/step - loss: 0.1629 - accuracy: 0.9413\n",
      "Epoch 173/1500\n",
      "97/97 [==============================] - 0s 939us/step - loss: 0.1591 - accuracy: 0.9396\n",
      "Epoch 174/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9338\n",
      "Epoch 175/1500\n",
      "97/97 [==============================] - 0s 971us/step - loss: 0.1750 - accuracy: 0.9283\n",
      "Epoch 176/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9409\n",
      "Epoch 177/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9371\n",
      "Epoch 178/1500\n",
      "97/97 [==============================] - 0s 991us/step - loss: 0.1755 - accuracy: 0.9315\n",
      "Epoch 179/1500\n",
      "97/97 [==============================] - 0s 979us/step - loss: 0.1714 - accuracy: 0.9354\n",
      "Epoch 180/1500\n",
      "97/97 [==============================] - 0s 988us/step - loss: 0.1656 - accuracy: 0.9409\n",
      "Epoch 181/1500\n",
      "97/97 [==============================] - 0s 965us/step - loss: 0.1656 - accuracy: 0.9374\n",
      "Epoch 182/1500\n",
      "97/97 [==============================] - 0s 942us/step - loss: 0.1592 - accuracy: 0.9455\n",
      "Epoch 183/1500\n",
      "97/97 [==============================] - 0s 975us/step - loss: 0.1908 - accuracy: 0.9276\n",
      "Epoch 184/1500\n",
      "97/97 [==============================] - 0s 988us/step - loss: 0.1698 - accuracy: 0.9338\n",
      "Epoch 185/1500\n",
      "97/97 [==============================] - 0s 966us/step - loss: 0.1646 - accuracy: 0.9390\n",
      "Epoch 186/1500\n",
      "97/97 [==============================] - 0s 929us/step - loss: 0.1718 - accuracy: 0.9338\n",
      "Epoch 187/1500\n",
      "97/97 [==============================] - 0s 945us/step - loss: 0.1495 - accuracy: 0.9442\n",
      "Epoch 188/1500\n",
      "97/97 [==============================] - 0s 986us/step - loss: 0.1647 - accuracy: 0.9322\n",
      "Epoch 189/1500\n",
      "97/97 [==============================] - 0s 989us/step - loss: 0.1726 - accuracy: 0.9400\n",
      "Epoch 190/1500\n",
      "97/97 [==============================] - 0s 963us/step - loss: 0.1459 - accuracy: 0.9478\n",
      "Epoch 191/1500\n",
      "97/97 [==============================] - 0s 934us/step - loss: 0.1548 - accuracy: 0.9481\n",
      "Epoch 192/1500\n",
      "97/97 [==============================] - 0s 941us/step - loss: 0.1545 - accuracy: 0.9435\n",
      "Epoch 193/1500\n",
      "97/97 [==============================] - 0s 945us/step - loss: 0.1750 - accuracy: 0.9377\n",
      "Epoch 194/1500\n",
      "97/97 [==============================] - 0s 925us/step - loss: 0.1574 - accuracy: 0.9396\n",
      "Epoch 195/1500\n",
      "97/97 [==============================] - 0s 941us/step - loss: 0.1543 - accuracy: 0.9413\n",
      "Epoch 196/1500\n",
      "97/97 [==============================] - 0s 936us/step - loss: 0.1629 - accuracy: 0.9351\n",
      "Epoch 197/1500\n",
      "97/97 [==============================] - 0s 930us/step - loss: 0.1632 - accuracy: 0.9403\n",
      "Epoch 198/1500\n",
      "97/97 [==============================] - 0s 930us/step - loss: 0.1533 - accuracy: 0.9409\n",
      "Epoch 199/1500\n",
      "97/97 [==============================] - 0s 930us/step - loss: 0.1597 - accuracy: 0.9406\n",
      "Epoch 200/1500\n",
      "97/97 [==============================] - 0s 916us/step - loss: 0.1569 - accuracy: 0.9465\n",
      "Epoch 201/1500\n",
      "97/97 [==============================] - 0s 955us/step - loss: 0.1596 - accuracy: 0.9403\n",
      "Epoch 202/1500\n",
      "97/97 [==============================] - 0s 947us/step - loss: 0.1497 - accuracy: 0.9442\n",
      "Epoch 203/1500\n",
      "97/97 [==============================] - 0s 934us/step - loss: 0.1647 - accuracy: 0.9341\n",
      "Epoch 204/1500\n",
      "97/97 [==============================] - 0s 938us/step - loss: 0.1636 - accuracy: 0.9348\n",
      "Epoch 205/1500\n",
      "97/97 [==============================] - 0s 943us/step - loss: 0.1596 - accuracy: 0.9461\n",
      "Epoch 206/1500\n",
      "97/97 [==============================] - 0s 931us/step - loss: 0.1691 - accuracy: 0.9348\n",
      "Epoch 207/1500\n",
      "97/97 [==============================] - 0s 921us/step - loss: 0.1628 - accuracy: 0.9413\n",
      "Epoch 208/1500\n",
      "97/97 [==============================] - 0s 926us/step - loss: 0.1529 - accuracy: 0.9478\n",
      "Epoch 209/1500\n",
      "97/97 [==============================] - 0s 933us/step - loss: 0.1539 - accuracy: 0.9396\n",
      "Epoch 210/1500\n",
      "97/97 [==============================] - 0s 937us/step - loss: 0.1511 - accuracy: 0.9448\n",
      "Epoch 211/1500\n",
      "97/97 [==============================] - 0s 937us/step - loss: 0.1547 - accuracy: 0.9387\n",
      "Epoch 212/1500\n",
      "97/97 [==============================] - 0s 930us/step - loss: 0.1694 - accuracy: 0.9358\n",
      "Epoch 213/1500\n",
      "97/97 [==============================] - 0s 933us/step - loss: 0.1587 - accuracy: 0.9409\n",
      "Epoch 214/1500\n",
      "97/97 [==============================] - 0s 954us/step - loss: 0.1391 - accuracy: 0.9507\n",
      "Epoch 215/1500\n",
      "97/97 [==============================] - 0s 930us/step - loss: 0.1438 - accuracy: 0.9461\n",
      "Epoch 216/1500\n",
      "97/97 [==============================] - 0s 933us/step - loss: 0.1500 - accuracy: 0.9458\n",
      "Epoch 217/1500\n",
      "97/97 [==============================] - 0s 915us/step - loss: 0.1359 - accuracy: 0.9504\n",
      "Epoch 218/1500\n",
      "97/97 [==============================] - 0s 932us/step - loss: 0.1538 - accuracy: 0.9432\n",
      "Epoch 219/1500\n",
      "97/97 [==============================] - 0s 926us/step - loss: 0.1732 - accuracy: 0.9396\n",
      "Epoch 220/1500\n",
      "97/97 [==============================] - 0s 899us/step - loss: 0.1479 - accuracy: 0.9429\n",
      "Epoch 221/1500\n",
      "97/97 [==============================] - 0s 911us/step - loss: 0.1509 - accuracy: 0.9448\n",
      "Epoch 222/1500\n",
      "97/97 [==============================] - 0s 922us/step - loss: 0.1547 - accuracy: 0.9429\n",
      "Epoch 223/1500\n",
      "97/97 [==============================] - 0s 982us/step - loss: 0.1419 - accuracy: 0.9510\n",
      "Epoch 224/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9455\n",
      "Epoch 225/1500\n",
      "97/97 [==============================] - 0s 964us/step - loss: 0.1532 - accuracy: 0.9422\n",
      "Epoch 226/1500\n",
      "97/97 [==============================] - 0s 993us/step - loss: 0.1627 - accuracy: 0.9422\n",
      "Epoch 227/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9445\n",
      "Epoch 228/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9491\n",
      "Epoch 229/1500\n",
      "97/97 [==============================] - 0s 991us/step - loss: 0.1481 - accuracy: 0.9474\n",
      "Epoch 230/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9432\n",
      "Epoch 231/1500\n",
      "97/97 [==============================] - 0s 977us/step - loss: 0.1404 - accuracy: 0.9461\n",
      "Epoch 232/1500\n",
      "97/97 [==============================] - 0s 989us/step - loss: 0.1422 - accuracy: 0.9442\n",
      "Epoch 233/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9445\n",
      "Epoch 234/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9458\n",
      "Epoch 235/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9429\n",
      "Epoch 236/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9455\n",
      "Epoch 237/1500\n",
      "97/97 [==============================] - 0s 991us/step - loss: 0.1642 - accuracy: 0.9371\n",
      "Epoch 238/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9409\n",
      "Epoch 239/1500\n",
      "97/97 [==============================] - 0s 985us/step - loss: 0.1523 - accuracy: 0.9445\n",
      "Epoch 240/1500\n",
      "97/97 [==============================] - 0s 958us/step - loss: 0.1564 - accuracy: 0.9396\n",
      "Epoch 241/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.9435\n",
      "Epoch 242/1500\n",
      "97/97 [==============================] - 0s 982us/step - loss: 0.1460 - accuracy: 0.9432\n",
      "Epoch 243/1500\n",
      "97/97 [==============================] - 0s 990us/step - loss: 0.1631 - accuracy: 0.9409\n",
      "Epoch 244/1500\n",
      "97/97 [==============================] - 0s 966us/step - loss: 0.1427 - accuracy: 0.9491\n",
      "Epoch 245/1500\n",
      "97/97 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9442\n",
      "Epoch 246/1500\n",
      "97/97 [==============================] - 0s 996us/step - loss: 0.1413 - accuracy: 0.9484\n",
      "Epoch 247/1500\n",
      "53/97 [===============>..............] - ETA: 0s - loss: 0.1373 - accuracy: 0.9493Restoring model weights from the end of the best epoch: 217.\n",
      "97/97 [==============================] - 0s 975us/step - loss: 0.1406 - accuracy: 0.9478\n",
      "Epoch 247: early stopping\n",
      "7/7 [==============================] - 0s 792us/step - loss: 0.6242 - accuracy: 0.7740\n",
      "7/7 [==============================] - 0s 606us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.71 (20/28)\n",
      "Before appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "After appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "Final Test Results - Loss: 0.6242465972900391, Accuracy: 0.7740384340286255, Precision: 0.6128246753246753, Recall: 0.6868517866228326, F1 Score: 0.6428913765362364\n",
      "Confusion Matrix:\n",
      " [[137   8  22]\n",
      " [  3  14   0]\n",
      " [ 14   0  10]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "016A    10\n",
      "071A    10\n",
      "065A     9\n",
      "045A     9\n",
      "022A     9\n",
      "015A     9\n",
      "033A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "031A     7\n",
      "117A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "053A     6\n",
      "075A     5\n",
      "044A     5\n",
      "021A     5\n",
      "025C     5\n",
      "034A     5\n",
      "023B     5\n",
      "009A     4\n",
      "003A     4\n",
      "104A     4\n",
      "062A     4\n",
      "113A     3\n",
      "014A     3\n",
      "056A     3\n",
      "060A     3\n",
      "064A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "018A     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "011A     2\n",
      "100A     1\n",
      "090A     1\n",
      "115A     1\n",
      "088A     1\n",
      "024A     1\n",
      "019B     1\n",
      "096A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "043A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "116A    12\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "037A     6\n",
      "023A     6\n",
      "070A     5\n",
      "052A     4\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "012A     3\n",
      "006A     3\n",
      "058A     3\n",
      "032A     2\n",
      "076A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    235\n",
      "X    186\n",
      "F    169\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    162\n",
      "M    102\n",
      "F     83\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 071A, 097B, 028A, 019A, 074A, 067...\n",
      "kitten    [044A, 047A, 109A, 050A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 095A, 072A, 023A, 027...\n",
      "kitten                 [014B, 111A, 040A, 046A, 042A, 110A]\n",
      "senior                       [057A, 106A, 116A, 051B, 058A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A' '010A'\n",
      " '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '031A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '047A' '048A'\n",
      " '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A' '073A'\n",
      " '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A' '096A'\n",
      " '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '109A' '113A'\n",
      " '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A'\n",
      " '032A' '035A' '037A' '040A' '042A' '046A' '051B' '052A' '057A' '058A'\n",
      " '070A' '072A' '076A' '095A' '103A' '105A' '106A' '110A' '111A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A', '000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'019A', '043A'}\n",
      "Moved to Test Set:\n",
      "{'019A', '043A'}\n",
      "Removed from Test Set\n",
      "{'046A', '000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A'\n",
      " '010A' '011A' '014A' '015A' '016A' '018A' '019B' '020A' '021A' '022A'\n",
      " '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '031A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '044A' '045A' '046A' '047A' '048A'\n",
      " '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A' '073A'\n",
      " '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A' '096A'\n",
      " '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '109A' '113A'\n",
      " '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '006A' '012A' '013B' '014B' '019A' '023A' '025A' '026A' '027A'\n",
      " '032A' '035A' '037A' '040A' '042A' '043A' '051B' '052A' '057A' '058A'\n",
      " '070A' '072A' '076A' '095A' '103A' '105A' '106A' '110A' '111A' '116A']\n",
      "Length of X_train_val:\n",
      "674\n",
      "Length of y_train_val:\n",
      "674\n",
      "Length of groups_train_val:\n",
      "674\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     417\n",
      "senior    113\n",
      "kitten     60\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     171\n",
      "kitten    111\n",
      "senior     65\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     439\n",
      "kitten    122\n",
      "senior    113\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     149\n",
      "senior     65\n",
      "kitten     49\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1083, 1: 854, 2: 777})\n",
      "Epoch 1/1500\n",
      "85/85 [==============================] - 0s 987us/step - loss: 0.9464 - accuracy: 0.5999\n",
      "Epoch 2/1500\n",
      "85/85 [==============================] - 0s 953us/step - loss: 0.6950 - accuracy: 0.7027\n",
      "Epoch 3/1500\n",
      "85/85 [==============================] - 0s 956us/step - loss: 0.5977 - accuracy: 0.7517\n",
      "Epoch 4/1500\n",
      "85/85 [==============================] - 0s 1000us/step - loss: 0.5706 - accuracy: 0.7568\n",
      "Epoch 5/1500\n",
      "85/85 [==============================] - 0s 922us/step - loss: 0.5305 - accuracy: 0.7937\n",
      "Epoch 6/1500\n",
      "85/85 [==============================] - 0s 922us/step - loss: 0.5253 - accuracy: 0.7856\n",
      "Epoch 7/1500\n",
      "85/85 [==============================] - 0s 973us/step - loss: 0.4986 - accuracy: 0.7933\n",
      "Epoch 8/1500\n",
      "85/85 [==============================] - 0s 981us/step - loss: 0.4959 - accuracy: 0.8025\n",
      "Epoch 9/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.4481 - accuracy: 0.8095\n",
      "Epoch 10/1500\n",
      "85/85 [==============================] - 0s 954us/step - loss: 0.4480 - accuracy: 0.8235\n",
      "Epoch 11/1500\n",
      "85/85 [==============================] - 0s 951us/step - loss: 0.4413 - accuracy: 0.8231\n",
      "Epoch 12/1500\n",
      "85/85 [==============================] - 0s 900us/step - loss: 0.4399 - accuracy: 0.8202\n",
      "Epoch 13/1500\n",
      "85/85 [==============================] - 0s 995us/step - loss: 0.4345 - accuracy: 0.8246\n",
      "Epoch 14/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.4135 - accuracy: 0.8335\n",
      "Epoch 15/1500\n",
      "85/85 [==============================] - 0s 939us/step - loss: 0.4202 - accuracy: 0.8279\n",
      "Epoch 16/1500\n",
      "85/85 [==============================] - 0s 957us/step - loss: 0.3968 - accuracy: 0.8364\n",
      "Epoch 17/1500\n",
      "85/85 [==============================] - 0s 909us/step - loss: 0.3840 - accuracy: 0.8456\n",
      "Epoch 18/1500\n",
      "85/85 [==============================] - 0s 901us/step - loss: 0.3959 - accuracy: 0.8438\n",
      "Epoch 19/1500\n",
      "85/85 [==============================] - 0s 924us/step - loss: 0.3699 - accuracy: 0.8482\n",
      "Epoch 20/1500\n",
      "85/85 [==============================] - 0s 973us/step - loss: 0.3710 - accuracy: 0.8508\n",
      "Epoch 21/1500\n",
      "85/85 [==============================] - 0s 995us/step - loss: 0.3795 - accuracy: 0.8511\n",
      "Epoch 22/1500\n",
      "85/85 [==============================] - 0s 970us/step - loss: 0.3416 - accuracy: 0.8714\n",
      "Epoch 23/1500\n",
      "85/85 [==============================] - 0s 997us/step - loss: 0.3462 - accuracy: 0.8688\n",
      "Epoch 24/1500\n",
      "85/85 [==============================] - 0s 924us/step - loss: 0.3306 - accuracy: 0.8696\n",
      "Epoch 25/1500\n",
      "85/85 [==============================] - 0s 953us/step - loss: 0.3317 - accuracy: 0.8648\n",
      "Epoch 26/1500\n",
      "85/85 [==============================] - 0s 987us/step - loss: 0.3559 - accuracy: 0.8651\n",
      "Epoch 27/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3400 - accuracy: 0.8725\n",
      "Epoch 28/1500\n",
      "85/85 [==============================] - 0s 969us/step - loss: 0.3332 - accuracy: 0.8699\n",
      "Epoch 29/1500\n",
      "85/85 [==============================] - 0s 935us/step - loss: 0.3301 - accuracy: 0.8740\n",
      "Epoch 30/1500\n",
      "85/85 [==============================] - 0s 898us/step - loss: 0.3499 - accuracy: 0.8629\n",
      "Epoch 31/1500\n",
      "85/85 [==============================] - 0s 914us/step - loss: 0.3349 - accuracy: 0.8688\n",
      "Epoch 32/1500\n",
      "85/85 [==============================] - 0s 922us/step - loss: 0.3060 - accuracy: 0.8880\n",
      "Epoch 33/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3003 - accuracy: 0.8758\n",
      "Epoch 34/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2859 - accuracy: 0.8950\n",
      "Epoch 35/1500\n",
      "85/85 [==============================] - 0s 949us/step - loss: 0.3028 - accuracy: 0.8777\n",
      "Epoch 36/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2976 - accuracy: 0.8803\n",
      "Epoch 37/1500\n",
      "85/85 [==============================] - 0s 964us/step - loss: 0.2964 - accuracy: 0.8865\n",
      "Epoch 38/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.3064 - accuracy: 0.8766\n",
      "Epoch 39/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2817 - accuracy: 0.8931\n",
      "Epoch 40/1500\n",
      "85/85 [==============================] - 0s 987us/step - loss: 0.2889 - accuracy: 0.8832\n",
      "Epoch 41/1500\n",
      "85/85 [==============================] - 0s 982us/step - loss: 0.2658 - accuracy: 0.9001\n",
      "Epoch 42/1500\n",
      "85/85 [==============================] - 0s 997us/step - loss: 0.2845 - accuracy: 0.8869\n",
      "Epoch 43/1500\n",
      "85/85 [==============================] - 0s 971us/step - loss: 0.2671 - accuracy: 0.8946\n",
      "Epoch 44/1500\n",
      "85/85 [==============================] - 0s 919us/step - loss: 0.2788 - accuracy: 0.8965\n",
      "Epoch 45/1500\n",
      "85/85 [==============================] - 0s 928us/step - loss: 0.2791 - accuracy: 0.8976\n",
      "Epoch 46/1500\n",
      "85/85 [==============================] - 0s 937us/step - loss: 0.2757 - accuracy: 0.8906\n",
      "Epoch 47/1500\n",
      "85/85 [==============================] - 0s 969us/step - loss: 0.2806 - accuracy: 0.8906\n",
      "Epoch 48/1500\n",
      "85/85 [==============================] - 0s 975us/step - loss: 0.2801 - accuracy: 0.8939\n",
      "Epoch 49/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2453 - accuracy: 0.9068\n",
      "Epoch 50/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2501 - accuracy: 0.9068\n",
      "Epoch 51/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2590 - accuracy: 0.9020\n",
      "Epoch 52/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2502 - accuracy: 0.9046\n",
      "Epoch 53/1500\n",
      "85/85 [==============================] - 0s 942us/step - loss: 0.2658 - accuracy: 0.9005\n",
      "Epoch 54/1500\n",
      "85/85 [==============================] - 0s 967us/step - loss: 0.2418 - accuracy: 0.9075\n",
      "Epoch 55/1500\n",
      "85/85 [==============================] - 0s 989us/step - loss: 0.2653 - accuracy: 0.8957\n",
      "Epoch 56/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2480 - accuracy: 0.8994\n",
      "Epoch 57/1500\n",
      "85/85 [==============================] - 0s 914us/step - loss: 0.2476 - accuracy: 0.9068\n",
      "Epoch 58/1500\n",
      "85/85 [==============================] - 0s 933us/step - loss: 0.2515 - accuracy: 0.9042\n",
      "Epoch 59/1500\n",
      "85/85 [==============================] - 0s 949us/step - loss: 0.2528 - accuracy: 0.8976\n",
      "Epoch 60/1500\n",
      "85/85 [==============================] - 0s 968us/step - loss: 0.2494 - accuracy: 0.9042\n",
      "Epoch 61/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2476 - accuracy: 0.9053\n",
      "Epoch 62/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.9049\n",
      "Epoch 63/1500\n",
      "85/85 [==============================] - 0s 988us/step - loss: 0.2388 - accuracy: 0.9046\n",
      "Epoch 64/1500\n",
      "85/85 [==============================] - 0s 929us/step - loss: 0.2245 - accuracy: 0.9156\n",
      "Epoch 65/1500\n",
      "85/85 [==============================] - 0s 904us/step - loss: 0.2316 - accuracy: 0.9134\n",
      "Epoch 66/1500\n",
      "85/85 [==============================] - 0s 888us/step - loss: 0.2395 - accuracy: 0.9112\n",
      "Epoch 67/1500\n",
      "85/85 [==============================] - 0s 942us/step - loss: 0.2449 - accuracy: 0.9038\n",
      "Epoch 68/1500\n",
      "85/85 [==============================] - 0s 973us/step - loss: 0.2328 - accuracy: 0.9083\n",
      "Epoch 69/1500\n",
      "85/85 [==============================] - 0s 963us/step - loss: 0.2206 - accuracy: 0.9119\n",
      "Epoch 70/1500\n",
      "85/85 [==============================] - 0s 923us/step - loss: 0.2267 - accuracy: 0.9094\n",
      "Epoch 71/1500\n",
      "85/85 [==============================] - 0s 989us/step - loss: 0.2148 - accuracy: 0.9274\n",
      "Epoch 72/1500\n",
      "85/85 [==============================] - 0s 980us/step - loss: 0.2284 - accuracy: 0.9105\n",
      "Epoch 73/1500\n",
      "85/85 [==============================] - 0s 972us/step - loss: 0.2193 - accuracy: 0.9134\n",
      "Epoch 74/1500\n",
      "85/85 [==============================] - 0s 983us/step - loss: 0.2210 - accuracy: 0.9141\n",
      "Epoch 75/1500\n",
      "85/85 [==============================] - 0s 980us/step - loss: 0.2188 - accuracy: 0.9112\n",
      "Epoch 76/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2207 - accuracy: 0.9167\n",
      "Epoch 77/1500\n",
      "85/85 [==============================] - 0s 967us/step - loss: 0.2101 - accuracy: 0.9182\n",
      "Epoch 78/1500\n",
      "85/85 [==============================] - 0s 965us/step - loss: 0.2178 - accuracy: 0.9141\n",
      "Epoch 79/1500\n",
      "85/85 [==============================] - 0s 921us/step - loss: 0.2136 - accuracy: 0.9153\n",
      "Epoch 80/1500\n",
      "85/85 [==============================] - 0s 979us/step - loss: 0.2350 - accuracy: 0.9123\n",
      "Epoch 81/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2168 - accuracy: 0.9175\n",
      "Epoch 82/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9123\n",
      "Epoch 83/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9153\n",
      "Epoch 84/1500\n",
      "85/85 [==============================] - 0s 988us/step - loss: 0.1837 - accuracy: 0.9322\n",
      "Epoch 85/1500\n",
      "85/85 [==============================] - 0s 915us/step - loss: 0.2054 - accuracy: 0.9274\n",
      "Epoch 86/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2034 - accuracy: 0.9259\n",
      "Epoch 87/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9219\n",
      "Epoch 88/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9282\n",
      "Epoch 89/1500\n",
      "85/85 [==============================] - 0s 995us/step - loss: 0.2108 - accuracy: 0.9189\n",
      "Epoch 90/1500\n",
      "85/85 [==============================] - 0s 947us/step - loss: 0.1956 - accuracy: 0.9252\n",
      "Epoch 91/1500\n",
      "85/85 [==============================] - 0s 923us/step - loss: 0.1921 - accuracy: 0.9289\n",
      "Epoch 92/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.2023 - accuracy: 0.9189\n",
      "Epoch 93/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1910 - accuracy: 0.9307\n",
      "Epoch 94/1500\n",
      "85/85 [==============================] - 0s 980us/step - loss: 0.1751 - accuracy: 0.9381\n",
      "Epoch 95/1500\n",
      "85/85 [==============================] - 0s 923us/step - loss: 0.1959 - accuracy: 0.9285\n",
      "Epoch 96/1500\n",
      "85/85 [==============================] - 0s 969us/step - loss: 0.1950 - accuracy: 0.9278\n",
      "Epoch 97/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9289\n",
      "Epoch 98/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.9344\n",
      "Epoch 99/1500\n",
      "85/85 [==============================] - 0s 967us/step - loss: 0.2052 - accuracy: 0.9189\n",
      "Epoch 100/1500\n",
      "85/85 [==============================] - 0s 966us/step - loss: 0.2051 - accuracy: 0.9278\n",
      "Epoch 101/1500\n",
      "85/85 [==============================] - 0s 965us/step - loss: 0.1915 - accuracy: 0.9267\n",
      "Epoch 102/1500\n",
      "85/85 [==============================] - 0s 941us/step - loss: 0.1962 - accuracy: 0.9215\n",
      "Epoch 103/1500\n",
      "85/85 [==============================] - 0s 912us/step - loss: 0.1856 - accuracy: 0.9293\n",
      "Epoch 104/1500\n",
      "85/85 [==============================] - 0s 969us/step - loss: 0.1720 - accuracy: 0.9355\n",
      "Epoch 105/1500\n",
      "85/85 [==============================] - 0s 958us/step - loss: 0.1745 - accuracy: 0.9366\n",
      "Epoch 106/1500\n",
      "85/85 [==============================] - 0s 952us/step - loss: 0.1767 - accuracy: 0.9366\n",
      "Epoch 107/1500\n",
      "85/85 [==============================] - 0s 912us/step - loss: 0.1716 - accuracy: 0.9355\n",
      "Epoch 108/1500\n",
      "85/85 [==============================] - 0s 906us/step - loss: 0.1848 - accuracy: 0.9256\n",
      "Epoch 109/1500\n",
      "85/85 [==============================] - 0s 907us/step - loss: 0.1932 - accuracy: 0.9256\n",
      "Epoch 110/1500\n",
      "85/85 [==============================] - 0s 923us/step - loss: 0.1891 - accuracy: 0.9282\n",
      "Epoch 111/1500\n",
      "85/85 [==============================] - 0s 903us/step - loss: 0.1750 - accuracy: 0.9322\n",
      "Epoch 112/1500\n",
      "85/85 [==============================] - 0s 904us/step - loss: 0.1785 - accuracy: 0.9359\n",
      "Epoch 113/1500\n",
      "85/85 [==============================] - 0s 899us/step - loss: 0.1693 - accuracy: 0.9340\n",
      "Epoch 114/1500\n",
      "85/85 [==============================] - 0s 897us/step - loss: 0.1668 - accuracy: 0.9370\n",
      "Epoch 115/1500\n",
      "85/85 [==============================] - 0s 903us/step - loss: 0.1769 - accuracy: 0.9322\n",
      "Epoch 116/1500\n",
      "85/85 [==============================] - 0s 906us/step - loss: 0.1634 - accuracy: 0.9381\n",
      "Epoch 117/1500\n",
      "85/85 [==============================] - 0s 901us/step - loss: 0.1701 - accuracy: 0.9340\n",
      "Epoch 118/1500\n",
      "85/85 [==============================] - 0s 910us/step - loss: 0.1893 - accuracy: 0.9267\n",
      "Epoch 119/1500\n",
      "85/85 [==============================] - 0s 913us/step - loss: 0.1739 - accuracy: 0.9374\n",
      "Epoch 120/1500\n",
      "85/85 [==============================] - 0s 945us/step - loss: 0.1711 - accuracy: 0.9374\n",
      "Epoch 121/1500\n",
      "85/85 [==============================] - 0s 915us/step - loss: 0.1683 - accuracy: 0.9385\n",
      "Epoch 122/1500\n",
      "85/85 [==============================] - 0s 901us/step - loss: 0.1706 - accuracy: 0.9381\n",
      "Epoch 123/1500\n",
      "85/85 [==============================] - 0s 921us/step - loss: 0.1567 - accuracy: 0.9392\n",
      "Epoch 124/1500\n",
      "85/85 [==============================] - 0s 914us/step - loss: 0.1640 - accuracy: 0.9359\n",
      "Epoch 125/1500\n",
      "85/85 [==============================] - 0s 906us/step - loss: 0.1691 - accuracy: 0.9359\n",
      "Epoch 126/1500\n",
      "85/85 [==============================] - 0s 896us/step - loss: 0.1624 - accuracy: 0.9366\n",
      "Epoch 127/1500\n",
      "85/85 [==============================] - 0s 918us/step - loss: 0.1534 - accuracy: 0.9455\n",
      "Epoch 128/1500\n",
      "85/85 [==============================] - 0s 905us/step - loss: 0.1567 - accuracy: 0.9392\n",
      "Epoch 129/1500\n",
      "85/85 [==============================] - 0s 897us/step - loss: 0.1815 - accuracy: 0.9366\n",
      "Epoch 130/1500\n",
      "85/85 [==============================] - 0s 926us/step - loss: 0.1680 - accuracy: 0.9359\n",
      "Epoch 131/1500\n",
      "85/85 [==============================] - 0s 964us/step - loss: 0.1550 - accuracy: 0.9444\n",
      "Epoch 132/1500\n",
      "85/85 [==============================] - 0s 921us/step - loss: 0.1805 - accuracy: 0.9304\n",
      "Epoch 133/1500\n",
      "85/85 [==============================] - 0s 977us/step - loss: 0.1534 - accuracy: 0.9407\n",
      "Epoch 134/1500\n",
      "85/85 [==============================] - 0s 912us/step - loss: 0.1687 - accuracy: 0.9304\n",
      "Epoch 135/1500\n",
      "85/85 [==============================] - 0s 957us/step - loss: 0.1510 - accuracy: 0.9403\n",
      "Epoch 136/1500\n",
      "85/85 [==============================] - 0s 959us/step - loss: 0.1717 - accuracy: 0.9355\n",
      "Epoch 137/1500\n",
      "85/85 [==============================] - 0s 921us/step - loss: 0.1570 - accuracy: 0.9425\n",
      "Epoch 138/1500\n",
      "85/85 [==============================] - 0s 925us/step - loss: 0.1630 - accuracy: 0.9429\n",
      "Epoch 139/1500\n",
      "85/85 [==============================] - 0s 964us/step - loss: 0.1677 - accuracy: 0.9388\n",
      "Epoch 140/1500\n",
      "85/85 [==============================] - 0s 935us/step - loss: 0.1567 - accuracy: 0.9377\n",
      "Epoch 141/1500\n",
      "85/85 [==============================] - 0s 933us/step - loss: 0.1512 - accuracy: 0.9473\n",
      "Epoch 142/1500\n",
      "85/85 [==============================] - 0s 927us/step - loss: 0.1621 - accuracy: 0.9381\n",
      "Epoch 143/1500\n",
      "85/85 [==============================] - 0s 981us/step - loss: 0.1525 - accuracy: 0.9458\n",
      "Epoch 144/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1618 - accuracy: 0.9425\n",
      "Epoch 145/1500\n",
      "85/85 [==============================] - 0s 994us/step - loss: 0.1624 - accuracy: 0.9377\n",
      "Epoch 146/1500\n",
      "85/85 [==============================] - 0s 919us/step - loss: 0.1533 - accuracy: 0.9388\n",
      "Epoch 147/1500\n",
      "85/85 [==============================] - 0s 961us/step - loss: 0.1374 - accuracy: 0.9525\n",
      "Epoch 148/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9444\n",
      "Epoch 149/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9455\n",
      "Epoch 150/1500\n",
      "85/85 [==============================] - 0s 921us/step - loss: 0.1561 - accuracy: 0.9410\n",
      "Epoch 151/1500\n",
      "85/85 [==============================] - 0s 937us/step - loss: 0.1720 - accuracy: 0.9370\n",
      "Epoch 152/1500\n",
      "85/85 [==============================] - 0s 995us/step - loss: 0.1549 - accuracy: 0.9425\n",
      "Epoch 153/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9458\n",
      "Epoch 154/1500\n",
      "85/85 [==============================] - 0s 937us/step - loss: 0.1490 - accuracy: 0.9407\n",
      "Epoch 155/1500\n",
      "85/85 [==============================] - 0s 904us/step - loss: 0.1468 - accuracy: 0.9499\n",
      "Epoch 156/1500\n",
      "85/85 [==============================] - 0s 932us/step - loss: 0.1398 - accuracy: 0.9480\n",
      "Epoch 157/1500\n",
      "85/85 [==============================] - 0s 903us/step - loss: 0.1445 - accuracy: 0.9440\n",
      "Epoch 158/1500\n",
      "85/85 [==============================] - 0s 904us/step - loss: 0.1389 - accuracy: 0.9495\n",
      "Epoch 159/1500\n",
      "85/85 [==============================] - 0s 969us/step - loss: 0.1482 - accuracy: 0.9477\n",
      "Epoch 160/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1547 - accuracy: 0.9422\n",
      "Epoch 161/1500\n",
      "85/85 [==============================] - 0s 996us/step - loss: 0.1582 - accuracy: 0.9396\n",
      "Epoch 162/1500\n",
      "85/85 [==============================] - 0s 960us/step - loss: 0.1578 - accuracy: 0.9425\n",
      "Epoch 163/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9410\n",
      "Epoch 164/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9429\n",
      "Epoch 165/1500\n",
      "85/85 [==============================] - 0s 975us/step - loss: 0.1562 - accuracy: 0.9451\n",
      "Epoch 166/1500\n",
      "85/85 [==============================] - 0s 951us/step - loss: 0.1460 - accuracy: 0.9469\n",
      "Epoch 167/1500\n",
      "85/85 [==============================] - 0s 957us/step - loss: 0.1438 - accuracy: 0.9469\n",
      "Epoch 168/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9506\n",
      "Epoch 169/1500\n",
      "85/85 [==============================] - 0s 965us/step - loss: 0.1396 - accuracy: 0.9499\n",
      "Epoch 170/1500\n",
      "85/85 [==============================] - 0s 927us/step - loss: 0.1538 - accuracy: 0.9462\n",
      "Epoch 171/1500\n",
      "85/85 [==============================] - 0s 931us/step - loss: 0.1573 - accuracy: 0.9451\n",
      "Epoch 172/1500\n",
      "85/85 [==============================] - 0s 913us/step - loss: 0.1515 - accuracy: 0.9444\n",
      "Epoch 173/1500\n",
      "85/85 [==============================] - 0s 954us/step - loss: 0.1459 - accuracy: 0.9466\n",
      "Epoch 174/1500\n",
      "85/85 [==============================] - 0s 986us/step - loss: 0.1538 - accuracy: 0.9414\n",
      "Epoch 175/1500\n",
      "85/85 [==============================] - 0s 935us/step - loss: 0.1380 - accuracy: 0.9517\n",
      "Epoch 176/1500\n",
      "85/85 [==============================] - 0s 954us/step - loss: 0.1260 - accuracy: 0.9528\n",
      "Epoch 177/1500\n",
      "85/85 [==============================] - 0s 969us/step - loss: 0.1385 - accuracy: 0.9488\n",
      "Epoch 178/1500\n",
      "85/85 [==============================] - 0s 955us/step - loss: 0.1420 - accuracy: 0.9477\n",
      "Epoch 179/1500\n",
      "85/85 [==============================] - 0s 952us/step - loss: 0.1330 - accuracy: 0.9532\n",
      "Epoch 180/1500\n",
      "85/85 [==============================] - 0s 911us/step - loss: 0.1294 - accuracy: 0.9558\n",
      "Epoch 181/1500\n",
      "85/85 [==============================] - 0s 962us/step - loss: 0.1368 - accuracy: 0.9503\n",
      "Epoch 182/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1330 - accuracy: 0.9510\n",
      "Epoch 183/1500\n",
      "85/85 [==============================] - 0s 964us/step - loss: 0.1211 - accuracy: 0.9565\n",
      "Epoch 184/1500\n",
      "85/85 [==============================] - 0s 952us/step - loss: 0.1305 - accuracy: 0.9521\n",
      "Epoch 185/1500\n",
      "85/85 [==============================] - 0s 956us/step - loss: 0.1321 - accuracy: 0.9525\n",
      "Epoch 186/1500\n",
      "85/85 [==============================] - 0s 970us/step - loss: 0.1456 - accuracy: 0.9462\n",
      "Epoch 187/1500\n",
      "85/85 [==============================] - 0s 957us/step - loss: 0.1372 - accuracy: 0.9444\n",
      "Epoch 188/1500\n",
      "85/85 [==============================] - 0s 965us/step - loss: 0.1289 - accuracy: 0.9565\n",
      "Epoch 189/1500\n",
      "85/85 [==============================] - 0s 984us/step - loss: 0.1284 - accuracy: 0.9565\n",
      "Epoch 190/1500\n",
      "85/85 [==============================] - 0s 908us/step - loss: 0.1267 - accuracy: 0.9525\n",
      "Epoch 191/1500\n",
      "85/85 [==============================] - 0s 923us/step - loss: 0.1252 - accuracy: 0.9550\n",
      "Epoch 192/1500\n",
      "85/85 [==============================] - 0s 977us/step - loss: 0.1349 - accuracy: 0.9543\n",
      "Epoch 193/1500\n",
      "85/85 [==============================] - 0s 997us/step - loss: 0.1346 - accuracy: 0.9484\n",
      "Epoch 194/1500\n",
      "85/85 [==============================] - 0s 955us/step - loss: 0.1401 - accuracy: 0.9484\n",
      "Epoch 195/1500\n",
      "85/85 [==============================] - 0s 918us/step - loss: 0.1103 - accuracy: 0.9635\n",
      "Epoch 196/1500\n",
      "85/85 [==============================] - 0s 908us/step - loss: 0.1200 - accuracy: 0.9628\n",
      "Epoch 197/1500\n",
      "85/85 [==============================] - 0s 980us/step - loss: 0.1263 - accuracy: 0.9565\n",
      "Epoch 198/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1111 - accuracy: 0.9573\n",
      "Epoch 199/1500\n",
      "85/85 [==============================] - 0s 998us/step - loss: 0.1253 - accuracy: 0.9543\n",
      "Epoch 200/1500\n",
      "85/85 [==============================] - 0s 935us/step - loss: 0.1316 - accuracy: 0.9510\n",
      "Epoch 201/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9503\n",
      "Epoch 202/1500\n",
      "85/85 [==============================] - 0s 994us/step - loss: 0.1254 - accuracy: 0.9550\n",
      "Epoch 203/1500\n",
      "85/85 [==============================] - 0s 935us/step - loss: 0.1176 - accuracy: 0.9550\n",
      "Epoch 204/1500\n",
      "85/85 [==============================] - 0s 922us/step - loss: 0.1214 - accuracy: 0.9562\n",
      "Epoch 205/1500\n",
      "85/85 [==============================] - 0s 935us/step - loss: 0.1208 - accuracy: 0.9613\n",
      "Epoch 206/1500\n",
      "85/85 [==============================] - 0s 918us/step - loss: 0.1222 - accuracy: 0.9536\n",
      "Epoch 207/1500\n",
      "85/85 [==============================] - 0s 928us/step - loss: 0.1049 - accuracy: 0.9632\n",
      "Epoch 208/1500\n",
      "85/85 [==============================] - 0s 919us/step - loss: 0.1393 - accuracy: 0.9469\n",
      "Epoch 209/1500\n",
      "85/85 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9584\n",
      "Epoch 210/1500\n",
      "85/85 [==============================] - 0s 953us/step - loss: 0.1265 - accuracy: 0.9539\n",
      "Epoch 211/1500\n",
      "85/85 [==============================] - 0s 937us/step - loss: 0.1156 - accuracy: 0.9587\n",
      "Epoch 212/1500\n",
      "85/85 [==============================] - 0s 964us/step - loss: 0.1305 - accuracy: 0.9528\n",
      "Epoch 213/1500\n",
      "85/85 [==============================] - 0s 943us/step - loss: 0.1239 - accuracy: 0.9587\n",
      "Epoch 214/1500\n",
      "85/85 [==============================] - 0s 956us/step - loss: 0.1129 - accuracy: 0.9609\n",
      "Epoch 215/1500\n",
      "85/85 [==============================] - 0s 967us/step - loss: 0.1267 - accuracy: 0.9536\n",
      "Epoch 216/1500\n",
      "85/85 [==============================] - 0s 950us/step - loss: 0.1343 - accuracy: 0.9506\n",
      "Epoch 217/1500\n",
      "85/85 [==============================] - 0s 934us/step - loss: 0.1192 - accuracy: 0.9547\n",
      "Epoch 218/1500\n",
      "85/85 [==============================] - 0s 937us/step - loss: 0.1220 - accuracy: 0.9536\n",
      "Epoch 219/1500\n",
      "85/85 [==============================] - 0s 911us/step - loss: 0.1111 - accuracy: 0.9620\n",
      "Epoch 220/1500\n",
      "85/85 [==============================] - 0s 909us/step - loss: 0.1134 - accuracy: 0.9613\n",
      "Epoch 221/1500\n",
      "85/85 [==============================] - 0s 969us/step - loss: 0.1249 - accuracy: 0.9539\n",
      "Epoch 222/1500\n",
      "85/85 [==============================] - 0s 990us/step - loss: 0.1416 - accuracy: 0.9458\n",
      "Epoch 223/1500\n",
      "85/85 [==============================] - 0s 970us/step - loss: 0.1186 - accuracy: 0.9576\n",
      "Epoch 224/1500\n",
      "85/85 [==============================] - 0s 949us/step - loss: 0.1125 - accuracy: 0.9562\n",
      "Epoch 225/1500\n",
      "85/85 [==============================] - 0s 933us/step - loss: 0.1068 - accuracy: 0.9620\n",
      "Epoch 226/1500\n",
      "85/85 [==============================] - 0s 955us/step - loss: 0.1218 - accuracy: 0.9547\n",
      "Epoch 227/1500\n",
      "85/85 [==============================] - 0s 961us/step - loss: 0.1246 - accuracy: 0.9554\n",
      "Epoch 228/1500\n",
      "85/85 [==============================] - 0s 993us/step - loss: 0.1146 - accuracy: 0.9580\n",
      "Epoch 229/1500\n",
      "85/85 [==============================] - 0s 935us/step - loss: 0.1103 - accuracy: 0.9620\n",
      "Epoch 230/1500\n",
      "85/85 [==============================] - 0s 910us/step - loss: 0.1287 - accuracy: 0.9536\n",
      "Epoch 231/1500\n",
      "85/85 [==============================] - 0s 921us/step - loss: 0.1233 - accuracy: 0.9499\n",
      "Epoch 232/1500\n",
      "85/85 [==============================] - 0s 958us/step - loss: 0.1354 - accuracy: 0.9503\n",
      "Epoch 233/1500\n",
      "85/85 [==============================] - 0s 951us/step - loss: 0.1374 - accuracy: 0.9462\n",
      "Epoch 234/1500\n",
      "85/85 [==============================] - 0s 965us/step - loss: 0.1209 - accuracy: 0.9543\n",
      "Epoch 235/1500\n",
      "85/85 [==============================] - 0s 958us/step - loss: 0.1208 - accuracy: 0.9558\n",
      "Epoch 236/1500\n",
      "85/85 [==============================] - 0s 929us/step - loss: 0.1150 - accuracy: 0.9632\n",
      "Epoch 237/1500\n",
      "56/85 [==================>...........] - ETA: 0s - loss: 0.1232 - accuracy: 0.9531Restoring model weights from the end of the best epoch: 207.\n",
      "85/85 [==============================] - 0s 939us/step - loss: 0.1198 - accuracy: 0.9525\n",
      "Epoch 237: early stopping\n",
      "9/9 [==============================] - 0s 731us/step - loss: 0.7106 - accuracy: 0.7833\n",
      "9/9 [==============================] - 0s 613us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.93 (28/30)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "After appending - Cat IDs: 618, Predictions: 618, Actuals: 618, Gender: 618\n",
      "Final Test Results - Loss: 0.71061772108078, Accuracy: 0.7832699418067932, Precision: 0.7905144841712582, Recall: 0.7802696504518173, F1 Score: 0.7849792265899649\n",
      "Confusion Matrix:\n",
      " [[121   2  26]\n",
      " [  4  44   1]\n",
      " [ 24   0  41]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "097B    14\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "068A    11\n",
      "025A    11\n",
      "063A    11\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "005A    10\n",
      "072A     9\n",
      "065A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "099A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "062A     4\n",
      "009A     4\n",
      "012A     3\n",
      "058A     3\n",
      "060A     3\n",
      "006A     3\n",
      "056A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "032A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "066A     1\n",
      "026C     1\n",
      "076A     1\n",
      "096A     1\n",
      "041A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "047A    28\n",
      "074A    25\n",
      "000B    19\n",
      "097A    16\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "036A    11\n",
      "016A    10\n",
      "022A     9\n",
      "117A     7\n",
      "031A     7\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "003A     4\n",
      "064A     3\n",
      "113A     3\n",
      "093A     2\n",
      "069A     2\n",
      "073A     1\n",
      "091A     1\n",
      "092A     1\n",
      "048A     1\n",
      "115A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    266\n",
      "F    163\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    89\n",
      "X    75\n",
      "M    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [014B, 111A, 040A, 046A, 042A, 109A, 050A, 043...\n",
      "senior    [057A, 106A, 104A, 055A, 116A, 051B, 054A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [028A, 074A, 022A, 034A, 091A, 002A, 007A, 069...\n",
      "kitten                             [044A, 047A, 048A, 115A]\n",
      "senior     [093A, 097A, 059A, 113A, 117A, 051A, 016A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "Length of X_train_val:\n",
      "702\n",
      "Length of y_train_val:\n",
      "702\n",
      "Length of groups_train_val:\n",
      "702\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1103, 1: 888, 2: 736})\n",
      "Epoch 1/1500\n",
      "86/86 [==============================] - 0s 954us/step - loss: 0.8941 - accuracy: 0.6175\n",
      "Epoch 2/1500\n",
      "86/86 [==============================] - 0s 886us/step - loss: 0.7641 - accuracy: 0.6740\n",
      "Epoch 3/1500\n",
      "86/86 [==============================] - 0s 901us/step - loss: 0.6822 - accuracy: 0.7044\n",
      "Epoch 4/1500\n",
      "86/86 [==============================] - 0s 929us/step - loss: 0.6309 - accuracy: 0.7389\n",
      "Epoch 5/1500\n",
      "86/86 [==============================] - 0s 913us/step - loss: 0.5845 - accuracy: 0.7477\n",
      "Epoch 6/1500\n",
      "86/86 [==============================] - 0s 907us/step - loss: 0.5599 - accuracy: 0.7671\n",
      "Epoch 7/1500\n",
      "86/86 [==============================] - 0s 902us/step - loss: 0.5590 - accuracy: 0.7682\n",
      "Epoch 8/1500\n",
      "86/86 [==============================] - 0s 937us/step - loss: 0.5386 - accuracy: 0.7767\n",
      "Epoch 9/1500\n",
      "86/86 [==============================] - 0s 943us/step - loss: 0.5369 - accuracy: 0.7829\n",
      "Epoch 10/1500\n",
      "86/86 [==============================] - 0s 956us/step - loss: 0.5168 - accuracy: 0.7924\n",
      "Epoch 11/1500\n",
      "86/86 [==============================] - 0s 920us/step - loss: 0.5177 - accuracy: 0.7884\n",
      "Epoch 12/1500\n",
      "86/86 [==============================] - 0s 917us/step - loss: 0.4798 - accuracy: 0.8042\n",
      "Epoch 13/1500\n",
      "86/86 [==============================] - 0s 900us/step - loss: 0.4788 - accuracy: 0.8045\n",
      "Epoch 14/1500\n",
      "86/86 [==============================] - 0s 901us/step - loss: 0.4719 - accuracy: 0.8064\n",
      "Epoch 15/1500\n",
      "86/86 [==============================] - 0s 932us/step - loss: 0.4512 - accuracy: 0.8203\n",
      "Epoch 16/1500\n",
      "86/86 [==============================] - 0s 939us/step - loss: 0.4626 - accuracy: 0.8181\n",
      "Epoch 17/1500\n",
      "86/86 [==============================] - 0s 944us/step - loss: 0.4443 - accuracy: 0.8185\n",
      "Epoch 18/1500\n",
      "86/86 [==============================] - 0s 915us/step - loss: 0.4204 - accuracy: 0.8313\n",
      "Epoch 19/1500\n",
      "86/86 [==============================] - 0s 921us/step - loss: 0.4251 - accuracy: 0.8254\n",
      "Epoch 20/1500\n",
      "86/86 [==============================] - 0s 956us/step - loss: 0.4163 - accuracy: 0.8387\n",
      "Epoch 21/1500\n",
      "86/86 [==============================] - 0s 972us/step - loss: 0.4199 - accuracy: 0.8405\n",
      "Epoch 22/1500\n",
      "86/86 [==============================] - 0s 952us/step - loss: 0.3964 - accuracy: 0.8390\n",
      "Epoch 23/1500\n",
      "86/86 [==============================] - 0s 927us/step - loss: 0.3883 - accuracy: 0.8409\n",
      "Epoch 24/1500\n",
      "86/86 [==============================] - 0s 911us/step - loss: 0.4047 - accuracy: 0.8379\n",
      "Epoch 25/1500\n",
      "86/86 [==============================] - 0s 923us/step - loss: 0.3754 - accuracy: 0.8504\n",
      "Epoch 26/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.3795 - accuracy: 0.8566\n",
      "Epoch 27/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.3584 - accuracy: 0.8559\n",
      "Epoch 28/1500\n",
      "86/86 [==============================] - 0s 994us/step - loss: 0.3784 - accuracy: 0.8508\n",
      "Epoch 29/1500\n",
      "86/86 [==============================] - 0s 966us/step - loss: 0.3676 - accuracy: 0.8541\n",
      "Epoch 30/1500\n",
      "86/86 [==============================] - 0s 966us/step - loss: 0.3644 - accuracy: 0.8621\n",
      "Epoch 31/1500\n",
      "86/86 [==============================] - 0s 980us/step - loss: 0.3680 - accuracy: 0.8533\n",
      "Epoch 32/1500\n",
      "86/86 [==============================] - 0s 944us/step - loss: 0.3528 - accuracy: 0.8555\n",
      "Epoch 33/1500\n",
      "86/86 [==============================] - 0s 922us/step - loss: 0.3426 - accuracy: 0.8662\n",
      "Epoch 34/1500\n",
      "86/86 [==============================] - 0s 923us/step - loss: 0.3434 - accuracy: 0.8691\n",
      "Epoch 35/1500\n",
      "86/86 [==============================] - 0s 965us/step - loss: 0.3227 - accuracy: 0.8706\n",
      "Epoch 36/1500\n",
      "86/86 [==============================] - 0s 990us/step - loss: 0.3335 - accuracy: 0.8684\n",
      "Epoch 37/1500\n",
      "86/86 [==============================] - 0s 983us/step - loss: 0.3314 - accuracy: 0.8702\n",
      "Epoch 38/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.3150 - accuracy: 0.8786\n",
      "Epoch 39/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8717\n",
      "Epoch 40/1500\n",
      "86/86 [==============================] - 0s 964us/step - loss: 0.3108 - accuracy: 0.8841\n",
      "Epoch 41/1500\n",
      "86/86 [==============================] - 0s 923us/step - loss: 0.3200 - accuracy: 0.8713\n",
      "Epoch 42/1500\n",
      "86/86 [==============================] - 0s 946us/step - loss: 0.3159 - accuracy: 0.8794\n",
      "Epoch 43/1500\n",
      "86/86 [==============================] - 0s 979us/step - loss: 0.3219 - accuracy: 0.8746\n",
      "Epoch 44/1500\n",
      "86/86 [==============================] - 0s 918us/step - loss: 0.3223 - accuracy: 0.8684\n",
      "Epoch 45/1500\n",
      "86/86 [==============================] - 0s 963us/step - loss: 0.3184 - accuracy: 0.8786\n",
      "Epoch 46/1500\n",
      "86/86 [==============================] - 0s 972us/step - loss: 0.3134 - accuracy: 0.8790\n",
      "Epoch 47/1500\n",
      "86/86 [==============================] - 0s 983us/step - loss: 0.3028 - accuracy: 0.8764\n",
      "Epoch 48/1500\n",
      "86/86 [==============================] - 0s 916us/step - loss: 0.3009 - accuracy: 0.8845\n",
      "Epoch 49/1500\n",
      "86/86 [==============================] - 0s 919us/step - loss: 0.3137 - accuracy: 0.8794\n",
      "Epoch 50/1500\n",
      "86/86 [==============================] - 0s 965us/step - loss: 0.2835 - accuracy: 0.8948\n",
      "Epoch 51/1500\n",
      "86/86 [==============================] - 0s 984us/step - loss: 0.2830 - accuracy: 0.8849\n",
      "Epoch 52/1500\n",
      "86/86 [==============================] - 0s 957us/step - loss: 0.2764 - accuracy: 0.8959\n",
      "Epoch 53/1500\n",
      "86/86 [==============================] - 0s 940us/step - loss: 0.2928 - accuracy: 0.8841\n",
      "Epoch 54/1500\n",
      "86/86 [==============================] - 0s 945us/step - loss: 0.2794 - accuracy: 0.8878\n",
      "Epoch 55/1500\n",
      "86/86 [==============================] - 0s 929us/step - loss: 0.2842 - accuracy: 0.8849\n",
      "Epoch 56/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2692 - accuracy: 0.8984\n",
      "Epoch 57/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2669 - accuracy: 0.8966\n",
      "Epoch 58/1500\n",
      "86/86 [==============================] - 0s 993us/step - loss: 0.2843 - accuracy: 0.8955\n",
      "Epoch 59/1500\n",
      "86/86 [==============================] - 0s 974us/step - loss: 0.2901 - accuracy: 0.8915\n",
      "Epoch 60/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2635 - accuracy: 0.8988\n",
      "Epoch 61/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2559 - accuracy: 0.9072\n",
      "Epoch 62/1500\n",
      "86/86 [==============================] - 0s 931us/step - loss: 0.2628 - accuracy: 0.8966\n",
      "Epoch 63/1500\n",
      "86/86 [==============================] - 0s 932us/step - loss: 0.2721 - accuracy: 0.8959\n",
      "Epoch 64/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2709 - accuracy: 0.8948\n",
      "Epoch 65/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2617 - accuracy: 0.8959\n",
      "Epoch 66/1500\n",
      "86/86 [==============================] - 0s 992us/step - loss: 0.2737 - accuracy: 0.8871\n",
      "Epoch 67/1500\n",
      "86/86 [==============================] - 0s 905us/step - loss: 0.2561 - accuracy: 0.8962\n",
      "Epoch 68/1500\n",
      "86/86 [==============================] - 0s 972us/step - loss: 0.2419 - accuracy: 0.9083\n",
      "Epoch 69/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 0.9032\n",
      "Epoch 70/1500\n",
      "86/86 [==============================] - 0s 975us/step - loss: 0.2637 - accuracy: 0.9025\n",
      "Epoch 71/1500\n",
      "86/86 [==============================] - 0s 954us/step - loss: 0.2508 - accuracy: 0.9047\n",
      "Epoch 72/1500\n",
      "86/86 [==============================] - 0s 989us/step - loss: 0.2560 - accuracy: 0.8970\n",
      "Epoch 73/1500\n",
      "86/86 [==============================] - 0s 976us/step - loss: 0.2320 - accuracy: 0.9109\n",
      "Epoch 74/1500\n",
      "86/86 [==============================] - 0s 957us/step - loss: 0.2242 - accuracy: 0.9120\n",
      "Epoch 75/1500\n",
      "86/86 [==============================] - 0s 969us/step - loss: 0.2561 - accuracy: 0.9087\n",
      "Epoch 76/1500\n",
      "86/86 [==============================] - 0s 993us/step - loss: 0.2241 - accuracy: 0.9164\n",
      "Epoch 77/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2421 - accuracy: 0.9083\n",
      "Epoch 78/1500\n",
      "86/86 [==============================] - 0s 927us/step - loss: 0.2444 - accuracy: 0.9069\n",
      "Epoch 79/1500\n",
      "86/86 [==============================] - 0s 953us/step - loss: 0.2378 - accuracy: 0.9109\n",
      "Epoch 80/1500\n",
      "86/86 [==============================] - 0s 916us/step - loss: 0.2213 - accuracy: 0.9127\n",
      "Epoch 81/1500\n",
      "86/86 [==============================] - 0s 917us/step - loss: 0.2432 - accuracy: 0.9058\n",
      "Epoch 82/1500\n",
      "86/86 [==============================] - 0s 962us/step - loss: 0.2322 - accuracy: 0.9080\n",
      "Epoch 83/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2169 - accuracy: 0.9182\n",
      "Epoch 84/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9135\n",
      "Epoch 85/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2307 - accuracy: 0.9149\n",
      "Epoch 86/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2389 - accuracy: 0.9087\n",
      "Epoch 87/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2214 - accuracy: 0.9201\n",
      "Epoch 88/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2290 - accuracy: 0.9091\n",
      "Epoch 89/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2084 - accuracy: 0.9212\n",
      "Epoch 90/1500\n",
      "86/86 [==============================] - 0s 964us/step - loss: 0.2175 - accuracy: 0.9186\n",
      "Epoch 91/1500\n",
      "86/86 [==============================] - 0s 982us/step - loss: 0.2216 - accuracy: 0.9124\n",
      "Epoch 92/1500\n",
      "86/86 [==============================] - 0s 929us/step - loss: 0.2290 - accuracy: 0.9157\n",
      "Epoch 93/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2284 - accuracy: 0.9120\n",
      "Epoch 94/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2205 - accuracy: 0.9160\n",
      "Epoch 95/1500\n",
      "86/86 [==============================] - 0s 948us/step - loss: 0.2213 - accuracy: 0.9142\n",
      "Epoch 96/1500\n",
      "86/86 [==============================] - 0s 939us/step - loss: 0.2242 - accuracy: 0.9124\n",
      "Epoch 97/1500\n",
      "86/86 [==============================] - 0s 924us/step - loss: 0.2087 - accuracy: 0.9256\n",
      "Epoch 98/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9208\n",
      "Epoch 99/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.2081 - accuracy: 0.9215\n",
      "Epoch 100/1500\n",
      "86/86 [==============================] - 0s 987us/step - loss: 0.2242 - accuracy: 0.9179\n",
      "Epoch 101/1500\n",
      "86/86 [==============================] - 0s 964us/step - loss: 0.2001 - accuracy: 0.9241\n",
      "Epoch 102/1500\n",
      "86/86 [==============================] - 0s 971us/step - loss: 0.2281 - accuracy: 0.9127\n",
      "Epoch 103/1500\n",
      "86/86 [==============================] - 0s 955us/step - loss: 0.2117 - accuracy: 0.9201\n",
      "Epoch 104/1500\n",
      "86/86 [==============================] - 0s 981us/step - loss: 0.2200 - accuracy: 0.9153\n",
      "Epoch 105/1500\n",
      "86/86 [==============================] - 0s 943us/step - loss: 0.2105 - accuracy: 0.9219\n",
      "Epoch 106/1500\n",
      "86/86 [==============================] - 0s 942us/step - loss: 0.1943 - accuracy: 0.9248\n",
      "Epoch 107/1500\n",
      "86/86 [==============================] - 0s 904us/step - loss: 0.2096 - accuracy: 0.9204\n",
      "Epoch 108/1500\n",
      "86/86 [==============================] - 0s 912us/step - loss: 0.2027 - accuracy: 0.9201\n",
      "Epoch 109/1500\n",
      "86/86 [==============================] - 0s 934us/step - loss: 0.1964 - accuracy: 0.9303\n",
      "Epoch 110/1500\n",
      "86/86 [==============================] - 0s 909us/step - loss: 0.1958 - accuracy: 0.9263\n",
      "Epoch 111/1500\n",
      "86/86 [==============================] - 0s 912us/step - loss: 0.2042 - accuracy: 0.9241\n",
      "Epoch 112/1500\n",
      "86/86 [==============================] - 0s 920us/step - loss: 0.2118 - accuracy: 0.9160\n",
      "Epoch 113/1500\n",
      "86/86 [==============================] - 0s 980us/step - loss: 0.2159 - accuracy: 0.9175\n",
      "Epoch 114/1500\n",
      "86/86 [==============================] - 0s 1000us/step - loss: 0.1997 - accuracy: 0.9226\n",
      "Epoch 115/1500\n",
      "86/86 [==============================] - 0s 947us/step - loss: 0.1916 - accuracy: 0.9289\n",
      "Epoch 116/1500\n",
      "86/86 [==============================] - 0s 915us/step - loss: 0.2022 - accuracy: 0.9259\n",
      "Epoch 117/1500\n",
      "86/86 [==============================] - 0s 916us/step - loss: 0.2063 - accuracy: 0.9230\n",
      "Epoch 118/1500\n",
      "86/86 [==============================] - 0s 927us/step - loss: 0.1946 - accuracy: 0.9289\n",
      "Epoch 119/1500\n",
      "86/86 [==============================] - 0s 955us/step - loss: 0.1984 - accuracy: 0.9190\n",
      "Epoch 120/1500\n",
      "86/86 [==============================] - 0s 956us/step - loss: 0.1985 - accuracy: 0.9208\n",
      "Epoch 121/1500\n",
      "86/86 [==============================] - 0s 960us/step - loss: 0.1974 - accuracy: 0.9267\n",
      "Epoch 122/1500\n",
      "86/86 [==============================] - 0s 955us/step - loss: 0.1897 - accuracy: 0.9263\n",
      "Epoch 123/1500\n",
      "86/86 [==============================] - 0s 962us/step - loss: 0.1818 - accuracy: 0.9366\n",
      "Epoch 124/1500\n",
      "86/86 [==============================] - 0s 941us/step - loss: 0.2111 - accuracy: 0.9215\n",
      "Epoch 125/1500\n",
      "86/86 [==============================] - 0s 966us/step - loss: 0.1949 - accuracy: 0.9267\n",
      "Epoch 126/1500\n",
      "86/86 [==============================] - 0s 941us/step - loss: 0.1947 - accuracy: 0.9263\n",
      "Epoch 127/1500\n",
      "86/86 [==============================] - 0s 914us/step - loss: 0.1993 - accuracy: 0.9274\n",
      "Epoch 128/1500\n",
      "86/86 [==============================] - 0s 956us/step - loss: 0.1689 - accuracy: 0.9362\n",
      "Epoch 129/1500\n",
      "86/86 [==============================] - 0s 970us/step - loss: 0.1984 - accuracy: 0.9248\n",
      "Epoch 130/1500\n",
      "86/86 [==============================] - 0s 939us/step - loss: 0.1794 - accuracy: 0.9369\n",
      "Epoch 131/1500\n",
      "86/86 [==============================] - 0s 937us/step - loss: 0.2032 - accuracy: 0.9237\n",
      "Epoch 132/1500\n",
      "86/86 [==============================] - 0s 928us/step - loss: 0.1898 - accuracy: 0.9274\n",
      "Epoch 133/1500\n",
      "86/86 [==============================] - 0s 990us/step - loss: 0.1931 - accuracy: 0.9226\n",
      "Epoch 134/1500\n",
      "86/86 [==============================] - 0s 947us/step - loss: 0.1861 - accuracy: 0.9314\n",
      "Epoch 135/1500\n",
      "86/86 [==============================] - 0s 959us/step - loss: 0.1912 - accuracy: 0.9292\n",
      "Epoch 136/1500\n",
      "86/86 [==============================] - 0s 981us/step - loss: 0.1768 - accuracy: 0.9358\n",
      "Epoch 137/1500\n",
      "86/86 [==============================] - 0s 988us/step - loss: 0.1848 - accuracy: 0.9307\n",
      "Epoch 138/1500\n",
      "86/86 [==============================] - 0s 988us/step - loss: 0.1728 - accuracy: 0.9358\n",
      "Epoch 139/1500\n",
      "86/86 [==============================] - 0s 932us/step - loss: 0.1942 - accuracy: 0.9285\n",
      "Epoch 140/1500\n",
      "86/86 [==============================] - 0s 906us/step - loss: 0.1838 - accuracy: 0.9336\n",
      "Epoch 141/1500\n",
      "86/86 [==============================] - 0s 979us/step - loss: 0.1681 - accuracy: 0.9384\n",
      "Epoch 142/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9347\n",
      "Epoch 143/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.9450\n",
      "Epoch 144/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9428\n",
      "Epoch 145/1500\n",
      "86/86 [==============================] - 0s 962us/step - loss: 0.1718 - accuracy: 0.9384\n",
      "Epoch 146/1500\n",
      "86/86 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9417\n",
      "Epoch 147/1500\n",
      "86/86 [==============================] - 0s 966us/step - loss: 0.1847 - accuracy: 0.9311\n",
      "Epoch 148/1500\n",
      "86/86 [==============================] - 0s 961us/step - loss: 0.1647 - accuracy: 0.9402\n",
      "Epoch 149/1500\n",
      "86/86 [==============================] - 0s 962us/step - loss: 0.1561 - accuracy: 0.9413\n",
      "Epoch 150/1500\n",
      "86/86 [==============================] - 0s 943us/step - loss: 0.1787 - accuracy: 0.9325\n",
      "Epoch 151/1500\n",
      "86/86 [==============================] - 0s 928us/step - loss: 0.1547 - accuracy: 0.9402\n",
      "Epoch 152/1500\n",
      "86/86 [==============================] - 0s 927us/step - loss: 0.2010 - accuracy: 0.9234\n",
      "Epoch 153/1500\n",
      "86/86 [==============================] - 0s 938us/step - loss: 0.1847 - accuracy: 0.9355\n",
      "Epoch 154/1500\n",
      "86/86 [==============================] - 0s 967us/step - loss: 0.1665 - accuracy: 0.9366\n",
      "Epoch 155/1500\n",
      "86/86 [==============================] - 0s 940us/step - loss: 0.1666 - accuracy: 0.9406\n",
      "Epoch 156/1500\n",
      "86/86 [==============================] - 0s 961us/step - loss: 0.1482 - accuracy: 0.9472\n",
      "Epoch 157/1500\n",
      "86/86 [==============================] - 0s 980us/step - loss: 0.1476 - accuracy: 0.9501\n",
      "Epoch 158/1500\n",
      "86/86 [==============================] - 0s 980us/step - loss: 0.1707 - accuracy: 0.9366\n",
      "Epoch 159/1500\n",
      "86/86 [==============================] - 0s 999us/step - loss: 0.1756 - accuracy: 0.9333\n",
      "Epoch 160/1500\n",
      "86/86 [==============================] - 0s 986us/step - loss: 0.1618 - accuracy: 0.9380\n",
      "Epoch 161/1500\n",
      "86/86 [==============================] - 0s 941us/step - loss: 0.1700 - accuracy: 0.9355\n",
      "Epoch 162/1500\n",
      "86/86 [==============================] - 0s 919us/step - loss: 0.1691 - accuracy: 0.9377\n",
      "Epoch 163/1500\n",
      "86/86 [==============================] - 0s 977us/step - loss: 0.1741 - accuracy: 0.9384\n",
      "Epoch 164/1500\n",
      "86/86 [==============================] - 0s 948us/step - loss: 0.1674 - accuracy: 0.9373\n",
      "Epoch 165/1500\n",
      "86/86 [==============================] - 0s 913us/step - loss: 0.1416 - accuracy: 0.9549\n",
      "Epoch 166/1500\n",
      "86/86 [==============================] - 0s 980us/step - loss: 0.1595 - accuracy: 0.9417\n",
      "Epoch 167/1500\n",
      "86/86 [==============================] - 0s 958us/step - loss: 0.1634 - accuracy: 0.9380\n",
      "Epoch 168/1500\n",
      "86/86 [==============================] - 0s 945us/step - loss: 0.1571 - accuracy: 0.9410\n",
      "Epoch 169/1500\n",
      "86/86 [==============================] - 0s 915us/step - loss: 0.1537 - accuracy: 0.9410\n",
      "Epoch 170/1500\n",
      "86/86 [==============================] - 0s 928us/step - loss: 0.1620 - accuracy: 0.9443\n",
      "Epoch 171/1500\n",
      "86/86 [==============================] - 0s 929us/step - loss: 0.1648 - accuracy: 0.9377\n",
      "Epoch 172/1500\n",
      "86/86 [==============================] - 0s 944us/step - loss: 0.1633 - accuracy: 0.9369\n",
      "Epoch 173/1500\n",
      "86/86 [==============================] - 0s 939us/step - loss: 0.1622 - accuracy: 0.9424\n",
      "Epoch 174/1500\n",
      "86/86 [==============================] - 0s 962us/step - loss: 0.1445 - accuracy: 0.9483\n",
      "Epoch 175/1500\n",
      "86/86 [==============================] - 0s 916us/step - loss: 0.1584 - accuracy: 0.9461\n",
      "Epoch 176/1500\n",
      "86/86 [==============================] - 0s 918us/step - loss: 0.1567 - accuracy: 0.9432\n",
      "Epoch 177/1500\n",
      "86/86 [==============================] - 0s 919us/step - loss: 0.1618 - accuracy: 0.9472\n",
      "Epoch 178/1500\n",
      "86/86 [==============================] - 0s 919us/step - loss: 0.1558 - accuracy: 0.9424\n",
      "Epoch 179/1500\n",
      "86/86 [==============================] - 0s 923us/step - loss: 0.1606 - accuracy: 0.9406\n",
      "Epoch 180/1500\n",
      "86/86 [==============================] - 0s 921us/step - loss: 0.1600 - accuracy: 0.9373\n",
      "Epoch 181/1500\n",
      "86/86 [==============================] - 0s 917us/step - loss: 0.1592 - accuracy: 0.9395\n",
      "Epoch 182/1500\n",
      "86/86 [==============================] - 0s 947us/step - loss: 0.1463 - accuracy: 0.9490\n",
      "Epoch 183/1500\n",
      "86/86 [==============================] - 0s 961us/step - loss: 0.1589 - accuracy: 0.9410\n",
      "Epoch 184/1500\n",
      "86/86 [==============================] - 0s 923us/step - loss: 0.1475 - accuracy: 0.9476\n",
      "Epoch 185/1500\n",
      "86/86 [==============================] - 0s 933us/step - loss: 0.1561 - accuracy: 0.9468\n",
      "Epoch 186/1500\n",
      "86/86 [==============================] - 0s 954us/step - loss: 0.1521 - accuracy: 0.9432\n",
      "Epoch 187/1500\n",
      "86/86 [==============================] - 0s 968us/step - loss: 0.1493 - accuracy: 0.9432\n",
      "Epoch 188/1500\n",
      "86/86 [==============================] - 0s 967us/step - loss: 0.1520 - accuracy: 0.9439\n",
      "Epoch 189/1500\n",
      "86/86 [==============================] - 0s 954us/step - loss: 0.1502 - accuracy: 0.9417\n",
      "Epoch 190/1500\n",
      "86/86 [==============================] - 0s 937us/step - loss: 0.1455 - accuracy: 0.9439\n",
      "Epoch 191/1500\n",
      "86/86 [==============================] - 0s 912us/step - loss: 0.1564 - accuracy: 0.9388\n",
      "Epoch 192/1500\n",
      "86/86 [==============================] - 0s 918us/step - loss: 0.1448 - accuracy: 0.9490\n",
      "Epoch 193/1500\n",
      "86/86 [==============================] - 0s 944us/step - loss: 0.1540 - accuracy: 0.9417\n",
      "Epoch 194/1500\n",
      "86/86 [==============================] - 0s 949us/step - loss: 0.1472 - accuracy: 0.9446\n",
      "Epoch 195/1500\n",
      "54/86 [=================>............] - ETA: 0s - loss: 0.1522 - accuracy: 0.9444Restoring model weights from the end of the best epoch: 165.\n",
      "86/86 [==============================] - 0s 987us/step - loss: 0.1532 - accuracy: 0.9439\n",
      "Epoch 195: early stopping\n",
      "8/8 [==============================] - 0s 824us/step - loss: 0.7493 - accuracy: 0.7404\n",
      "8/8 [==============================] - 0s 634us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (22/29)\n",
      "Before appending - Cat IDs: 618, Predictions: 618, Actuals: 618, Gender: 618\n",
      "After appending - Cat IDs: 853, Predictions: 853, Actuals: 853, Gender: 853\n",
      "Final Test Results - Loss: 0.749269425868988, Accuracy: 0.7404255270957947, Precision: 0.8015432722374563, Recall: 0.6765567765567765, F1 Score: 0.713076621772274\n",
      "Confusion Matrix:\n",
      " [[119   2   9]\n",
      " [ 12  23   0]\n",
      " [ 38   0  32]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.710153660805082\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.6735846698284149\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7700117081403732\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.7112546215387896\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7452397288464531\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[3]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0522ad-89f9-479c-b073-c5e720fc6221",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "ecac477e-680c-4a82-abcb-2b3a2dbfbea4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 853, Predictions: 853, Actuals: 853, Gender: 853\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ce33a298-d7a3-40ec-8f78-00c109118230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "23d083a1-d61a-4fce-9b56-667a9930bfe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.81 (89/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "431e9ecf-be90-466a-a1b7-dd98f7fb2a9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "42e4a5c0-56dd-4771-820f-bcecaf391f0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, kitten, adult, kitten, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, senior, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, kitten, adult, kitten, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, senior, adult, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[senior, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, kitten, adult, adult, adult, ad...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A              [adult, adult, senior, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, kitten, adult, kitten, adult, senior, ...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, adult, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "64    058A                            [senior, adult, senior]        senior           senior                   True\n",
       "63    057A  [senior, adult, adult, senior, adult, adult, s...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "58    052A                      [adult, senior, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "49    043A                                   [kitten, kitten]        kitten           kitten                   True\n",
       "81    075A              [adult, adult, senior, senior, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A     [senior, senior, adult, adult, senior, senior]        senior           senior                   True\n",
       "100   105A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "99    104A                    [adult, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "50    044A             [kitten, adult, adult, kitten, kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "48    042A  [adult, senior, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B               [adult, adult, adult, kitten, adult]         adult            adult                   True\n",
       "25    023A        [adult, adult, senior, adult, adult, adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, senior, senior, adult, adult, senior, ...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "18    016A  [senior, adult, adult, senior, senior, senior,...        senior           senior                   True\n",
       "17    015A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "29    025B                                    [senior, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, senior, adult, senior, a...         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "40    034A              [adult, senior, senior, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, kitten, adult, kitten, adult, adult, a...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "41    035A                     [senior, adult, senior, adult]         adult            adult                   True\n",
       "106   113A                             [adult, senior, adult]         adult           senior                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "103   109A      [adult, adult, kitten, kitten, kitten, adult]         adult           kitten                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "101   106A  [adult, senior, adult, senior, adult, adult, a...         adult           senior                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "56    051A  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "61    055A  [adult, adult, senior, adult, senior, senior, ...         adult           senior                  False\n",
       "42    036A  [senior, senior, senior, senior, adult, adult,...        senior            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "92    097A  [adult, senior, senior, adult, adult, senior, ...         adult           senior                  False\n",
       "90    095A  [senior, senior, adult, senior, senior, senior...        senior            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "66    060A                           [senior, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bc2e5ff6-b761-4b71-b7cd-9eb24aae0ebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     64\n",
      "kitten    13\n",
      "senior    12\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "ff925f9d-efd5-46a0-89a0-1fb3da66a46a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             64  87.671233\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22             12  54.545455\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "d8f08817-c78c-4654-b195-911f2a9ccb2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6f284-fe35-4071-8054-5e5a086b4ed9",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "4394c6ec-6da8-42de-aaaa-bc1b17bbf74b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          566            470  83.038869\n",
      "1           kitten          109             89  81.651376\n",
      "2           senior          178             97  54.494382\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8d541c77-590d-4b79-b526-85520ea5c4a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c98d75-f483-4819-b292-975bd229cfd2",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c7325b11-ab06-45bd-8e77-d498737b7552",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      154     72.30\n",
      "1          M    337      270     80.12\n",
      "2          X    303      232     76.57\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "777ae0eb-dab5-47c0-86a7-0d3c6b01b9c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40270db-853d-46a1-acb4-8dab8a4f9c81",
   "metadata": {},
   "source": [
    "# RANDOM SEED 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "588e6cfb-6ee5-4b51-a9ee-79dcbfbb208c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    1020\n",
      "kitten     992\n",
      "adult      842\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[4]))\n",
    "np.random.seed(int(random_seeds[4]))\n",
    "tf.random.set_seed(int(random_seeds[4]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_3.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "432a9515-e7f1-482c-bdaf-73cb07a9132e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "8231e441-4472-4d9b-8ddf-dc285c07923b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9ed6b-6d2f-491e-95d8-73d33edeef6a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6cd63258-8a3b-4bc0-9ac6-ae8aa4b515b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "106A    14\n",
      "097B    14\n",
      "028A    13\n",
      "111A    13\n",
      "051A    12\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "068A    11\n",
      "014B    10\n",
      "016A    10\n",
      "071A    10\n",
      "005A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "045A     9\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "095A     8\n",
      "010A     8\n",
      "050A     7\n",
      "027A     7\n",
      "031A     7\n",
      "099A     7\n",
      "117A     7\n",
      "053A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "075A     5\n",
      "021A     5\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "062A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "009A     4\n",
      "035A     4\n",
      "056A     3\n",
      "014A     3\n",
      "058A     3\n",
      "060A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "032A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "073A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "041A     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "000B    19\n",
      "001A    14\n",
      "002A    13\n",
      "040A    10\n",
      "022A     9\n",
      "065A     9\n",
      "109A     6\n",
      "108A     6\n",
      "044A     5\n",
      "026A     4\n",
      "113A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "011A     2\n",
      "054A     2\n",
      "018A     2\n",
      "092A     1\n",
      "049A     1\n",
      "004A     1\n",
      "019B     1\n",
      "115A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    280\n",
      "X    256\n",
      "F    186\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    92\n",
      "F    66\n",
      "M    57\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [014B, 111A, 047A, 042A, 050A, 043A, 041A, 045...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 001A, 103A, 022A, 065A, 002A, 000B, 026...\n",
      "kitten                 [044A, 040A, 046A, 109A, 049A, 115A]\n",
      "senior                             [113A, 054A, 108A, 011A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 59, 'kitten': 10, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 15, 'kitten': 6, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '041A' '042A' '043A' '045A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '044A' '046A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'041A'}\n",
      "Moved to Test Set:\n",
      "{'041A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '042A' '043A' '045A' '046A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '041A' '044A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "Length of X_train_val:\n",
      "784\n",
      "Length of y_train_val:\n",
      "784\n",
      "Length of groups_train_val:\n",
      "784\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten     85\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     116\n",
      "kitten     86\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten    147\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     116\n",
      "kitten     24\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1156, 2: 1109, 1: 1003})\n",
      "Epoch 1/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.8818 - accuracy: 0.6313\n",
      "Epoch 2/1500\n",
      "103/103 [==============================] - 0s 981us/step - loss: 0.7283 - accuracy: 0.7072\n",
      "Epoch 3/1500\n",
      "103/103 [==============================] - 0s 918us/step - loss: 0.6421 - accuracy: 0.7286\n",
      "Epoch 4/1500\n",
      "103/103 [==============================] - 0s 923us/step - loss: 0.6091 - accuracy: 0.7491\n",
      "Epoch 5/1500\n",
      "103/103 [==============================] - 0s 962us/step - loss: 0.5997 - accuracy: 0.7592\n",
      "Epoch 6/1500\n",
      "103/103 [==============================] - 0s 978us/step - loss: 0.5679 - accuracy: 0.7729\n",
      "Epoch 7/1500\n",
      "103/103 [==============================] - 0s 950us/step - loss: 0.5428 - accuracy: 0.7772\n",
      "Epoch 8/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.5101 - accuracy: 0.7919\n",
      "Epoch 9/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.5084 - accuracy: 0.7950\n",
      "Epoch 10/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4824 - accuracy: 0.8048\n",
      "Epoch 11/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4782 - accuracy: 0.8069\n",
      "Epoch 12/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4652 - accuracy: 0.8109\n",
      "Epoch 13/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4491 - accuracy: 0.8185\n",
      "Epoch 14/1500\n",
      "103/103 [==============================] - 0s 984us/step - loss: 0.4345 - accuracy: 0.8256\n",
      "Epoch 15/1500\n",
      "103/103 [==============================] - 0s 941us/step - loss: 0.4541 - accuracy: 0.8149\n",
      "Epoch 16/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4230 - accuracy: 0.8293\n",
      "Epoch 17/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4094 - accuracy: 0.8320\n",
      "Epoch 18/1500\n",
      "103/103 [==============================] - 0s 941us/step - loss: 0.4190 - accuracy: 0.8277\n",
      "Epoch 19/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.4215 - accuracy: 0.8320\n",
      "Epoch 20/1500\n",
      "103/103 [==============================] - 0s 978us/step - loss: 0.4122 - accuracy: 0.8293\n",
      "Epoch 21/1500\n",
      "103/103 [==============================] - 0s 953us/step - loss: 0.4043 - accuracy: 0.8372\n",
      "Epoch 22/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8403\n",
      "Epoch 23/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3925 - accuracy: 0.8446\n",
      "Epoch 24/1500\n",
      "103/103 [==============================] - 0s 964us/step - loss: 0.3899 - accuracy: 0.8464\n",
      "Epoch 25/1500\n",
      "103/103 [==============================] - 0s 961us/step - loss: 0.3806 - accuracy: 0.8473\n",
      "Epoch 26/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8562\n",
      "Epoch 27/1500\n",
      "103/103 [==============================] - 0s 974us/step - loss: 0.3625 - accuracy: 0.8501\n",
      "Epoch 28/1500\n",
      "103/103 [==============================] - 0s 944us/step - loss: 0.3792 - accuracy: 0.8528\n",
      "Epoch 29/1500\n",
      "103/103 [==============================] - 0s 968us/step - loss: 0.3578 - accuracy: 0.8565\n",
      "Epoch 30/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.8635\n",
      "Epoch 31/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3414 - accuracy: 0.8626\n",
      "Epoch 32/1500\n",
      "103/103 [==============================] - 0s 928us/step - loss: 0.3481 - accuracy: 0.8669\n",
      "Epoch 33/1500\n",
      "103/103 [==============================] - 0s 935us/step - loss: 0.3464 - accuracy: 0.8608\n",
      "Epoch 34/1500\n",
      "103/103 [==============================] - 0s 965us/step - loss: 0.3419 - accuracy: 0.8660\n",
      "Epoch 35/1500\n",
      "103/103 [==============================] - 0s 986us/step - loss: 0.3625 - accuracy: 0.8522\n",
      "Epoch 36/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3444 - accuracy: 0.8647\n",
      "Epoch 37/1500\n",
      "103/103 [==============================] - 0s 991us/step - loss: 0.3250 - accuracy: 0.8669\n",
      "Epoch 38/1500\n",
      "103/103 [==============================] - 0s 985us/step - loss: 0.3332 - accuracy: 0.8718\n",
      "Epoch 39/1500\n",
      "103/103 [==============================] - 0s 963us/step - loss: 0.3299 - accuracy: 0.8660\n",
      "Epoch 40/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3375 - accuracy: 0.8678\n",
      "Epoch 41/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8703\n",
      "Epoch 42/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3114 - accuracy: 0.8810\n",
      "Epoch 43/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3127 - accuracy: 0.8791\n",
      "Epoch 44/1500\n",
      "103/103 [==============================] - 0s 949us/step - loss: 0.3213 - accuracy: 0.8758\n",
      "Epoch 45/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3005 - accuracy: 0.8840\n",
      "Epoch 46/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3029 - accuracy: 0.8770\n",
      "Epoch 47/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.3119 - accuracy: 0.8794\n",
      "Epoch 48/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2919 - accuracy: 0.8849\n",
      "Epoch 49/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2993 - accuracy: 0.8742\n",
      "Epoch 50/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2959 - accuracy: 0.8828\n",
      "Epoch 51/1500\n",
      "103/103 [==============================] - 0s 954us/step - loss: 0.2905 - accuracy: 0.8831\n",
      "Epoch 52/1500\n",
      "103/103 [==============================] - 0s 953us/step - loss: 0.2962 - accuracy: 0.8810\n",
      "Epoch 53/1500\n",
      "103/103 [==============================] - 0s 942us/step - loss: 0.2754 - accuracy: 0.8905\n",
      "Epoch 54/1500\n",
      "103/103 [==============================] - 0s 928us/step - loss: 0.2682 - accuracy: 0.8963\n",
      "Epoch 55/1500\n",
      "103/103 [==============================] - 0s 935us/step - loss: 0.2779 - accuracy: 0.8926\n",
      "Epoch 56/1500\n",
      "103/103 [==============================] - 0s 922us/step - loss: 0.2791 - accuracy: 0.8941\n",
      "Epoch 57/1500\n",
      "103/103 [==============================] - 0s 940us/step - loss: 0.2734 - accuracy: 0.8908\n",
      "Epoch 58/1500\n",
      "103/103 [==============================] - 0s 945us/step - loss: 0.2769 - accuracy: 0.8960\n",
      "Epoch 59/1500\n",
      "103/103 [==============================] - 0s 969us/step - loss: 0.2620 - accuracy: 0.9030\n",
      "Epoch 60/1500\n",
      "103/103 [==============================] - 0s 938us/step - loss: 0.3063 - accuracy: 0.8810\n",
      "Epoch 61/1500\n",
      "103/103 [==============================] - 0s 958us/step - loss: 0.2679 - accuracy: 0.8908\n",
      "Epoch 62/1500\n",
      "103/103 [==============================] - 0s 962us/step - loss: 0.2733 - accuracy: 0.8905\n",
      "Epoch 63/1500\n",
      "103/103 [==============================] - 0s 987us/step - loss: 0.2693 - accuracy: 0.8944\n",
      "Epoch 64/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.9009\n",
      "Epoch 65/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2665 - accuracy: 0.8957\n",
      "Epoch 66/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2627 - accuracy: 0.8996\n",
      "Epoch 67/1500\n",
      "103/103 [==============================] - 0s 981us/step - loss: 0.2531 - accuracy: 0.8981\n",
      "Epoch 68/1500\n",
      "103/103 [==============================] - 0s 961us/step - loss: 0.2522 - accuracy: 0.9002\n",
      "Epoch 69/1500\n",
      "103/103 [==============================] - 0s 917us/step - loss: 0.2468 - accuracy: 0.8963\n",
      "Epoch 70/1500\n",
      "103/103 [==============================] - 0s 955us/step - loss: 0.2514 - accuracy: 0.9070\n",
      "Epoch 71/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2766 - accuracy: 0.8901\n",
      "Epoch 72/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2375 - accuracy: 0.9119\n",
      "Epoch 73/1500\n",
      "103/103 [==============================] - 0s 985us/step - loss: 0.2413 - accuracy: 0.9064\n",
      "Epoch 74/1500\n",
      "103/103 [==============================] - 0s 977us/step - loss: 0.2535 - accuracy: 0.8981\n",
      "Epoch 75/1500\n",
      "103/103 [==============================] - 0s 941us/step - loss: 0.2644 - accuracy: 0.8972\n",
      "Epoch 76/1500\n",
      "103/103 [==============================] - 0s 956us/step - loss: 0.2411 - accuracy: 0.9082\n",
      "Epoch 77/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2614 - accuracy: 0.8993\n",
      "Epoch 78/1500\n",
      "103/103 [==============================] - 0s 982us/step - loss: 0.2453 - accuracy: 0.9131\n",
      "Epoch 79/1500\n",
      "103/103 [==============================] - 0s 947us/step - loss: 0.2258 - accuracy: 0.9128\n",
      "Epoch 80/1500\n",
      "103/103 [==============================] - 0s 952us/step - loss: 0.2435 - accuracy: 0.9051\n",
      "Epoch 81/1500\n",
      "103/103 [==============================] - 0s 943us/step - loss: 0.2549 - accuracy: 0.9021\n",
      "Epoch 82/1500\n",
      "103/103 [==============================] - 0s 981us/step - loss: 0.2471 - accuracy: 0.8996\n",
      "Epoch 83/1500\n",
      "103/103 [==============================] - 0s 984us/step - loss: 0.2387 - accuracy: 0.9076\n",
      "Epoch 84/1500\n",
      "103/103 [==============================] - 0s 983us/step - loss: 0.2446 - accuracy: 0.9100\n",
      "Epoch 85/1500\n",
      "103/103 [==============================] - 0s 959us/step - loss: 0.2518 - accuracy: 0.9033\n",
      "Epoch 86/1500\n",
      "103/103 [==============================] - 0s 988us/step - loss: 0.2339 - accuracy: 0.9091\n",
      "Epoch 87/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2533 - accuracy: 0.9024\n",
      "Epoch 88/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.9027\n",
      "Epoch 89/1500\n",
      "103/103 [==============================] - 0s 939us/step - loss: 0.2364 - accuracy: 0.9067\n",
      "Epoch 90/1500\n",
      "103/103 [==============================] - 0s 971us/step - loss: 0.2469 - accuracy: 0.9024\n",
      "Epoch 91/1500\n",
      "103/103 [==============================] - 0s 957us/step - loss: 0.2390 - accuracy: 0.9064\n",
      "Epoch 92/1500\n",
      "103/103 [==============================] - 0s 945us/step - loss: 0.2337 - accuracy: 0.9082\n",
      "Epoch 93/1500\n",
      "103/103 [==============================] - 0s 948us/step - loss: 0.2391 - accuracy: 0.9070\n",
      "Epoch 94/1500\n",
      "103/103 [==============================] - 0s 968us/step - loss: 0.2313 - accuracy: 0.9054\n",
      "Epoch 95/1500\n",
      "103/103 [==============================] - 0s 958us/step - loss: 0.2185 - accuracy: 0.9162\n",
      "Epoch 96/1500\n",
      "103/103 [==============================] - 0s 927us/step - loss: 0.2253 - accuracy: 0.9159\n",
      "Epoch 97/1500\n",
      "103/103 [==============================] - 0s 943us/step - loss: 0.2234 - accuracy: 0.9152\n",
      "Epoch 98/1500\n",
      "103/103 [==============================] - 0s 973us/step - loss: 0.2242 - accuracy: 0.9131\n",
      "Epoch 99/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2328 - accuracy: 0.9137\n",
      "Epoch 100/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2483 - accuracy: 0.9045\n",
      "Epoch 101/1500\n",
      "103/103 [==============================] - 0s 944us/step - loss: 0.2324 - accuracy: 0.9085\n",
      "Epoch 102/1500\n",
      "103/103 [==============================] - 0s 963us/step - loss: 0.2276 - accuracy: 0.9100\n",
      "Epoch 103/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2141 - accuracy: 0.9198\n",
      "Epoch 104/1500\n",
      "103/103 [==============================] - 0s 980us/step - loss: 0.2198 - accuracy: 0.9159\n",
      "Epoch 105/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2241 - accuracy: 0.9116\n",
      "Epoch 106/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2360 - accuracy: 0.9100\n",
      "Epoch 107/1500\n",
      "103/103 [==============================] - 0s 951us/step - loss: 0.2206 - accuracy: 0.9177\n",
      "Epoch 108/1500\n",
      "103/103 [==============================] - 0s 972us/step - loss: 0.2319 - accuracy: 0.9131\n",
      "Epoch 109/1500\n",
      "103/103 [==============================] - 0s 970us/step - loss: 0.2144 - accuracy: 0.9177\n",
      "Epoch 110/1500\n",
      "103/103 [==============================] - 0s 972us/step - loss: 0.2163 - accuracy: 0.9155\n",
      "Epoch 111/1500\n",
      "103/103 [==============================] - 0s 974us/step - loss: 0.2171 - accuracy: 0.9186\n",
      "Epoch 112/1500\n",
      "103/103 [==============================] - 0s 4ms/step - loss: 0.2297 - accuracy: 0.9152\n",
      "Epoch 113/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2100 - accuracy: 0.9211\n",
      "Epoch 114/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2010 - accuracy: 0.9192\n",
      "Epoch 115/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2368 - accuracy: 0.9085\n",
      "Epoch 116/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2193 - accuracy: 0.9165\n",
      "Epoch 117/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2174 - accuracy: 0.9195\n",
      "Epoch 118/1500\n",
      "103/103 [==============================] - 0s 991us/step - loss: 0.2084 - accuracy: 0.9186\n",
      "Epoch 119/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1942 - accuracy: 0.9278\n",
      "Epoch 120/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2039 - accuracy: 0.9232\n",
      "Epoch 121/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9250\n",
      "Epoch 122/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1947 - accuracy: 0.9253\n",
      "Epoch 123/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.9211\n",
      "Epoch 124/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.9168\n",
      "Epoch 125/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2191 - accuracy: 0.9113\n",
      "Epoch 126/1500\n",
      "103/103 [==============================] - 0s 995us/step - loss: 0.2074 - accuracy: 0.9201\n",
      "Epoch 127/1500\n",
      "103/103 [==============================] - 0s 970us/step - loss: 0.1989 - accuracy: 0.9195\n",
      "Epoch 128/1500\n",
      "103/103 [==============================] - 0s 931us/step - loss: 0.1868 - accuracy: 0.9302\n",
      "Epoch 129/1500\n",
      "103/103 [==============================] - 0s 962us/step - loss: 0.1965 - accuracy: 0.9253\n",
      "Epoch 130/1500\n",
      "103/103 [==============================] - 0s 955us/step - loss: 0.1959 - accuracy: 0.9287\n",
      "Epoch 131/1500\n",
      "103/103 [==============================] - 0s 983us/step - loss: 0.2011 - accuracy: 0.9223\n",
      "Epoch 132/1500\n",
      "103/103 [==============================] - 0s 926us/step - loss: 0.2007 - accuracy: 0.9207\n",
      "Epoch 133/1500\n",
      "103/103 [==============================] - 0s 924us/step - loss: 0.2081 - accuracy: 0.9162\n",
      "Epoch 134/1500\n",
      "103/103 [==============================] - 0s 934us/step - loss: 0.2087 - accuracy: 0.9186\n",
      "Epoch 135/1500\n",
      "103/103 [==============================] - 0s 964us/step - loss: 0.2109 - accuracy: 0.9134\n",
      "Epoch 136/1500\n",
      "103/103 [==============================] - 0s 938us/step - loss: 0.1904 - accuracy: 0.9263\n",
      "Epoch 137/1500\n",
      "103/103 [==============================] - 0s 938us/step - loss: 0.1997 - accuracy: 0.9235\n",
      "Epoch 138/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.9189\n",
      "Epoch 139/1500\n",
      "103/103 [==============================] - 0s 962us/step - loss: 0.1973 - accuracy: 0.9247\n",
      "Epoch 140/1500\n",
      "103/103 [==============================] - 0s 920us/step - loss: 0.2029 - accuracy: 0.9266\n",
      "Epoch 141/1500\n",
      "103/103 [==============================] - 0s 905us/step - loss: 0.1978 - accuracy: 0.9259\n",
      "Epoch 142/1500\n",
      "103/103 [==============================] - 0s 909us/step - loss: 0.1859 - accuracy: 0.9315\n",
      "Epoch 143/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9238\n",
      "Epoch 144/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9290\n",
      "Epoch 145/1500\n",
      "103/103 [==============================] - 0s 945us/step - loss: 0.1825 - accuracy: 0.9327\n",
      "Epoch 146/1500\n",
      "103/103 [==============================] - 0s 949us/step - loss: 0.1954 - accuracy: 0.9220\n",
      "Epoch 147/1500\n",
      "103/103 [==============================] - 0s 975us/step - loss: 0.1956 - accuracy: 0.9214\n",
      "Epoch 148/1500\n",
      "103/103 [==============================] - 0s 962us/step - loss: 0.1992 - accuracy: 0.9232\n",
      "Epoch 149/1500\n",
      "103/103 [==============================] - 0s 937us/step - loss: 0.1959 - accuracy: 0.9226\n",
      "Epoch 150/1500\n",
      "103/103 [==============================] - 0s 947us/step - loss: 0.1939 - accuracy: 0.9296\n",
      "Epoch 151/1500\n",
      "103/103 [==============================] - 0s 983us/step - loss: 0.1845 - accuracy: 0.9299\n",
      "Epoch 152/1500\n",
      "103/103 [==============================] - 0s 972us/step - loss: 0.1873 - accuracy: 0.9266\n",
      "Epoch 153/1500\n",
      "103/103 [==============================] - 0s 936us/step - loss: 0.1963 - accuracy: 0.9238\n",
      "Epoch 154/1500\n",
      "103/103 [==============================] - 0s 960us/step - loss: 0.2064 - accuracy: 0.9232\n",
      "Epoch 155/1500\n",
      "103/103 [==============================] - 0s 952us/step - loss: 0.1971 - accuracy: 0.9244\n",
      "Epoch 156/1500\n",
      "103/103 [==============================] - 0s 941us/step - loss: 0.1871 - accuracy: 0.9275\n",
      "Epoch 157/1500\n",
      "103/103 [==============================] - 0s 936us/step - loss: 0.1832 - accuracy: 0.9302\n",
      "Epoch 158/1500\n",
      "103/103 [==============================] - 0s 968us/step - loss: 0.1795 - accuracy: 0.9336\n",
      "Epoch 159/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9281\n",
      "Epoch 160/1500\n",
      "103/103 [==============================] - 0s 948us/step - loss: 0.1854 - accuracy: 0.9256\n",
      "Epoch 161/1500\n",
      "103/103 [==============================] - 0s 929us/step - loss: 0.1940 - accuracy: 0.9269\n",
      "Epoch 162/1500\n",
      "103/103 [==============================] - 0s 982us/step - loss: 0.2065 - accuracy: 0.9211\n",
      "Epoch 163/1500\n",
      "103/103 [==============================] - 0s 989us/step - loss: 0.1838 - accuracy: 0.9269\n",
      "Epoch 164/1500\n",
      "103/103 [==============================] - 0s 951us/step - loss: 0.1947 - accuracy: 0.9296\n",
      "Epoch 165/1500\n",
      "103/103 [==============================] - 0s 932us/step - loss: 0.1875 - accuracy: 0.9281\n",
      "Epoch 166/1500\n",
      "103/103 [==============================] - 0s 920us/step - loss: 0.1852 - accuracy: 0.9287\n",
      "Epoch 167/1500\n",
      "103/103 [==============================] - 0s 945us/step - loss: 0.1794 - accuracy: 0.9330\n",
      "Epoch 168/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1899 - accuracy: 0.9351\n",
      "Epoch 169/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.9272\n",
      "Epoch 170/1500\n",
      "103/103 [==============================] - 0s 947us/step - loss: 0.1769 - accuracy: 0.9345\n",
      "Epoch 171/1500\n",
      "103/103 [==============================] - 0s 934us/step - loss: 0.1744 - accuracy: 0.9364\n",
      "Epoch 172/1500\n",
      "103/103 [==============================] - 0s 934us/step - loss: 0.1573 - accuracy: 0.9385\n",
      "Epoch 173/1500\n",
      "103/103 [==============================] - 0s 960us/step - loss: 0.1803 - accuracy: 0.9321\n",
      "Epoch 174/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9293\n",
      "Epoch 175/1500\n",
      "103/103 [==============================] - 0s 990us/step - loss: 0.1823 - accuracy: 0.9296\n",
      "Epoch 176/1500\n",
      "103/103 [==============================] - 0s 953us/step - loss: 0.1808 - accuracy: 0.9318\n",
      "Epoch 177/1500\n",
      "103/103 [==============================] - 0s 958us/step - loss: 0.1816 - accuracy: 0.9299\n",
      "Epoch 178/1500\n",
      "103/103 [==============================] - 0s 959us/step - loss: 0.1631 - accuracy: 0.9397\n",
      "Epoch 179/1500\n",
      "103/103 [==============================] - 0s 956us/step - loss: 0.1762 - accuracy: 0.9333\n",
      "Epoch 180/1500\n",
      "103/103 [==============================] - 0s 962us/step - loss: 0.1724 - accuracy: 0.9391\n",
      "Epoch 181/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9357\n",
      "Epoch 182/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9412\n",
      "Epoch 183/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1493 - accuracy: 0.9419\n",
      "Epoch 184/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9422\n",
      "Epoch 185/1500\n",
      "103/103 [==============================] - 0s 965us/step - loss: 0.1693 - accuracy: 0.9388\n",
      "Epoch 186/1500\n",
      "103/103 [==============================] - 0s 952us/step - loss: 0.1639 - accuracy: 0.9416\n",
      "Epoch 187/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9370\n",
      "Epoch 188/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9357\n",
      "Epoch 189/1500\n",
      "103/103 [==============================] - 0s 969us/step - loss: 0.1927 - accuracy: 0.9263\n",
      "Epoch 190/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9394\n",
      "Epoch 191/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1785 - accuracy: 0.9318\n",
      "Epoch 192/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9345\n",
      "Epoch 193/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9382\n",
      "Epoch 194/1500\n",
      "103/103 [==============================] - 0s 981us/step - loss: 0.1576 - accuracy: 0.9422\n",
      "Epoch 195/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9382\n",
      "Epoch 196/1500\n",
      "103/103 [==============================] - 0s 966us/step - loss: 0.1621 - accuracy: 0.9394\n",
      "Epoch 197/1500\n",
      "103/103 [==============================] - 0s 952us/step - loss: 0.1725 - accuracy: 0.9364\n",
      "Epoch 198/1500\n",
      "103/103 [==============================] - 0s 943us/step - loss: 0.1634 - accuracy: 0.9373\n",
      "Epoch 199/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9357\n",
      "Epoch 200/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9425\n",
      "Epoch 201/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.9324\n",
      "Epoch 202/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9302\n",
      "Epoch 203/1500\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1748 - accuracy: 0.9345\n",
      "Epoch 204/1500\n",
      "103/103 [==============================] - 0s 924us/step - loss: 0.1768 - accuracy: 0.9321\n",
      "Epoch 205/1500\n",
      "103/103 [==============================] - 0s 917us/step - loss: 0.1690 - accuracy: 0.9312\n",
      "Epoch 206/1500\n",
      "103/103 [==============================] - 0s 956us/step - loss: 0.1618 - accuracy: 0.9403\n",
      "Epoch 207/1500\n",
      "103/103 [==============================] - 0s 964us/step - loss: 0.1631 - accuracy: 0.9400\n",
      "Epoch 208/1500\n",
      "103/103 [==============================] - 0s 970us/step - loss: 0.1690 - accuracy: 0.9345\n",
      "Epoch 209/1500\n",
      "103/103 [==============================] - 0s 998us/step - loss: 0.1613 - accuracy: 0.9406\n",
      "Epoch 210/1500\n",
      "103/103 [==============================] - 0s 951us/step - loss: 0.1645 - accuracy: 0.9293\n",
      "Epoch 211/1500\n",
      "103/103 [==============================] - 0s 961us/step - loss: 0.1746 - accuracy: 0.9272\n",
      "Epoch 212/1500\n",
      "103/103 [==============================] - 0s 985us/step - loss: 0.1617 - accuracy: 0.9354\n",
      "Epoch 213/1500\n",
      "103/103 [==============================] - ETA: 0s - loss: 0.1638 - accuracy: 0.9379Restoring model weights from the end of the best epoch: 183.\n",
      "103/103 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9379\n",
      "Epoch 213: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.7153 - accuracy: 0.6601\n",
      "5/5 [==============================] - 0s 826us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.68 (17/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "Final Test Results - Loss: 0.7153176665306091, Accuracy: 0.6601307392120361, Precision: 0.5594138730761121, Recall: 0.5120100206307102, F1 Score: 0.5265499026966917\n",
      "Confusion Matrix:\n",
      " [[83  4 29]\n",
      " [ 8 16  0]\n",
      " [11  0  2]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "005A    10\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "027A     7\n",
      "008A     6\n",
      "108A     6\n",
      "109A     6\n",
      "053A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "003A     4\n",
      "105A     4\n",
      "009A     4\n",
      "026A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "012A     3\n",
      "113A     3\n",
      "014A     3\n",
      "060A     3\n",
      "011A     2\n",
      "102A     2\n",
      "032A     2\n",
      "069A     2\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "038A     2\n",
      "019B     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "092A     1\n",
      "004A     1\n",
      "041A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "073A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "067A    19\n",
      "042A    14\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "063A    11\n",
      "016A    10\n",
      "033A     9\n",
      "051B     9\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "117A     7\n",
      "050A     7\n",
      "007A     6\n",
      "075A     5\n",
      "025C     5\n",
      "087A     2\n",
      "061A     2\n",
      "025B     2\n",
      "043A     1\n",
      "066A     1\n",
      "048A     1\n",
      "088A     1\n",
      "091A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    263\n",
      "M    226\n",
      "F    177\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    111\n",
      "X     85\n",
      "F     75\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 015A, 001A, 103A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 109A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 113...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 067A, 002B, 091A, 039A, 063A, 013...\n",
      "kitten                       [047A, 042A, 050A, 043A, 048A]\n",
      "senior                 [116A, 051B, 117A, 016A, 061A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 11, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 5, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A' '011A'\n",
      " '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A' '045A'\n",
      " '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '062A' '064A' '065A' '069A' '070A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A'\n",
      " '039A' '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A'\n",
      " '067A' '068A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'026A'}\n",
      "Moved to Test Set:\n",
      "{'026A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A'\n",
      " '011A' '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A' '045A'\n",
      " '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '062A' '064A' '065A' '069A' '070A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '026A' '033A'\n",
      " '039A' '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A'\n",
      " '067A' '068A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "Length of X_train_val:\n",
      "701\n",
      "Length of y_train_val:\n",
      "701\n",
      "Length of groups_train_val:\n",
      "701\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     409\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     179\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     444\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     144\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1071, 2: 941, 1: 788})\n",
      "Epoch 1/1500\n",
      "88/88 [==============================] - 0s 990us/step - loss: 0.9111 - accuracy: 0.5993\n",
      "Epoch 2/1500\n",
      "88/88 [==============================] - 0s 946us/step - loss: 0.7009 - accuracy: 0.7032\n",
      "Epoch 3/1500\n",
      "88/88 [==============================] - 0s 935us/step - loss: 0.6570 - accuracy: 0.7179\n",
      "Epoch 4/1500\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.6152 - accuracy: 0.7454\n",
      "Epoch 5/1500\n",
      "88/88 [==============================] - 0s 943us/step - loss: 0.6018 - accuracy: 0.7454\n",
      "Epoch 6/1500\n",
      "88/88 [==============================] - 0s 971us/step - loss: 0.5532 - accuracy: 0.7575\n",
      "Epoch 7/1500\n",
      "88/88 [==============================] - 0s 946us/step - loss: 0.5509 - accuracy: 0.7721\n",
      "Epoch 8/1500\n",
      "88/88 [==============================] - 0s 963us/step - loss: 0.5345 - accuracy: 0.7696\n",
      "Epoch 9/1500\n",
      "88/88 [==============================] - 0s 928us/step - loss: 0.5170 - accuracy: 0.7743\n",
      "Epoch 10/1500\n",
      "88/88 [==============================] - 0s 938us/step - loss: 0.4914 - accuracy: 0.7957\n",
      "Epoch 11/1500\n",
      "88/88 [==============================] - 0s 942us/step - loss: 0.4974 - accuracy: 0.7875\n",
      "Epoch 12/1500\n",
      "88/88 [==============================] - 0s 931us/step - loss: 0.4766 - accuracy: 0.7989\n",
      "Epoch 13/1500\n",
      "88/88 [==============================] - 0s 922us/step - loss: 0.4845 - accuracy: 0.8007\n",
      "Epoch 14/1500\n",
      "88/88 [==============================] - 0s 914us/step - loss: 0.4413 - accuracy: 0.8111\n",
      "Epoch 15/1500\n",
      "88/88 [==============================] - 0s 930us/step - loss: 0.4719 - accuracy: 0.8000\n",
      "Epoch 16/1500\n",
      "88/88 [==============================] - 0s 929us/step - loss: 0.4408 - accuracy: 0.8114\n",
      "Epoch 17/1500\n",
      "88/88 [==============================] - 0s 941us/step - loss: 0.4300 - accuracy: 0.8239\n",
      "Epoch 18/1500\n",
      "88/88 [==============================] - 0s 910us/step - loss: 0.4180 - accuracy: 0.8254\n",
      "Epoch 19/1500\n",
      "88/88 [==============================] - 0s 928us/step - loss: 0.3955 - accuracy: 0.8382\n",
      "Epoch 20/1500\n",
      "88/88 [==============================] - 0s 913us/step - loss: 0.4114 - accuracy: 0.8311\n",
      "Epoch 21/1500\n",
      "88/88 [==============================] - 0s 901us/step - loss: 0.3846 - accuracy: 0.8307\n",
      "Epoch 22/1500\n",
      "88/88 [==============================] - 0s 932us/step - loss: 0.3876 - accuracy: 0.8396\n",
      "Epoch 23/1500\n",
      "88/88 [==============================] - 0s 953us/step - loss: 0.3786 - accuracy: 0.8357\n",
      "Epoch 24/1500\n",
      "88/88 [==============================] - 0s 917us/step - loss: 0.3725 - accuracy: 0.8514\n",
      "Epoch 25/1500\n",
      "88/88 [==============================] - 0s 925us/step - loss: 0.3730 - accuracy: 0.8418\n",
      "Epoch 26/1500\n",
      "88/88 [==============================] - 0s 934us/step - loss: 0.3525 - accuracy: 0.8514\n",
      "Epoch 27/1500\n",
      "88/88 [==============================] - 0s 912us/step - loss: 0.3591 - accuracy: 0.8554\n",
      "Epoch 28/1500\n",
      "88/88 [==============================] - 0s 978us/step - loss: 0.3533 - accuracy: 0.8557\n",
      "Epoch 29/1500\n",
      "88/88 [==============================] - 0s 972us/step - loss: 0.3506 - accuracy: 0.8482\n",
      "Epoch 30/1500\n",
      "88/88 [==============================] - 0s 931us/step - loss: 0.3392 - accuracy: 0.8643\n",
      "Epoch 31/1500\n",
      "88/88 [==============================] - 0s 932us/step - loss: 0.3314 - accuracy: 0.8596\n",
      "Epoch 32/1500\n",
      "88/88 [==============================] - 0s 921us/step - loss: 0.3429 - accuracy: 0.8596\n",
      "Epoch 33/1500\n",
      "88/88 [==============================] - 0s 930us/step - loss: 0.3197 - accuracy: 0.8696\n",
      "Epoch 34/1500\n",
      "88/88 [==============================] - 0s 927us/step - loss: 0.3103 - accuracy: 0.8739\n",
      "Epoch 35/1500\n",
      "88/88 [==============================] - 0s 928us/step - loss: 0.3276 - accuracy: 0.8589\n",
      "Epoch 36/1500\n",
      "88/88 [==============================] - 0s 923us/step - loss: 0.3145 - accuracy: 0.8664\n",
      "Epoch 37/1500\n",
      "88/88 [==============================] - 0s 926us/step - loss: 0.3054 - accuracy: 0.8743\n",
      "Epoch 38/1500\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.3111 - accuracy: 0.8757\n",
      "Epoch 39/1500\n",
      "88/88 [==============================] - 0s 958us/step - loss: 0.2891 - accuracy: 0.8875\n",
      "Epoch 40/1500\n",
      "88/88 [==============================] - 0s 932us/step - loss: 0.2916 - accuracy: 0.8907\n",
      "Epoch 41/1500\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.3092 - accuracy: 0.8761\n",
      "Epoch 42/1500\n",
      "88/88 [==============================] - 0s 957us/step - loss: 0.2933 - accuracy: 0.8814\n",
      "Epoch 43/1500\n",
      "88/88 [==============================] - 0s 958us/step - loss: 0.3088 - accuracy: 0.8750\n",
      "Epoch 44/1500\n",
      "88/88 [==============================] - 0s 920us/step - loss: 0.2956 - accuracy: 0.8836\n",
      "Epoch 45/1500\n",
      "88/88 [==============================] - 0s 932us/step - loss: 0.2949 - accuracy: 0.8768\n",
      "Epoch 46/1500\n",
      "88/88 [==============================] - 0s 941us/step - loss: 0.2962 - accuracy: 0.8804\n",
      "Epoch 47/1500\n",
      "88/88 [==============================] - 0s 966us/step - loss: 0.2782 - accuracy: 0.8886\n",
      "Epoch 48/1500\n",
      "88/88 [==============================] - 0s 930us/step - loss: 0.2804 - accuracy: 0.8911\n",
      "Epoch 49/1500\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.2849 - accuracy: 0.8846\n",
      "Epoch 50/1500\n",
      "88/88 [==============================] - 0s 945us/step - loss: 0.2740 - accuracy: 0.8943\n",
      "Epoch 51/1500\n",
      "88/88 [==============================] - 0s 941us/step - loss: 0.2591 - accuracy: 0.8939\n",
      "Epoch 52/1500\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.2710 - accuracy: 0.8964\n",
      "Epoch 53/1500\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.3015 - accuracy: 0.8743\n",
      "Epoch 54/1500\n",
      "88/88 [==============================] - 0s 928us/step - loss: 0.2790 - accuracy: 0.8861\n",
      "Epoch 55/1500\n",
      "88/88 [==============================] - 0s 922us/step - loss: 0.2744 - accuracy: 0.8854\n",
      "Epoch 56/1500\n",
      "88/88 [==============================] - 0s 935us/step - loss: 0.2559 - accuracy: 0.8939\n",
      "Epoch 57/1500\n",
      "88/88 [==============================] - 0s 945us/step - loss: 0.2553 - accuracy: 0.8971\n",
      "Epoch 58/1500\n",
      "88/88 [==============================] - 0s 949us/step - loss: 0.2481 - accuracy: 0.9018\n",
      "Epoch 59/1500\n",
      "88/88 [==============================] - 0s 931us/step - loss: 0.2583 - accuracy: 0.8925\n",
      "Epoch 60/1500\n",
      "88/88 [==============================] - 0s 959us/step - loss: 0.2597 - accuracy: 0.8971\n",
      "Epoch 61/1500\n",
      "88/88 [==============================] - 0s 923us/step - loss: 0.2510 - accuracy: 0.8989\n",
      "Epoch 62/1500\n",
      "88/88 [==============================] - 0s 915us/step - loss: 0.2533 - accuracy: 0.9018\n",
      "Epoch 63/1500\n",
      "88/88 [==============================] - 0s 940us/step - loss: 0.2449 - accuracy: 0.9043\n",
      "Epoch 64/1500\n",
      "88/88 [==============================] - 0s 923us/step - loss: 0.2569 - accuracy: 0.8964\n",
      "Epoch 65/1500\n",
      "88/88 [==============================] - 0s 960us/step - loss: 0.2458 - accuracy: 0.8986\n",
      "Epoch 66/1500\n",
      "88/88 [==============================] - 0s 938us/step - loss: 0.2236 - accuracy: 0.9068\n",
      "Epoch 67/1500\n",
      "88/88 [==============================] - 0s 940us/step - loss: 0.2425 - accuracy: 0.9064\n",
      "Epoch 68/1500\n",
      "88/88 [==============================] - 0s 935us/step - loss: 0.2507 - accuracy: 0.9007\n",
      "Epoch 69/1500\n",
      "88/88 [==============================] - 0s 932us/step - loss: 0.2385 - accuracy: 0.9079\n",
      "Epoch 70/1500\n",
      "88/88 [==============================] - 0s 923us/step - loss: 0.2276 - accuracy: 0.9182\n",
      "Epoch 71/1500\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.2297 - accuracy: 0.9046\n",
      "Epoch 72/1500\n",
      "88/88 [==============================] - 0s 921us/step - loss: 0.2396 - accuracy: 0.9061\n",
      "Epoch 73/1500\n",
      "88/88 [==============================] - 0s 946us/step - loss: 0.2299 - accuracy: 0.9121\n",
      "Epoch 74/1500\n",
      "88/88 [==============================] - 0s 947us/step - loss: 0.2399 - accuracy: 0.9089\n",
      "Epoch 75/1500\n",
      "88/88 [==============================] - 0s 940us/step - loss: 0.2159 - accuracy: 0.9193\n",
      "Epoch 76/1500\n",
      "88/88 [==============================] - 0s 916us/step - loss: 0.2196 - accuracy: 0.9207\n",
      "Epoch 77/1500\n",
      "88/88 [==============================] - 0s 933us/step - loss: 0.2226 - accuracy: 0.9121\n",
      "Epoch 78/1500\n",
      "88/88 [==============================] - 0s 936us/step - loss: 0.2272 - accuracy: 0.9129\n",
      "Epoch 79/1500\n",
      "88/88 [==============================] - 0s 995us/step - loss: 0.2187 - accuracy: 0.9168\n",
      "Epoch 80/1500\n",
      "88/88 [==============================] - 0s 965us/step - loss: 0.2137 - accuracy: 0.9161\n",
      "Epoch 81/1500\n",
      "88/88 [==============================] - 0s 948us/step - loss: 0.2317 - accuracy: 0.9132\n",
      "Epoch 82/1500\n",
      "88/88 [==============================] - 0s 960us/step - loss: 0.2182 - accuracy: 0.9150\n",
      "Epoch 83/1500\n",
      "88/88 [==============================] - 0s 945us/step - loss: 0.2312 - accuracy: 0.9025\n",
      "Epoch 84/1500\n",
      "88/88 [==============================] - 0s 975us/step - loss: 0.2168 - accuracy: 0.9164\n",
      "Epoch 85/1500\n",
      "88/88 [==============================] - 0s 993us/step - loss: 0.2063 - accuracy: 0.9168\n",
      "Epoch 86/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9139\n",
      "Epoch 87/1500\n",
      "88/88 [==============================] - 0s 981us/step - loss: 0.2279 - accuracy: 0.9089\n",
      "Epoch 88/1500\n",
      "88/88 [==============================] - 0s 960us/step - loss: 0.2038 - accuracy: 0.9264\n",
      "Epoch 89/1500\n",
      "88/88 [==============================] - 0s 971us/step - loss: 0.1957 - accuracy: 0.9207\n",
      "Epoch 90/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1947 - accuracy: 0.9289\n",
      "Epoch 91/1500\n",
      "88/88 [==============================] - 0s 979us/step - loss: 0.1949 - accuracy: 0.9264\n",
      "Epoch 92/1500\n",
      "88/88 [==============================] - 0s 984us/step - loss: 0.2031 - accuracy: 0.9161\n",
      "Epoch 93/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2023 - accuracy: 0.9207\n",
      "Epoch 94/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1906 - accuracy: 0.9311\n",
      "Epoch 95/1500\n",
      "88/88 [==============================] - 0s 955us/step - loss: 0.1972 - accuracy: 0.9243\n",
      "Epoch 96/1500\n",
      "88/88 [==============================] - 0s 984us/step - loss: 0.1969 - accuracy: 0.9229\n",
      "Epoch 97/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.9179\n",
      "Epoch 98/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9261\n",
      "Epoch 99/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9186\n",
      "Epoch 100/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9236\n",
      "Epoch 101/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9221\n",
      "Epoch 102/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9225\n",
      "Epoch 103/1500\n",
      "88/88 [==============================] - 0s 988us/step - loss: 0.1845 - accuracy: 0.9336\n",
      "Epoch 104/1500\n",
      "88/88 [==============================] - 0s 941us/step - loss: 0.1886 - accuracy: 0.9275\n",
      "Epoch 105/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9286\n",
      "Epoch 106/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.9329\n",
      "Epoch 107/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2001 - accuracy: 0.9254\n",
      "Epoch 108/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.2056 - accuracy: 0.9214\n",
      "Epoch 109/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.9354\n",
      "Epoch 110/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.9343\n",
      "Epoch 111/1500\n",
      "88/88 [==============================] - 0s 970us/step - loss: 0.1897 - accuracy: 0.9232\n",
      "Epoch 112/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1867 - accuracy: 0.9254\n",
      "Epoch 113/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1970 - accuracy: 0.9229\n",
      "Epoch 114/1500\n",
      "88/88 [==============================] - 0s 990us/step - loss: 0.1895 - accuracy: 0.9243\n",
      "Epoch 115/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9300\n",
      "Epoch 116/1500\n",
      "88/88 [==============================] - 0s 967us/step - loss: 0.1709 - accuracy: 0.9364\n",
      "Epoch 117/1500\n",
      "88/88 [==============================] - 0s 960us/step - loss: 0.1862 - accuracy: 0.9318\n",
      "Epoch 118/1500\n",
      "88/88 [==============================] - 0s 957us/step - loss: 0.1801 - accuracy: 0.9282\n",
      "Epoch 119/1500\n",
      "88/88 [==============================] - 0s 978us/step - loss: 0.1789 - accuracy: 0.9332\n",
      "Epoch 120/1500\n",
      "88/88 [==============================] - 0s 996us/step - loss: 0.1823 - accuracy: 0.9336\n",
      "Epoch 121/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1879 - accuracy: 0.9268\n",
      "Epoch 122/1500\n",
      "88/88 [==============================] - 0s 965us/step - loss: 0.1837 - accuracy: 0.9250\n",
      "Epoch 123/1500\n",
      "88/88 [==============================] - 0s 966us/step - loss: 0.1772 - accuracy: 0.9289\n",
      "Epoch 124/1500\n",
      "88/88 [==============================] - 0s 993us/step - loss: 0.1726 - accuracy: 0.9375\n",
      "Epoch 125/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.9354\n",
      "Epoch 126/1500\n",
      "88/88 [==============================] - 0s 954us/step - loss: 0.1765 - accuracy: 0.9296\n",
      "Epoch 127/1500\n",
      "88/88 [==============================] - 0s 992us/step - loss: 0.1894 - accuracy: 0.9250\n",
      "Epoch 128/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1683 - accuracy: 0.9371\n",
      "Epoch 129/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1608 - accuracy: 0.9436\n",
      "Epoch 130/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.9307\n",
      "Epoch 131/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9307\n",
      "Epoch 132/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.9379\n",
      "Epoch 133/1500\n",
      "88/88 [==============================] - 0s 957us/step - loss: 0.1893 - accuracy: 0.9279\n",
      "Epoch 134/1500\n",
      "88/88 [==============================] - 0s 987us/step - loss: 0.1525 - accuracy: 0.9446\n",
      "Epoch 135/1500\n",
      "88/88 [==============================] - 0s 980us/step - loss: 0.1573 - accuracy: 0.9432\n",
      "Epoch 136/1500\n",
      "88/88 [==============================] - 0s 981us/step - loss: 0.1628 - accuracy: 0.9361\n",
      "Epoch 137/1500\n",
      "88/88 [==============================] - 0s 987us/step - loss: 0.1751 - accuracy: 0.9275\n",
      "Epoch 138/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.9368\n",
      "Epoch 139/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9343\n",
      "Epoch 140/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.9286\n",
      "Epoch 141/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9364\n",
      "Epoch 142/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9314\n",
      "Epoch 143/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1584 - accuracy: 0.9432\n",
      "Epoch 144/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9393\n",
      "Epoch 145/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.9357\n",
      "Epoch 146/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9361\n",
      "Epoch 147/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9368\n",
      "Epoch 148/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9414\n",
      "Epoch 149/1500\n",
      "88/88 [==============================] - 0s 1000us/step - loss: 0.1668 - accuracy: 0.9350\n",
      "Epoch 150/1500\n",
      "88/88 [==============================] - 0s 951us/step - loss: 0.1532 - accuracy: 0.9450\n",
      "Epoch 151/1500\n",
      "88/88 [==============================] - 0s 934us/step - loss: 0.1650 - accuracy: 0.9343\n",
      "Epoch 152/1500\n",
      "88/88 [==============================] - 0s 946us/step - loss: 0.1611 - accuracy: 0.9414\n",
      "Epoch 153/1500\n",
      "88/88 [==============================] - 0s 972us/step - loss: 0.1742 - accuracy: 0.9304\n",
      "Epoch 154/1500\n",
      "88/88 [==============================] - 0s 954us/step - loss: 0.1478 - accuracy: 0.9454\n",
      "Epoch 155/1500\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.1561 - accuracy: 0.9443\n",
      "Epoch 156/1500\n",
      "88/88 [==============================] - 0s 947us/step - loss: 0.1556 - accuracy: 0.9407\n",
      "Epoch 157/1500\n",
      "88/88 [==============================] - 0s 917us/step - loss: 0.1584 - accuracy: 0.9421\n",
      "Epoch 158/1500\n",
      "88/88 [==============================] - 0s 946us/step - loss: 0.1421 - accuracy: 0.9461\n",
      "Epoch 159/1500\n",
      "88/88 [==============================] - 0s 938us/step - loss: 0.1591 - accuracy: 0.9379\n",
      "Epoch 160/1500\n",
      "88/88 [==============================] - 0s 927us/step - loss: 0.1669 - accuracy: 0.9382\n",
      "Epoch 161/1500\n",
      "88/88 [==============================] - 0s 942us/step - loss: 0.1523 - accuracy: 0.9411\n",
      "Epoch 162/1500\n",
      "88/88 [==============================] - 0s 936us/step - loss: 0.1597 - accuracy: 0.9371\n",
      "Epoch 163/1500\n",
      "88/88 [==============================] - 0s 904us/step - loss: 0.1485 - accuracy: 0.9425\n",
      "Epoch 164/1500\n",
      "88/88 [==============================] - 0s 948us/step - loss: 0.1510 - accuracy: 0.9454\n",
      "Epoch 165/1500\n",
      "88/88 [==============================] - 0s 964us/step - loss: 0.1562 - accuracy: 0.9418\n",
      "Epoch 166/1500\n",
      "88/88 [==============================] - 0s 918us/step - loss: 0.1649 - accuracy: 0.9404\n",
      "Epoch 167/1500\n",
      "88/88 [==============================] - 0s 993us/step - loss: 0.1489 - accuracy: 0.9429\n",
      "Epoch 168/1500\n",
      "88/88 [==============================] - 0s 941us/step - loss: 0.1513 - accuracy: 0.9436\n",
      "Epoch 169/1500\n",
      "88/88 [==============================] - 0s 966us/step - loss: 0.1536 - accuracy: 0.9418\n",
      "Epoch 170/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9464\n",
      "Epoch 171/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9543\n",
      "Epoch 172/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9536\n",
      "Epoch 173/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1429 - accuracy: 0.9439\n",
      "Epoch 174/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9404\n",
      "Epoch 175/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1494 - accuracy: 0.9432\n",
      "Epoch 176/1500\n",
      "88/88 [==============================] - 0s 984us/step - loss: 0.1606 - accuracy: 0.9404\n",
      "Epoch 177/1500\n",
      "88/88 [==============================] - 0s 970us/step - loss: 0.1417 - accuracy: 0.9479\n",
      "Epoch 178/1500\n",
      "88/88 [==============================] - 0s 985us/step - loss: 0.1360 - accuracy: 0.9471\n",
      "Epoch 179/1500\n",
      "88/88 [==============================] - 0s 990us/step - loss: 0.1443 - accuracy: 0.9457\n",
      "Epoch 180/1500\n",
      "88/88 [==============================] - 0s 995us/step - loss: 0.1346 - accuracy: 0.9500\n",
      "Epoch 181/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9511\n",
      "Epoch 182/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9493\n",
      "Epoch 183/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9518\n",
      "Epoch 184/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9436\n",
      "Epoch 185/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9439\n",
      "Epoch 186/1500\n",
      "88/88 [==============================] - 0s 998us/step - loss: 0.1380 - accuracy: 0.9468\n",
      "Epoch 187/1500\n",
      "88/88 [==============================] - 0s 986us/step - loss: 0.1490 - accuracy: 0.9414\n",
      "Epoch 188/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9504\n",
      "Epoch 189/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.9450\n",
      "Epoch 190/1500\n",
      "88/88 [==============================] - 0s 993us/step - loss: 0.1325 - accuracy: 0.9539\n",
      "Epoch 191/1500\n",
      "88/88 [==============================] - 0s 979us/step - loss: 0.1457 - accuracy: 0.9493\n",
      "Epoch 192/1500\n",
      "88/88 [==============================] - 0s 934us/step - loss: 0.1506 - accuracy: 0.9411\n",
      "Epoch 193/1500\n",
      "88/88 [==============================] - 0s 981us/step - loss: 0.1396 - accuracy: 0.9500\n",
      "Epoch 194/1500\n",
      "88/88 [==============================] - 0s 978us/step - loss: 0.1265 - accuracy: 0.9543\n",
      "Epoch 195/1500\n",
      "88/88 [==============================] - 0s 982us/step - loss: 0.1392 - accuracy: 0.9504\n",
      "Epoch 196/1500\n",
      "88/88 [==============================] - 0s 948us/step - loss: 0.1499 - accuracy: 0.9432\n",
      "Epoch 197/1500\n",
      "88/88 [==============================] - 0s 933us/step - loss: 0.1298 - accuracy: 0.9486\n",
      "Epoch 198/1500\n",
      "88/88 [==============================] - 0s 943us/step - loss: 0.1333 - accuracy: 0.9511\n",
      "Epoch 199/1500\n",
      "88/88 [==============================] - 0s 968us/step - loss: 0.1393 - accuracy: 0.9504\n",
      "Epoch 200/1500\n",
      "88/88 [==============================] - 0s 966us/step - loss: 0.1215 - accuracy: 0.9582\n",
      "Epoch 201/1500\n",
      "88/88 [==============================] - 0s 941us/step - loss: 0.1474 - accuracy: 0.9486\n",
      "Epoch 202/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9557\n",
      "Epoch 203/1500\n",
      "88/88 [==============================] - 0s 983us/step - loss: 0.1421 - accuracy: 0.9518\n",
      "Epoch 204/1500\n",
      "88/88 [==============================] - 0s 960us/step - loss: 0.1151 - accuracy: 0.9625\n",
      "Epoch 205/1500\n",
      "88/88 [==============================] - 0s 952us/step - loss: 0.1414 - accuracy: 0.9479\n",
      "Epoch 206/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9532\n",
      "Epoch 207/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.9496\n",
      "Epoch 208/1500\n",
      "88/88 [==============================] - 0s 967us/step - loss: 0.1396 - accuracy: 0.9486\n",
      "Epoch 209/1500\n",
      "88/88 [==============================] - 0s 989us/step - loss: 0.1440 - accuracy: 0.9454\n",
      "Epoch 210/1500\n",
      "88/88 [==============================] - 0s 997us/step - loss: 0.1424 - accuracy: 0.9443\n",
      "Epoch 211/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9539\n",
      "Epoch 212/1500\n",
      "88/88 [==============================] - 0s 989us/step - loss: 0.1385 - accuracy: 0.9507\n",
      "Epoch 213/1500\n",
      "88/88 [==============================] - 0s 959us/step - loss: 0.1304 - accuracy: 0.9507\n",
      "Epoch 214/1500\n",
      "88/88 [==============================] - 0s 961us/step - loss: 0.1306 - accuracy: 0.9546\n",
      "Epoch 215/1500\n",
      "88/88 [==============================] - 0s 938us/step - loss: 0.1419 - accuracy: 0.9429\n",
      "Epoch 216/1500\n",
      "88/88 [==============================] - 0s 986us/step - loss: 0.1385 - accuracy: 0.9443\n",
      "Epoch 217/1500\n",
      "88/88 [==============================] - 0s 946us/step - loss: 0.1315 - accuracy: 0.9521\n",
      "Epoch 218/1500\n",
      "88/88 [==============================] - 0s 970us/step - loss: 0.1406 - accuracy: 0.9486\n",
      "Epoch 219/1500\n",
      "88/88 [==============================] - 0s 961us/step - loss: 0.1284 - accuracy: 0.9543\n",
      "Epoch 220/1500\n",
      "88/88 [==============================] - 0s 976us/step - loss: 0.1235 - accuracy: 0.9525\n",
      "Epoch 221/1500\n",
      "88/88 [==============================] - 0s 930us/step - loss: 0.1319 - accuracy: 0.9475\n",
      "Epoch 222/1500\n",
      "88/88 [==============================] - 0s 965us/step - loss: 0.1146 - accuracy: 0.9625\n",
      "Epoch 223/1500\n",
      "88/88 [==============================] - 0s 937us/step - loss: 0.1173 - accuracy: 0.9554\n",
      "Epoch 224/1500\n",
      "88/88 [==============================] - 0s 950us/step - loss: 0.1097 - accuracy: 0.9600\n",
      "Epoch 225/1500\n",
      "88/88 [==============================] - 0s 939us/step - loss: 0.1283 - accuracy: 0.9564\n",
      "Epoch 226/1500\n",
      "88/88 [==============================] - 0s 974us/step - loss: 0.1105 - accuracy: 0.9646\n",
      "Epoch 227/1500\n",
      "88/88 [==============================] - 0s 950us/step - loss: 0.1275 - accuracy: 0.9493\n",
      "Epoch 228/1500\n",
      "88/88 [==============================] - 0s 936us/step - loss: 0.1117 - accuracy: 0.9614\n",
      "Epoch 229/1500\n",
      "88/88 [==============================] - 0s 947us/step - loss: 0.1333 - accuracy: 0.9461\n",
      "Epoch 230/1500\n",
      "88/88 [==============================] - 0s 936us/step - loss: 0.1322 - accuracy: 0.9500\n",
      "Epoch 231/1500\n",
      "88/88 [==============================] - 0s 934us/step - loss: 0.1269 - accuracy: 0.9504\n",
      "Epoch 232/1500\n",
      "88/88 [==============================] - 0s 966us/step - loss: 0.1103 - accuracy: 0.9582\n",
      "Epoch 233/1500\n",
      "88/88 [==============================] - 0s 920us/step - loss: 0.1210 - accuracy: 0.9543\n",
      "Epoch 234/1500\n",
      "88/88 [==============================] - 0s 940us/step - loss: 0.1349 - accuracy: 0.9550\n",
      "Epoch 235/1500\n",
      "88/88 [==============================] - 0s 955us/step - loss: 0.1329 - accuracy: 0.9532\n",
      "Epoch 236/1500\n",
      "88/88 [==============================] - 0s 987us/step - loss: 0.1164 - accuracy: 0.9557\n",
      "Epoch 237/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9646\n",
      "Epoch 238/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9504\n",
      "Epoch 239/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9539\n",
      "Epoch 240/1500\n",
      "88/88 [==============================] - 0s 972us/step - loss: 0.1165 - accuracy: 0.9571\n",
      "Epoch 241/1500\n",
      "88/88 [==============================] - 0s 979us/step - loss: 0.1183 - accuracy: 0.9611\n",
      "Epoch 242/1500\n",
      "88/88 [==============================] - 0s 950us/step - loss: 0.1328 - accuracy: 0.9532\n",
      "Epoch 243/1500\n",
      "88/88 [==============================] - 0s 995us/step - loss: 0.1082 - accuracy: 0.9625\n",
      "Epoch 244/1500\n",
      "88/88 [==============================] - 0s 996us/step - loss: 0.1368 - accuracy: 0.9446\n",
      "Epoch 245/1500\n",
      "88/88 [==============================] - 0s 955us/step - loss: 0.1212 - accuracy: 0.9536\n",
      "Epoch 246/1500\n",
      "88/88 [==============================] - 0s 984us/step - loss: 0.1186 - accuracy: 0.9568\n",
      "Epoch 247/1500\n",
      "88/88 [==============================] - 0s 939us/step - loss: 0.1239 - accuracy: 0.9550\n",
      "Epoch 248/1500\n",
      "88/88 [==============================] - 0s 953us/step - loss: 0.1424 - accuracy: 0.9446\n",
      "Epoch 249/1500\n",
      "88/88 [==============================] - 0s 978us/step - loss: 0.1115 - accuracy: 0.9564\n",
      "Epoch 250/1500\n",
      "88/88 [==============================] - 0s 966us/step - loss: 0.1142 - accuracy: 0.9643\n",
      "Epoch 251/1500\n",
      "88/88 [==============================] - 0s 963us/step - loss: 0.1130 - accuracy: 0.9561\n",
      "Epoch 252/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9600\n",
      "Epoch 253/1500\n",
      "88/88 [==============================] - 0s 953us/step - loss: 0.1160 - accuracy: 0.9607\n",
      "Epoch 254/1500\n",
      "88/88 [==============================] - 0s 945us/step - loss: 0.1107 - accuracy: 0.9625\n",
      "Epoch 255/1500\n",
      "88/88 [==============================] - 0s 973us/step - loss: 0.1045 - accuracy: 0.9621\n",
      "Epoch 256/1500\n",
      "88/88 [==============================] - 0s 979us/step - loss: 0.1147 - accuracy: 0.9575\n",
      "Epoch 257/1500\n",
      "88/88 [==============================] - 0s 970us/step - loss: 0.1123 - accuracy: 0.9575\n",
      "Epoch 258/1500\n",
      "88/88 [==============================] - 0s 998us/step - loss: 0.1181 - accuracy: 0.9550\n",
      "Epoch 259/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9507\n",
      "Epoch 260/1500\n",
      "88/88 [==============================] - 0s 950us/step - loss: 0.1451 - accuracy: 0.9468\n",
      "Epoch 261/1500\n",
      "88/88 [==============================] - 0s 959us/step - loss: 0.1014 - accuracy: 0.9611\n",
      "Epoch 262/1500\n",
      "88/88 [==============================] - 0s 976us/step - loss: 0.1017 - accuracy: 0.9636\n",
      "Epoch 263/1500\n",
      "88/88 [==============================] - 0s 951us/step - loss: 0.1040 - accuracy: 0.9604\n",
      "Epoch 264/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9639\n",
      "Epoch 265/1500\n",
      "88/88 [==============================] - 0s 967us/step - loss: 0.1192 - accuracy: 0.9561\n",
      "Epoch 266/1500\n",
      "88/88 [==============================] - 0s 990us/step - loss: 0.1278 - accuracy: 0.9525\n",
      "Epoch 267/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9507\n",
      "Epoch 268/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9539\n",
      "Epoch 269/1500\n",
      "88/88 [==============================] - 0s 952us/step - loss: 0.1129 - accuracy: 0.9600\n",
      "Epoch 270/1500\n",
      "88/88 [==============================] - 0s 949us/step - loss: 0.0986 - accuracy: 0.9654\n",
      "Epoch 271/1500\n",
      "88/88 [==============================] - 0s 946us/step - loss: 0.1050 - accuracy: 0.9629\n",
      "Epoch 272/1500\n",
      "88/88 [==============================] - 0s 940us/step - loss: 0.0996 - accuracy: 0.9636\n",
      "Epoch 273/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9643\n",
      "Epoch 274/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9629\n",
      "Epoch 275/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9518\n",
      "Epoch 276/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9600\n",
      "Epoch 277/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9521\n",
      "Epoch 278/1500\n",
      "88/88 [==============================] - 0s 965us/step - loss: 0.1100 - accuracy: 0.9614\n",
      "Epoch 279/1500\n",
      "88/88 [==============================] - 0s 960us/step - loss: 0.1001 - accuracy: 0.9621\n",
      "Epoch 280/1500\n",
      "88/88 [==============================] - 0s 971us/step - loss: 0.1066 - accuracy: 0.9607\n",
      "Epoch 281/1500\n",
      "88/88 [==============================] - 0s 965us/step - loss: 0.1162 - accuracy: 0.9561\n",
      "Epoch 282/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9629\n",
      "Epoch 283/1500\n",
      "88/88 [==============================] - 0s 992us/step - loss: 0.1104 - accuracy: 0.9600\n",
      "Epoch 284/1500\n",
      "88/88 [==============================] - 0s 988us/step - loss: 0.1064 - accuracy: 0.9579\n",
      "Epoch 285/1500\n",
      "88/88 [==============================] - 0s 997us/step - loss: 0.1009 - accuracy: 0.9643\n",
      "Epoch 286/1500\n",
      "88/88 [==============================] - 0s 991us/step - loss: 0.1085 - accuracy: 0.9636\n",
      "Epoch 287/1500\n",
      "88/88 [==============================] - 0s 971us/step - loss: 0.1027 - accuracy: 0.9657\n",
      "Epoch 288/1500\n",
      "88/88 [==============================] - 0s 971us/step - loss: 0.1057 - accuracy: 0.9604\n",
      "Epoch 289/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9632\n",
      "Epoch 290/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9564\n",
      "Epoch 291/1500\n",
      "88/88 [==============================] - 0s 989us/step - loss: 0.1251 - accuracy: 0.9539\n",
      "Epoch 292/1500\n",
      "88/88 [==============================] - 0s 965us/step - loss: 0.1139 - accuracy: 0.9618\n",
      "Epoch 293/1500\n",
      "88/88 [==============================] - 0s 961us/step - loss: 0.1020 - accuracy: 0.9629\n",
      "Epoch 294/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9629\n",
      "Epoch 295/1500\n",
      "88/88 [==============================] - 0s 984us/step - loss: 0.1062 - accuracy: 0.9621\n",
      "Epoch 296/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.9675\n",
      "Epoch 297/1500\n",
      "88/88 [==============================] - 0s 992us/step - loss: 0.0980 - accuracy: 0.9639\n",
      "Epoch 298/1500\n",
      "88/88 [==============================] - 0s 949us/step - loss: 0.1093 - accuracy: 0.9611\n",
      "Epoch 299/1500\n",
      "88/88 [==============================] - 0s 977us/step - loss: 0.1214 - accuracy: 0.9561\n",
      "Epoch 300/1500\n",
      "88/88 [==============================] - 0s 992us/step - loss: 0.1082 - accuracy: 0.9654\n",
      "Epoch 301/1500\n",
      "88/88 [==============================] - 0s 994us/step - loss: 0.1008 - accuracy: 0.9611\n",
      "Epoch 302/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9629\n",
      "Epoch 303/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9589\n",
      "Epoch 304/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9604\n",
      "Epoch 305/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9607\n",
      "Epoch 306/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9625\n",
      "Epoch 307/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9550\n",
      "Epoch 308/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9636\n",
      "Epoch 309/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9632\n",
      "Epoch 310/1500\n",
      "88/88 [==============================] - 0s 978us/step - loss: 0.1021 - accuracy: 0.9639\n",
      "Epoch 311/1500\n",
      "88/88 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 0.9639\n",
      "Epoch 312/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9600\n",
      "Epoch 313/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9554\n",
      "Epoch 314/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9618\n",
      "Epoch 315/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9621\n",
      "Epoch 316/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9511\n",
      "Epoch 317/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9661\n",
      "Epoch 318/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9611\n",
      "Epoch 319/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9618\n",
      "Epoch 320/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9625\n",
      "Epoch 321/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9618\n",
      "Epoch 322/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9618\n",
      "Epoch 323/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9675\n",
      "Epoch 324/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9639\n",
      "Epoch 325/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9589\n",
      "Epoch 326/1500\n",
      "88/88 [==============================] - 0s 993us/step - loss: 0.1060 - accuracy: 0.9600\n",
      "Epoch 327/1500\n",
      "88/88 [==============================] - 0s 945us/step - loss: 0.0986 - accuracy: 0.9700\n",
      "Epoch 328/1500\n",
      "88/88 [==============================] - 0s 981us/step - loss: 0.1043 - accuracy: 0.9621\n",
      "Epoch 329/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9650\n",
      "Epoch 330/1500\n",
      "88/88 [==============================] - 0s 982us/step - loss: 0.0883 - accuracy: 0.9675\n",
      "Epoch 331/1500\n",
      "88/88 [==============================] - 0s 967us/step - loss: 0.1086 - accuracy: 0.9632\n",
      "Epoch 332/1500\n",
      "88/88 [==============================] - 0s 973us/step - loss: 0.1044 - accuracy: 0.9604\n",
      "Epoch 333/1500\n",
      "88/88 [==============================] - 0s 993us/step - loss: 0.1026 - accuracy: 0.9671\n",
      "Epoch 334/1500\n",
      "88/88 [==============================] - 0s 988us/step - loss: 0.0962 - accuracy: 0.9657\n",
      "Epoch 335/1500\n",
      "88/88 [==============================] - 0s 991us/step - loss: 0.1139 - accuracy: 0.9586\n",
      "Epoch 336/1500\n",
      "88/88 [==============================] - 0s 972us/step - loss: 0.0876 - accuracy: 0.9729\n",
      "Epoch 337/1500\n",
      "88/88 [==============================] - 0s 987us/step - loss: 0.1045 - accuracy: 0.9611\n",
      "Epoch 338/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9682\n",
      "Epoch 339/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9639\n",
      "Epoch 340/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9650\n",
      "Epoch 341/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9696\n",
      "Epoch 342/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9671\n",
      "Epoch 343/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9639\n",
      "Epoch 344/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.9514\n",
      "Epoch 345/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9654\n",
      "Epoch 346/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9664\n",
      "Epoch 347/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9664\n",
      "Epoch 348/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9664\n",
      "Epoch 349/1500\n",
      "88/88 [==============================] - 0s 988us/step - loss: 0.1000 - accuracy: 0.9639\n",
      "Epoch 350/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9586\n",
      "Epoch 351/1500\n",
      "88/88 [==============================] - 0s 966us/step - loss: 0.1110 - accuracy: 0.9582\n",
      "Epoch 352/1500\n",
      "88/88 [==============================] - 0s 965us/step - loss: 0.1200 - accuracy: 0.9575\n",
      "Epoch 353/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9614\n",
      "Epoch 354/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9675\n",
      "Epoch 355/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.9621\n",
      "Epoch 356/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9625\n",
      "Epoch 357/1500\n",
      "88/88 [==============================] - 0s 994us/step - loss: 0.1015 - accuracy: 0.9639\n",
      "Epoch 358/1500\n",
      "88/88 [==============================] - 0s 981us/step - loss: 0.0978 - accuracy: 0.9629\n",
      "Epoch 359/1500\n",
      "88/88 [==============================] - 0s 967us/step - loss: 0.0849 - accuracy: 0.9707\n",
      "Epoch 360/1500\n",
      "88/88 [==============================] - 0s 977us/step - loss: 0.0916 - accuracy: 0.9661\n",
      "Epoch 361/1500\n",
      "88/88 [==============================] - 0s 950us/step - loss: 0.0965 - accuracy: 0.9625\n",
      "Epoch 362/1500\n",
      "88/88 [==============================] - 0s 965us/step - loss: 0.0917 - accuracy: 0.9650\n",
      "Epoch 363/1500\n",
      "88/88 [==============================] - 0s 951us/step - loss: 0.0907 - accuracy: 0.9718\n",
      "Epoch 364/1500\n",
      "88/88 [==============================] - 0s 948us/step - loss: 0.1056 - accuracy: 0.9643\n",
      "Epoch 365/1500\n",
      "88/88 [==============================] - 0s 982us/step - loss: 0.0956 - accuracy: 0.9675\n",
      "Epoch 366/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9682\n",
      "Epoch 367/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9618\n",
      "Epoch 368/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9682\n",
      "Epoch 369/1500\n",
      "88/88 [==============================] - 0s 969us/step - loss: 0.0995 - accuracy: 0.9636\n",
      "Epoch 370/1500\n",
      "88/88 [==============================] - 0s 960us/step - loss: 0.0988 - accuracy: 0.9654\n",
      "Epoch 371/1500\n",
      "88/88 [==============================] - 0s 964us/step - loss: 0.0879 - accuracy: 0.9657\n",
      "Epoch 372/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9632\n",
      "Epoch 373/1500\n",
      "88/88 [==============================] - 0s 991us/step - loss: 0.0769 - accuracy: 0.9721\n",
      "Epoch 374/1500\n",
      "88/88 [==============================] - 0s 994us/step - loss: 0.0920 - accuracy: 0.9679\n",
      "Epoch 375/1500\n",
      "88/88 [==============================] - 0s 976us/step - loss: 0.0796 - accuracy: 0.9743\n",
      "Epoch 376/1500\n",
      "88/88 [==============================] - 0s 969us/step - loss: 0.0920 - accuracy: 0.9661\n",
      "Epoch 377/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9682\n",
      "Epoch 378/1500\n",
      "88/88 [==============================] - 0s 999us/step - loss: 0.0873 - accuracy: 0.9668\n",
      "Epoch 379/1500\n",
      "88/88 [==============================] - 0s 992us/step - loss: 0.0906 - accuracy: 0.9675\n",
      "Epoch 380/1500\n",
      "88/88 [==============================] - 0s 974us/step - loss: 0.0893 - accuracy: 0.9675\n",
      "Epoch 381/1500\n",
      "88/88 [==============================] - 0s 957us/step - loss: 0.0868 - accuracy: 0.9711\n",
      "Epoch 382/1500\n",
      "88/88 [==============================] - 0s 986us/step - loss: 0.0993 - accuracy: 0.9636\n",
      "Epoch 383/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9621\n",
      "Epoch 384/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9657\n",
      "Epoch 385/1500\n",
      "88/88 [==============================] - 0s 968us/step - loss: 0.0989 - accuracy: 0.9618\n",
      "Epoch 386/1500\n",
      "88/88 [==============================] - 0s 995us/step - loss: 0.0952 - accuracy: 0.9614\n",
      "Epoch 387/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9657\n",
      "Epoch 388/1500\n",
      "88/88 [==============================] - 0s 981us/step - loss: 0.0979 - accuracy: 0.9657\n",
      "Epoch 389/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9646\n",
      "Epoch 390/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9696\n",
      "Epoch 391/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9679\n",
      "Epoch 392/1500\n",
      "88/88 [==============================] - 0s 997us/step - loss: 0.1085 - accuracy: 0.9600\n",
      "Epoch 393/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.9675\n",
      "Epoch 394/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9625\n",
      "Epoch 395/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9629\n",
      "Epoch 396/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9721\n",
      "Epoch 397/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9718\n",
      "Epoch 398/1500\n",
      "88/88 [==============================] - 0s 979us/step - loss: 0.0904 - accuracy: 0.9693\n",
      "Epoch 399/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9646\n",
      "Epoch 400/1500\n",
      "88/88 [==============================] - 0s 998us/step - loss: 0.1028 - accuracy: 0.9607\n",
      "Epoch 401/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9757\n",
      "Epoch 402/1500\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9682\n",
      "Epoch 403/1500\n",
      "49/88 [===============>..............] - ETA: 0s - loss: 0.0852 - accuracy: 0.9751Restoring model weights from the end of the best epoch: 373.\n",
      "88/88 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9671\n",
      "Epoch 403: early stopping\n",
      "8/8 [==============================] - 0s 753us/step - loss: 0.8821 - accuracy: 0.6949\n",
      "8/8 [==============================] - 0s 660us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.66 (19/29)\n",
      "Before appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "After appending - Cat IDs: 389, Predictions: 389, Actuals: 389, Gender: 389\n",
      "Final Test Results - Loss: 0.8821309208869934, Accuracy: 0.694915235042572, Precision: 0.6735294117647058, Recall: 0.591426882406079, F1 Score: 0.6185601855404633\n",
      "Confusion Matrix:\n",
      " [[121   8  15]\n",
      " [ 27  24   0]\n",
      " [ 22   0  19]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "001A    14\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "071A    10\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "007A     6\n",
      "109A     6\n",
      "108A     6\n",
      "037A     6\n",
      "008A     6\n",
      "044A     5\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "034A     5\n",
      "023B     5\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "060A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "054A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "018A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "091A     1\n",
      "110A     1\n",
      "115A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "057A    27\n",
      "074A    25\n",
      "029A    17\n",
      "097A    16\n",
      "106A    14\n",
      "028A    13\n",
      "025A    11\n",
      "005A    10\n",
      "015A     9\n",
      "045A     9\n",
      "072A     9\n",
      "094A     8\n",
      "023A     6\n",
      "053A     6\n",
      "021A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "003A     4\n",
      "058A     3\n",
      "056A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "038A     2\n",
      "073A     1\n",
      "076A     1\n",
      "026C     1\n",
      "100A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    266\n",
      "M    237\n",
      "F    211\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    100\n",
      "X     82\n",
      "F     41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 103A, 071A, 097B, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [055A, 059A, 113A, 116A, 051B, 054A, 117A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 028A, 074A, 062A, 029A, 005A, 072A, 009...\n",
      "kitten                                               [045A]\n",
      "senior     [093A, 097A, 057A, 106A, 104A, 056A, 058A, 094A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 15, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 1, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "Length of X_train_val:\n",
      "714\n",
      "Length of y_train_val:\n",
      "714\n",
      "Length of groups_train_val:\n",
      "714\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({1: 1098, 0: 1085, 2: 673})\n",
      "Epoch 1/1500\n",
      "90/90 [==============================] - 0s 977us/step - loss: 0.8389 - accuracy: 0.6513\n",
      "Epoch 2/1500\n",
      "90/90 [==============================] - 0s 954us/step - loss: 0.6395 - accuracy: 0.7377\n",
      "Epoch 3/1500\n",
      "90/90 [==============================] - 0s 959us/step - loss: 0.5885 - accuracy: 0.7556\n",
      "Epoch 4/1500\n",
      "90/90 [==============================] - 0s 955us/step - loss: 0.5393 - accuracy: 0.7833\n",
      "Epoch 5/1500\n",
      "90/90 [==============================] - 0s 947us/step - loss: 0.4983 - accuracy: 0.8071\n",
      "Epoch 6/1500\n",
      "90/90 [==============================] - 0s 928us/step - loss: 0.4700 - accuracy: 0.8085\n",
      "Epoch 7/1500\n",
      "90/90 [==============================] - 0s 938us/step - loss: 0.4591 - accuracy: 0.8109\n",
      "Epoch 8/1500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 0.4247 - accuracy: 0.8302\n",
      "Epoch 9/1500\n",
      "90/90 [==============================] - 0s 991us/step - loss: 0.4514 - accuracy: 0.8176\n",
      "Epoch 10/1500\n",
      "90/90 [==============================] - 0s 926us/step - loss: 0.4307 - accuracy: 0.8239\n",
      "Epoch 11/1500\n",
      "90/90 [==============================] - 0s 941us/step - loss: 0.4056 - accuracy: 0.8386\n",
      "Epoch 12/1500\n",
      "90/90 [==============================] - 0s 924us/step - loss: 0.3918 - accuracy: 0.8456\n",
      "Epoch 13/1500\n",
      "90/90 [==============================] - 0s 921us/step - loss: 0.3784 - accuracy: 0.8519\n",
      "Epoch 14/1500\n",
      "90/90 [==============================] - 0s 979us/step - loss: 0.3712 - accuracy: 0.8473\n",
      "Epoch 15/1500\n",
      "90/90 [==============================] - 0s 980us/step - loss: 0.3810 - accuracy: 0.8470\n",
      "Epoch 16/1500\n",
      "90/90 [==============================] - 0s 989us/step - loss: 0.3860 - accuracy: 0.8424\n",
      "Epoch 17/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.3513 - accuracy: 0.8519\n",
      "Epoch 18/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.3557 - accuracy: 0.8599\n",
      "Epoch 19/1500\n",
      "90/90 [==============================] - 0s 936us/step - loss: 0.3471 - accuracy: 0.8627\n",
      "Epoch 20/1500\n",
      "90/90 [==============================] - 0s 948us/step - loss: 0.3636 - accuracy: 0.8540\n",
      "Epoch 21/1500\n",
      "90/90 [==============================] - 0s 953us/step - loss: 0.3357 - accuracy: 0.8613\n",
      "Epoch 22/1500\n",
      "90/90 [==============================] - 0s 939us/step - loss: 0.3343 - accuracy: 0.8666\n",
      "Epoch 23/1500\n",
      "90/90 [==============================] - 0s 926us/step - loss: 0.3317 - accuracy: 0.8701\n",
      "Epoch 24/1500\n",
      "90/90 [==============================] - 0s 926us/step - loss: 0.3195 - accuracy: 0.8746\n",
      "Epoch 25/1500\n",
      "90/90 [==============================] - 0s 926us/step - loss: 0.3265 - accuracy: 0.8718\n",
      "Epoch 26/1500\n",
      "90/90 [==============================] - 0s 927us/step - loss: 0.3077 - accuracy: 0.8754\n",
      "Epoch 27/1500\n",
      "90/90 [==============================] - 0s 943us/step - loss: 0.3013 - accuracy: 0.8817\n",
      "Epoch 28/1500\n",
      "90/90 [==============================] - 0s 937us/step - loss: 0.3108 - accuracy: 0.8764\n",
      "Epoch 29/1500\n",
      "90/90 [==============================] - 0s 924us/step - loss: 0.3055 - accuracy: 0.8806\n",
      "Epoch 30/1500\n",
      "90/90 [==============================] - 0s 930us/step - loss: 0.3159 - accuracy: 0.8739\n",
      "Epoch 31/1500\n",
      "90/90 [==============================] - 0s 921us/step - loss: 0.2794 - accuracy: 0.8803\n",
      "Epoch 32/1500\n",
      "90/90 [==============================] - 0s 915us/step - loss: 0.2944 - accuracy: 0.8855\n",
      "Epoch 33/1500\n",
      "90/90 [==============================] - 0s 907us/step - loss: 0.3043 - accuracy: 0.8778\n",
      "Epoch 34/1500\n",
      "90/90 [==============================] - 0s 949us/step - loss: 0.2866 - accuracy: 0.8936\n",
      "Epoch 35/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.2953 - accuracy: 0.8792\n",
      "Epoch 36/1500\n",
      "90/90 [==============================] - 0s 994us/step - loss: 0.2873 - accuracy: 0.8817\n",
      "Epoch 37/1500\n",
      "90/90 [==============================] - 0s 958us/step - loss: 0.2824 - accuracy: 0.8838\n",
      "Epoch 38/1500\n",
      "90/90 [==============================] - 0s 985us/step - loss: 0.2694 - accuracy: 0.8932\n",
      "Epoch 39/1500\n",
      "90/90 [==============================] - 0s 952us/step - loss: 0.2835 - accuracy: 0.8918\n",
      "Epoch 40/1500\n",
      "90/90 [==============================] - 0s 960us/step - loss: 0.2618 - accuracy: 0.9030\n",
      "Epoch 41/1500\n",
      "90/90 [==============================] - 0s 943us/step - loss: 0.2595 - accuracy: 0.8988\n",
      "Epoch 42/1500\n",
      "90/90 [==============================] - 0s 989us/step - loss: 0.2663 - accuracy: 0.8988\n",
      "Epoch 43/1500\n",
      "90/90 [==============================] - 0s 977us/step - loss: 0.2562 - accuracy: 0.9020\n",
      "Epoch 44/1500\n",
      "90/90 [==============================] - 0s 952us/step - loss: 0.2676 - accuracy: 0.8936\n",
      "Epoch 45/1500\n",
      "90/90 [==============================] - 0s 934us/step - loss: 0.2512 - accuracy: 0.8995\n",
      "Epoch 46/1500\n",
      "90/90 [==============================] - 0s 942us/step - loss: 0.2560 - accuracy: 0.8967\n",
      "Epoch 47/1500\n",
      "90/90 [==============================] - 0s 918us/step - loss: 0.2678 - accuracy: 0.8932\n",
      "Epoch 48/1500\n",
      "90/90 [==============================] - 0s 921us/step - loss: 0.2534 - accuracy: 0.8929\n",
      "Epoch 49/1500\n",
      "90/90 [==============================] - 0s 981us/step - loss: 0.2429 - accuracy: 0.9104\n",
      "Epoch 50/1500\n",
      "90/90 [==============================] - 0s 953us/step - loss: 0.2367 - accuracy: 0.9086\n",
      "Epoch 51/1500\n",
      "90/90 [==============================] - 0s 976us/step - loss: 0.2406 - accuracy: 0.9044\n",
      "Epoch 52/1500\n",
      "90/90 [==============================] - 0s 947us/step - loss: 0.2446 - accuracy: 0.9013\n",
      "Epoch 53/1500\n",
      "90/90 [==============================] - 0s 949us/step - loss: 0.2118 - accuracy: 0.9142\n",
      "Epoch 54/1500\n",
      "90/90 [==============================] - 0s 944us/step - loss: 0.2148 - accuracy: 0.9167\n",
      "Epoch 55/1500\n",
      "90/90 [==============================] - 0s 965us/step - loss: 0.2549 - accuracy: 0.9013\n",
      "Epoch 56/1500\n",
      "90/90 [==============================] - 0s 944us/step - loss: 0.2299 - accuracy: 0.9093\n",
      "Epoch 57/1500\n",
      "90/90 [==============================] - 0s 965us/step - loss: 0.2413 - accuracy: 0.9090\n",
      "Epoch 58/1500\n",
      "90/90 [==============================] - 0s 965us/step - loss: 0.2269 - accuracy: 0.9125\n",
      "Epoch 59/1500\n",
      "90/90 [==============================] - 0s 945us/step - loss: 0.2351 - accuracy: 0.9118\n",
      "Epoch 60/1500\n",
      "90/90 [==============================] - 0s 983us/step - loss: 0.2224 - accuracy: 0.9163\n",
      "Epoch 61/1500\n",
      "90/90 [==============================] - 0s 971us/step - loss: 0.2323 - accuracy: 0.9065\n",
      "Epoch 62/1500\n",
      "90/90 [==============================] - 0s 975us/step - loss: 0.2137 - accuracy: 0.9149\n",
      "Epoch 63/1500\n",
      "90/90 [==============================] - 0s 955us/step - loss: 0.2212 - accuracy: 0.9107\n",
      "Epoch 64/1500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 0.2215 - accuracy: 0.9198\n",
      "Epoch 65/1500\n",
      "90/90 [==============================] - 0s 933us/step - loss: 0.2101 - accuracy: 0.9153\n",
      "Epoch 66/1500\n",
      "90/90 [==============================] - 0s 954us/step - loss: 0.2060 - accuracy: 0.9188\n",
      "Epoch 67/1500\n",
      "90/90 [==============================] - 0s 938us/step - loss: 0.2050 - accuracy: 0.9209\n",
      "Epoch 68/1500\n",
      "90/90 [==============================] - 0s 913us/step - loss: 0.2282 - accuracy: 0.9156\n",
      "Epoch 69/1500\n",
      "90/90 [==============================] - 0s 928us/step - loss: 0.2138 - accuracy: 0.9188\n",
      "Epoch 70/1500\n",
      "90/90 [==============================] - 0s 940us/step - loss: 0.2084 - accuracy: 0.9163\n",
      "Epoch 71/1500\n",
      "90/90 [==============================] - 0s 917us/step - loss: 0.1975 - accuracy: 0.9254\n",
      "Epoch 72/1500\n",
      "90/90 [==============================] - 0s 917us/step - loss: 0.2086 - accuracy: 0.9195\n",
      "Epoch 73/1500\n",
      "90/90 [==============================] - 0s 931us/step - loss: 0.1993 - accuracy: 0.9219\n",
      "Epoch 74/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9237\n",
      "Epoch 75/1500\n",
      "90/90 [==============================] - 0s 957us/step - loss: 0.2175 - accuracy: 0.9240\n",
      "Epoch 76/1500\n",
      "90/90 [==============================] - 0s 936us/step - loss: 0.2043 - accuracy: 0.9216\n",
      "Epoch 77/1500\n",
      "90/90 [==============================] - 0s 925us/step - loss: 0.2196 - accuracy: 0.9230\n",
      "Epoch 78/1500\n",
      "90/90 [==============================] - 0s 981us/step - loss: 0.1998 - accuracy: 0.9244\n",
      "Epoch 79/1500\n",
      "90/90 [==============================] - 0s 928us/step - loss: 0.1911 - accuracy: 0.9261\n",
      "Epoch 80/1500\n",
      "90/90 [==============================] - 0s 971us/step - loss: 0.1842 - accuracy: 0.9317\n",
      "Epoch 81/1500\n",
      "90/90 [==============================] - 0s 949us/step - loss: 0.2001 - accuracy: 0.9219\n",
      "Epoch 82/1500\n",
      "90/90 [==============================] - 0s 910us/step - loss: 0.1986 - accuracy: 0.9251\n",
      "Epoch 83/1500\n",
      "90/90 [==============================] - 0s 926us/step - loss: 0.1847 - accuracy: 0.9296\n",
      "Epoch 84/1500\n",
      "90/90 [==============================] - 0s 916us/step - loss: 0.2015 - accuracy: 0.9216\n",
      "Epoch 85/1500\n",
      "90/90 [==============================] - 0s 916us/step - loss: 0.1868 - accuracy: 0.9279\n",
      "Epoch 86/1500\n",
      "90/90 [==============================] - 0s 924us/step - loss: 0.1853 - accuracy: 0.9310\n",
      "Epoch 87/1500\n",
      "90/90 [==============================] - 0s 923us/step - loss: 0.1856 - accuracy: 0.9300\n",
      "Epoch 88/1500\n",
      "90/90 [==============================] - 0s 922us/step - loss: 0.1996 - accuracy: 0.9251\n",
      "Epoch 89/1500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 0.1873 - accuracy: 0.9310\n",
      "Epoch 90/1500\n",
      "90/90 [==============================] - 0s 950us/step - loss: 0.1904 - accuracy: 0.9293\n",
      "Epoch 91/1500\n",
      "90/90 [==============================] - 0s 940us/step - loss: 0.1828 - accuracy: 0.9303\n",
      "Epoch 92/1500\n",
      "90/90 [==============================] - 0s 928us/step - loss: 0.1997 - accuracy: 0.9216\n",
      "Epoch 93/1500\n",
      "90/90 [==============================] - 0s 922us/step - loss: 0.1957 - accuracy: 0.9251\n",
      "Epoch 94/1500\n",
      "90/90 [==============================] - 0s 956us/step - loss: 0.1778 - accuracy: 0.9310\n",
      "Epoch 95/1500\n",
      "90/90 [==============================] - 0s 1000us/step - loss: 0.1751 - accuracy: 0.9314\n",
      "Epoch 96/1500\n",
      "90/90 [==============================] - 0s 974us/step - loss: 0.1805 - accuracy: 0.9349\n",
      "Epoch 97/1500\n",
      "90/90 [==============================] - 0s 936us/step - loss: 0.1849 - accuracy: 0.9300\n",
      "Epoch 98/1500\n",
      "90/90 [==============================] - 0s 933us/step - loss: 0.1749 - accuracy: 0.9296\n",
      "Epoch 99/1500\n",
      "90/90 [==============================] - 0s 932us/step - loss: 0.1905 - accuracy: 0.9303\n",
      "Epoch 100/1500\n",
      "90/90 [==============================] - 0s 919us/step - loss: 0.1823 - accuracy: 0.9296\n",
      "Epoch 101/1500\n",
      "90/90 [==============================] - 0s 923us/step - loss: 0.1765 - accuracy: 0.9356\n",
      "Epoch 102/1500\n",
      "90/90 [==============================] - 0s 938us/step - loss: 0.1730 - accuracy: 0.9363\n",
      "Epoch 103/1500\n",
      "90/90 [==============================] - 0s 973us/step - loss: 0.1761 - accuracy: 0.9366\n",
      "Epoch 104/1500\n",
      "90/90 [==============================] - 0s 931us/step - loss: 0.1726 - accuracy: 0.9345\n",
      "Epoch 105/1500\n",
      "90/90 [==============================] - 0s 926us/step - loss: 0.1826 - accuracy: 0.9324\n",
      "Epoch 106/1500\n",
      "90/90 [==============================] - 0s 976us/step - loss: 0.1661 - accuracy: 0.9391\n",
      "Epoch 107/1500\n",
      "90/90 [==============================] - 0s 930us/step - loss: 0.1731 - accuracy: 0.9384\n",
      "Epoch 108/1500\n",
      "90/90 [==============================] - 0s 942us/step - loss: 0.1628 - accuracy: 0.9401\n",
      "Epoch 109/1500\n",
      "90/90 [==============================] - 0s 992us/step - loss: 0.1559 - accuracy: 0.9436\n",
      "Epoch 110/1500\n",
      "90/90 [==============================] - 0s 954us/step - loss: 0.1663 - accuracy: 0.9405\n",
      "Epoch 111/1500\n",
      "90/90 [==============================] - 0s 922us/step - loss: 0.1573 - accuracy: 0.9433\n",
      "Epoch 112/1500\n",
      "90/90 [==============================] - 0s 937us/step - loss: 0.1536 - accuracy: 0.9433\n",
      "Epoch 113/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1628 - accuracy: 0.9401\n",
      "Epoch 114/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9412\n",
      "Epoch 115/1500\n",
      "90/90 [==============================] - 0s 995us/step - loss: 0.1745 - accuracy: 0.9282\n",
      "Epoch 116/1500\n",
      "90/90 [==============================] - 0s 984us/step - loss: 0.1713 - accuracy: 0.9380\n",
      "Epoch 117/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1631 - accuracy: 0.9342\n",
      "Epoch 118/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9387\n",
      "Epoch 119/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1643 - accuracy: 0.9412\n",
      "Epoch 120/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9380\n",
      "Epoch 121/1500\n",
      "90/90 [==============================] - 0s 963us/step - loss: 0.1651 - accuracy: 0.9394\n",
      "Epoch 122/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9461\n",
      "Epoch 123/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9471\n",
      "Epoch 124/1500\n",
      "90/90 [==============================] - 0s 939us/step - loss: 0.1434 - accuracy: 0.9454\n",
      "Epoch 125/1500\n",
      "90/90 [==============================] - 0s 950us/step - loss: 0.1418 - accuracy: 0.9464\n",
      "Epoch 126/1500\n",
      "90/90 [==============================] - 0s 940us/step - loss: 0.1394 - accuracy: 0.9461\n",
      "Epoch 127/1500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 0.1573 - accuracy: 0.9373\n",
      "Epoch 128/1500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 0.1504 - accuracy: 0.9426\n",
      "Epoch 129/1500\n",
      "90/90 [==============================] - 0s 956us/step - loss: 0.1462 - accuracy: 0.9499\n",
      "Epoch 130/1500\n",
      "90/90 [==============================] - 0s 917us/step - loss: 0.1506 - accuracy: 0.9447\n",
      "Epoch 131/1500\n",
      "90/90 [==============================] - 0s 921us/step - loss: 0.1546 - accuracy: 0.9373\n",
      "Epoch 132/1500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 0.1468 - accuracy: 0.9464\n",
      "Epoch 133/1500\n",
      "90/90 [==============================] - 0s 928us/step - loss: 0.1439 - accuracy: 0.9450\n",
      "Epoch 134/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9569\n",
      "Epoch 135/1500\n",
      "90/90 [==============================] - 0s 965us/step - loss: 0.1619 - accuracy: 0.9363\n",
      "Epoch 136/1500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 0.1353 - accuracy: 0.9492\n",
      "Epoch 137/1500\n",
      "90/90 [==============================] - 0s 926us/step - loss: 0.1500 - accuracy: 0.9394\n",
      "Epoch 138/1500\n",
      "90/90 [==============================] - 0s 930us/step - loss: 0.1476 - accuracy: 0.9457\n",
      "Epoch 139/1500\n",
      "90/90 [==============================] - 0s 955us/step - loss: 0.1552 - accuracy: 0.9468\n",
      "Epoch 140/1500\n",
      "90/90 [==============================] - 0s 953us/step - loss: 0.1523 - accuracy: 0.9436\n",
      "Epoch 141/1500\n",
      "90/90 [==============================] - 0s 967us/step - loss: 0.1558 - accuracy: 0.9447\n",
      "Epoch 142/1500\n",
      "90/90 [==============================] - 0s 980us/step - loss: 0.1572 - accuracy: 0.9380\n",
      "Epoch 143/1500\n",
      "90/90 [==============================] - 0s 952us/step - loss: 0.1587 - accuracy: 0.9422\n",
      "Epoch 144/1500\n",
      "90/90 [==============================] - 0s 991us/step - loss: 0.1471 - accuracy: 0.9496\n",
      "Epoch 145/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9517\n",
      "Epoch 146/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9499\n",
      "Epoch 147/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9503\n",
      "Epoch 148/1500\n",
      "90/90 [==============================] - 0s 947us/step - loss: 0.1414 - accuracy: 0.9478\n",
      "Epoch 149/1500\n",
      "90/90 [==============================] - 0s 930us/step - loss: 0.1279 - accuracy: 0.9576\n",
      "Epoch 150/1500\n",
      "90/90 [==============================] - 0s 992us/step - loss: 0.1586 - accuracy: 0.9440\n",
      "Epoch 151/1500\n",
      "90/90 [==============================] - 0s 972us/step - loss: 0.1414 - accuracy: 0.9513\n",
      "Epoch 152/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1422 - accuracy: 0.9503\n",
      "Epoch 153/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9531\n",
      "Epoch 154/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9443\n",
      "Epoch 155/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.9489\n",
      "Epoch 156/1500\n",
      "90/90 [==============================] - 0s 953us/step - loss: 0.1495 - accuracy: 0.9471\n",
      "Epoch 157/1500\n",
      "90/90 [==============================] - 0s 966us/step - loss: 0.1499 - accuracy: 0.9450\n",
      "Epoch 158/1500\n",
      "90/90 [==============================] - 0s 960us/step - loss: 0.1370 - accuracy: 0.9524\n",
      "Epoch 159/1500\n",
      "90/90 [==============================] - 0s 964us/step - loss: 0.1557 - accuracy: 0.9412\n",
      "Epoch 160/1500\n",
      "90/90 [==============================] - 0s 929us/step - loss: 0.1535 - accuracy: 0.9461\n",
      "Epoch 161/1500\n",
      "90/90 [==============================] - 0s 926us/step - loss: 0.1340 - accuracy: 0.9531\n",
      "Epoch 162/1500\n",
      "90/90 [==============================] - 0s 931us/step - loss: 0.1337 - accuracy: 0.9492\n",
      "Epoch 163/1500\n",
      "90/90 [==============================] - 0s 950us/step - loss: 0.1410 - accuracy: 0.9468\n",
      "Epoch 164/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9496\n",
      "Epoch 165/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9580\n",
      "Epoch 166/1500\n",
      "90/90 [==============================] - 0s 984us/step - loss: 0.1273 - accuracy: 0.9503\n",
      "Epoch 167/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9545\n",
      "Epoch 168/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.9499\n",
      "Epoch 169/1500\n",
      "90/90 [==============================] - 0s 980us/step - loss: 0.1476 - accuracy: 0.9485\n",
      "Epoch 170/1500\n",
      "90/90 [==============================] - 0s 1000us/step - loss: 0.1223 - accuracy: 0.9538\n",
      "Epoch 171/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9608\n",
      "Epoch 172/1500\n",
      "90/90 [==============================] - 0s 999us/step - loss: 0.1259 - accuracy: 0.9531\n",
      "Epoch 173/1500\n",
      "90/90 [==============================] - 0s 925us/step - loss: 0.1291 - accuracy: 0.9517\n",
      "Epoch 174/1500\n",
      "90/90 [==============================] - 0s 944us/step - loss: 0.1283 - accuracy: 0.9559\n",
      "Epoch 175/1500\n",
      "90/90 [==============================] - 0s 941us/step - loss: 0.1152 - accuracy: 0.9566\n",
      "Epoch 176/1500\n",
      "90/90 [==============================] - 0s 922us/step - loss: 0.1486 - accuracy: 0.9415\n",
      "Epoch 177/1500\n",
      "90/90 [==============================] - 0s 913us/step - loss: 0.1350 - accuracy: 0.9499\n",
      "Epoch 178/1500\n",
      "90/90 [==============================] - 0s 959us/step - loss: 0.1277 - accuracy: 0.9545\n",
      "Epoch 179/1500\n",
      "90/90 [==============================] - 0s 957us/step - loss: 0.1213 - accuracy: 0.9559\n",
      "Epoch 180/1500\n",
      "90/90 [==============================] - 0s 941us/step - loss: 0.1246 - accuracy: 0.9545\n",
      "Epoch 181/1500\n",
      "90/90 [==============================] - 0s 945us/step - loss: 0.1177 - accuracy: 0.9576\n",
      "Epoch 182/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9541\n",
      "Epoch 183/1500\n",
      "90/90 [==============================] - 0s 955us/step - loss: 0.1205 - accuracy: 0.9583\n",
      "Epoch 184/1500\n",
      "90/90 [==============================] - 0s 940us/step - loss: 0.1359 - accuracy: 0.9531\n",
      "Epoch 185/1500\n",
      "90/90 [==============================] - 0s 939us/step - loss: 0.1243 - accuracy: 0.9527\n",
      "Epoch 186/1500\n",
      "90/90 [==============================] - 0s 921us/step - loss: 0.1299 - accuracy: 0.9552\n",
      "Epoch 187/1500\n",
      "90/90 [==============================] - 0s 949us/step - loss: 0.1437 - accuracy: 0.9454\n",
      "Epoch 188/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.9580\n",
      "Epoch 189/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9548\n",
      "Epoch 190/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9503\n",
      "Epoch 191/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9538\n",
      "Epoch 192/1500\n",
      "90/90 [==============================] - 0s 992us/step - loss: 0.1206 - accuracy: 0.9534\n",
      "Epoch 193/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9548\n",
      "Epoch 194/1500\n",
      "90/90 [==============================] - 0s 987us/step - loss: 0.1175 - accuracy: 0.9583\n",
      "Epoch 195/1500\n",
      "90/90 [==============================] - 0s 933us/step - loss: 0.1364 - accuracy: 0.9496\n",
      "Epoch 196/1500\n",
      "90/90 [==============================] - 0s 941us/step - loss: 0.1264 - accuracy: 0.9534\n",
      "Epoch 197/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9559\n",
      "Epoch 198/1500\n",
      "90/90 [==============================] - 0s 958us/step - loss: 0.1367 - accuracy: 0.9524\n",
      "Epoch 199/1500\n",
      "90/90 [==============================] - 0s 968us/step - loss: 0.1257 - accuracy: 0.9503\n",
      "Epoch 200/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9587\n",
      "Epoch 201/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9590\n",
      "Epoch 202/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9629\n",
      "Epoch 203/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.9562\n",
      "Epoch 204/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9534\n",
      "Epoch 205/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9566\n",
      "Epoch 206/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9587\n",
      "Epoch 207/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9625\n",
      "Epoch 208/1500\n",
      "90/90 [==============================] - 0s 966us/step - loss: 0.1102 - accuracy: 0.9611\n",
      "Epoch 209/1500\n",
      "90/90 [==============================] - 0s 950us/step - loss: 0.1150 - accuracy: 0.9587\n",
      "Epoch 210/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9601\n",
      "Epoch 211/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9597\n",
      "Epoch 212/1500\n",
      "90/90 [==============================] - 0s 970us/step - loss: 0.1254 - accuracy: 0.9541\n",
      "Epoch 213/1500\n",
      "90/90 [==============================] - 0s 969us/step - loss: 0.1186 - accuracy: 0.9576\n",
      "Epoch 214/1500\n",
      "90/90 [==============================] - 0s 936us/step - loss: 0.1160 - accuracy: 0.9604\n",
      "Epoch 215/1500\n",
      "90/90 [==============================] - 0s 968us/step - loss: 0.1141 - accuracy: 0.9590\n",
      "Epoch 216/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9562\n",
      "Epoch 217/1500\n",
      "90/90 [==============================] - 0s 993us/step - loss: 0.1137 - accuracy: 0.9618\n",
      "Epoch 218/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9566\n",
      "Epoch 219/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9587\n",
      "Epoch 220/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9587\n",
      "Epoch 221/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9576\n",
      "Epoch 222/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.9685\n",
      "Epoch 223/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9632\n",
      "Epoch 224/1500\n",
      "90/90 [==============================] - 0s 989us/step - loss: 0.1112 - accuracy: 0.9597\n",
      "Epoch 225/1500\n",
      "90/90 [==============================] - 0s 961us/step - loss: 0.1096 - accuracy: 0.9660\n",
      "Epoch 226/1500\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 0.1099 - accuracy: 0.9632\n",
      "Epoch 227/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9583\n",
      "Epoch 228/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9618\n",
      "Epoch 229/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9646\n",
      "Epoch 230/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9555\n",
      "Epoch 231/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9618\n",
      "Epoch 232/1500\n",
      "90/90 [==============================] - 0s 986us/step - loss: 0.1042 - accuracy: 0.9622\n",
      "Epoch 233/1500\n",
      "90/90 [==============================] - 0s 988us/step - loss: 0.1020 - accuracy: 0.9660\n",
      "Epoch 234/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9562\n",
      "Epoch 235/1500\n",
      "90/90 [==============================] - 0s 987us/step - loss: 0.1087 - accuracy: 0.9590\n",
      "Epoch 236/1500\n",
      "90/90 [==============================] - 0s 955us/step - loss: 0.0979 - accuracy: 0.9699\n",
      "Epoch 237/1500\n",
      "90/90 [==============================] - 0s 963us/step - loss: 0.1104 - accuracy: 0.9611\n",
      "Epoch 238/1500\n",
      "90/90 [==============================] - 0s 994us/step - loss: 0.1109 - accuracy: 0.9587\n",
      "Epoch 239/1500\n",
      "90/90 [==============================] - 0s 987us/step - loss: 0.1186 - accuracy: 0.9580\n",
      "Epoch 240/1500\n",
      "90/90 [==============================] - 0s 956us/step - loss: 0.1134 - accuracy: 0.9583\n",
      "Epoch 241/1500\n",
      "90/90 [==============================] - 0s 999us/step - loss: 0.1055 - accuracy: 0.9636\n",
      "Epoch 242/1500\n",
      "90/90 [==============================] - 0s 985us/step - loss: 0.1119 - accuracy: 0.9587\n",
      "Epoch 243/1500\n",
      "90/90 [==============================] - 0s 957us/step - loss: 0.1019 - accuracy: 0.9590\n",
      "Epoch 244/1500\n",
      "90/90 [==============================] - 0s 970us/step - loss: 0.0932 - accuracy: 0.9671\n",
      "Epoch 245/1500\n",
      "90/90 [==============================] - 0s 955us/step - loss: 0.1113 - accuracy: 0.9611\n",
      "Epoch 246/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9587\n",
      "Epoch 247/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9678\n",
      "Epoch 248/1500\n",
      "90/90 [==============================] - 0s 960us/step - loss: 0.1301 - accuracy: 0.9562\n",
      "Epoch 249/1500\n",
      "90/90 [==============================] - 0s 999us/step - loss: 0.1180 - accuracy: 0.9611\n",
      "Epoch 250/1500\n",
      "90/90 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9604\n",
      "Epoch 251/1500\n",
      "90/90 [==============================] - 0s 966us/step - loss: 0.0980 - accuracy: 0.9657\n",
      "Epoch 252/1500\n",
      "53/90 [================>.............] - ETA: 0s - loss: 0.1213 - accuracy: 0.9564Restoring model weights from the end of the best epoch: 222.\n",
      "90/90 [==============================] - 0s 987us/step - loss: 0.1174 - accuracy: 0.9590\n",
      "Epoch 252: early stopping\n",
      "7/7 [==============================] - 0s 869us/step - loss: 0.8342 - accuracy: 0.7489\n",
      "7/7 [==============================] - 0s 647us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (25/30)\n",
      "Before appending - Cat IDs: 389, Predictions: 389, Actuals: 389, Gender: 389\n",
      "After appending - Cat IDs: 612, Predictions: 612, Actuals: 612, Gender: 612\n",
      "Final Test Results - Loss: 0.8342409133911133, Accuracy: 0.7488788962364197, Precision: 0.7316968745540174, Recall: 0.7684737538752137, F1 Score: 0.7455399061032862\n",
      "Confusion Matrix:\n",
      " [[114   3  20]\n",
      " [  1   8   0]\n",
      " [ 32   0  45]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "063A    11\n",
      "068A    11\n",
      "040A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "022A     9\n",
      "045A     9\n",
      "065A     9\n",
      "072A     9\n",
      "033A     9\n",
      "015A     9\n",
      "094A     8\n",
      "010A     8\n",
      "013B     8\n",
      "117A     7\n",
      "099A     7\n",
      "050A     7\n",
      "007A     6\n",
      "053A     6\n",
      "108A     6\n",
      "109A     6\n",
      "023A     6\n",
      "021A     5\n",
      "025C     5\n",
      "044A     5\n",
      "075A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "056A     3\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "113A     3\n",
      "069A     2\n",
      "025B     2\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "087A     2\n",
      "011A     2\n",
      "093A     2\n",
      "032A     2\n",
      "054A     2\n",
      "088A     1\n",
      "115A     1\n",
      "100A     1\n",
      "024A     1\n",
      "019B     1\n",
      "043A     1\n",
      "091A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "055A    20\n",
      "019A    17\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "111A    13\n",
      "051A    12\n",
      "036A    11\n",
      "014B    10\n",
      "071A    10\n",
      "095A     8\n",
      "027A     7\n",
      "031A     7\n",
      "037A     6\n",
      "008A     6\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "105A     4\n",
      "052A     4\n",
      "014A     3\n",
      "060A     3\n",
      "102A     2\n",
      "110A     1\n",
      "096A     1\n",
      "041A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    268\n",
      "X    259\n",
      "F    182\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    89\n",
      "F    70\n",
      "M    69\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 028A, 074...\n",
      "kitten    [044A, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 097B, 019A, 020A, 101A, 095A, 034A, 027...\n",
      "kitten                             [014B, 111A, 041A, 110A]\n",
      "senior                             [055A, 059A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "Length of X_train_val:\n",
      "709\n",
      "Length of y_train_val:\n",
      "709\n",
      "Length of groups_train_val:\n",
      "709\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1053, 1: 1014, 2: 871})\n",
      "Epoch 1/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.9147 - accuracy: 0.6099\n",
      "Epoch 2/1500\n",
      "92/92 [==============================] - 0s 971us/step - loss: 0.7388 - accuracy: 0.6961\n",
      "Epoch 3/1500\n",
      "92/92 [==============================] - 0s 983us/step - loss: 0.6750 - accuracy: 0.7182\n",
      "Epoch 4/1500\n",
      "92/92 [==============================] - 0s 974us/step - loss: 0.6324 - accuracy: 0.7243\n",
      "Epoch 5/1500\n",
      "92/92 [==============================] - 0s 968us/step - loss: 0.6222 - accuracy: 0.7420\n",
      "Epoch 6/1500\n",
      "92/92 [==============================] - 0s 988us/step - loss: 0.5646 - accuracy: 0.7668\n",
      "Epoch 7/1500\n",
      "92/92 [==============================] - 0s 972us/step - loss: 0.5724 - accuracy: 0.7607\n",
      "Epoch 8/1500\n",
      "92/92 [==============================] - 0s 971us/step - loss: 0.5512 - accuracy: 0.7634\n",
      "Epoch 9/1500\n",
      "92/92 [==============================] - 0s 959us/step - loss: 0.5338 - accuracy: 0.7822\n",
      "Epoch 10/1500\n",
      "92/92 [==============================] - 0s 983us/step - loss: 0.5036 - accuracy: 0.7937\n",
      "Epoch 11/1500\n",
      "92/92 [==============================] - 0s 985us/step - loss: 0.4942 - accuracy: 0.7910\n",
      "Epoch 12/1500\n",
      "92/92 [==============================] - 0s 952us/step - loss: 0.4772 - accuracy: 0.8033\n",
      "Epoch 13/1500\n",
      "92/92 [==============================] - 0s 947us/step - loss: 0.4715 - accuracy: 0.8091\n",
      "Epoch 14/1500\n",
      "92/92 [==============================] - 0s 981us/step - loss: 0.4522 - accuracy: 0.8179\n",
      "Epoch 15/1500\n",
      "92/92 [==============================] - 0s 978us/step - loss: 0.4539 - accuracy: 0.8155\n",
      "Epoch 16/1500\n",
      "92/92 [==============================] - 0s 969us/step - loss: 0.4422 - accuracy: 0.8176\n",
      "Epoch 17/1500\n",
      "92/92 [==============================] - 0s 951us/step - loss: 0.4308 - accuracy: 0.8305\n",
      "Epoch 18/1500\n",
      "92/92 [==============================] - 0s 949us/step - loss: 0.4480 - accuracy: 0.8199\n",
      "Epoch 19/1500\n",
      "92/92 [==============================] - 0s 956us/step - loss: 0.4171 - accuracy: 0.8332\n",
      "Epoch 20/1500\n",
      "92/92 [==============================] - 0s 931us/step - loss: 0.4058 - accuracy: 0.8336\n",
      "Epoch 21/1500\n",
      "92/92 [==============================] - 0s 991us/step - loss: 0.4086 - accuracy: 0.8336\n",
      "Epoch 22/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3915 - accuracy: 0.8417\n",
      "Epoch 23/1500\n",
      "92/92 [==============================] - 0s 973us/step - loss: 0.3845 - accuracy: 0.8479\n",
      "Epoch 24/1500\n",
      "92/92 [==============================] - 0s 944us/step - loss: 0.3839 - accuracy: 0.8482\n",
      "Epoch 25/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3984 - accuracy: 0.8465\n",
      "Epoch 26/1500\n",
      "92/92 [==============================] - 0s 998us/step - loss: 0.3823 - accuracy: 0.8438\n",
      "Epoch 27/1500\n",
      "92/92 [==============================] - 0s 950us/step - loss: 0.3630 - accuracy: 0.8560\n",
      "Epoch 28/1500\n",
      "92/92 [==============================] - 0s 974us/step - loss: 0.3660 - accuracy: 0.8513\n",
      "Epoch 29/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3555 - accuracy: 0.8601\n",
      "Epoch 30/1500\n",
      "92/92 [==============================] - 0s 971us/step - loss: 0.3555 - accuracy: 0.8625\n",
      "Epoch 31/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3639 - accuracy: 0.8509\n",
      "Epoch 32/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3503 - accuracy: 0.8628\n",
      "Epoch 33/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3464 - accuracy: 0.8536\n",
      "Epoch 34/1500\n",
      "92/92 [==============================] - 0s 972us/step - loss: 0.3438 - accuracy: 0.8652\n",
      "Epoch 35/1500\n",
      "92/92 [==============================] - 0s 939us/step - loss: 0.3288 - accuracy: 0.8693\n",
      "Epoch 36/1500\n",
      "92/92 [==============================] - 0s 930us/step - loss: 0.3279 - accuracy: 0.8686\n",
      "Epoch 37/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3445 - accuracy: 0.8635\n",
      "Epoch 38/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.3126 - accuracy: 0.8795\n",
      "Epoch 39/1500\n",
      "92/92 [==============================] - 0s 962us/step - loss: 0.3306 - accuracy: 0.8676\n",
      "Epoch 40/1500\n",
      "92/92 [==============================] - 0s 940us/step - loss: 0.3284 - accuracy: 0.8710\n",
      "Epoch 41/1500\n",
      "92/92 [==============================] - 0s 962us/step - loss: 0.3079 - accuracy: 0.8826\n",
      "Epoch 42/1500\n",
      "92/92 [==============================] - 0s 963us/step - loss: 0.3269 - accuracy: 0.8747\n",
      "Epoch 43/1500\n",
      "92/92 [==============================] - 0s 983us/step - loss: 0.3077 - accuracy: 0.8822\n",
      "Epoch 44/1500\n",
      "92/92 [==============================] - 0s 967us/step - loss: 0.3022 - accuracy: 0.8819\n",
      "Epoch 45/1500\n",
      "92/92 [==============================] - 0s 950us/step - loss: 0.3082 - accuracy: 0.8775\n",
      "Epoch 46/1500\n",
      "92/92 [==============================] - 0s 976us/step - loss: 0.3081 - accuracy: 0.8812\n",
      "Epoch 47/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2818 - accuracy: 0.8860\n",
      "Epoch 48/1500\n",
      "92/92 [==============================] - 0s 973us/step - loss: 0.3019 - accuracy: 0.8833\n",
      "Epoch 49/1500\n",
      "92/92 [==============================] - 0s 974us/step - loss: 0.3135 - accuracy: 0.8809\n",
      "Epoch 50/1500\n",
      "92/92 [==============================] - 0s 987us/step - loss: 0.2976 - accuracy: 0.8836\n",
      "Epoch 51/1500\n",
      "92/92 [==============================] - 0s 947us/step - loss: 0.2968 - accuracy: 0.8856\n",
      "Epoch 52/1500\n",
      "92/92 [==============================] - 0s 940us/step - loss: 0.2876 - accuracy: 0.8938\n",
      "Epoch 53/1500\n",
      "92/92 [==============================] - 0s 944us/step - loss: 0.3046 - accuracy: 0.8856\n",
      "Epoch 54/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2825 - accuracy: 0.8945\n",
      "Epoch 55/1500\n",
      "92/92 [==============================] - 0s 980us/step - loss: 0.2937 - accuracy: 0.8819\n",
      "Epoch 56/1500\n",
      "92/92 [==============================] - 0s 945us/step - loss: 0.2995 - accuracy: 0.8788\n",
      "Epoch 57/1500\n",
      "92/92 [==============================] - 0s 967us/step - loss: 0.2914 - accuracy: 0.8826\n",
      "Epoch 58/1500\n",
      "92/92 [==============================] - 0s 978us/step - loss: 0.2763 - accuracy: 0.8955\n",
      "Epoch 59/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2831 - accuracy: 0.8853\n",
      "Epoch 60/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2590 - accuracy: 0.9027\n",
      "Epoch 61/1500\n",
      "92/92 [==============================] - 0s 980us/step - loss: 0.2785 - accuracy: 0.8901\n",
      "Epoch 62/1500\n",
      "92/92 [==============================] - 0s 962us/step - loss: 0.2770 - accuracy: 0.8938\n",
      "Epoch 63/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2761 - accuracy: 0.8911\n",
      "Epoch 64/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2573 - accuracy: 0.8999\n",
      "Epoch 65/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2735 - accuracy: 0.8945\n",
      "Epoch 66/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2741 - accuracy: 0.8952\n",
      "Epoch 67/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2625 - accuracy: 0.8986\n",
      "Epoch 68/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2672 - accuracy: 0.8935\n",
      "Epoch 69/1500\n",
      "92/92 [==============================] - 0s 974us/step - loss: 0.2670 - accuracy: 0.8935\n",
      "Epoch 70/1500\n",
      "92/92 [==============================] - 0s 989us/step - loss: 0.2470 - accuracy: 0.9074\n",
      "Epoch 71/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2539 - accuracy: 0.8948\n",
      "Epoch 72/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2464 - accuracy: 0.9084\n",
      "Epoch 73/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2643 - accuracy: 0.8955\n",
      "Epoch 74/1500\n",
      "92/92 [==============================] - 0s 940us/step - loss: 0.2638 - accuracy: 0.8941\n",
      "Epoch 75/1500\n",
      "92/92 [==============================] - 0s 978us/step - loss: 0.2462 - accuracy: 0.9091\n",
      "Epoch 76/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2729 - accuracy: 0.8945\n",
      "Epoch 77/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.9033\n",
      "Epoch 78/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2525 - accuracy: 0.9033\n",
      "Epoch 79/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2764 - accuracy: 0.8901\n",
      "Epoch 80/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2462 - accuracy: 0.9033\n",
      "Epoch 81/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2405 - accuracy: 0.9081\n",
      "Epoch 82/1500\n",
      "92/92 [==============================] - 0s 950us/step - loss: 0.2319 - accuracy: 0.9163\n",
      "Epoch 83/1500\n",
      "92/92 [==============================] - 0s 966us/step - loss: 0.2508 - accuracy: 0.9037\n",
      "Epoch 84/1500\n",
      "92/92 [==============================] - 0s 926us/step - loss: 0.2392 - accuracy: 0.9071\n",
      "Epoch 85/1500\n",
      "92/92 [==============================] - 0s 945us/step - loss: 0.2410 - accuracy: 0.9027\n",
      "Epoch 86/1500\n",
      "92/92 [==============================] - 0s 919us/step - loss: 0.2412 - accuracy: 0.9050\n",
      "Epoch 87/1500\n",
      "92/92 [==============================] - 0s 987us/step - loss: 0.2334 - accuracy: 0.9044\n",
      "Epoch 88/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2337 - accuracy: 0.9132\n",
      "Epoch 89/1500\n",
      "92/92 [==============================] - 0s 956us/step - loss: 0.2407 - accuracy: 0.9027\n",
      "Epoch 90/1500\n",
      "92/92 [==============================] - 0s 952us/step - loss: 0.2389 - accuracy: 0.9105\n",
      "Epoch 91/1500\n",
      "92/92 [==============================] - 0s 979us/step - loss: 0.2374 - accuracy: 0.9129\n",
      "Epoch 92/1500\n",
      "92/92 [==============================] - 0s 960us/step - loss: 0.2049 - accuracy: 0.9275\n",
      "Epoch 93/1500\n",
      "92/92 [==============================] - 0s 980us/step - loss: 0.2203 - accuracy: 0.9197\n",
      "Epoch 94/1500\n",
      "92/92 [==============================] - 0s 989us/step - loss: 0.2155 - accuracy: 0.9197\n",
      "Epoch 95/1500\n",
      "92/92 [==============================] - 0s 987us/step - loss: 0.2254 - accuracy: 0.9159\n",
      "Epoch 96/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9135\n",
      "Epoch 97/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.9142\n",
      "Epoch 98/1500\n",
      "92/92 [==============================] - 0s 984us/step - loss: 0.2279 - accuracy: 0.9149\n",
      "Epoch 99/1500\n",
      "92/92 [==============================] - 0s 949us/step - loss: 0.2271 - accuracy: 0.9135\n",
      "Epoch 100/1500\n",
      "92/92 [==============================] - 0s 958us/step - loss: 0.2264 - accuracy: 0.9129\n",
      "Epoch 101/1500\n",
      "92/92 [==============================] - 0s 990us/step - loss: 0.2072 - accuracy: 0.9217\n",
      "Epoch 102/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2129 - accuracy: 0.9200\n",
      "Epoch 103/1500\n",
      "92/92 [==============================] - 0s 992us/step - loss: 0.2121 - accuracy: 0.9200\n",
      "Epoch 104/1500\n",
      "92/92 [==============================] - 0s 967us/step - loss: 0.2133 - accuracy: 0.9258\n",
      "Epoch 105/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.9071\n",
      "Epoch 106/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2213 - accuracy: 0.9132\n",
      "Epoch 107/1500\n",
      "92/92 [==============================] - 0s 986us/step - loss: 0.2057 - accuracy: 0.9173\n",
      "Epoch 108/1500\n",
      "92/92 [==============================] - 0s 998us/step - loss: 0.2115 - accuracy: 0.9231\n",
      "Epoch 109/1500\n",
      "92/92 [==============================] - 0s 956us/step - loss: 0.2036 - accuracy: 0.9217\n",
      "Epoch 110/1500\n",
      "92/92 [==============================] - 0s 971us/step - loss: 0.2025 - accuracy: 0.9251\n",
      "Epoch 111/1500\n",
      "92/92 [==============================] - 0s 974us/step - loss: 0.2091 - accuracy: 0.9166\n",
      "Epoch 112/1500\n",
      "92/92 [==============================] - 0s 996us/step - loss: 0.2141 - accuracy: 0.9241\n",
      "Epoch 113/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2323 - accuracy: 0.9084\n",
      "Epoch 114/1500\n",
      "92/92 [==============================] - 0s 963us/step - loss: 0.2233 - accuracy: 0.9149\n",
      "Epoch 115/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2353 - accuracy: 0.9101\n",
      "Epoch 116/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2223 - accuracy: 0.9166\n",
      "Epoch 117/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9238\n",
      "Epoch 118/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9193\n",
      "Epoch 119/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9258\n",
      "Epoch 120/1500\n",
      "92/92 [==============================] - 0s 990us/step - loss: 0.1965 - accuracy: 0.9265\n",
      "Epoch 121/1500\n",
      "92/92 [==============================] - 0s 976us/step - loss: 0.1873 - accuracy: 0.9278\n",
      "Epoch 122/1500\n",
      "92/92 [==============================] - 0s 952us/step - loss: 0.1908 - accuracy: 0.9268\n",
      "Epoch 123/1500\n",
      "92/92 [==============================] - 0s 987us/step - loss: 0.1932 - accuracy: 0.9289\n",
      "Epoch 124/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9224\n",
      "Epoch 125/1500\n",
      "92/92 [==============================] - 0s 964us/step - loss: 0.2015 - accuracy: 0.9227\n",
      "Epoch 126/1500\n",
      "92/92 [==============================] - 0s 962us/step - loss: 0.2212 - accuracy: 0.9176\n",
      "Epoch 127/1500\n",
      "92/92 [==============================] - 0s 932us/step - loss: 0.2064 - accuracy: 0.9207\n",
      "Epoch 128/1500\n",
      "92/92 [==============================] - 0s 944us/step - loss: 0.2123 - accuracy: 0.9193\n",
      "Epoch 129/1500\n",
      "92/92 [==============================] - 0s 957us/step - loss: 0.1915 - accuracy: 0.9299\n",
      "Epoch 130/1500\n",
      "92/92 [==============================] - 0s 962us/step - loss: 0.1946 - accuracy: 0.9251\n",
      "Epoch 131/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9210\n",
      "Epoch 132/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2043 - accuracy: 0.9200\n",
      "Epoch 133/1500\n",
      "92/92 [==============================] - 0s 962us/step - loss: 0.1975 - accuracy: 0.9265\n",
      "Epoch 134/1500\n",
      "92/92 [==============================] - 0s 946us/step - loss: 0.2044 - accuracy: 0.9227\n",
      "Epoch 135/1500\n",
      "92/92 [==============================] - 0s 964us/step - loss: 0.1844 - accuracy: 0.9323\n",
      "Epoch 136/1500\n",
      "92/92 [==============================] - 0s 988us/step - loss: 0.1850 - accuracy: 0.9316\n",
      "Epoch 137/1500\n",
      "92/92 [==============================] - 0s 979us/step - loss: 0.1990 - accuracy: 0.9241\n",
      "Epoch 138/1500\n",
      "92/92 [==============================] - 0s 959us/step - loss: 0.2103 - accuracy: 0.9210\n",
      "Epoch 139/1500\n",
      "92/92 [==============================] - 0s 950us/step - loss: 0.1941 - accuracy: 0.9282\n",
      "Epoch 140/1500\n",
      "92/92 [==============================] - 0s 992us/step - loss: 0.1868 - accuracy: 0.9289\n",
      "Epoch 141/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1912 - accuracy: 0.9289\n",
      "Epoch 142/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1981 - accuracy: 0.9231\n",
      "Epoch 143/1500\n",
      "92/92 [==============================] - 0s 985us/step - loss: 0.1878 - accuracy: 0.9278\n",
      "Epoch 144/1500\n",
      "92/92 [==============================] - 0s 962us/step - loss: 0.1930 - accuracy: 0.9268\n",
      "Epoch 145/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.2037 - accuracy: 0.9190\n",
      "Epoch 146/1500\n",
      "92/92 [==============================] - 0s 987us/step - loss: 0.1904 - accuracy: 0.9275\n",
      "Epoch 147/1500\n",
      "92/92 [==============================] - 0s 986us/step - loss: 0.1847 - accuracy: 0.9319\n",
      "Epoch 148/1500\n",
      "92/92 [==============================] - 0s 979us/step - loss: 0.1751 - accuracy: 0.9367\n",
      "Epoch 149/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9231\n",
      "Epoch 150/1500\n",
      "92/92 [==============================] - 0s 989us/step - loss: 0.1916 - accuracy: 0.9299\n",
      "Epoch 151/1500\n",
      "92/92 [==============================] - 0s 951us/step - loss: 0.1842 - accuracy: 0.9312\n",
      "Epoch 152/1500\n",
      "92/92 [==============================] - 0s 984us/step - loss: 0.1869 - accuracy: 0.9278\n",
      "Epoch 153/1500\n",
      "92/92 [==============================] - 0s 995us/step - loss: 0.1768 - accuracy: 0.9319\n",
      "Epoch 154/1500\n",
      "92/92 [==============================] - 0s 992us/step - loss: 0.1829 - accuracy: 0.9309\n",
      "Epoch 155/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1880 - accuracy: 0.9299\n",
      "Epoch 156/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9360\n",
      "Epoch 157/1500\n",
      "92/92 [==============================] - 0s 967us/step - loss: 0.1809 - accuracy: 0.9312\n",
      "Epoch 158/1500\n",
      "92/92 [==============================] - 0s 944us/step - loss: 0.1896 - accuracy: 0.9302\n",
      "Epoch 159/1500\n",
      "92/92 [==============================] - 0s 949us/step - loss: 0.1933 - accuracy: 0.9261\n",
      "Epoch 160/1500\n",
      "92/92 [==============================] - 0s 949us/step - loss: 0.1714 - accuracy: 0.9391\n",
      "Epoch 161/1500\n",
      "92/92 [==============================] - 0s 983us/step - loss: 0.1648 - accuracy: 0.9398\n",
      "Epoch 162/1500\n",
      "92/92 [==============================] - 0s 956us/step - loss: 0.1634 - accuracy: 0.9404\n",
      "Epoch 163/1500\n",
      "92/92 [==============================] - 0s 951us/step - loss: 0.1655 - accuracy: 0.9381\n",
      "Epoch 164/1500\n",
      "92/92 [==============================] - 0s 943us/step - loss: 0.1773 - accuracy: 0.9350\n",
      "Epoch 165/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.9299\n",
      "Epoch 166/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.9326\n",
      "Epoch 167/1500\n",
      "92/92 [==============================] - 0s 980us/step - loss: 0.1783 - accuracy: 0.9329\n",
      "Epoch 168/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.9384\n",
      "Epoch 169/1500\n",
      "92/92 [==============================] - 0s 958us/step - loss: 0.1676 - accuracy: 0.9343\n",
      "Epoch 170/1500\n",
      "92/92 [==============================] - 0s 935us/step - loss: 0.1750 - accuracy: 0.9350\n",
      "Epoch 171/1500\n",
      "92/92 [==============================] - 0s 966us/step - loss: 0.1664 - accuracy: 0.9398\n",
      "Epoch 172/1500\n",
      "92/92 [==============================] - 0s 938us/step - loss: 0.1761 - accuracy: 0.9282\n",
      "Epoch 173/1500\n",
      "92/92 [==============================] - 0s 943us/step - loss: 0.1680 - accuracy: 0.9346\n",
      "Epoch 174/1500\n",
      "92/92 [==============================] - 0s 939us/step - loss: 0.1840 - accuracy: 0.9261\n",
      "Epoch 175/1500\n",
      "92/92 [==============================] - 0s 941us/step - loss: 0.1703 - accuracy: 0.9353\n",
      "Epoch 176/1500\n",
      "92/92 [==============================] - 0s 960us/step - loss: 0.1602 - accuracy: 0.9398\n",
      "Epoch 177/1500\n",
      "92/92 [==============================] - 0s 962us/step - loss: 0.1615 - accuracy: 0.9391\n",
      "Epoch 178/1500\n",
      "92/92 [==============================] - 0s 945us/step - loss: 0.1676 - accuracy: 0.9394\n",
      "Epoch 179/1500\n",
      "92/92 [==============================] - 0s 941us/step - loss: 0.1725 - accuracy: 0.9360\n",
      "Epoch 180/1500\n",
      "92/92 [==============================] - 0s 960us/step - loss: 0.1627 - accuracy: 0.9408\n",
      "Epoch 181/1500\n",
      "92/92 [==============================] - 0s 989us/step - loss: 0.1659 - accuracy: 0.9329\n",
      "Epoch 182/1500\n",
      "92/92 [==============================] - 0s 986us/step - loss: 0.1646 - accuracy: 0.9387\n",
      "Epoch 183/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9469\n",
      "Epoch 184/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9425\n",
      "Epoch 185/1500\n",
      "92/92 [==============================] - 0s 965us/step - loss: 0.1847 - accuracy: 0.9309\n",
      "Epoch 186/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9340\n",
      "Epoch 187/1500\n",
      "92/92 [==============================] - 0s 981us/step - loss: 0.1645 - accuracy: 0.9435\n",
      "Epoch 188/1500\n",
      "92/92 [==============================] - 0s 943us/step - loss: 0.1627 - accuracy: 0.9357\n",
      "Epoch 189/1500\n",
      "92/92 [==============================] - 0s 931us/step - loss: 0.1550 - accuracy: 0.9421\n",
      "Epoch 190/1500\n",
      "92/92 [==============================] - 0s 940us/step - loss: 0.1555 - accuracy: 0.9425\n",
      "Epoch 191/1500\n",
      "92/92 [==============================] - 0s 949us/step - loss: 0.1505 - accuracy: 0.9411\n",
      "Epoch 192/1500\n",
      "92/92 [==============================] - 0s 949us/step - loss: 0.1588 - accuracy: 0.9367\n",
      "Epoch 193/1500\n",
      "92/92 [==============================] - 0s 982us/step - loss: 0.1680 - accuracy: 0.9391\n",
      "Epoch 194/1500\n",
      "92/92 [==============================] - 0s 940us/step - loss: 0.1668 - accuracy: 0.9377\n",
      "Epoch 195/1500\n",
      "92/92 [==============================] - 0s 949us/step - loss: 0.1515 - accuracy: 0.9421\n",
      "Epoch 196/1500\n",
      "92/92 [==============================] - 0s 964us/step - loss: 0.1523 - accuracy: 0.9411\n",
      "Epoch 197/1500\n",
      "92/92 [==============================] - 0s 953us/step - loss: 0.1593 - accuracy: 0.9374\n",
      "Epoch 198/1500\n",
      "92/92 [==============================] - 0s 936us/step - loss: 0.1570 - accuracy: 0.9384\n",
      "Epoch 199/1500\n",
      "92/92 [==============================] - 0s 944us/step - loss: 0.1613 - accuracy: 0.9435\n",
      "Epoch 200/1500\n",
      "92/92 [==============================] - 0s 959us/step - loss: 0.1601 - accuracy: 0.9415\n",
      "Epoch 201/1500\n",
      "92/92 [==============================] - 0s 939us/step - loss: 0.1679 - accuracy: 0.9381\n",
      "Epoch 202/1500\n",
      "92/92 [==============================] - 0s 949us/step - loss: 0.1738 - accuracy: 0.9381\n",
      "Epoch 203/1500\n",
      "92/92 [==============================] - 0s 944us/step - loss: 0.1607 - accuracy: 0.9415\n",
      "Epoch 204/1500\n",
      "92/92 [==============================] - 0s 947us/step - loss: 0.1740 - accuracy: 0.9357\n",
      "Epoch 205/1500\n",
      "92/92 [==============================] - 0s 943us/step - loss: 0.1507 - accuracy: 0.9432\n",
      "Epoch 206/1500\n",
      "92/92 [==============================] - 0s 964us/step - loss: 0.1605 - accuracy: 0.9408\n",
      "Epoch 207/1500\n",
      "92/92 [==============================] - 0s 969us/step - loss: 0.1637 - accuracy: 0.9408\n",
      "Epoch 208/1500\n",
      "92/92 [==============================] - 0s 964us/step - loss: 0.1476 - accuracy: 0.9479\n",
      "Epoch 209/1500\n",
      "92/92 [==============================] - 0s 941us/step - loss: 0.1469 - accuracy: 0.9466\n",
      "Epoch 210/1500\n",
      "92/92 [==============================] - 0s 940us/step - loss: 0.1571 - accuracy: 0.9459\n",
      "Epoch 211/1500\n",
      "92/92 [==============================] - 0s 947us/step - loss: 0.1631 - accuracy: 0.9408\n",
      "Epoch 212/1500\n",
      "92/92 [==============================] - 0s 950us/step - loss: 0.1557 - accuracy: 0.9438\n",
      "Epoch 213/1500\n",
      "92/92 [==============================] - 0s 950us/step - loss: 0.1440 - accuracy: 0.9425\n",
      "Epoch 214/1500\n",
      "92/92 [==============================] - 0s 938us/step - loss: 0.1550 - accuracy: 0.9415\n",
      "Epoch 215/1500\n",
      "92/92 [==============================] - 0s 935us/step - loss: 0.1537 - accuracy: 0.9455\n",
      "Epoch 216/1500\n",
      "92/92 [==============================] - 0s 943us/step - loss: 0.1401 - accuracy: 0.9496\n",
      "Epoch 217/1500\n",
      "92/92 [==============================] - 0s 959us/step - loss: 0.1409 - accuracy: 0.9455\n",
      "Epoch 218/1500\n",
      "92/92 [==============================] - 0s 937us/step - loss: 0.1687 - accuracy: 0.9377\n",
      "Epoch 219/1500\n",
      "92/92 [==============================] - 0s 957us/step - loss: 0.1553 - accuracy: 0.9445\n",
      "Epoch 220/1500\n",
      "92/92 [==============================] - 0s 935us/step - loss: 0.1611 - accuracy: 0.9404\n",
      "Epoch 221/1500\n",
      "92/92 [==============================] - 0s 945us/step - loss: 0.1579 - accuracy: 0.9401\n",
      "Epoch 222/1500\n",
      "92/92 [==============================] - 0s 973us/step - loss: 0.1571 - accuracy: 0.9398\n",
      "Epoch 223/1500\n",
      "92/92 [==============================] - 0s 1000us/step - loss: 0.1406 - accuracy: 0.9496\n",
      "Epoch 224/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.9425\n",
      "Epoch 225/1500\n",
      "92/92 [==============================] - 0s 948us/step - loss: 0.1571 - accuracy: 0.9404\n",
      "Epoch 226/1500\n",
      "92/92 [==============================] - 0s 969us/step - loss: 0.1747 - accuracy: 0.9319\n",
      "Epoch 227/1500\n",
      "92/92 [==============================] - 0s 980us/step - loss: 0.1560 - accuracy: 0.9394\n",
      "Epoch 228/1500\n",
      "92/92 [==============================] - 0s 962us/step - loss: 0.1598 - accuracy: 0.9391\n",
      "Epoch 229/1500\n",
      "92/92 [==============================] - 0s 973us/step - loss: 0.1594 - accuracy: 0.9442\n",
      "Epoch 230/1500\n",
      "92/92 [==============================] - 0s 962us/step - loss: 0.1495 - accuracy: 0.9418\n",
      "Epoch 231/1500\n",
      "92/92 [==============================] - 0s 971us/step - loss: 0.1348 - accuracy: 0.9523\n",
      "Epoch 232/1500\n",
      "92/92 [==============================] - 0s 986us/step - loss: 0.1608 - accuracy: 0.9370\n",
      "Epoch 233/1500\n",
      "92/92 [==============================] - 0s 938us/step - loss: 0.1632 - accuracy: 0.9442\n",
      "Epoch 234/1500\n",
      "92/92 [==============================] - 0s 955us/step - loss: 0.1405 - accuracy: 0.9483\n",
      "Epoch 235/1500\n",
      "92/92 [==============================] - 0s 961us/step - loss: 0.1528 - accuracy: 0.9462\n",
      "Epoch 236/1500\n",
      "92/92 [==============================] - 0s 953us/step - loss: 0.1521 - accuracy: 0.9387\n",
      "Epoch 237/1500\n",
      "92/92 [==============================] - 0s 920us/step - loss: 0.1587 - accuracy: 0.9381\n",
      "Epoch 238/1500\n",
      "92/92 [==============================] - 0s 959us/step - loss: 0.1355 - accuracy: 0.9472\n",
      "Epoch 239/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9500\n",
      "Epoch 240/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9394\n",
      "Epoch 241/1500\n",
      "92/92 [==============================] - 0s 971us/step - loss: 0.1322 - accuracy: 0.9564\n",
      "Epoch 242/1500\n",
      "92/92 [==============================] - 0s 948us/step - loss: 0.1586 - accuracy: 0.9357\n",
      "Epoch 243/1500\n",
      "92/92 [==============================] - 0s 959us/step - loss: 0.1525 - accuracy: 0.9479\n",
      "Epoch 244/1500\n",
      "92/92 [==============================] - 0s 975us/step - loss: 0.1364 - accuracy: 0.9503\n",
      "Epoch 245/1500\n",
      "92/92 [==============================] - 0s 984us/step - loss: 0.1503 - accuracy: 0.9466\n",
      "Epoch 246/1500\n",
      "92/92 [==============================] - 0s 941us/step - loss: 0.1570 - accuracy: 0.9445\n",
      "Epoch 247/1500\n",
      "92/92 [==============================] - 0s 949us/step - loss: 0.1488 - accuracy: 0.9483\n",
      "Epoch 248/1500\n",
      "92/92 [==============================] - 0s 983us/step - loss: 0.1467 - accuracy: 0.9452\n",
      "Epoch 249/1500\n",
      "92/92 [==============================] - 0s 977us/step - loss: 0.1375 - accuracy: 0.9500\n",
      "Epoch 250/1500\n",
      "92/92 [==============================] - 0s 968us/step - loss: 0.1465 - accuracy: 0.9442\n",
      "Epoch 251/1500\n",
      "92/92 [==============================] - 0s 938us/step - loss: 0.1553 - accuracy: 0.9408\n",
      "Epoch 252/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1562 - accuracy: 0.9435\n",
      "Epoch 253/1500\n",
      "92/92 [==============================] - 0s 940us/step - loss: 0.1523 - accuracy: 0.9428\n",
      "Epoch 254/1500\n",
      "92/92 [==============================] - 0s 942us/step - loss: 0.1607 - accuracy: 0.9370\n",
      "Epoch 255/1500\n",
      "92/92 [==============================] - 0s 945us/step - loss: 0.1437 - accuracy: 0.9479\n",
      "Epoch 256/1500\n",
      "92/92 [==============================] - 0s 946us/step - loss: 0.1563 - accuracy: 0.9418\n",
      "Epoch 257/1500\n",
      "92/92 [==============================] - 0s 948us/step - loss: 0.1630 - accuracy: 0.9418\n",
      "Epoch 258/1500\n",
      "92/92 [==============================] - 0s 947us/step - loss: 0.1325 - accuracy: 0.9513\n",
      "Epoch 259/1500\n",
      "92/92 [==============================] - 0s 939us/step - loss: 0.1465 - accuracy: 0.9452\n",
      "Epoch 260/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9483\n",
      "Epoch 261/1500\n",
      "92/92 [==============================] - 0s 980us/step - loss: 0.1690 - accuracy: 0.9374\n",
      "Epoch 262/1500\n",
      "92/92 [==============================] - 0s 966us/step - loss: 0.1576 - accuracy: 0.9408\n",
      "Epoch 263/1500\n",
      "92/92 [==============================] - 0s 979us/step - loss: 0.1409 - accuracy: 0.9486\n",
      "Epoch 264/1500\n",
      "92/92 [==============================] - 0s 962us/step - loss: 0.1386 - accuracy: 0.9466\n",
      "Epoch 265/1500\n",
      "92/92 [==============================] - 0s 996us/step - loss: 0.1443 - accuracy: 0.9500\n",
      "Epoch 266/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9483\n",
      "Epoch 267/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9455\n",
      "Epoch 268/1500\n",
      "92/92 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9561\n",
      "Epoch 269/1500\n",
      "92/92 [==============================] - 0s 957us/step - loss: 0.1482 - accuracy: 0.9486\n",
      "Epoch 270/1500\n",
      "92/92 [==============================] - 0s 947us/step - loss: 0.1400 - accuracy: 0.9479\n",
      "Epoch 271/1500\n",
      "56/92 [=================>............] - ETA: 0s - loss: 0.1295 - accuracy: 0.9559Restoring model weights from the end of the best epoch: 241.\n",
      "92/92 [==============================] - 0s 953us/step - loss: 0.1336 - accuracy: 0.9520\n",
      "Epoch 271: early stopping\n",
      "8/8 [==============================] - 0s 707us/step - loss: 0.5166 - accuracy: 0.7675\n",
      "8/8 [==============================] - 0s 633us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (22/28)\n",
      "Before appending - Cat IDs: 612, Predictions: 612, Actuals: 612, Gender: 612\n",
      "After appending - Cat IDs: 840, Predictions: 840, Actuals: 840, Gender: 840\n",
      "Final Test Results - Loss: 0.5166271328926086, Accuracy: 0.7675438523292542, Precision: 0.7206461749399146, Recall: 0.791312965993817, F1 Score: 0.749636803874092\n",
      "Confusion Matrix:\n",
      " [[121   7  28]\n",
      " [  1  24   0]\n",
      " [ 17   0  30]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6600716995536333\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.7370791584253311\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7178671807050705\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6713215835836874\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6658059057264549\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[4]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90d80b-198d-4293-a1a0-73a65f6588d0",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "e9dd5399-64d4-435b-9e73-cb31f16d042d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 840, Predictions: 840, Actuals: 840, Gender: 840\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9d9869f0-e23f-4453-a329-395fa44f1208",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a7f81185-7b51-497f-98cb-80c4b8f35388",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.74 (81/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "adbd2ca0-3dea-4b42-bfad-93138cb1ae30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b1ea4fe7-6ee1-4185-a009-b864d9aa6b85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, kitten, senior, kitten, adult, senior...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[senior, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, kit...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, adult, adult, adult, adult, kitten, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, adult, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, adult, adult, kitten, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, senior, adult, senior, adult, adult, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, s...         adult            adult                   True\n",
       "69    063A  [senior, kitten, senior, kitten, adult, senior...         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, adult, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "64    058A                            [senior, adult, senior]        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "59    053A        [adult, adult, senior, adult, adult, adult]         adult            adult                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "51    045A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "50    044A            [kitten, adult, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A              [adult, adult, senior, senior, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [adult, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, senior, adult, senior, senior, ...         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "17    015A  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "31    026A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "29    025B                                    [kitten, adult]         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, senior, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                     [senior, adult, adult, senior]         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, kit...         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "7     006A                             [senior, adult, adult]         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, senior, adult, adult, adult, se...         adult            adult                   True\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "5     004A                                           [kitten]        kitten            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, adult, senior...        senior            adult                  False\n",
       "102   108A         [adult, adult, adult, adult, adult, adult]         adult           senior                  False\n",
       "101   106A  [adult, senior, senior, senior, adult, adult, ...         adult           senior                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "103   109A        [adult, adult, kitten, adult, adult, adult]         adult           kitten                  False\n",
       "47    041A                                    [adult, kitten]         adult           kitten                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "52    047A  [kitten, adult, adult, adult, adult, kitten, k...         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "54    049A                                            [adult]         adult           kitten                  False\n",
       "42    036A  [senior, senior, senior, senior, adult, adult,...        senior            adult                  False\n",
       "57    051B  [senior, adult, adult, adult, senior, adult, a...         adult           senior                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "40    034A            [adult, senior, senior, senior, senior]        senior            adult                  False\n",
       "61    055A  [adult, adult, senior, adult, adult, senior, s...         adult           senior                  False\n",
       "18    016A  [senior, adult, adult, senior, adult, senior, ...         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, senior, s...         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "30    025C             [senior, senior, adult, senior, adult]        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "48    042A  [kitten, adult, adult, kitten, adult, adult, a...         adult           kitten                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e03366c5-a807-4159-b0c1-8dc88c04d91e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     62\n",
      "kitten     9\n",
      "senior    10\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0ae8f86b-0e19-49df-a07c-f64383bce76b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             62  84.931507\n",
      "1           kitten           15              9  60.000000\n",
      "2           senior           22             10  45.454545\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6350c1b2-050d-4bf3-98f1-5a80b3078d3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABngElEQVR4nO3dd3iN9//H8edJRCJDEiGIvTVVe6RG7VmrpapTqVVb1Vdrt/j22xqtUaWUoqqltVdpqZlQI0pFzBBii8hCxvn9kSv3L0cSskg4r8d1ua6c+77Pfb/v49znvM7n/tyf22Q2m82IiIiIiFgJm+wuQERERETkSVIAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4g8xWJjY7O7hCz3LO6TiOQsubK7AJG0io6OplWrVkRGRgJQoUIFli5dms1VSWacOXOGb775hiNHjhAZGUm+fPlo2LAhI0aMSPU5NWvWtHicN29e/vjjD2xsLH/Pf/HFF6xYscJi2rhx42jXrl2Gaj1w4AB9+/YFoHDhwqxbty5D60mP8ePHs379egB69epFnz59LOZv2bKFFStWMG/evCzd7v3792nZsiXh4eEAvPfeewwYMCDV5du2bcuVK1cA6Nmzp/E6pVd4eDjfffcdbm5uvP/++xlaR1Zbt24dn376KQDVq1fnu+++y9Z6Pv30U4v33rJlyyhXrlw2VpR2YWFhbNiwge3bt3Pp0iVCQ0PJlSsXBQoUoFKlSrRt25batWtnd5liJdQCLE+NrVu3GuEXIDAwkH///TcbK5LMiImJoV+/fuzcuZOwsDBiY2O5du0aV69eTdd67ty5Q0BAQLLp+/fvz6pSc5wbN27Qq1cvRo4caQTPrJQ7d26aNm1qPN66dWuqyx47dsyihtatW2dom9u3b+fVV19l2bJlagFORWRkJH/88YfFtJUrV2ZTNemze/duunTpwrRp0zh8+DDXrl0jJiaG6OhoLly4wMaNG+nXrx8jR47k/v372V2uWAG1AMtTY82aNcmmrVq1iueffz4bqpHMOnPmDDdv3jQet27dGjc3NypXrpzude3fv9/ifXDt2jXOnz+fJXUmKlSoEN26dQPAxcUlS9edmvr16+Ph4QFA1apVjelBQUEcPnz4sW67VatWrF69GoBLly7x77//pnis/fnnn8bf3t7elChRIkPb27FjB6GhoRl6rrXYunUr0dHRFtM2bdrE4MGDcXBwyKaqHm3btm385z//MR47OjpSp04dChcuzO3bt9m3b5/xWbBlyxacnJwYNWpUdpUrVkIBWJ4KQUFBHDlyBEg45X3nzh0g4cNy6NChODk5ZWd5kgFJW/M9PT2ZMGFCutfh4ODA3bt32b9/P927dzemJ239zZMnT7LQkBFFixZl4MCBmV5PejRr1oxmzZo90W0mqlGjBgULFjRa5Ldu3ZpiAN62bZvxd6tWrZ5YfdYoaSNA4udgREQEW7ZsoX379tlYWeouXrxodCEBqF27NpMmTcLd3d2Ydv/+fSZMmMCmTZsAWL16NW+//XaGf0yJpIUCsDwVkn7wv/baa/j5+fHvv/8SFRXF5s2b6dSpU6rPPXHiBEuWLOHQoUPcvn2bfPnyUaZMGbp27UrdunWTLR8REcHSpUvZvn07Fy9exM7ODi8vL1q0aMFrr72Go6OjsezD+mg+rM9oYj9WDw8P5s2bx/jx4wkICCBv3rz85z//oWnTpty/f5+lS5eydetWgoODuXfvHk5OTpQqVYpOnTrx8ssvZ7j2Hj168M8//wAwZMgQ3n77bYv1LFu2jKlTpwIJrZBff/11qq9votjYWNatW8fGjRs5d+4c0dHRFCxYkHr16vHOO+/g6elpLNuuXTsuX75sPL527ZrxmqxduxYvL69Hbg+gcuXK7N+/n3/++Yd79+5hb28PwN9//20sU6VKFfz8/FJ8/o0bN/j+++/x9fXl2rVrxMXF4ebmhre3N927d7dojU5LH+AtW7awdu1aTp06RXh4OB4eHtSuXZt33nmHkiVLWiw7d+5co+/uxx9/zJ07d/jpp5+Ijo7G29vbeF88+P5KOg3g8uXL1KxZk8KFCzNq1Cijr66rqyu///47uXL9/8d8bGwsrVq14vbt2wAsXrwYb2/vFF8bk8lEy5YtWbx4MZAQgAcPHozJZDKWCQgI4NKlSwDY2trSokULY97t27dZsWIF27ZtIyQkBLPZTIkSJWjevDldunSxaLF8sF/3vHnzmDdvXrJj6o8//mD58uUEBgYSFxdHsWLFaN68OW+++WayFtCoqCiWLFnCjh07CA4O5v79+zg7O1OuXDk6dOiQ4a4aN27cYMaMGezevZuYmBgqVKhAt27daNCgAQDx8fG0a9fO+OHwxRdfWHQnAZg6dSrLli0DEj7PHtbnPdGZM2c4evQo8P9nI7744gsg4UzYwwLwxYsXmTNnDn5+fkRHR1OxYkV69eqFg4MDPXv2BBL6cY8fP97ieel5vVOzaNEi48du4cKFmTJlisVnKCR0uRk1ahS3bt3C09OTMmXKYGdnZ8xPy7GS6OjRoyxfvhx/f39u3LiBi4sLlSpVokuXLvj4+Fhs91HHdNLPqTlz5hjv06TH4FdffYWLiwvfffcdx44dw87Ojtq1a9O/f3+KFi2aptdIsocCsOR4sbGxbNiwwXjcrl07ChUqZPT/XbVqVaoBeP369UyYMIG4uDhj2tWrV7l69Sp79+5lwIABvPfee8a8K1eu8MEHHxAcHGxMu3v3LoGBgQQGBvLnn38yZ86cZB/gGXX37l0GDBhASEgIADdv3qR8+fLEx8czatQotm/fbrF8eHg4//zzD//88w8XL160CAfpqb19+/ZGAN6yZUuyAJy0z2fbtm0fuR+3b99m2LBhRit9ogsXLnDhwgXWr1/P5MmTkwWdzKpRowb79+/n3r17HD582PiCO3DgAADFixcnf/78KT43NDSU3r17c+HCBYvpN2/eZNeuXezdu5cZM2ZQp06dR9Zx7949Ro4cyY4dOyymX758mTVr1rBp0ybGjRtHy5YtU3z+ypUrOXnypPG4UKFCj9xmSmrXrk2hQoW4cuUKYWFh+Pn5Ub9+fWP+gQMHjPBbunTpVMNvotatWxsB+OrVq/zzzz9UqVLFmJ+0+0OtWrWM1zogIIBhw4Zx7do1i/UFBAQQEBDA+vXrmTlzJgULFkzzvqV0UeOpU6c4deoUf/zxB99++y2urq5Awvu+Z8+eFq8pJFyEdeDAAQ4cOMDFixfp1atXmrcPCe+Nbt26WfRT9/f3x9/fnw8//JA333wTGxsb2rZty/fffw8kHF9JA7DZbLZ43dJ6UWbSRoC2bdvSunVrvv76a+7du8fRo0c5ffo0ZcuWTfa8EydO8MEHHxgXNAIcOXKEgQMH8sorr6S6vfS83qmJj4+3OEPQqVOnVD87HRwc+Oabbx66Pnj4sbJgwQLmzJlDfHy8Me3WrVvs3LmTnTt38sYbbzBs2LBHbiM9du7cydq1ay2+Y7Zu3cq+ffuYM2cO5cuXz9LtSdbRRXCS4+3atYtbt24BUK1aNYoWLUqLFi3IkycPkPABn9JFUGfPnmXSpEnGB1O5cuV47bXXLFoBZs2aRWBgoPF41KhRRoB0dnambdu2dOjQwehicfz4cb799tss27fIyEhCQkJo0KABr7zyCnXq1KFYsWLs3r3bCL9OTk506NCBrl27WnyY/vTTT5jN5gzV3qJFC+OL6Pjx41y8eNFYz5UrV4yWprx58/LSSy89cj8+/fRTI/zmypWLxo0b88orrxgBJzw8nI8++sjYTqdOnSzCoJOTE926daNbt244Ozun+fWrUaOG8Xdiq+/58+eNgJJ0/oN++OEHI/wWKVKErl278uqrrxohLi4ujp9//jlNdcyYMcMIvyaTibp169KpUyfjFO79+/cZN26c8bo+6OTJk+TPn58uXbpQvXr1VIMyJLTIp/TaderUCRsbG4tAtWXLFovnpveHTbly5ShTpkyKz4eUuz+Eh4czfPhwI/y6ubnRrl07WrZsabznzp49y4cffmhc7NatWzeL7VSpUoVu3boZ/Z43bNhghDGTycRLL71Ep06djLMKJ0+e5MsvvzSev3HjRiMkubu70759e958802LEQbmzZtn8b5Pi8T3Vv369Xn11VctAvz06dMJCgoCEkJtYkv57t27iYqKMpY7cuSI8dqk5UcIJFwwunHjRmP/27Zti7Ozs0WwTuliuPj4eMaMGWOEX3t7e1q3bk2bNm1wdHRM9QK69L7eqQkJCSEsLMx4nLQfe0aldqxs27aN2bNnG+G3YsWKvPbaa1SvXt147rJly/jxxx8zXUNSq1atws7OjtatW9O6dWvjLNSdO3cYPXq0xWe05CxqAZYcL2nLR+KXu5OTE82aNTNOWa1cuTLZRRPLli0jJiYGgEaNGvG///3POB08ceJEVq9ejZOTE/v376dChQocOXLECHFOTk78+OOPximsdu3a0bNnT2xtbfn333+Jj49PNuxWRjVu3JjJkydbTMudOzcdO3bk1KlT9O3blxdffBFIaNlq3rw50dHRREZGcvv2bdzd3dNdu6OjI82aNWPt2rVAQlDq0aMHkHDaM/FDu0WLFuTOnfuh9R85coRdu3YBCafBv/32W6pVqwYkdMno168fx48fJyIigvnz5zN+/Hjee+89Dhw4wO+//w4kBO2M9K+tVKmSRT9gsOz+UKNGjVS7PxQrVoyWLVty4cIFpk+fTr58+YCEVs/ElsHE0/sPc+XKFYuWsgkTJhhh8P79+4wYMYJdu3YRGxvLzJkzUx1Ga+bMmWkazqpZs2a4ubml+tq1b9+e+fPnYzab2bFjh9E1JDY2lr/++gtI+H9q06bNI7cFCa/HrFmzgIT3xocffoiNjQ0nT540fkDY29vTuHFjAFasWGGMCuHl5cWCBQuMHxVBQUF069aNyMhIAgMD2bRpE+3atWPgwIHcvHmTM2fOAAkt2UnPbixatMj4++OPPzbO+PTv35+uXbty7do1tm7dysCBAylUqJDF/1v//v3p2LGj8fibb77hypUrlCpVyqLVLq3+85//0KVLFyAh5PTo0YOgoCDi4uJYs2YNgwcPpmjRotSsWZO///6be/fusXPnTuM9kfRHRErdmFKyY8cOo+U+sREAoEOHDkYw3rRpE4MGDbLomnDgwAHOnTsHJPyff/fdd0Y/7qCgIN566y3u3buXbHvpfb1Tk/QiV8A4xhLt27eP/v37p/jclLpkJErpWEl8j0LCD+wRI0YYn9ELFy40WpfnzZtHx44d0/VD+2FsbW2ZP38+FStWBKBz58707NkTs9nM2bNn2b9/f5rOIsmTpxZgydGuXbuGr68vkHAxU9ILgjp06GD8vWXLFotWFvj/0+AAXbp0segL2b9/f1avXs1ff/3FO++8k2z5l156yaL/VtWqVfnxxx/ZuXMnCxYsyLLwC6TY2ufj48Po0aNZtGgRL774Ivfu3cPf358lS5ZYtCgkfnllpPYHX79ESYdZSksrYdLlW7RoYYRfSGiJTjp+7I4dOyxOT2ZWrly5jH66gYGBhIWFWVwA97AuF507d2bSpEksWbKEfPnyERYWxu7duy2626QUDh60bds2Y5+qVq1qcSFY7ty5LU65Hj582AgySZUuXTrLxnItXLiw0dIZGRnJnj17gIQLAxNb4+rUqZNq15AHtWrVymjNvHHjBocOHQIsuz+89NJLxpmGpO+HHj16WGynZMmSdO3a1Xj8YBeflNy4cYOzZ88CYGdnZxFm8+bNS8OGDYGE1s7EHz+JYQRg8uTJfPTRR/zyyy9Gd4AJEybQo0ePdF9k5erqatHdKm/evLz66qvG42PHjhl/Jz2+En+sJO0SYGtrm+YA/GD3h0TVq1enWLFiQELL+4NDpCXtkvTiiy9aXMRYsmTJFH8EZeT1Tk1ia2iijPzgeFBKx0pgYKDxY8zBwYFBgwZZfEa/++67FC5cGEg4Jh5Vd3o0btzY4v1WpUoVo8ECSNYtTHIOtQBLjrZu3TrjQ9PW1paPPvrIYr7JZMJsNhMZGcnvv/9u0actaf/DxA+/RO7u7hZXIT9qebD8Uk2LtJ76SmlbkNCyuHLlSvz8/IyLUB6UGLwyUnuVKlUoWbIkQUFBnD59mnPnzpEnTx7jS7xkyZJUqlTpkfUn7XOc0naSTgsPDycsLCzZa58Zif2AE7+QDx48CECJEiUeGfKOHTvGmjVrOHjwYLK+wECawvqj9r9o0aI4OTkRGRmJ2Wzm0qVLuLm5WSyT2nsgozp06MC+ffuAhBbHJk2apLv7Q6JChQpRrVo1I/hu3bqVmjVrWnR/SBqk0vN+SEsXhKRjDMfExDy0NS2xtbNZs2bGj5l79+7x119/Ga3fefPmpVGjRrzzzjuUKlXqkdtPqkiRItja2lpMS3pxY9IWz8aNG+Pi4kJ4eDh+fn6Eh4dz6tQprl+/DqT9R8iVK1eM/0tIGCFh8+bNxuO7d+8af69cudLi/zZxW0CKYT+l/c/I652aB/t4X7161WKbXl5extCCkNBdJPEsQGpSOlaSvueKFSuWbFQgW1tbypUrZ1zQlnT5h0nL8Z/S61qyZEn27t0LJG8Fl5xDAVhyLLPZbJyih4TT6Q+7ucGqVatSvagjvS0PGWmpeDDwJna/eJSUhnBLvEglKioKk8lE1apVqV69OpUrV2bixIkWX2wPSk/tHTp0YPr06UBCK3DSC1TSGpKStqyn5MHXJekoAlkhaT/fH3/80WjlfFj/X0joIjNt2jTMZjMODg40bNiQqlWrUqhQIT755JM0b/9R+/+glPY/q4fxa9SoEa6uroSFhbFr1y7u3Llj9FF2cXExWvHSqlWrVkYA3rZtG506dTLCj6urq0WLV3rfD4+SNITY2Ng89MdT4rpNJhOffvopr7zyCps2bcLX19e40PTOnTusXbuWTZs2MWfOHIuL+h4lpRt0JD3eku67vb09rVq1YsWKFcTExLB9+3aLaxXS2vq7bt06i9cg8eLVlPzzzz+cOXPG6E+d9LVO65mXjLzeqXF3d6dIkSJGl5QDBw5YXINRrFgxi+47SbvBpCalYyUtx2DSWlM6BlN6fdJyQ5aUbtqRdASLrP68k6yjACw51sGDB9PUBzPR8ePHCQwMpEKFCkDC2LKJv/SDgoIsWmouXLjAb7/9RunSpalQoQIVK1a0GKYrpZsofPvtt7i4uFCmTBmqVauGg4ODxWm2pC0xQIqnulOS9MMy0bRp04wuHUn7lELKH8oZqR0SvoS/+eYbYmNjjQHoIeGLL619RJO2yCS9oDClaXnz5n3klePp9fzzzxv9gJOegn5YAL5z5w4zZ87EbDZjZ2fH8uXLjaHXEk//ptWj9v/ixYvGMFA2NjYUKVIk2TIpvQcyI3fu3LRu3Zqff/6Zu3fvMnnyZGPs7ObNmyc7Nf0ozZo1Y/LkycTExBAaGmpxAVTz5s0tAkjhwoWNi64CAwOTtQInfY2KFy/+yG0nfW/b2dmxadMmi+MuLi4uWatsopIlSzJ8+HBy5crFlStX8Pf359dff8Xf35+YmBjmz5/PzJkzH1lDoosXL3L37l2LfrZJzxw82KLboUMHo3/45s2bjXDn7OxMo0aNHrk9s9mc7ltur1q1yjhTVqBAgRTrTHT69Olk0zLzeqekVatWxogYieP7PngGJFFaQnpKx0rSYzA4OJjIyEiLoBwXF2exr4ndRpLux4Of3/Hx8cYx8zApvYZJX+uk/weSs6gPsORYiXehAujatasxfNGD/5Je2Z30quakAWj58uUWLbLLly9n6dKlTJgwwfhwTrq8r6+vRUvEiRMn+P777/n6668ZMmSI8as/b968xjIPBqekfSQfJqUWglOnThl/J/2y8PX1tbhbVuIXRkZqh4SLUhLHLz1//jzHjx8HEi5CSvpF+DBJR4n4/fff8ff3Nx5HRkZaDG3UqFGjLG8RsbOzS/HucQ8LwOfPnzdeB1tbW4s7uyVeVARp+0JOuv+HDx+26GoQExPDV199ZVFTSj8A0vuaJP3iTq2VKmkf1MQbDED6uj8kyps3L/Xq1TMeJ/0/fvDmF0lfjwULFnDjxg3j8fnz5/nll1+Mx4kXzgEWISvpPhUqVMj40XDv3j1+++03Y150dDQdO3akQ4cODB061AgjY8aMoUWLFjRr1sz4TChUqBCtWrWic+fOxvPTe9vtxLGFE0VERFhcAPngKAcVK1Y0fpDv37/fOB2e1h8h+/btM1quXV1d8fPzS/EzMOlNZDZu3Gj0XU/aH9/X19c4viFhNIWkXSkSZeT1fpguXboYn2G3b99m6NChyYbHu3//PgsXLkw2aklKUjpWypcvb4Tgu3fvMmvWLIsW3yVLlhjdH5ydnalVqxZgeUfHO3fuWLxXd+zYkaazeIn/J4lOnz5tdH8Ay/8DyVnUAiw5Unh4uMUFMg+7G1bLli2NrhGbN29myJAh5MmTh65du7J+/XpiY2PZv38/b7zxBrVq1eLSpUsWH1Cvv/46kPDlVblyZeOmCt27d6dhw4Y4ODhYhJo2bdoYwTfpxRh79+7l888/p0KFCuzYscO4+Cgj8ufPb3zxjRw5khYtWnDz5k127txpsVziF11Gak/UoUOHZBcjpSck1ahRg2rVqnH48GHi4uLo27cvL730Eq6urvj6+hp9Cl1cXNI97mpaVa9e3aJ7zKP6/yadd/fuXbp3706dOnUICAiwOMWclovgihYtSuvWrY2QOXLkSNavX0/hwoU5cOCAMTSWnZ2dxQWBmZG0dev69euMGzcOwOKOW+XKlcPb29si9BQvXjxDt5qGhKCb2I82UZEiRZKFvs6dO/Pbb78RGhrKpUuXeOONN6hfvz6xsbHs2LHDOLPh7e1tEZ6T7tPatWuJiIigXLlyvPrqq7z55pvGSClffPEFu3btonjx4uzbt88INrGxsUZ/zLJlyxr/H1OnTsXX15dixYoZY8ImSk/3h0Rz587ln3/+oWjRouzdu9c4S2Vvb5/izSg6dOiQbMiwtB5fSS9+a9SoUaqn+hs2bIi9vT337t3jzp07/PHHH7z88svUqFGD0qVLc/bsWeLj4+nduzdNmjTBbDazffv2FE/fA+l+vR/Gw8OD0aNHM2LECOLi4jh69CivvPIKdevWpXDhwoSGhuLr65vsjFl6ugWZTCbef/99Jk6cCCSMRHLs2DEqVarEmTNnjO47AH369DHWXbx4ceN1M5vNDBkyhFdeeYWQkJA0D4FoNpsZOHAgjRo1wsHBgW3bthmfG+XLl7cYhk1yFrUAS460adMm40OkQIECD/2iatKkiXFaLPFiOEj4Evzkk0+M1rKgoCBWrFhhEX67d+9uMVLAxIkTjdaPqKgoNm3axKpVq4iIiAASrkAeMmSIxbaTntL+7bff+O9//8uePXt47bXXMrz/iSNTQELLxK+//sr27duJi4uzGL4n6cUc6a090Ysvvmhxms7JySlNp2cT2djY8Pnnn/Pcc88BCV+M27ZtY9WqVUb4zZs3L1OnTs3yi70SPTjaw6P6/xYuXNjiR1VQUBC//PIL//zzD7ly5TJOcYeFhaXpNOgnn3xi9G00m83s2bOHX3/91Qi/9vb2TJgwIcVbCWdEqVKlLFqSN2zYwKZNm5K1Bj8YyDLS+puoQYMGyUJJSiOY5M+fny+//BIPDw8g4YYj69atY9OmTUb4LVu2LFOmTLFoyU4apG/evMmKFSuMK+hfe+01i23t3buXn3/+2eiH7OzszBdffGF8Drz99ts0b94cSDj9vWvXLn766Sc2b95s1FCyZEn69euXrtegefPmeHh44Ovry4oVK4zwa2Njw8cff5zikGBJx4aFhNCVluAdFhZmcWOVhzUCODo6WrS8r1q1yqhrwoQJxv/b3bt32bhxI5s2bSI+Pt54jcCyZTW9r/ejNGrUiG+++cZ4T9y7d4/t27fz008/sWnTJovw6+LiQp8+fRg6dGia1p2oY8eOvPfee8Z+BAQEsGLFCovw+9Zbb/HGG28Yj3Pnzm00gEDC2bLPP/+cRYsWUbBgQYuzi6mpWbMmNjY2bN26lXXr1hndnVxdXTN0e3d5chSAJUdK2vLRpEmTh54idnFxsbilceKHPyS0vixcuND44rK1tSVv3rzUqVOHKVOmJBuD0svLiyVLltCjRw9KlSqFvb099vb2lClTht69e7No0SKL4JEnTx7mz59P69atcXNzw8HBgUqVKjFx4sQUw2Zavfbaa/zvf//D29sbR0dH8uTJQ6VKlZgwYYLFepN2s0hv7YlsbW0tglmzZs3SfJvTRPnz52fhwoV88sknVK9eHVdXV3Lnzk2xYsV44403+OWXXx5rS0hiP+BEjwrAAJ999hn9+vWjZMmS5M6dG1dXV+rXr8/8+fONU/Nms9kY7eDBi4OScnR0ZObMmUycOJG6devi4eGBnZ0dhQoVokOHDvz0008PDTDpZWdnx+TJk/H29sbOzo68efNSs2bNZC3WSVt7TSZTmvt1p8Te3p4mTZpYTEvtdsLVqlXj559/plevXpQvX954Dz/33HMMHjyYH374IVkXmyZNmtCnTx88PT3JlSsXBQsWNFoYbWxsmDhxIhMmTKBWrVoW769XX32VpUuXWoxYYmtry6RJk/jyyy/x8fGhcOHC5MqVCycnJ5577jn69u3L4sWL0z0aiZeXF0uXLqVdu3bG8V69enVmzZqV6h3dXFxcLFpK0/p/sGnTJqOF1tXV1Thtn5qkgdXf398IqxUqVGDRokU0btyYvHnzkidPHurUqcOCBQssgnjijYUg/a93WtSsWZPffvuNYcOGUbt2bfLly4etrS1OTk4UL16cVq1aMX78eDZu3EivXr3SfXEpwIABA5g/fz5t2rShcOHC2NnZ4e7uzksvvcTs2bNTDNUDBw5kyJAhlChRgty5c1O4cGHeeecdFi9enKbrFapVq8b3339PrVq1cHBwwNXV1biFeNKbu0jOYzLrNiUiVu3ChQt07drV+LKdO3dumgKktfnhhx+MwfbLlClj0Zc1p/rss8+MkVRq1KjB3Llzs7ki63Po0CF69+4NJPwIWbNmjXHB5eN25coVNm3ahJubG66urlSrVs0i9H/66afGRXZDhgxJdkt0Sdn48eNZv349AL169bK4aYs8PdQHWMQKXb58meXLlxMXF8fmzZuN8FumTBmF3wds3ryZyZMnW9zS9XF15cgKv/76K9euXePEiRMW3X0y0yVH0ufEiRNs3bqVqKgoixur1KtX74mFX0g4g5H0ItRixYpRt25dbGxsOH36tHFDCJPJRP369Z9YXSI5QY4NwFevXuX1119nypQpFv37goODmTZtGocPH8bW1pZmzZoxcOBAi36RUVFRzJw5k23bthEVFUW1atX48MMPLYbBErFmJpPJ4mp2SDitPnz48GyqKOf6999/LcIvJNzxLqc6fvy4xfjZkHBnwaZNm2ZTRdYnOjra4nbCkNBvdvDgwU+0jsKFC/PKK68Y3cKCg4NTPHPx5ptv6vtRrE6ODMBXrlxh4MCBxsU7icLDw+nbty8eHh6MHz+e0NBQZsyYQUhIiMVYjqNGjeLYsWMMGjQIJycn5s2bR9++fVm+fHmyK+BFrFGBAgUoVqwY165dw8HBgQoVKtCjR4+H3jrYmrm6uhIVFYWXlxevv/56pvrSPm7ly5fHzc2N6OhoChQoQLNmzejZs6cG5H+CvLy8KFSoELdu3cLFxYVKlSrRu3fvdN95LiuMHDmSKlWq8Pvvv3Pq1CnjgjNXV1cqVKhAx44dk/XtFrEGOaoPcHx8PBs2bODrr78GEq6CnTNnjvGlvHDhQr7//nvWr19vjCu4Z88eBg8ezPz586latSr//PMPPXr0YPr06ca4laGhobRv35733nuP999/Pzt2TURERERyiBw1CsSpU6f4/PPPefnlly3Gs0zk6+tLtWrVLG4M4OPjg5OTkzHmqq+vL3ny5LG43aK7uzvVq1fP1LisIiIiIvJsyFEBuFChQqxatYoPP/wwxWGYgoKCkt0609bWFi8vL+P2r0FBQRQpUiTZrRqLFSuW4i1iRURERMS65Kg+wK6urg8ddy8iIiLFu8M4Ojoag0+nZZn0CgwMNJ6b1oG/RUREROTJiomJwWQyPfI21DkqAD9K0oHoH5Q4MH1alsmIxK7Sqd06UkRERESeDk9VAHZ2djZuY5lUZGSkcVchZ2dnbt26leIySYdKS48KFSpw9OhRzGYzZcuWzdA6REREROTxOn36dJpGvXmqAnCJEiUIDg62mBYXF0dISIhx69ISJUrg5+dHfHy8RYtvcHBwpsc5NJlMODo6ZmodIiIiIvJ4pHXIxxx1Edyj+Pj4cOjQIUJDQ41pfn5+REVFGaM++Pj4EBkZia+vr7FMaGgohw8fthgZQkRERESs01MVgDt37oy9vT39+/dn+/btrF69mjFjxlC3bl2qVKkCQPXq1alRowZjxoxh9erVbN++nX79+uHi4kLnzp2zeQ9EREREJLs9VV0g3N3dmTNnDtOmTWP06NE4OTnRtGlThgwZYrHc5MmT+eqrr5g+fTrx8fFUqVKFzz//XHeBExEREZGcdSe4nOzo0aMAvPDCC9lciYiIiIikJK157anqAiEiIiIiklkKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQCWHGXVqlV06dKF+vXr07lzZ5YvX47ZbE5x2WXLllGzZk1CQkIeud5169bRpUsX6tatS4cOHZg3bx6xsbFZXb6IiIg8BXJldwEiiVavXs2kSZN4/fXXadiwIYcPH2by5Mncv3+ft99+22LZ8+fPM2vWrDStd9myZUydOpWmTZsyePBgQkNDmTt3LidPnmTy5MmPY1dEREQkB1MAlhxj7dq1VK1aleHDhwNQu3Ztzp8/z/Llyy0CcFxcHJ9++ilubm5cvXr1oeuMi4tj/vz51KlThy+++MKYXrFiRbp27Yqfnx8+Pj6PZ4dEREQkR1IXCMkx7t27h5OTk8U0V1dXwsLCLKYtWbKEmzdv8t577z1ynbdu3SIsLIwGDRpYTC9btixubm7s2bMn03WLiIjI00UBWHKMN954Az8/PzZu3EhERAS+vr5s2LCBNm3aGMucOXOGefPmMXbsWBwcHB65ThcXF2xtbbl8+bLF9Dt37hAeHs7FixezfD9EREQkZ1MXCMkxWrZsycGDBxk7dqwx7cUXX2TYsGEAxMbGMm7cODp06ECNGjXSdPGbg4MDLVq0YPny5ZQuXZrGjRtz69Ytpk6diq2tLXfv3n1s+yMiIiI5kwKw5BjDhg3D39+fQYMG8fzzz3P69Gm+++47RowYwZQpU1iwYAHh4eEMHDgwXev95JNPsLOzY+LEiUyYMAF7e3vee+89IiMj09SKLCIiIs8WBWDJEY4cOcLevXsZPXo0HTt2BKBGjRoUKVKEIUOG8P3337Nw4UKmT5+OnZ0dsbGxxMfHAxAfH09cXBy2trYprtvR0ZGxY8fy0UcfcfnyZQoXLoyjoyOrV6+mWLFiT2oXRUREJIdQAJYcIbGPbpUqVSymV69eHYCFCxcSExNDv379kj23Y8eOVK9ene+++y7Fde/atQsXFxeqVq1KmTJlgISL465du0bFihWzcjdERETkKaAALDlCyZIlATh8+DClSpUyph85cgRI6MZQunRpi+fs2rWLefPmMW3aNIoXL57qun/77TfCwsJYuHChMW3ZsmXY2NgkGx1CREREnn0KwJIjVKxYkSZNmvDVV19x584dKlWqxNmzZ/nuu+947rnnaNWqFblyWb5dz5w5AyQMaebl5WVMP3r0KO7u7hQtWhSArl27MmDAAKZOnUrDhg3Zv38/CxcupFu3bsYyIiIiYj0UgCXHmDRpEt9//z0rV65k7ty5FCpUiHbt2tGrV69k4fdhunfvTtu2bRk/fjwAPj4+TJw4kQULFrBy5UoKFy7MRx99RNeuXR/TnoiIiEhOZjKbzebsLuJpcPToUQBeeOGFbK5ERERERFKS1rymG2GIiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFICtVLyGf87R9P8jIiLy+OhOcFbKxmTiZ7+TXLsTld2lyAM88zrS1ad8dpchIiLyzFIAtmLX7kQREhqZ3WWIiIiIPFHqAiEiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqPJXDoK1atYply5YREhJCoUKF6NKlC6+99homkwmA4OBgpk2bxuHDh7G1taVZs2YMHDgQZ2fnbK5cRERERLLbUxeAV69ezaRJk3j99ddp2LAhhw8fZvLkydy/f5+3336b8PBw+vbti4eHB+PHjyc0NJQZM2YQEhLCzJkzs7t8EREREclmT10AXrt2LVWrVmX48OEA1K5dm/Pnz7N8+XLefvttfv31V8LCwli6dClubm4AeHp6MnjwYPz9/alatWr2FS8iIiIi2e6p6wN87949nJycLKa5uroSFhYGgK+vL9WqVTPCL4CPjw9OTk7s2bPnSZYqIiIiIjnQUxeA33jjDfz8/Ni4cSMRERH4+vqyYcMG2rRpA0BQUBDFixe3eI6trS1eXl6cP38+O0oWERERkRzkqesC0bJlSw4ePMjYsWONaS+++CLDhg0DICIiIlkLMYCjoyORkZGZ2rbZbCYqKipT68gJTCYTefLkye4y5BGio6Mxm83ZXYaIiMhTw2w2G4MiPMxTF4CHDRuGv78/gwYN4vnnn+f06dN89913jBgxgilTphAfH5/qc21sMtfgHRMTQ0BAQKbWkRPkyZMHb2/v7C5DHuHcuXNER0dndxkiIiJPldy5cz9ymacqAB85coS9e/cyevRoOnbsCECNGjUoUqQIQ4YMYffu3Tg7O6fYShsZGYmnp2emtm9nZ0fZsmUztY6cIC2/jCT7lSpVSi3AIiIi6XD69Ok0LfdUBeDLly8DUKVKFYvp1atXB+DMmTOUKFGC4OBgi/lxcXGEhITQuHHjTG3fZDLh6OiYqXWIpJW6qYiIiKRPWhv5nqqL4EqWLAnA4cOHLaYfOXIEgKJFi+Lj48OhQ4cIDQ015vv5+REVFYWPj88Tq1VEREREcqanqgW4YsWKNGnShK+++oo7d+5QqVIlzp49y3fffcdzzz1Ho0aNqFGjBr/88gv9+/enV69ehIWFMWPGDOrWrZus5VhERERErI/J/JR1MoyJieH7779n48aNXL9+nUKFCtGoUSN69epldE84ffo006ZN48iRIzg5OdGwYUOGDBmS4ugQaXX06FEAXnjhhSzZj5xgxhZ/QkIzNzKGZD0vdycGtaia3WWIiIg8ddKa156qFmBIuBCtb9++9O3bN9VlypYty+zZs59gVSIiIiLytHiq+gCLiIiIiGSWArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKxKrsw8+eLFi1y9epXQ0FBy5cqFm5sbpUuXJm/evFlVn4iIiIhIlkp3AD527BirVq3Cz8+P69evp7hM8eLFadCgAe3ataN06dKZLlJEREREJKukOQD7+/szY8YMjh07BoDZbE512fPnz3PhwgWWLl1K1apVGTJkCN7e3pmvVkREREQkk9IUgCdNmsTatWuJj48HoGTJkrzwwguUK1eOAgUK4OTkBMCdO3e4fv06p06d4sSJE5w9e5bDhw/TvXt32rRpw7hx4x7fnoiIiIiIpEGaAvDq1avx9PTk1VdfpVmzZpQoUSJNK7958yZ//PEHK1euZMOGDQrAIiIiIpLt0hSAv/zySxo2bIiNTfoGjfDw8OD111/n9ddfx8/PL0MFioiIiIhkpTQF4MaNG2d6Qz4+Ppleh4iIiIhIZmVqGDSAiIgIvv32W3bv3s3Nmzfx9PSkVatWdO/eHTs7u6yoUUREREQky2Q6AH/22Wds377deBwcHMz8+fOJjo5m8ODBmV29iIiIiEiWylQAjomJYceOHTRp0oR33nkHNzc3IiIiWLNmDb///rsCsIiIiIjkOGm6qm3SpEncuHEj2fR79+4RHx9P6dKlef755ylatCgVK1bk+eef5969e1lerIiIpOzo0aP06dOH+vXr06JFC8aNG8etW7eM+cHBwQwdOpRGjRrRtGlTPv/8cyIiIh653uPHj9O7d28aNGhAq1at+Oabb4iJiXmcuyIi8tileRi0TZs20aVLF9577z3jVsfOzs6UK1eO77//nqVLl+Li4kJUVBSRkZE0bNjwsRYuIiIJAgIC6Nu3L7Vr12bKlClcv36dWbNmERwczIIFCwgPD6dv3754eHgwfvx4QkNDmTFjBiEhIcycOTPV9V68eJF+/fpRuXJlPv/8c4KCgpg9ezZhYWGMHDnyCe6hiEjWSlMA/vTTT5k7dy5Llixh1apVvPvuu7zxxhs4ODjw6aefMmrUKM6dO0d0dDQAVapUYfjw4Y+1cBERSTBjxgwqVKjA1KlTjeEqnZycmDp1KpcuXWLLli2EhYWxdOlS3NzcAPD09GTw4MH4+/tTtWrVFNe7aNEiYz12dnbUr18fBwcHvvzyS3r06EGhQoWe0B6KiGStNHWBaNOmDb/99hvDhw/H3t6e2bNn07FjR3799VdKly7NL7/8wpo1a1i4cCEbNmxg/vz5eHp6Pu7aRUSs3u3btzl48CCdO3e2GKu9SZMmbNiwgSJFiuDr60u1atWM8AsJQ1M6OTmxZ8+eVNft5+dHvXr1LEb0adq0KfHx8fj6+j6W/REReRLSfGeLXLly0aVLF1avXs0HH3zA/fv3+fLLL+ncuTO///47Xl5eVKpUScFXROQJOn36NPHx8bi7uzN69GheeuklGjRowNixYwkPDwcgKCiI4sWLWzzP1tYWLy8vzp8/n+J67969y+XLl5M9z93dHScnp1SfJyLyNEjfrd0ABwcHevTowZo1a3jnnXe4fv06Y8eO5c0333xoS4KIiGS90NBQIGFISnt7e6ZMmcLgwYPZtWsXQ4YMwWw2ExERgZOTU7LnOjo6EhkZmeJ6Ey+Qc3Z2TjbPyckp1eeJiDwN0jwM2s2bN/Hz8+PWrVt4enpSr149Bg4cyBtvvMG8efNYu3YtQ4cOpWrVqgwYMIDKlSs/zrpFRASMERkqVqzImDFjAKhduzYuLi6MGjWKffv2ER8fn+rzU7vFvdlsfuh2TSZTBisWEcl+aQrABw4cYNiwYcZFbpBwGmzu3LmULFmSTz75hHfeeYdvv/2WrVu30rNnT+rXr8+0adMeW+EiIpLQigvQoEEDi+l169YF4MSJEzg7OxMVFZXsuZGRkal2W0tsMU6ppTcyMjLFlmERkadFmrpAzJgxg1y5clGvXj1atmxJw4YNyZUrF7NnzzaWKVq0KJMmTeLHH3/kxRdfZPfu3Y+taBERSZDYR/f+/fsW02NjY4GEbmslSpQgODjYYn5cXBwhISGULFkyxfU6Ojri6enJxYsXLabfunWLyMhISpUqlUV7ICLy5KWpBTgoKIgZM2ZYDJUTHh5Oz549ky1bvnx5pk+fjr+/f1bVKCIiqShVqhReXl5s2bKF119/3eiasGPHDgCqVq1KeHg4ixcvJjQ0FHd3dyBhhIeoqCh8fHxSXXedOnXYtWsXQ4cOJXfu3ABs27YNW1tbatWq9Zj3TETk8UlTAC5UqBATJkygbt26ODs7Ex0djb+/P4ULF071OamNKykiIlnHZDIxaNAgPvnkE0aOHEnHjh05d+4cs2fPpkmTJlSsWJGCBQvyyy+/0L9/f3r16kVYWBgzZsygbt26VKlSxVjX0aNHcXd3p2jRogB069aNLVu2MGjQIN566y3Onz/P7NmzeeWVVzQGsIg81UzmR13pAGzevJlx48YRHx+PyWTCbDZjZ2fH7NmzrSboHj16FIAXXnghmyvJOjO2+BMSqiu5cxovdycGtaia3WXIU2bXrl3MmzeP06dPkzdvXlq3bs0HH3xgtNyePn2aadOmceTIEZycnGjYsCFDhgyxGB2iZs2atG3blvHjxxvTDh8+zPTp0zl58iRubm60adOGvn37kitXmq+hFhF5YtKa19IUgAECAwPZsWOHMQpEixYtjFYCa6AALE+KArCIiEjGpDWvpfknfIUKFahQoULmqhIRERERyWZpGgVi2LBh7N+/P8MbOX78OKNHj87w8x909OhR+vTpQ/369WnRogXjxo3j1q1bxvzg4GCGDh1Ko0aNaNq0KZ9//rkxqLuIiIiIWLc0tQDv2rWLXbt2UbRoUZo2bUqjRo147rnnUh1APTY2liNHjrB//3527drF6dOnAZg4cWKmCw4ICKBv377Url2bKVOmcP36dWbNmkVwcDALFiwgPDycvn374uHhwfjx4wkNDWXGjBmEhIQwc+bMTG9fRERERJ5uaQrA8+bN44svvuDUqVMsWrSIRYsWYWdnR6lSpShQoABOTk6YTCaioqK4cuUKFy5c4N69e0DC3YQqVqzIsGHDsqTgGTNmUKFCBaZOnWoEcCcnJ6ZOncqlS5fYsmULYWFhLF26FDc3NwA8PT0ZPHgw/v7+VnPRnoiIiIikLE0BuEqVKvz444/8+eefLFmyhICAAO7fv09gYCAnT560WDbxmjqTyUTt2rXp1KkTjRo1ypLbZt6+fZuDBw8yfvx4i9bnJk2a0KRJEwB8fX2pVq2aEX4BfHx8cHJyYs+ePQrAIiIiIlYuzRfB2djY0Lx5c5o3b05ISAh79+7lyJEjXL9+3eh/my9fPooWLUrVqlWpVasWBQsWzNJiT58+TXx8PO7u7owePZqdO3diNptp3Lgxw4cPx8XFhaCgIJo3b27xPFtbW7y8vDh//nymtm82m1O8nejTxmQykSdPnuwuQx4hOjqaNA7SIk9QVvyYl8dHx4yIdTObzWn6nM7QQI5eXl507tyZzp07Z+TpGRYaGgrAZ599Rt26dZkyZQoXLlzgm2++4dKlS8yfP5+IiAiLcS0TOTo6pnhP+/SIiYkhICAgU+vICfLkyYO3t3d2lyGPcO7cOaKjo7O7DEnCzs4O7+efJ5etbXaXIimIjYvj+L//EhMTk92liEg2Shz//GGeqpHMEz/UKlasyJgxYwCoXbs2Li4ujBo1in379hEfH5/q81O7aC+t7OzsKFu2bKbWkROoBevpUKpUKbVm5TAmk4lctrb87HeSa3ee/rNBzxLPvI509SlPuXLldNyIWLHEgRce5akKwI6OjgA0aNDAYnrdunUBOHHiBM7Ozil2U4iMjMTT0zNT2zeZTEYNIo+buqnkXNfuROkmMjmUjhsR65bWRr7MNYk+YcWLFwfg/v37FtNjY2MBcHBwoESJEgQHB1vMj4uLIyQkhJIlSz6ROkVEREQk53qqAnCpUqXw8vJiy5YtFqe4duzYAUDVqlXx8fHh0KFDRn9hAD8/P6KiovDx8XniNYuIiIhIzvJUBWCTycSgQYM4evQoI0eOZN++ffz8889MmzaNJk2aULFiRTp37oy9vT39+/dn+/btrF69mjFjxlC3bl2qVKmS3bsgIiIiItksQ32Ajx07RqVKlbK6ljRp1qwZ9vb2zJs3j6FDh5I3b146derEBx98AIC7uztz5sxh2rRpjB49GicnJ5o2bcqQIUOypV4RERERyVkyFIC7d+9OqVKlePnll2nTpg0FChTI6roeqkGDBskuhEuqbNmyzJ49+wlWJCIiIiJPiwx3gQgKCuKbb76hbdu2DBgwgN9//924/bGIiIiISE6VoRbgbt268eeff3Lx4kXMZjP79+9n//79ODo60rx5c15++WXdclhEREREcqQMBeABAwYwYMAAAgMD+eOPP/jzzz8JDg4mMjKSNWvWsGbNGry8vGjbti1t27alUKFCWV23iIiIiEiGZGoUiAoVKtC/f39WrlzJ0qVL6dChA2azGbPZTEhICN999x0dO3Zk8uTJD71Dm4iIiIjIk5LpO8GFh4fz559/snXrVg4ePIjJZDJCMCTchGLFihXkzZuXPn36ZLpgEREREZHMyFAAjoqK4q+//mLLli3s37/fuBOb2WzGxsaGOnXq0L59e0wmEzNnziQkJITNmzcrAIuIiIhItstQAG7evDkxMTEARkuvl5cX7dq1S9bn19PTk/fff59r165lQbkiIiIiIpmToQB8//59AHLnzk2TJk3o0KEDNWvWTHFZLy8vAFxcXDJYooiIiIhI1slQAH7uuedo3749rVq1wtnZ+aHL5smTh2+++YYiRYpkqEARERERkayUoQC8ePFiIKEvcExMDHZ2dgCcP3+e/Pnz4+TkZCzr5ORE7dq1s6BUEREREZHMy/AwaGvWrKFt27YcPXrUmPbjjz/SunVr1q5dmyXFiYiIiIhktQwF4D179jBx4kQiIiI4ffq0MT0oKIjo6GgmTpzI/v37s6xIEREREZGskqEAvHTpUgAKFy5MmTJljOlvvfUWxYoVw2w2s2TJkqypUEREREQkC2WoD/CZM2cwmUyMHTuWGjVqGNMbNWqEq6srvXv35tSpU1lWpIiIiIhIVslQC3BERAQA7u7uyeYlDncWHh6eibJERERERB6PDAXgggULArBy5UqL6WazmZ9//tliGRERERGRnCRDXSAaNWrEkiVLWL58OX5+fpQrV47Y2FhOnjzJ5cuXMZlMNGzYMKtrFRERERHJtAwF4B49evDXX38RHBzMhQsXuHDhgjHPbDZTrFgx3n///SwrUkRERORxGj58OCdOnGDdunXGtPfff58jR44kW3bx4sV4e3unuJ579+7x0ksvERcXZzE9T5487Nq1K2uLlgzLUAB2dnZm4cKFzJo1iz///NPo7+vs7EyzZs3o37//I+8QJyIiIpITbNy4ke3bt1O4cGFjmtls5vTp07z11ls0a9bMYvlSpUqluq4zZ84QFxfHhAkTKFq0qDHdxibDt16QxyBDARjA1dWVUaNGMXLkSG7fvo3ZbMbd3R2TyZSV9YmIiIg8NtevX2fKlCnJrl26ePEikZGR1KtXjxdeeCHN6zt58iS2trY0bdqU3LlzZ3W5kkUy/XPEZDLh7u5Ovnz5jPAbHx/P3r17M12ciIiIyOM0YcIE6tSpQ61atSymBwYGAlC+fPl0rS8wMJCSJUsq/OZwGWoBNpvNLFiwgJ07d3Lnzh3i4+ONebGxsdy+fZvY2Fj27duXZYWKiIiIZKXVq1dz4sQJli9fztdff20x7+TJkzg6OjJ9+nR27txJdHQ0NWvW5MMPP6RkyZKprjOxBbh///4cOXKE3Llz07RpU4YMGYKTk9Pj3SFJswwF4F9++YU5c+ZgMpkwm80W8xKnqSuEiIiI5FSXL1/mq6++YuzYsbi5uSWbf/LkSaKionBxcWHKlClcvnyZefPm0atXL3766ScKFCiQ7DmJ/YbNZjMdO3bk/fff5/jx48ybN49z587x3XffqS9wDpGhALxhwwYg4YpGDw8PLl68iLe3N1FRUZw7dw6TycSIESOytFARERGRrGA2m/nss8+oW7cuTZs2TXGZfv368e6771K9enUAqlWrRuXKlXnttddYtmwZgwYNSnG9U6dOxd3dnTJlygBQvXp1PDw8GDNmDL6+vtSrV+/x7ZikWYZ+hly8eBGTycQXX3zB559/jtlspk+fPixfvpw333wTs9lMUFBQFpcqIiIiknnLly/n1KlTDBs2jNjYWGJjY40z2rGxscTHx1O+fHkj/CYqWrQopUqV4tSpUymu18bGhpo1axrhN1H9+vUBUn2ePHkZCsD37t0DoHjx4pQvXx5HR0eOHTsGwCuvvALAnj17sqhEERERkazz559/cvv2bVq1aoWPjw8+Pj5s2LCBy5cv4+Pjw5w5c1i/fj3//PNPsufevXs3xS4TkDCixKpVq7hy5YrF9MTclNrz5MnLUBeIfPnyce3aNQIDA/Hy8qJcuXLs2bOHXr16cfHiRQCuXbuWpYWKiIiIZIWRI0cSFRVlMW3evHkEBAQwbdo0ChQoQM+ePcmfPz/ff/+9scyJEye4ePEi3bp1S3G9cXFxTJo0ie7du9O/f39j+pYtW7C1taVatWqPZ4ck3TIUgKtUqcKWLVsYM2YMy5Yto1q1aixatIguXboYv3ry5cuXpYWKiIiIZIWURnFwdXXFzs7OuMNbr169GD9+PGPHjqVNmzZcuXKFOXPmUL58edq2bQvA/fv3CQwMxNPTk4IFC1KoUCHatWvHkiVLsLe3p3Llyvj7+7Nw4UK6dOlCiRIlnuRuykNkKAD37NkTPz8/IiIiKFCgAC1btmTx4sUEBQUZI0A8eNcUERERkadF27Ztsbe3Z/HixXz00UfkyZOHRo0aMWDAAGxtbQG4ceMG3bt3p1evXvTp0weATz75hCJFirBx40YWLFiAp6cnffr04d13383O3ZEHmMwPjmOWRiEhIWzcuJGePXsCCbcR/Pbbb4mKiqJJkyZ89NFH2NvbZ2mx2eno0aMA6bobTE43Y4s/IaGR2V2GPMDL3YlBLapmdxnyEDp2ch4dNyICac9rGWoB3rNnD5UrVzbCL0CbNm1o06ZNRlYnIiIiIvLEZGgUiLFjx9KqVSt27tyZ1fWIiIiIiDxWGQrAd+/eJSYm5qG3AhQRERERyYkyFIAT75qyffv2LC1GRERERORxy1Af4PLly7N7926++eYbVq5cSenSpXF2diZXrv9fnclkYuzYsVlWqIiIiIhIVshQAJ4+fTomkwmAy5cvc/ny5RSXUwAWERERkZwmQwEY4FGjpyUGZBERERGRnCRDAXjt2rVZXYeIiIg8w+LNZmzUOJYjWeP/TYYCcOHChbO6DhEREXmG2ZhM/Ox3kmt3orK7FEnCM68jXX3KZ3cZT1yGAvChQ4fStFz16tUzsnoRERF5Bl27E6W7KEqOkKEA3KdPn0f28TWZTOzbty9DRYmIiIiIPC6P7SI4EREREZGcKEMBuFevXhaPzWYz9+/f58qVK2zfvp2KFSvSo0ePLClQRERERCQrZSgA9+7dO9V5f/zxByNHjiQ8PDzDRYmIiIiIPC4ZuhXywzRp0gSAZcuWZfWqRUREREQyLcsD8N9//43ZbObMmTNZvWoRERERkUzLUBeIvn37JpsWHx9PREQEZ8+eBSBfvnyZq0xERERE5DHIUAA+ePBgqsOgJY4O0bZt24xXJSIiIiLymGTpMGh2dnYUKFCAli1b0rNnz0wVllbDhw/nxIkTrFu3zpgWHBzMtGnTOHz4MLa2tjRr1oyBAwfi7Oz8RGoSERERkZwrQwH477//zuo6MmTjxo1s377d4tbM4eHh9O3bFw8PD8aPH09oaCgzZswgJCSEmTNnZmO1IiIiIpITZLgFOCUxMTHY2dll5SpTdf36daZMmULBggUtpv/666+EhYWxdOlS3NzcAPD09GTw4MH4+/tTtWrVJ1KfiIiIiORMGR4FIjAwkH79+nHixAlj2owZM+jZsyenTp3KkuIeZsKECdSpU4datWpZTPf19aVatWpG+AXw8fHBycmJPXv2PPa6RERERCRny1AAPnv2LH369OHAgQMWYTcoKIgjR47Qu3dvgoKCsqrGZFavXs2JEycYMWJEsnlBQUEUL17cYpqtrS1eXl6cP3/+sdUkIiIiIk+HDHWBWLBgAZGRkeTOndtiNIjnnnuOQ4cOERkZyQ8//MD48eOzqk7D5cuX+eqrrxg7dqxFK2+iiIgInJyckk13dHQkMjIyU9s2m81ERUVlah05gclkIk+ePNldhjxCdHR0ihebSvbRsZPz6bjJmXTs5HzPyrFjNptTHaksqQwFYH9/f0wmE6NHj6Z169bG9H79+lG2bFlGjRrF4cOHM7LqhzKbzXz22WfUrVuXpk2bprhMfHx8qs+3scncfT9iYmIICAjI1Dpygjx58uDt7Z3dZcgjnDt3jujo6OwuQ5LQsZPz6bjJmXTs5HzP0rGTO3fuRy6ToQB869YtACpVqpRsXoUKFQC4ceNGRlb9UMuXL+fUqVP8/PPPxMbGAv8/HFtsbCw2NjY4Ozun2EobGRmJp6dnprZvZ2dH2bJlM7WOnCAtv4wk+5UqVeqZ+DX+LNGxk/PpuMmZdOzkfM/KsXP69Ok0LZehAOzq6srNmzf5+++/KVasmMW8vXv3AuDi4pKRVT/Un3/+ye3bt2nVqlWyeT4+PvTq1YsSJUoQHBxsMS8uLo6QkBAaN26cqe2bTCYcHR0ztQ6RtNLpQpH003EjkjHPyrGT1h9bGQrANWvWZPPmzUydOpWAgAAqVKhAbGwsx48fZ+vWrZhMpmSjM2SFkSNHJmvdnTdvHgEBAUybNo0CBQpgY2PD4sWLCQ0Nxd3dHQA/Pz+ioqLw8fHJ8ppERERE5OmSoQDcs2dPdu7cSXR0NGvWrLGYZzabyZMnD++//36WFJhUyZIlk01zdXXFzs7O6FvUuXNnfvnlF/r370+vXr0ICwtjxowZ1K1blypVqmR5TSIiIiLydMnQVWElSpRg5syZFC9eHLPZbPGvePHizJw5M8Ww+iS4u7szZ84c3NzcGD16NLNnz6Zp06Z8/vnn2VKPiIiIiOQsGb4TXOXKlfn1118JDAwkODgYs9lMsWLFqFChwhPt7J7SUGtly5Zl9uzZT6wGEREREXl6ZOpWyFFRUZQuXdoY+eH8+fNERUWlOA6viIiIiEhOkOGBcdesWUPbtm05evSoMe3HH3+kdevWrF27NkuKExERERHJahkKwHv27GHixIlERERYjLcWFBREdHQ0EydOZP/+/VlWpIiIiIhIVslQAF66dCkAhQsXpkyZMsb0t956i2LFimE2m1myZEnWVCgiIiIikoUy1Af4zJkzmEwmxo4dS40aNYzpjRo1wtXVld69e3Pq1KksK1JEREREJKtkqAU4IiICwLjRRFKJd4ALDw/PRFkiIiIiIo9HhgJwwYIFAVi5cqXFdLPZzM8//2yxjIiIiIhITpKhLhCNGjViyZIlLF++HD8/P8qVK0dsbCwnT57k8uXLmEwmGjZsmNW1ioiIiIhkWoYCcI8ePfjrr78IDg7mwoULXLhwwZiXeEOMx3ErZBERERGRzMpQFwhnZ2cWLlxIx44dcXZ2Nm6D7OTkRMeOHVmwYAHOzs5ZXauIiIiISKZl+E5wrq6ujBo1ipEjR3L79m3MZjPu7u5P9DbIIiIiIiLpleE7wSUymUy4u7uTL18+TCYT0dHRrFq1infffTcr6hMRERERyVIZbgF+UEBAACtXrmTLli1ER0dn1WpFRERERLJUpgJwVFQUmzZtYvXq1QQGBhrTzWazukKIiIiISI6UoQD877//smrVKrZu3Wq09prNZgBsbW1p2LAhnTp1yroqRURERESySJoDcGRkJJs2bWLVqlXGbY4TQ28ik8nE+vXryZ8/f9ZWKSIiIiKSRdIUgD/77DP++OMP7t69axF6HR0dadKkCYUKFWL+/PkACr8iIiIikqOlKQCvW7cOk8mE2WwmV65c+Pj40Lp1axo2bIi9vT2+vr6Pu04RERERkSyRrmHQTCYTnp6eVKpUCW9vb+zt7R9XXSIiIiIij0WaWoCrVq2Kv78/AJcvX2bu3LnMnTsXb29vWrVqpbu+iYiIiMhTI00BeN68eVy4cIHVq1ezceNGbt68CcDx48c5fvy4xbJxcXHY2tpmfaUiIiIiIlkgzV0gihcvzqBBg9iwYQOTJ0+mfv36Rr/gpOP+tmrViq+//pozZ848tqJFRERERDIq3eMA29ra0qhRIxo1asSNGzdYu3Yt69at4+LFiwCEhYXx008/sWzZMvbt25flBYuIiIiIZEa6LoJ7UP78+enRowerVq3i22+/pVWrVtjZ2RmtwiIiIiIiOU2mboWcVM2aNalZsyYjRoxg48aNrF27NqtWLSIiIiKSZbIsACdydnamS5cudOnSJatXLSIiIiKSaZnqAiEiIiIi8rRRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVXJldwHpFR8fz8qVK/n111+5dOkS+fLl46WXXqJPnz44OzsDEBwczLRp0zh8+DC2trY0a9aMgQMHGvNFRERExHo9dQF48eLFfPvtt7zzzjvUqlWLCxcuMGfOHM6cOcM333xDREQEffv2xcPDg/HjxxMaGsqMGTMICQlh5syZ2V2+iIiIiGSzpyoAx8fHs2jRIl599VUGDBgAQJ06dXB1dWXkyJEEBASwb98+wsLCWLp0KW5ubgB4enoyePBg/P39qVq1avbtgIiIiIhku6eqD3BkZCRt2rShZcuWFtNLliwJwMWLF/H19aVatWpG+AXw8fHBycmJPXv2PMFqRURERCQneqpagF1cXBg+fHiy6X/99RcApUuXJigoiObNm1vMt7W1xcvLi/Pnzz+JMkVEREQkB3uqAnBKjh07xqJFi2jQoAFly5YlIiICJyenZMs5OjoSGRmZqW2ZzWaioqIytY6cwGQykSdPnuwuQx4hOjoas9mc3WVIEjp2cj4dNzmTjp2c71k5dsxmMyaT6ZHLPdUB2N/fn6FDh+Ll5cW4ceOAhH7CqbGxyVyPj5iYGAICAjK1jpwgT548eHt7Z3cZ8gjnzp0jOjo6u8uQJHTs5Hw6bnImHTs537N07OTOnfuRyzy1AXjLli18+umnFC9enJkzZxp9fp2dnVNspY2MjMTT0zNT27Szs6Ns2bKZWkdOkJZfRpL9SpUq9Uz8Gn+W6NjJ+XTc5Ew6dnK+Z+XYOX36dJqWeyoD8JIlS5gxYwY1atRgypQpFuP7lihRguDgYIvl4+LiCAkJoXHjxpnarslkwtHRMVPrEEkrnS4UST8dNyIZ86wcO2n9sfVUjQIB8NtvvzF9+nSaNWvGzJkzk93cwsfHh0OHDhEaGmpM8/PzIyoqCh8fnyddroiIiIjkME9VC/CNGzeYNm0aXl5evP7665w4ccJiftGiRencuTO//PIL/fv3p1evXoSFhTFjxgzq1q1LlSpVsqlyEREREckpnqoAvGfPHu7du0dISAg9e/ZMNn/cuHG0a9eOOXPmMG3aNEaPHo2TkxNNmzZlyJAhT75gEREREclxnqoA3KFDBzp06PDI5cqWLcvs2bOfQEUiIiIi8rR56voAi4iIiIhkhgKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKooAIuIiIiIVVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsAiIiIiYlUUgEVERETEqigAi4iIiIhVUQAWEREREauiACwiIiIiVuWZDsB+fn68++671KtXj/bt27NkyRLMZnN2lyUiIiIi2eiZDcBHjx5lyJAhlChRgsmTJ9OqVStmzJjBokWLsrs0EREREclGubK7gMdl7ty5VKhQgQkTJgBQt25dYmNjWbhwIV27dsXBwSGbKxQRERGR7PBMtgDfv3+fgwcP0rhxY4vpTZs2JTIyEn9//+wpTERERESy3TMZgC9dukRMTAzFixe3mF6sWDEAzp8/nx1liYiIiEgO8Ex2gYiIiADAycnJYrqjoyMAkZGR6VpfYGAg9+/fB+Cff/7Jggqzn8lkona+eOLc1BUkp7G1iefo0aO6YDOH0rGTM+m4yfl07ORMz9qxExMTg8lkeuRyz2QAjo+Pf+h8G5v0N3wnvphpeVGfFk72dtldgjzEs/Ree9bo2Mm5dNzkbDp2cq5n5dgxmUzWG4CdnZ0BiIqKspie2PKbOD+tKlSokDWFiYiIiEi2eyb7ABctWhRbW1uCg4Mtpic+LlmyZDZUJSIiIiI5wTMZgO3t7alWrRrbt2+36NOybds2nJ2dqVSpUjZWJyIiIiLZ6ZkMwADvv/8+x44d4+OPP2bPnj18++23LFmyhO7du2sMYBERERErZjI/K5f9pWD79u3MnTuX8+fP4+npyWuvvcbbb7+d3WWJiIiISDZ6pgOwiIiIiMiDntkuECIiIiIiKVEAFhERERGrogAsIiIiIlZFAVhERERErIoCsIiIiIhYFQVgEREREbEqCsBi9TQSoDzrUnqP630vItZMAVieSiEhIdSsWZN169Zl+Dnh4eGMHTuWw4cPP64yRR6Ldu3aMX78+BTnzZ07l5o1axqP/f39GTx4sMUy8+fPZ8mSJY+zRBGrkpHvJMleCsBitQIDA9m4cSPx8fHZXYpIlunYsSMLFy40Hq9evZpz585ZLDNnzhyio6OfdGkiz6z8+fOzcOFC6tevn92lSBrlyu4CREQk6xQsWJCCBQtmdxkiViV37ty88MIL2V2GpINagCXb3b17l1mzZvHKK6/w4osv0rBhQ/r160dgYKCxzLZt23jjjTeoV68eb731FidPnrRYx7p166hZsyYhISEW01M7VXzgwAH69u0LQN++fendu3fW75jIE7JmzRpq1arF/PnzLbpAjB8/nvXr13P58mXj9GzivHnz5ll0lTh9+jRDhgyhYcOGNGzYkI8++oiLFy8a8w8cOEDNmjXZv38//fv3p169erRs2ZIZM2YQFxf3ZHdYJB0CAgL44IMPaNiwIS+99BL9+vXj6NGjxvzDhw/Tu3dv6tWrR5MmTRg3bhyhoaHG/HXr1lGnTh2OHTtG9+7dqVu3Lm3btrXoRpRSF4gLFy7wn//8h5YtW1K/fn369OmDv79/suf8+OOPdOrUiXr16rF27drH+2KIQQFYst24ceNYu3Yt7733HrNmzWLo0KGcPXuW0aNHYzab2blzJyNGjKBs2bJMmTKF5s2bM2bMmExts2LFiowYMQKAESNG8PHHH2fFrog8cVu2bGHSpEn07NmTnj17Wszr2bMn9erVw8PDwzg9m9g9okOHDsbf58+f5/333+fWrVuMHz+eMWPGcOnSJWNaUmPGjKFatWp8/fXXtGzZksWLF7N69eonsq8i6RUREcHAgQNxc3Pjyy+/5L///S/R0dEMGDCAiIgIDh06xAcffICDgwP/+9//+PDDDzl48CB9+vTh7t27xnri4+P5+OOPadGiBdOnT6dq1apMnz4dX1/fFLd79uxZ3nnnHS5fvszw4cOZOHEiJpOJvn37cvDgQYtl582bR7du3fjss8+oU6fOY3095P+pC4Rkq5iYGKKiohg+fDjNmzcHoEaNGkRERPD1119z8+ZN5s+fz/PPP8+ECRMAePHFFwGYNWtWhrfr7OxMqVKlAChVqhSlS5fO5J6IPHm7du1i7NixvPfee/Tp0yfZ/KJFi+Lu7m5xetbd3R0AT09PY9q8efNwcHBg9uzZODs7A1CrVi06dOjAkiVLLC6i69ixoxG0a9WqxY4dO9i9ezedOnV6rPsqkhHnzp3j9u3bdO3alSpVqgBQsmRJVq5cSWRkJLNmzaJEiRJ89dVX2NraAvDCCy/QpUsX1q5dS5cuXYCEUVN69uxJx44dAahSpQrbt29n165dxndSUvPmzcPOzo45c+bg5OQEQP369Xn99deZPn06ixcvNpZt1qwZ7du3f5wvg6RALcCSrezs7Jg5cybNmzfn2rVrHDhwgN9++43du3cDCQE5ICCABg0aWDwvMSyLWKuAgAA+/vhjPD09je48GfX3339TvXp1HBwciI2NJTY2FicnJ6pVq8a+ffssln2wn6Onp6cuqJMcq0yZMri7uzN06FD++9//sn37djw8PBg0aBCurq4cO3aM+vXrYzabjfd+kSJFKFmyZLL3fuXKlY2/c+fOjZubW6rv/YMHD9KgQQMj/ALkypWLFi1aEBAQQFRUlDG9fPnyWbzXkhZqAZZs5+vry9SpUwkKCsLJyYly5crh6OgIwLVr1zCbzbi5uVk8J3/+/NlQqUjOcebMGerXr8/u3btZvnw5Xbt2zfC6bt++zdatW9m6dWuyeYktxokcHBwsHptMJo2kIjmWo6Mj8+bN4/vvv2fr1q2sXLkSe3t7Xn75Zbp37058fDyLFi1i0aJFyZ5rb29v8fjB976NjU2q42mHhYXh4eGRbLqHhwdms5nIyEiLGuXJUwCWbHXx4kU++ugjGjZsyNdff02RIkUwmUysWLGCvXv34urqio2NTbJ+iGFhYRaPTSYTQLIv4qS/skWeJXXr1uXrr7/mk08+Yfbs2TRq1IhChQplaF0uLi7Url2bt99+O9m8xNPCIk+rkiVLMmHCBOLi4vj333/ZuHEjv/76K56enphMJt58801atmyZ7HkPBt70cHV15ebNm8mmJ05zdXXlxo0bGV6/ZJ66QEi2CggI4N69e7z33nsULVrUCLJ79+4FEk4ZVa5cmW3btln80t65c6fFehJPM129etWYFhQUlCwoJ6Uvdnma5cuXD4Bhw4ZhY2PD//73vxSXs7FJ/jH/4LTq1atz7tw5ypcvj7e3N97e3jz33HMsXbqUv/76K8trF3lS/vjjD5o1a8aNGzewtbWlcuXKfPzxx7i4uHDz5k0qVqxIUFCQ8b739vamdOnSzJ07N9nFaulRvXp1du3aZdHSGxcXx++//463tze5c+fOit2TTFAAlmxVsWJFbG1tmTlzJn5+fuzatYvhw4cbfYDv3r1L//79OXv2LMOHD2fv3r0sW7aMuXPnWqynZs2a2Nvb8/XXX7Nnzx62bNnCsGHDcHV1TXXbLi4uAOzZsyfZsGoiT4v8+fPTv39/du/ezebNm5PNd3Fx4datW+zZs8docXJxceHIkSMcOnQIs9lMr169CA4OZujQofz111/4+vryn//8hy1btlCuXLknvUsiWaZq1arEx8fz0Ucf8ddff/H3338zadIkIiIiaNq0Kf3798fPz4/Ro0eze/dudu7cyaBBg/j777+pWLFihrfbq1cv7t27R9++ffnjjz/YsWMHAwcO5NKlS/Tv3z8L91AySgFYslWxYsWYNGkSV69eZdiwYfz3v/8FEm7najKZOHz4MNWqVWPGjBlcu3aN4cOHs3LlSsaOHWuxHhcXFyZPnkxcXBwfffQRc+bMoVevXnh7e6e67dKlS9OyZUuWL1/O6NGjH+t+ijxOnTp14vnnn2fq1KnJznq0a9eOwoULM2zYMNavXw9A9+7dCQgIYNCgQVy9epVy5coxf/58TCYT48aNY8SIEdy4cYMpU6bQpEmT7NglkSyRP39+Zs6cibOzMxMmTGDIkCEEBgby5ZdfUrNmTXx8fJg5cyZXr15lxIgRjB07FltbW2bPnp2pG1uUKVOG+fPn4+7uzmeffWZ8Z82dO1dDneUQJnNqPbhFRERERJ5BagEWEREREauiACwiIiIiVkUBWERERESsigKwiIiIiFgVBWARERERsSoKwCIiIiJiVRSARURERMSq5MruAkREngW9evXi8OHDQMLNJ8aNG5fNFSV3+vRpfvvtN/bv38+NGze4f/8+7u7uPPfcc7Rv356GDRtmd4kiIk+EboQhIpJJ58+fp1OnTsZjBwcHNm/ejLOzczZWZemHH35gzpw5xMbGprpM69at+fTTT7Gx0clBEXm26VNORCST1qxZY/H47t27bNy4MZuqSW758uXMmjWL2NhYChYsyMiRI1mxYgU///wzQ4YMwcnJCYBNmzbx008/ZXO1IiKPn1qARUQyITY2lpdffpmbN2/i5eXF1atXiYuLo3z58jkiTN64cYN27doRExNDwYIFWbx4MR4eHhbL7Nmzh8GDBwNQoEABNm7ciMlkyo5yRUSeCPUBFhHJhN27d3Pz5k0A2rdvz7Fjx9i9ezcnT57k2LFjVKpUKdlzQkJCmDVrFn5+fsTExFCtWjU+/PBD/vvf/3Lo0CGqV6/Od999ZywfFBTE3Llz+fvvv4mKiqJw4cK0bt2ad955B3t7+4fWt379emJiYgDo2bNnsvALUK9ePYYMGYKXlxfe3t5G+F23bh2ffvopANOmTWPRokUcP34cd3d3lixZgoeHBzExMfz8889s3ryZ4OBgAMqUKUPHjh1p3769RZDu3bs3hw4dAuDAgQPG9AMHDtC3b18goS91nz59LJYvX748X3zxBdOnT+fvv//GZDLx4osvMnDgQLy8vB66/yIiKVEAFhHJhKTdH1q2bEmxYsXYvXs3ACtXrkwWgC9fvky3bt0IDQ01pu3du5fjx4+n2Gf433//pV+/fkRGRhrTzp8/z5w5c9i/fz+zZ88mV67UP8oTAyeAj49Pqsu9/fbbD9lLGDduHOHh4QB4eHjg4eFBVFQUvXv35sSJExbLHj16lKNHj7Jnzx4+//xzbG1tH7ruRwkNDaV79+7cvn3bmLZ161YOHTrEokWLKFSoUKbWLyLWR32ARUQy6Pr16+zduxcAb29vihUrRsOGDY0+tVu3biUiIsLiObNmzTLCb+vWrVm2bBnffvst+fLl4+LFixbLms1mPvvsMyIjI3Fzc2Py5Mn89ttvDB8+HBsbGw4dOsQvv/zy0BqvXr1q/F2gQAGLeTdu3ODq1avJ/t2/fz/ZemJiYpg2bRo//fQTH374IQBff/21EX5btGjBjz/+yIIFC6hTpw4A27ZtY8mSJQ9/EdPg+vXr5M2bl1mzZrFs2TJat24NwM2bN5k5c2am1y8i1kcBWEQkg9atW0dcXBwArVq1AhJGgGjcuDEA0dHRbN682Vg+Pj7eaB0uWLAg48aNo1y5ctSqVYtJkyYlW/+pU6c4c+YMAG3btsXb2xsHBwcaNWpE9erVAdiwYcNDa0w6osODI0C8++67vPzyy8n+/fPPP8nW06xZM1566SXKly9PtWrViIyMNLZdpkwZJkyYQMWKFalcuTJTpkwxulo8KqCn1ZgxY/Dx8aFcuXKMGzeOwoULA7Br1y7j/0BEJK0UgEVEMsBsNrN27VrjsbOzM3v37mXv3r0Wp+RXrVpl/B0aGmp0ZfD29rboulCuXDmj5TjRhQsXjL9//PFHi5Ca2If2zJkzKbbYJipYsKDxd0hISHp301CmTJlktd27dw+AmjVrWnRzyJMnD5UrVwYSWm+Tdl3ICJPJZNGVJFeuXHh7ewMQFRWV6fWLiPVRH2ARkQw4ePCgRZeFzz77LMXlAgMD+ffff3n++eexs7MzpqdlAJ609J2Ni4vjzp075M+fP8X5tWvXNlqdd+/eTenSpY15SYdqGz9+POvXr091Ow/2T35UbY/av7i4OGMdiUH6YeuKjY1N9fXTiBUikl5qARYRyYAHx/59mMRW4Lx58+Li4gJAQECARZeEEydOWFzoBlCsWDHj7379+nHgwAHj348//sjmzZs5cOBAquEXEvrmOjg4ALBo0aJUW4Ef3PaDHrzQrkiRIuTOnRtIGMUhPj7emBcdHc3Ro0eBhBZoNzc3AGP5B7d35cqVh24bEn5wJIqLiyMwMBBICOaJ6xcRSSsFYBGRdAoPD2fbtm0AuLq64uvraxFODxw4wObNm40Wzi1bthiBr2XLlkDCxWmffvopp0+fxs/Pj1GjRiXbTpkyZShfvjyQ0AXi999/5+LFi2zcuJFu3brRqlUrhg8f/tBa8+fPz9ChQwEICwuje/furFixgqCgIIKCgti8eTN9+vRh+/bt6XoNnJycaNq0KZDQDWPs2LGcOHGCo0eP8p///McYGq5Lly7Gc5JehLds2TLi4+MJDAxk0aJFj9ze//73P3bt2sXp06f53//+x6VLlwBo1KiR7lwnIummLhAiIum0adMm47R9mzZtLE7NJ8qfPz8NGzZk27ZtREVFsXnzZjp16kSPHj3Yvn07N2/eZNOmTWzatAmAQoUKkSdPHqKjo41T+iaTiWHDhjFo0CDu3LmTLCS7uroaY+Y+TKdOnYiJiWH69OncvHmTL774IsXlbG1t6dChg9G/9lGGDx/OyZMnOXPmDJs3b7a44A+gSZMmFsOrtWzZknXr1gEwb9485s+fj9ls5oUXXnhk/2Sz2WwE+UQFChRgwIABaapVRCQp/WwWEUmnpN0fOnTokOpynTp1Mv5O7Abh6enJ999/T+PGjXFycsLJyYkmTZowf/58o4tA0q4CNWrU4IcffqB58+Z4eHhgZ2dHwYIFadeuHT/88ANly5ZNU81du3ZlxYoVdO/enQoVKuDq6oqdnR358+endu3aDBgwgHXr1jFy5EgcHR3TtM68efOyZMkSBg8ezHPPPYejoyMODg5UqlSJ0aNH88UXX1j0Ffbx8WHChAmUKVOG3LlzU7hwYXr16sVXX331yG0lvmZ58uTB2dmZFi1asHDhwod2/xARSY1uhSwi8gT5+fmRO3duPD09KVSokNG3Nj4+ngYNGnDv3j1atGjBf//732yuNPulduc4EZHMUhcIEZEn6JdffmHXrl0AdOzYkW7dunH//n3Wr19vdKtIaxcEERHJGAVgEZEn6PXXX2fPnj3Ex8ezevVqVq9ebTG/YMGCtG/fPnuKExGxEuoDLCLyBPn4+DB79mwaNGiAh4cHtra25M6dm6JFi9KpUyd++OEH8ubNm91liog809QHWERERESsilqARURERMSqKACLiIiIiFVRABYRERERq6IALCIiIiJWRQFYRERERKyKArCIiIiIWBUFYBERERGxKgrAIiIiImJVFIBFRERExKr8H63cEdZde5GOAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ea2c7-0827-4ffa-9777-ad2891e5f49b",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "94fbe612-c62e-4abe-8ec3-d75940a993cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          553            439  79.385172\n",
      "1           kitten          109             72  66.055046\n",
      "2           senior          178             96  53.932584\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "50e84f5a-909e-47ca-93ca-2e92abf7ded1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ac46e-4cc6-4237-925c-524d47e83b46",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "1743599c-0d3f-4b10-a095-02c36218c679",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    217      160     73.73\n",
      "1          M    337      253     75.07\n",
      "2          X    286      194     67.83\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b8546267-aa7a-4e4b-b60e-810f836ebf8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613636d-22ae-4629-ac86-daf482925194",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
