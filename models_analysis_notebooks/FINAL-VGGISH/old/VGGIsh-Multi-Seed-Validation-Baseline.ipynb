{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17da2043-9a5b-40fe-b3cb-f755b9a98deb",
   "metadata": {},
   "source": [
    "# VGGIsh Multi Seed Validation\n",
    "#### 5 Random (but reproducible) seeds are selected for 5 different runs of train/test\n",
    "#### Models have been optimised and verified on validation sets thoroughly, running a final train/test evaluation here\n",
    "#### The setup:\n",
    "\n",
    "- 4 fold StratifiedGroupKFold for stratification and ensuring each cat_id group only appears in one set at a time\n",
    "- Final scores averaged over the 4 folds\n",
    "- For each seed run we will explore the cat_id predictions through majority voting\n",
    "- For each run we will explore the potential impact of gender\n",
    "\n",
    "The dataset is highly unbalanced, resources for an unbiased estimate have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ac1e09ae-387c-4347-b6f5-8d09dd1bf3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, SGD, RMSprop, AdamW\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import shap\n",
    "from keras.regularizers import l1, l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d005cfd2-3eda-44c4-bbb5-af343e979bc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [7270  860 5390 5191 5734]\n"
     ]
    }
   ],
   "source": [
    "# Set an initial seed for reproducibility\n",
    "np.random.seed(42)  \n",
    "\n",
    "# Generate a list of 5 random seeds\n",
    "random_seeds = np.random.randint(0, 10000, size=5)\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae122a-9f9c-47a3-8835-2d46d2f8e2f9",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d5097e68-5153-440c-9294-dfa9e6061652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_initial_group_split(groups_train, groups_test):\n",
    "    \"\"\"\n",
    "    Check if any group is present in both the train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - groups_train: Array of group identifiers for the train set\n",
    "    - groups_test: Array of group identifiers for the test set\n",
    "\n",
    "    Returns:\n",
    "    - Prints out any groups found in both sets and the count of such groups\n",
    "    \"\"\"\n",
    "    train_groups = set(groups_train)\n",
    "    test_groups = set(groups_test)\n",
    "    common_groups = train_groups.intersection(test_groups)\n",
    "\n",
    "    if common_groups:\n",
    "        print(f\"Warning: Found {len(common_groups)} common groups in both train/validation and test sets: {common_groups}\")\n",
    "    else:\n",
    "        print(\"No common groups found between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "88dfe2de-bb1e-4d49-9260-e056c39627bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to perform the swaps based on cat_id, ensuring swaps within the same age_group\n",
    "def swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids):\n",
    "    for cat_id in specific_cat_ids:\n",
    "        # Check if the specific cat_id is not in the training set\n",
    "        if cat_id not in dataframe.iloc[train_val_idx]['cat_id'].values:\n",
    "            # Get the age_group of this cat_id\n",
    "            age_group = dataframe[dataframe['cat_id'] == cat_id]['age_group'].iloc[0]\n",
    "                \n",
    "            # Find a different cat_id within the same age_group in the train set that is not in the test set\n",
    "            other_cat_ids_in_age_group = dataframe[(dataframe['age_group'] == age_group) & \n",
    "                                                   (dataframe['cat_id'] != cat_id) &\n",
    "                                                   (~dataframe['cat_id'].isin(dataframe.iloc[test_idx]['cat_id']))]['cat_id'].unique()\n",
    "            \n",
    "            # Choose one other cat_id for swapping\n",
    "            if len(other_cat_ids_in_age_group) > 0:\n",
    "                other_cat_id = np.random.choice(other_cat_ids_in_age_group)\n",
    "\n",
    "                # Find all instances of the other_cat_id in the train set\n",
    "                other_cat_id_train_val_indices = train_val_idx[dataframe.iloc[train_val_idx]['cat_id'] == other_cat_id]\n",
    "                \n",
    "                # Find all instances of the specific cat_id in the test set\n",
    "                cat_id_test_indices = test_idx[dataframe.iloc[test_idx]['cat_id'] == cat_id]\n",
    "                \n",
    "                # Swap the indices\n",
    "                train_val_idx = np.setdiff1d(train_val_idx, other_cat_id_train_val_indices, assume_unique=True)\n",
    "                test_idx = np.setdiff1d(test_idx, cat_id_test_indices, assume_unique=True)\n",
    "\n",
    "                train_val_idx = np.concatenate((train_val_idx, cat_id_test_indices))\n",
    "                test_idx = np.concatenate((test_idx, other_cat_id_train_val_indices))\n",
    "            else:\n",
    "                print(f\"No alternative cat_id found in the same age_group as {cat_id} for swapping.\")\n",
    "                \n",
    "    return train_val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2e6c2d87-cfab-493e-bca8-b3b8976a2bda",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify differences in groups\n",
    "def find_group_differences(original, new):\n",
    "    # Convert numpy arrays to sets for easy difference computation\n",
    "    original_set = set(original)\n",
    "    new_set = set(new)\n",
    "    # Find differences\n",
    "    moved_to_new = new_set - original_set\n",
    "    moved_to_original = original_set - new_set\n",
    "    return moved_to_new, moved_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6164b1c3-75dd-4549-aafd-cb6106297941",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create custom logger function for local logs & stored in a .txt\n",
    "def logger(message, file=None):\n",
    "    print(message)\n",
    "    if file is not None:\n",
    "        with open(file, \"a\") as log_file:\n",
    "            log_file.write(message + \"\\n\")\n",
    "\n",
    "log_file_path = \"multi-seed-val-D13.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "899fb535-0e25-4c7b-b6a5-13231e26c502",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "# Define a custom color palette\n",
    "colors = [\"#6aabd1\", \"#b6e2d3\", \"#dac292\"] \n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "# Function to create bar plots with enhanced style\n",
    "def styled_barplot(data, x, y, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bar_plot = sns.barplot(x=x, y=y, data=data, errorbar=None, width=0.5)  \n",
    "    plt.title(title, fontsize=16, fontweight='bold', color=\"#333333\")\n",
    "    plt.xlabel(xlabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.ylabel(ylabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.xticks(fontsize=12, color=\"#333333\")\n",
    "    plt.yticks(fontsize=12, color=\"#333333\")\n",
    "    plt.ylim(0, 100) \n",
    "\n",
    "    # Adding value labels on top of each bar\n",
    "    for p in bar_plot.patches:\n",
    "        height = p.get_height()\n",
    "        # Annotate the height value on the bar\n",
    "        bar_plot.annotate(f'{height:.1f}', \n",
    "                          (p.get_x() + p.get_width() / 2., height), \n",
    "                          ha='center', va='center', \n",
    "                          xytext=(0, 9), \n",
    "                          textcoords='offset points', fontsize=12, color=\"#333333\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936a24a-13bd-4bc3-be13-0b210a09b5da",
   "metadata": {},
   "source": [
    "# RANDOM SEED 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70136a37-0507-4c2e-8307-c2dd905432e8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "14bb802b-f4bd-4464-a5fd-1977438428b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[0])) \n",
    "np.random.seed(int(random_seeds[0]))\n",
    "tf.random.set_seed(int(random_seeds[0]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6ce97c28-4210-4150-a60b-acdbf0e7716c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c26f17c3-4d21-4537-a090-9ca00f2be51a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91a9c3-3474-4e83-8a5b-c5eb4a9a9223",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f4605c0b-f99d-48c3-8c2a-d87dd22c8946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "005A    10\n",
      "071A    10\n",
      "040A    10\n",
      "014B    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "022A     9\n",
      "072A     9\n",
      "095A     8\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "031A     7\n",
      "027A     7\n",
      "050A     7\n",
      "109A     6\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "037A     6\n",
      "075A     5\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "026A     4\n",
      "035A     4\n",
      "105A     4\n",
      "052A     4\n",
      "003A     4\n",
      "062A     4\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "090A     1\n",
      "100A     1\n",
      "115A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "049A     1\n",
      "048A     1\n",
      "066A     1\n",
      "096A     1\n",
      "026C     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "055A    20\n",
      "001A    14\n",
      "025A    11\n",
      "016A    10\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "117A     7\n",
      "008A     6\n",
      "053A     6\n",
      "104A     4\n",
      "009A     4\n",
      "060A     3\n",
      "056A     3\n",
      "058A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "073A     1\n",
      "076A     1\n",
      "092A     1\n",
      "091A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    325\n",
      "M    253\n",
      "F    197\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    84\n",
      "F    55\n",
      "X    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [097A, 057A, 106A, 059A, 113A, 116A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 001A, 103A, 091A, 009A, 025A, 069A, 032...\n",
      "kitten                                         [045A, 110A]\n",
      "senior    [093A, 104A, 055A, 117A, 056A, 058A, 016A, 094...\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 14, 'senior': 13}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 2, 'senior': 9}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "Length of X_train_val:\n",
      "775\n",
      "Length of y_train_val:\n",
      "775\n",
      "Length of groups_train_val:\n",
      "775\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 494, 1: 161, 2: 120})\n",
      "Epoch 1/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.2359 - accuracy: 0.4594\n",
      "Epoch 2/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.9353 - accuracy: 0.6065\n",
      "Epoch 3/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.8528 - accuracy: 0.6465\n",
      "Epoch 4/1500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.7410 - accuracy: 0.6968\n",
      "Epoch 5/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7051 - accuracy: 0.6955\n",
      "Epoch 6/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7391 - accuracy: 0.6826\n",
      "Epoch 7/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6785 - accuracy: 0.7161\n",
      "Epoch 8/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6458 - accuracy: 0.7329\n",
      "Epoch 9/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.6251 - accuracy: 0.7626\n",
      "Epoch 10/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5327 - accuracy: 0.7677\n",
      "Epoch 11/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5699 - accuracy: 0.7716\n",
      "Epoch 12/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5526 - accuracy: 0.7832\n",
      "Epoch 13/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5151 - accuracy: 0.8026\n",
      "Epoch 14/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5056 - accuracy: 0.8013\n",
      "Epoch 15/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4931 - accuracy: 0.8206\n",
      "Epoch 16/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5070 - accuracy: 0.8013\n",
      "Epoch 17/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4752 - accuracy: 0.8090\n",
      "Epoch 18/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5060 - accuracy: 0.8065\n",
      "Epoch 19/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4817 - accuracy: 0.8129\n",
      "Epoch 20/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4633 - accuracy: 0.8206\n",
      "Epoch 21/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4835 - accuracy: 0.8090\n",
      "Epoch 22/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4405 - accuracy: 0.8335\n",
      "Epoch 23/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4275 - accuracy: 0.8361\n",
      "Epoch 24/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4204 - accuracy: 0.8348\n",
      "Epoch 25/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4147 - accuracy: 0.8426\n",
      "Epoch 26/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3937 - accuracy: 0.8335\n",
      "Epoch 27/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4153 - accuracy: 0.8245\n",
      "Epoch 28/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3895 - accuracy: 0.8374\n",
      "Epoch 29/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4450 - accuracy: 0.8232\n",
      "Epoch 30/1500\n",
      "25/25 [==============================] - 0s 992us/step - loss: 0.3957 - accuracy: 0.8374\n",
      "Epoch 31/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4148 - accuracy: 0.8361\n",
      "Epoch 32/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3833 - accuracy: 0.8490\n",
      "Epoch 33/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3692 - accuracy: 0.8581\n",
      "Epoch 34/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3661 - accuracy: 0.8594\n",
      "Epoch 35/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3849 - accuracy: 0.8477\n",
      "Epoch 36/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4037 - accuracy: 0.8387\n",
      "Epoch 37/1500\n",
      "25/25 [==============================] - 0s 968us/step - loss: 0.3731 - accuracy: 0.8606\n",
      "Epoch 38/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3344 - accuracy: 0.8723\n",
      "Epoch 39/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3414 - accuracy: 0.8658\n",
      "Epoch 40/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3482 - accuracy: 0.8594\n",
      "Epoch 41/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3278 - accuracy: 0.8671\n",
      "Epoch 42/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8619\n",
      "Epoch 43/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3380 - accuracy: 0.8671\n",
      "Epoch 44/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3607 - accuracy: 0.8619\n",
      "Epoch 45/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3567 - accuracy: 0.8503\n",
      "Epoch 46/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3319 - accuracy: 0.8826\n",
      "Epoch 47/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3229 - accuracy: 0.8697\n",
      "Epoch 48/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3075 - accuracy: 0.8800\n",
      "Epoch 49/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3181 - accuracy: 0.8735\n",
      "Epoch 50/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.3284 - accuracy: 0.8723\n",
      "Epoch 51/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3047 - accuracy: 0.8710\n",
      "Epoch 52/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3045 - accuracy: 0.8852\n",
      "Epoch 53/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3252 - accuracy: 0.8787\n",
      "Epoch 54/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3133 - accuracy: 0.8826\n",
      "Epoch 55/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2918 - accuracy: 0.8852\n",
      "Epoch 56/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2932 - accuracy: 0.8852\n",
      "Epoch 57/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3635 - accuracy: 0.8619\n",
      "Epoch 58/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2916 - accuracy: 0.8839\n",
      "Epoch 59/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3090 - accuracy: 0.8748\n",
      "Epoch 60/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2597 - accuracy: 0.8994\n",
      "Epoch 61/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3094 - accuracy: 0.8632\n",
      "Epoch 62/1500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.2669 - accuracy: 0.8877\n",
      "Epoch 63/1500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.2814 - accuracy: 0.8826\n",
      "Epoch 64/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3049 - accuracy: 0.8723\n",
      "Epoch 65/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3174 - accuracy: 0.8826\n",
      "Epoch 66/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2617 - accuracy: 0.9097\n",
      "Epoch 67/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2576 - accuracy: 0.9006\n",
      "Epoch 68/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2695 - accuracy: 0.8787\n",
      "Epoch 69/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2574 - accuracy: 0.9097\n",
      "Epoch 70/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2916 - accuracy: 0.8839\n",
      "Epoch 71/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.9058\n",
      "Epoch 72/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2659 - accuracy: 0.8890\n",
      "Epoch 73/1500\n",
      "25/25 [==============================] - 0s 977us/step - loss: 0.2544 - accuracy: 0.8942\n",
      "Epoch 74/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.9058\n",
      "Epoch 75/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2578 - accuracy: 0.8903\n",
      "Epoch 76/1500\n",
      "25/25 [==============================] - 0s 946us/step - loss: 0.2551 - accuracy: 0.9084\n",
      "Epoch 77/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2576 - accuracy: 0.8903\n",
      "Epoch 78/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2606 - accuracy: 0.8994\n",
      "Epoch 79/1500\n",
      "25/25 [==============================] - 0s 995us/step - loss: 0.2473 - accuracy: 0.8968\n",
      "Epoch 80/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2390 - accuracy: 0.9058\n",
      "Epoch 81/1500\n",
      "25/25 [==============================] - 0s 995us/step - loss: 0.2537 - accuracy: 0.9045\n",
      "Epoch 82/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2533 - accuracy: 0.9006\n",
      "Epoch 83/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.8942\n",
      "Epoch 84/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.8968\n",
      "Epoch 85/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2716 - accuracy: 0.8929\n",
      "Epoch 86/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2309 - accuracy: 0.9097\n",
      "Epoch 87/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.8929\n",
      "Epoch 88/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2503 - accuracy: 0.8942\n",
      "Epoch 89/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2521 - accuracy: 0.8916\n",
      "Epoch 90/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2556 - accuracy: 0.8955\n",
      "Epoch 91/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2374 - accuracy: 0.9032\n",
      "Epoch 92/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2190 - accuracy: 0.9200\n",
      "Epoch 93/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2552 - accuracy: 0.9006\n",
      "Epoch 94/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2273 - accuracy: 0.9187\n",
      "Epoch 95/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2149 - accuracy: 0.9187\n",
      "Epoch 96/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2255 - accuracy: 0.9148\n",
      "Epoch 97/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2232 - accuracy: 0.9123\n",
      "Epoch 98/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2442 - accuracy: 0.8994\n",
      "Epoch 99/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2171 - accuracy: 0.9213\n",
      "Epoch 100/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2052 - accuracy: 0.9213\n",
      "Epoch 101/1500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.2353 - accuracy: 0.9084\n",
      "Epoch 102/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2292 - accuracy: 0.9187\n",
      "Epoch 103/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2463 - accuracy: 0.9084\n",
      "Epoch 104/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2166 - accuracy: 0.9045\n",
      "Epoch 105/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.9316\n",
      "Epoch 106/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2032 - accuracy: 0.9316\n",
      "Epoch 107/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9290\n",
      "Epoch 108/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9058\n",
      "Epoch 109/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2207 - accuracy: 0.9148\n",
      "Epoch 110/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2082 - accuracy: 0.9252\n",
      "Epoch 111/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1792 - accuracy: 0.9406\n",
      "Epoch 112/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9200\n",
      "Epoch 113/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2259 - accuracy: 0.9135\n",
      "Epoch 114/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2427 - accuracy: 0.9019\n",
      "Epoch 115/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9161\n",
      "Epoch 116/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.9200\n",
      "Epoch 117/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9368\n",
      "Epoch 118/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1982 - accuracy: 0.9239\n",
      "Epoch 119/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9239\n",
      "Epoch 120/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1978 - accuracy: 0.9226\n",
      "Epoch 121/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1925 - accuracy: 0.9226\n",
      "Epoch 122/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.9303\n",
      "Epoch 123/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2012 - accuracy: 0.9174\n",
      "Epoch 124/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9226\n",
      "Epoch 125/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9316\n",
      "Epoch 126/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1931 - accuracy: 0.9303\n",
      "Epoch 127/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9303\n",
      "Epoch 128/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2141 - accuracy: 0.9123\n",
      "Epoch 129/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2000 - accuracy: 0.9239\n",
      "Epoch 130/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9032\n",
      "Epoch 131/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9226\n",
      "Epoch 132/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1912 - accuracy: 0.9200\n",
      "Epoch 133/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1656 - accuracy: 0.9419\n",
      "Epoch 134/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.9277\n",
      "Epoch 135/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.9303\n",
      "Epoch 136/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1692 - accuracy: 0.9471\n",
      "Epoch 137/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.1999 - accuracy: 0.9226\n",
      "Epoch 138/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.9355\n",
      "Epoch 139/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9458\n",
      "Epoch 140/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9381\n",
      "Epoch 141/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9510\n",
      "Epoch 142/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9419\n",
      "Epoch 143/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9329\n",
      "Epoch 144/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9381\n",
      "Epoch 145/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9355\n",
      "Epoch 146/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9355\n",
      "Epoch 147/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.9252\n",
      "Epoch 148/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9381\n",
      "Epoch 149/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1562 - accuracy: 0.9484\n",
      "Epoch 150/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9342\n",
      "Epoch 151/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9497\n",
      "Epoch 152/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9394\n",
      "Epoch 153/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9316\n",
      "Epoch 154/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9394\n",
      "Epoch 155/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.9368\n",
      "Epoch 156/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1582 - accuracy: 0.9471\n",
      "Epoch 157/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9419\n",
      "Epoch 158/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9458\n",
      "Epoch 159/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9432\n",
      "Epoch 160/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9277\n",
      "Epoch 161/1500\n",
      "25/25 [==============================] - 0s 979us/step - loss: 0.1603 - accuracy: 0.9368\n",
      "Epoch 162/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9277\n",
      "Epoch 163/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.9523\n",
      "Epoch 164/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9368\n",
      "Epoch 165/1500\n",
      "25/25 [==============================] - 0s 995us/step - loss: 0.1800 - accuracy: 0.9290\n",
      "Epoch 166/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9419\n",
      "Epoch 167/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9535\n",
      "Epoch 168/1500\n",
      "25/25 [==============================] - 0s 959us/step - loss: 0.1793 - accuracy: 0.9329\n",
      "Epoch 169/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9471\n",
      "Epoch 170/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1562 - accuracy: 0.9406\n",
      "Epoch 171/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1721 - accuracy: 0.9406\n",
      "Epoch 172/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1404 - accuracy: 0.9432\n",
      "Epoch 173/1500\n",
      "25/25 [==============================] - 0s 976us/step - loss: 0.1458 - accuracy: 0.9432\n",
      "Epoch 174/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9419\n",
      "Epoch 175/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9523\n",
      "Epoch 176/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9497\n",
      "Epoch 177/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9613\n",
      "Epoch 178/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1409 - accuracy: 0.9497\n",
      "Epoch 179/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9394\n",
      "Epoch 180/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9458\n",
      "Epoch 181/1500\n",
      "25/25 [==============================] - 0s 935us/step - loss: 0.1508 - accuracy: 0.9406\n",
      "Epoch 182/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9419\n",
      "Epoch 183/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9484\n",
      "Epoch 184/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9484\n",
      "Epoch 185/1500\n",
      "25/25 [==============================] - 0s 11ms/step - loss: 0.1401 - accuracy: 0.9484\n",
      "Epoch 186/1500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.1543 - accuracy: 0.9394\n",
      "Epoch 187/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1533 - accuracy: 0.9316\n",
      "Epoch 188/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9432\n",
      "Epoch 189/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9626\n",
      "Epoch 190/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9613\n",
      "Epoch 191/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9419\n",
      "Epoch 192/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9458\n",
      "Epoch 193/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9535\n",
      "Epoch 194/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1683 - accuracy: 0.9303\n",
      "Epoch 195/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9355\n",
      "Epoch 196/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9484\n",
      "Epoch 197/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9548\n",
      "Epoch 198/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.9419\n",
      "Epoch 199/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9484\n",
      "Epoch 200/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9510\n",
      "Epoch 201/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9626\n",
      "Epoch 202/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9639\n",
      "Epoch 203/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1266 - accuracy: 0.9613\n",
      "Epoch 204/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9445\n",
      "Epoch 205/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1421 - accuracy: 0.9510\n",
      "Epoch 206/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9613\n",
      "Epoch 207/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9574\n",
      "Epoch 208/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1482 - accuracy: 0.9432\n",
      "Epoch 209/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1559 - accuracy: 0.9381\n",
      "Epoch 210/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9510\n",
      "Epoch 211/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9510\n",
      "Epoch 212/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9626\n",
      "Epoch 213/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9548\n",
      "Epoch 214/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9471\n",
      "Epoch 215/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9535\n",
      "Epoch 216/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9613\n",
      "Epoch 217/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9626\n",
      "Epoch 218/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9458\n",
      "Epoch 219/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9574\n",
      "Epoch 220/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9626\n",
      "Epoch 221/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9626\n",
      "Epoch 222/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9639\n",
      "Epoch 223/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9703\n",
      "Epoch 224/1500\n",
      "25/25 [==============================] - 0s 983us/step - loss: 0.1396 - accuracy: 0.9484\n",
      "Epoch 225/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9703\n",
      "Epoch 226/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9587\n",
      "Epoch 227/1500\n",
      "25/25 [==============================] - 0s 987us/step - loss: 0.1324 - accuracy: 0.9497\n",
      "Epoch 228/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9497\n",
      "Epoch 229/1500\n",
      "25/25 [==============================] - 0s 976us/step - loss: 0.1307 - accuracy: 0.9600\n",
      "Epoch 230/1500\n",
      "25/25 [==============================] - 0s 987us/step - loss: 0.1217 - accuracy: 0.9523\n",
      "Epoch 231/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9458\n",
      "Epoch 232/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9510\n",
      "Epoch 233/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9523\n",
      "Epoch 234/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9548\n",
      "Epoch 235/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.9613\n",
      "Epoch 236/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9652\n",
      "Epoch 237/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9432\n",
      "Epoch 238/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9613\n",
      "Epoch 239/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9510\n",
      "Epoch 240/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9574\n",
      "Epoch 241/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9665\n",
      "Epoch 242/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9626\n",
      "Epoch 243/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9652\n",
      "Epoch 244/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9523\n",
      "Epoch 245/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9548\n",
      "Epoch 246/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9639\n",
      "Epoch 247/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9548\n",
      "Epoch 248/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9587\n",
      "Epoch 249/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9471\n",
      "Epoch 250/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.1068 - accuracy: 0.9574\n",
      "Epoch 251/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9665\n",
      "Epoch 252/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.9794\n",
      "Epoch 253/1500\n",
      "25/25 [==============================] - 0s 980us/step - loss: 0.1305 - accuracy: 0.9574\n",
      "Epoch 254/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9690\n",
      "Epoch 255/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9652\n",
      "Epoch 256/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9587\n",
      "Epoch 257/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9703\n",
      "Epoch 258/1500\n",
      "25/25 [==============================] - 0s 983us/step - loss: 0.1036 - accuracy: 0.9600\n",
      "Epoch 259/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9652\n",
      "Epoch 260/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9665\n",
      "Epoch 261/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9639\n",
      "Epoch 262/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9652\n",
      "Epoch 263/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9755\n",
      "Epoch 264/1500\n",
      "25/25 [==============================] - 0s 960us/step - loss: 0.1121 - accuracy: 0.9639\n",
      "Epoch 265/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9510\n",
      "Epoch 266/1500\n",
      "25/25 [==============================] - 0s 937us/step - loss: 0.1183 - accuracy: 0.9574\n",
      "Epoch 267/1500\n",
      "25/25 [==============================] - 0s 943us/step - loss: 0.1115 - accuracy: 0.9652\n",
      "Epoch 268/1500\n",
      "25/25 [==============================] - 0s 991us/step - loss: 0.1151 - accuracy: 0.9690\n",
      "Epoch 269/1500\n",
      "25/25 [==============================] - 0s 970us/step - loss: 0.1168 - accuracy: 0.9510\n",
      "Epoch 270/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9690\n",
      "Epoch 271/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9600\n",
      "Epoch 272/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9613\n",
      "Epoch 273/1500\n",
      "25/25 [==============================] - 0s 993us/step - loss: 0.1065 - accuracy: 0.9613\n",
      "Epoch 274/1500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.1038 - accuracy: 0.9600\n",
      "Epoch 275/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1040 - accuracy: 0.9639\n",
      "Epoch 276/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9600\n",
      "Epoch 277/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9742\n",
      "Epoch 278/1500\n",
      "25/25 [==============================] - 0s 978us/step - loss: 0.1192 - accuracy: 0.9510\n",
      "Epoch 279/1500\n",
      "25/25 [==============================] - 0s 925us/step - loss: 0.1080 - accuracy: 0.9665\n",
      "Epoch 280/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9600\n",
      "Epoch 281/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9626\n",
      "Epoch 282/1500\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0350 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 252.\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9548\n",
      "Epoch 282: early stopping\n",
      "6/6 [==============================] - 0s 913us/step - loss: 1.0957 - accuracy: 0.6852\n",
      "6/6 [==============================] - 0s 608us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (19/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "Final Test Results - Loss: 1.0956519842147827, Accuracy: 0.6851851940155029, Precision: 0.67644726407613, Recall: 0.7321349963316214, F1 Score: 0.6991370568880244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[70  4 20]\n",
      " [ 1  9  0]\n",
      " [26  0 32]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "016A    10\n",
      "033A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "108A     6\n",
      "007A     6\n",
      "021A     5\n",
      "034A     5\n",
      "025C     5\n",
      "023B     5\n",
      "075A     5\n",
      "052A     4\n",
      "104A     4\n",
      "026A     4\n",
      "035A     4\n",
      "009A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "056A     3\n",
      "058A     3\n",
      "113A     3\n",
      "038A     2\n",
      "069A     2\n",
      "093A     2\n",
      "087A     2\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "032A     2\n",
      "018A     2\n",
      "115A     1\n",
      "110A     1\n",
      "019B     1\n",
      "090A     1\n",
      "004A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "076A     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "057A    27\n",
      "000B    19\n",
      "029A    17\n",
      "106A    14\n",
      "028A    13\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "071A    10\n",
      "005A    10\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "037A     6\n",
      "044A     5\n",
      "070A     5\n",
      "105A     4\n",
      "014A     3\n",
      "054A     2\n",
      "025B     2\n",
      "096A     1\n",
      "049A     1\n",
      "048A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    264\n",
      "X    198\n",
      "F    193\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    150\n",
      "M     73\n",
      "F     59\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 019...\n",
      "kitten    [014B, 111A, 040A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 116A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 028A, 022A, 029A, 095A, 005A, 039A, 013...\n",
      "kitten                             [044A, 046A, 049A, 048A]\n",
      "senior                             [057A, 106A, 051B, 054A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '047A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '046A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'110A'}\n",
      "Moved to Test Set:\n",
      "{'110A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '046A'\n",
      " '047A' '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A'\n",
      " '061A' '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A'\n",
      " '097B' '099A' '101A' '102A' '103A' '104A' '108A' '109A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '048A' '049A' '051B' '054A' '057A' '068A'\n",
      " '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A' '110A']\n",
      "Length of X_train_val:\n",
      "717\n",
      "Length of y_train_val:\n",
      "717\n",
      "Length of groups_train_val:\n",
      "717\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    126\n",
      "kitten    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     70\n",
      "senior     52\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "kitten    163\n",
      "senior    126\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "senior     52\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 428, 1: 163, 2: 126})\n",
      "Epoch 1/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.2066 - accuracy: 0.4826\n",
      "Epoch 2/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8884 - accuracy: 0.6290\n",
      "Epoch 3/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8395 - accuracy: 0.6513\n",
      "Epoch 4/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7614 - accuracy: 0.6918\n",
      "Epoch 5/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7824 - accuracy: 0.6625\n",
      "Epoch 6/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.7015 - accuracy: 0.7141\n",
      "Epoch 7/1500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.6973 - accuracy: 0.7211\n",
      "Epoch 8/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6413 - accuracy: 0.7462\n",
      "Epoch 9/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6161 - accuracy: 0.7559\n",
      "Epoch 10/1500\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.6192 - accuracy: 0.7503\n",
      "Epoch 11/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5412 - accuracy: 0.7936\n",
      "Epoch 12/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5199 - accuracy: 0.8075\n",
      "Epoch 13/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5381 - accuracy: 0.7908\n",
      "Epoch 14/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5159 - accuracy: 0.7894\n",
      "Epoch 15/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5014 - accuracy: 0.7964\n",
      "Epoch 16/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5234 - accuracy: 0.7908\n",
      "Epoch 17/1500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.5062 - accuracy: 0.8006\n",
      "Epoch 18/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.4761 - accuracy: 0.8103\n",
      "Epoch 19/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4482 - accuracy: 0.8187\n",
      "Epoch 20/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4304 - accuracy: 0.8285\n",
      "Epoch 21/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.4397 - accuracy: 0.8312\n",
      "Epoch 22/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4158 - accuracy: 0.8424\n",
      "Epoch 23/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4254 - accuracy: 0.8438\n",
      "Epoch 24/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3884 - accuracy: 0.8452\n",
      "Epoch 25/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3870 - accuracy: 0.8689\n",
      "Epoch 26/1500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.3827 - accuracy: 0.8563\n",
      "Epoch 27/1500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.3870 - accuracy: 0.8577\n",
      "Epoch 28/1500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.4119 - accuracy: 0.8215\n",
      "Epoch 29/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3723 - accuracy: 0.8522\n",
      "Epoch 30/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.3894 - accuracy: 0.8494\n",
      "Epoch 31/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.3869 - accuracy: 0.8522\n",
      "Epoch 32/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3855 - accuracy: 0.8452\n",
      "Epoch 33/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.3791 - accuracy: 0.8480\n",
      "Epoch 34/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.3712 - accuracy: 0.8661\n",
      "Epoch 35/1500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.3696 - accuracy: 0.8619\n",
      "Epoch 36/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3547 - accuracy: 0.8647\n",
      "Epoch 37/1500\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.3606 - accuracy: 0.8591\n",
      "Epoch 38/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3152 - accuracy: 0.8661\n",
      "Epoch 39/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.3177 - accuracy: 0.8787\n",
      "Epoch 40/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.3178 - accuracy: 0.8717\n",
      "Epoch 41/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3672 - accuracy: 0.8577\n",
      "Epoch 42/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3061 - accuracy: 0.8731\n",
      "Epoch 43/1500\n",
      "23/23 [==============================] - 0s 934us/step - loss: 0.3342 - accuracy: 0.8675\n",
      "Epoch 44/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3083 - accuracy: 0.8787\n",
      "Epoch 45/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3266 - accuracy: 0.8801\n",
      "Epoch 46/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8703\n",
      "Epoch 47/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.3167 - accuracy: 0.8828\n",
      "Epoch 48/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.3044 - accuracy: 0.8828\n",
      "Epoch 49/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3214 - accuracy: 0.8815\n",
      "Epoch 50/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2983 - accuracy: 0.8731\n",
      "Epoch 51/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.2968 - accuracy: 0.8842\n",
      "Epoch 52/1500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.2926 - accuracy: 0.8870\n",
      "Epoch 53/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8787\n",
      "Epoch 54/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2600 - accuracy: 0.9024\n",
      "Epoch 55/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2932 - accuracy: 0.8912\n",
      "Epoch 56/1500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.2796 - accuracy: 0.8856\n",
      "Epoch 57/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.2600 - accuracy: 0.9038\n",
      "Epoch 58/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2690 - accuracy: 0.8940\n",
      "Epoch 59/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2682 - accuracy: 0.9038\n",
      "Epoch 60/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2529 - accuracy: 0.9024\n",
      "Epoch 61/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2713 - accuracy: 0.8926\n",
      "Epoch 62/1500\n",
      "23/23 [==============================] - 0s 909us/step - loss: 0.2574 - accuracy: 0.9121\n",
      "Epoch 63/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.9177\n",
      "Epoch 64/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.9052\n",
      "Epoch 65/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.2743 - accuracy: 0.9079\n",
      "Epoch 66/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.2638 - accuracy: 0.8982\n",
      "Epoch 67/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.2576 - accuracy: 0.9038\n",
      "Epoch 68/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.2543 - accuracy: 0.9079\n",
      "Epoch 69/1500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.2642 - accuracy: 0.9052\n",
      "Epoch 70/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.2299 - accuracy: 0.9079\n",
      "Epoch 71/1500\n",
      "23/23 [==============================] - 0s 900us/step - loss: 0.2641 - accuracy: 0.9038\n",
      "Epoch 72/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2226 - accuracy: 0.9219\n",
      "Epoch 73/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2218 - accuracy: 0.9191\n",
      "Epoch 74/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.2432 - accuracy: 0.9010\n",
      "Epoch 75/1500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.2313 - accuracy: 0.9093\n",
      "Epoch 76/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.2481 - accuracy: 0.8996\n",
      "Epoch 77/1500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.2393 - accuracy: 0.9066\n",
      "Epoch 78/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.2288 - accuracy: 0.9093\n",
      "Epoch 79/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.2262 - accuracy: 0.9163\n",
      "Epoch 80/1500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.2329 - accuracy: 0.9121\n",
      "Epoch 81/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.2316 - accuracy: 0.9149\n",
      "Epoch 82/1500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.2181 - accuracy: 0.9233\n",
      "Epoch 83/1500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.2451 - accuracy: 0.9121\n",
      "Epoch 84/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2253 - accuracy: 0.9149\n",
      "Epoch 85/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.2451 - accuracy: 0.9052\n",
      "Epoch 86/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.2235 - accuracy: 0.9191\n",
      "Epoch 87/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.1979 - accuracy: 0.9275\n",
      "Epoch 88/1500\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.2019 - accuracy: 0.9331\n",
      "Epoch 89/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2179 - accuracy: 0.9163\n",
      "Epoch 90/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9289\n",
      "Epoch 91/1500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.2162 - accuracy: 0.9219\n",
      "Epoch 92/1500\n",
      "23/23 [==============================] - 0s 912us/step - loss: 0.2265 - accuracy: 0.9191\n",
      "Epoch 93/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.2057 - accuracy: 0.9289\n",
      "Epoch 94/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.9135\n",
      "Epoch 95/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2104 - accuracy: 0.9247\n",
      "Epoch 96/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1878 - accuracy: 0.9414\n",
      "Epoch 97/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.2010 - accuracy: 0.9191\n",
      "Epoch 98/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2086 - accuracy: 0.9261\n",
      "Epoch 99/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.2093 - accuracy: 0.9247\n",
      "Epoch 100/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2245 - accuracy: 0.9205\n",
      "Epoch 101/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.1916 - accuracy: 0.9303\n",
      "Epoch 102/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9442\n",
      "Epoch 103/1500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.1989 - accuracy: 0.9289\n",
      "Epoch 104/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.9331\n",
      "Epoch 105/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.9303\n",
      "Epoch 106/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9400\n",
      "Epoch 107/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.1799 - accuracy: 0.9358\n",
      "Epoch 108/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.2057 - accuracy: 0.9261\n",
      "Epoch 109/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.2036 - accuracy: 0.9191\n",
      "Epoch 110/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9191\n",
      "Epoch 111/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.1843 - accuracy: 0.9317\n",
      "Epoch 112/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9358\n",
      "Epoch 113/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.1785 - accuracy: 0.9372\n",
      "Epoch 114/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.1509 - accuracy: 0.9498\n",
      "Epoch 115/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1729 - accuracy: 0.9358\n",
      "Epoch 116/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.9484\n",
      "Epoch 117/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9512\n",
      "Epoch 118/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.1499 - accuracy: 0.9498\n",
      "Epoch 119/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1672 - accuracy: 0.9414\n",
      "Epoch 120/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.1710 - accuracy: 0.9428\n",
      "Epoch 121/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.1648 - accuracy: 0.9372\n",
      "Epoch 122/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1564 - accuracy: 0.9526\n",
      "Epoch 123/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.1967 - accuracy: 0.9331\n",
      "Epoch 124/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.1745 - accuracy: 0.9372\n",
      "Epoch 125/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.1728 - accuracy: 0.9331\n",
      "Epoch 126/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.1560 - accuracy: 0.9484\n",
      "Epoch 127/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.1689 - accuracy: 0.9344\n",
      "Epoch 128/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.9526\n",
      "Epoch 129/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.1693 - accuracy: 0.9470\n",
      "Epoch 130/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9442\n",
      "Epoch 131/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.1532 - accuracy: 0.9540\n",
      "Epoch 132/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.1561 - accuracy: 0.9470\n",
      "Epoch 133/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.1601 - accuracy: 0.9484\n",
      "Epoch 134/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9344\n",
      "Epoch 135/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.1580 - accuracy: 0.9386\n",
      "Epoch 136/1500\n",
      "23/23 [==============================] - 0s 6ms/step - loss: 0.1575 - accuracy: 0.9470\n",
      "Epoch 137/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1348 - accuracy: 0.9568\n",
      "Epoch 138/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9596\n",
      "Epoch 139/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9470\n",
      "Epoch 140/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9456\n",
      "Epoch 141/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.1867 - accuracy: 0.9344\n",
      "Epoch 142/1500\n",
      "23/23 [==============================] - 0s 925us/step - loss: 0.1558 - accuracy: 0.9484\n",
      "Epoch 143/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.1722 - accuracy: 0.9344\n",
      "Epoch 144/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9526\n",
      "Epoch 145/1500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.1688 - accuracy: 0.9512\n",
      "Epoch 146/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.1536 - accuracy: 0.9344\n",
      "Epoch 147/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9428\n",
      "Epoch 148/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.1486 - accuracy: 0.9512\n",
      "Epoch 149/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.9428\n",
      "Epoch 150/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9540\n",
      "Epoch 151/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9442\n",
      "Epoch 152/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9554\n",
      "Epoch 153/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9372\n",
      "Epoch 154/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.1498 - accuracy: 0.9442\n",
      "Epoch 155/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9526\n",
      "Epoch 156/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.1378 - accuracy: 0.9456\n",
      "Epoch 157/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.9442\n",
      "Epoch 158/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.1359 - accuracy: 0.9512\n",
      "Epoch 159/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.1308 - accuracy: 0.9568\n",
      "Epoch 160/1500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.1181 - accuracy: 0.9637\n",
      "Epoch 161/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9554\n",
      "Epoch 162/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.1103 - accuracy: 0.9665\n",
      "Epoch 163/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9637\n",
      "Epoch 164/1500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.1197 - accuracy: 0.9582\n",
      "Epoch 165/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9568\n",
      "Epoch 166/1500\n",
      "23/23 [==============================] - 0s 936us/step - loss: 0.1573 - accuracy: 0.9414\n",
      "Epoch 167/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.1361 - accuracy: 0.9470\n",
      "Epoch 168/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.9442\n",
      "Epoch 169/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9609\n",
      "Epoch 170/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9651\n",
      "Epoch 171/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1331 - accuracy: 0.9526\n",
      "Epoch 172/1500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.1255 - accuracy: 0.9623\n",
      "Epoch 173/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.1384 - accuracy: 0.9498\n",
      "Epoch 174/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.1166 - accuracy: 0.9554\n",
      "Epoch 175/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9512\n",
      "Epoch 176/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1279 - accuracy: 0.9540\n",
      "Epoch 177/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.1065 - accuracy: 0.9735\n",
      "Epoch 178/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.1127 - accuracy: 0.9693\n",
      "Epoch 179/1500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.1389 - accuracy: 0.9470\n",
      "Epoch 180/1500\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.1252 - accuracy: 0.9512\n",
      "Epoch 181/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9540\n",
      "Epoch 182/1500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.1261 - accuracy: 0.9442\n",
      "Epoch 183/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9609\n",
      "Epoch 184/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9665\n",
      "Epoch 185/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9623\n",
      "Epoch 186/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.1245 - accuracy: 0.9512\n",
      "Epoch 187/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.1180 - accuracy: 0.9596\n",
      "Epoch 188/1500\n",
      "23/23 [==============================] - 0s 930us/step - loss: 0.1028 - accuracy: 0.9693\n",
      "Epoch 189/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9651\n",
      "Epoch 190/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9637\n",
      "Epoch 191/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.1094 - accuracy: 0.9693\n",
      "Epoch 192/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.1059 - accuracy: 0.9623\n",
      "Epoch 193/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.1057 - accuracy: 0.9721\n",
      "Epoch 194/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9554\n",
      "Epoch 195/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.1186 - accuracy: 0.9609\n",
      "Epoch 196/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.0949 - accuracy: 0.9707\n",
      "Epoch 197/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9623\n",
      "Epoch 198/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9651\n",
      "Epoch 199/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.1035 - accuracy: 0.9679\n",
      "Epoch 200/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.1065 - accuracy: 0.9791\n",
      "Epoch 201/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.1130 - accuracy: 0.9637\n",
      "Epoch 202/1500\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.1163 - accuracy: 0.9623\n",
      "Epoch 203/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.0932 - accuracy: 0.9623\n",
      "Epoch 204/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.9665\n",
      "Epoch 205/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.1061 - accuracy: 0.9693\n",
      "Epoch 206/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1173 - accuracy: 0.9554\n",
      "Epoch 207/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9623\n",
      "Epoch 208/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9512\n",
      "Epoch 209/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9665\n",
      "Epoch 210/1500\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.1131 - accuracy: 0.9568\n",
      "Epoch 211/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.9805\n",
      "Epoch 212/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.1110 - accuracy: 0.9637\n",
      "Epoch 213/1500\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.1087 - accuracy: 0.9596\n",
      "Epoch 214/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0956 - accuracy: 0.9679\n",
      "Epoch 215/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.1101 - accuracy: 0.9679\n",
      "Epoch 216/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.0904 - accuracy: 0.9707\n",
      "Epoch 217/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.1058 - accuracy: 0.9679\n",
      "Epoch 218/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.0823 - accuracy: 0.9777\n",
      "Epoch 219/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9693\n",
      "Epoch 220/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9707\n",
      "Epoch 221/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.9707\n",
      "Epoch 222/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.9651\n",
      "Epoch 223/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.1074 - accuracy: 0.9609\n",
      "Epoch 224/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.1036 - accuracy: 0.9596\n",
      "Epoch 225/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.0925 - accuracy: 0.9721\n",
      "Epoch 226/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.0838 - accuracy: 0.9735\n",
      "Epoch 227/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.0830 - accuracy: 0.9735\n",
      "Epoch 228/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9707\n",
      "Epoch 229/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9707\n",
      "Epoch 230/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.0858 - accuracy: 0.9721\n",
      "Epoch 231/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.0919 - accuracy: 0.9679\n",
      "Epoch 232/1500\n",
      "23/23 [==============================] - 0s 934us/step - loss: 0.1286 - accuracy: 0.9568\n",
      "Epoch 233/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9637\n",
      "Epoch 234/1500\n",
      "23/23 [==============================] - 0s 939us/step - loss: 0.0918 - accuracy: 0.9665\n",
      "Epoch 235/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.0882 - accuracy: 0.9777\n",
      "Epoch 236/1500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.0956 - accuracy: 0.9665\n",
      "Epoch 237/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.0981 - accuracy: 0.9693\n",
      "Epoch 238/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9596\n",
      "Epoch 239/1500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0929 - accuracy: 0.9735\n",
      "Epoch 240/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0857 - accuracy: 0.9749\n",
      "Epoch 241/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9847\n",
      "Epoch 242/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9749\n",
      "Epoch 243/1500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.1078 - accuracy: 0.9609\n",
      "Epoch 244/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.1082 - accuracy: 0.9623\n",
      "Epoch 245/1500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.1094 - accuracy: 0.9554\n",
      "Epoch 246/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.0899 - accuracy: 0.9665\n",
      "Epoch 247/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9582\n",
      "Epoch 248/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.0994 - accuracy: 0.9568\n",
      "Epoch 249/1500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.0836 - accuracy: 0.9679\n",
      "Epoch 250/1500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.0869 - accuracy: 0.9749\n",
      "Epoch 251/1500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.0938 - accuracy: 0.9707\n",
      "Epoch 252/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.0807 - accuracy: 0.9735\n",
      "Epoch 253/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9749\n",
      "Epoch 254/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9665\n",
      "Epoch 255/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9791\n",
      "Epoch 256/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.0772 - accuracy: 0.9833\n",
      "Epoch 257/1500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.0794 - accuracy: 0.9763\n",
      "Epoch 258/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.9833\n",
      "Epoch 259/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.0814 - accuracy: 0.9735\n",
      "Epoch 260/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9805\n",
      "Epoch 261/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9721\n",
      "Epoch 262/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9679\n",
      "Epoch 263/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9735\n",
      "Epoch 264/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.0672 - accuracy: 0.9805\n",
      "Epoch 265/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.0767 - accuracy: 0.9819\n",
      "Epoch 266/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.0875 - accuracy: 0.9679\n",
      "Epoch 267/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9847\n",
      "Epoch 268/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.0821 - accuracy: 0.9735\n",
      "Epoch 269/1500\n",
      "23/23 [==============================] - 0s 959us/step - loss: 0.0739 - accuracy: 0.9763\n",
      "Epoch 270/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.9833\n",
      "Epoch 271/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.9833\n",
      "Epoch 272/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.0677 - accuracy: 0.9819\n",
      "Epoch 273/1500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.0772 - accuracy: 0.9707\n",
      "Epoch 274/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.0707 - accuracy: 0.9749\n",
      "Epoch 275/1500\n",
      "23/23 [==============================] - 0s 937us/step - loss: 0.0736 - accuracy: 0.9791\n",
      "Epoch 276/1500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.0675 - accuracy: 0.9791\n",
      "Epoch 277/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9721\n",
      "Epoch 278/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.0864 - accuracy: 0.9679\n",
      "Epoch 279/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.0812 - accuracy: 0.9763\n",
      "Epoch 280/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.0597 - accuracy: 0.9902\n",
      "Epoch 281/1500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.0670 - accuracy: 0.9777\n",
      "Epoch 282/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.0692 - accuracy: 0.9791\n",
      "Epoch 283/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9568\n",
      "Epoch 284/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.1045 - accuracy: 0.9623\n",
      "Epoch 285/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.0855 - accuracy: 0.9777\n",
      "Epoch 286/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.0668 - accuracy: 0.9833\n",
      "Epoch 287/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9777\n",
      "Epoch 288/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.0842 - accuracy: 0.9721\n",
      "Epoch 289/1500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.0694 - accuracy: 0.9833\n",
      "Epoch 290/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.0761 - accuracy: 0.9791\n",
      "Epoch 291/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.0644 - accuracy: 0.9861\n",
      "Epoch 292/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.0764 - accuracy: 0.9777\n",
      "Epoch 293/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.0671 - accuracy: 0.9819\n",
      "Epoch 294/1500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.0690 - accuracy: 0.9763\n",
      "Epoch 295/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.0652 - accuracy: 0.9819\n",
      "Epoch 296/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.0797 - accuracy: 0.9707\n",
      "Epoch 297/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.0763 - accuracy: 0.9791\n",
      "Epoch 298/1500\n",
      "23/23 [==============================] - 0s 934us/step - loss: 0.0640 - accuracy: 0.9819\n",
      "Epoch 299/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.0670 - accuracy: 0.9749\n",
      "Epoch 300/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.0619 - accuracy: 0.9847\n",
      "Epoch 301/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.0767 - accuracy: 0.9777\n",
      "Epoch 302/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.0581 - accuracy: 0.9874\n",
      "Epoch 303/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9805\n",
      "Epoch 304/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.0666 - accuracy: 0.9833\n",
      "Epoch 305/1500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.0599 - accuracy: 0.9874\n",
      "Epoch 306/1500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.0647 - accuracy: 0.9805\n",
      "Epoch 307/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.0563 - accuracy: 0.9902\n",
      "Epoch 308/1500\n",
      "23/23 [==============================] - 0s 924us/step - loss: 0.0974 - accuracy: 0.9568\n",
      "Epoch 309/1500\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.0789 - accuracy: 0.9679\n",
      "Epoch 310/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.0740 - accuracy: 0.9721\n",
      "Epoch 311/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9568\n",
      "Epoch 312/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.0693 - accuracy: 0.9735\n",
      "Epoch 313/1500\n",
      "23/23 [==============================] - 0s 943us/step - loss: 0.0623 - accuracy: 0.9805\n",
      "Epoch 314/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.9791\n",
      "Epoch 315/1500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.0615 - accuracy: 0.9805\n",
      "Epoch 316/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.0741 - accuracy: 0.9763\n",
      "Epoch 317/1500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.0566 - accuracy: 0.9833\n",
      "Epoch 318/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.0613 - accuracy: 0.9847\n",
      "Epoch 319/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.0520 - accuracy: 0.9916\n",
      "Epoch 320/1500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.0626 - accuracy: 0.9847\n",
      "Epoch 321/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.9847\n",
      "Epoch 322/1500\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.0687 - accuracy: 0.9763\n",
      "Epoch 323/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.0726 - accuracy: 0.9791\n",
      "Epoch 324/1500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.0615 - accuracy: 0.9819\n",
      "Epoch 325/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.0665 - accuracy: 0.9707\n",
      "Epoch 326/1500\n",
      "23/23 [==============================] - 0s 959us/step - loss: 0.0599 - accuracy: 0.9805\n",
      "Epoch 327/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.0765 - accuracy: 0.9721\n",
      "Epoch 328/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.0541 - accuracy: 0.9874\n",
      "Epoch 329/1500\n",
      "23/23 [==============================] - 0s 929us/step - loss: 0.0653 - accuracy: 0.9874\n",
      "Epoch 330/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9735\n",
      "Epoch 331/1500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.0707 - accuracy: 0.9777\n",
      "Epoch 332/1500\n",
      "23/23 [==============================] - 0s 935us/step - loss: 0.0882 - accuracy: 0.9707\n",
      "Epoch 333/1500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0932 - accuracy: 0.9637\n",
      "Epoch 334/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9805\n",
      "Epoch 335/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0630 - accuracy: 0.9861\n",
      "Epoch 336/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.9791\n",
      "Epoch 337/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.0526 - accuracy: 0.9902\n",
      "Epoch 338/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9777\n",
      "Epoch 339/1500\n",
      "23/23 [==============================] - 0s 942us/step - loss: 0.0702 - accuracy: 0.9777\n",
      "Epoch 340/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.0543 - accuracy: 0.9874\n",
      "Epoch 341/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.0802 - accuracy: 0.9763\n",
      "Epoch 342/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0500 - accuracy: 0.9833\n",
      "Epoch 343/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.0475 - accuracy: 0.9888\n",
      "Epoch 344/1500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.0683 - accuracy: 0.9749\n",
      "Epoch 345/1500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.0752 - accuracy: 0.9763\n",
      "Epoch 346/1500\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.0537 - accuracy: 0.9861\n",
      "Epoch 347/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0558 - accuracy: 0.9847\n",
      "Epoch 348/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0517 - accuracy: 0.9833\n",
      "Epoch 349/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.0598 - accuracy: 0.9847\n",
      "Epoch 350/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.0623 - accuracy: 0.9777\n",
      "Epoch 351/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.0519 - accuracy: 0.9847\n",
      "Epoch 352/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.9805\n",
      "Epoch 353/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.0544 - accuracy: 0.9861\n",
      "Epoch 354/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.0549 - accuracy: 0.9819\n",
      "Epoch 355/1500\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.0512 - accuracy: 0.9874\n",
      "Epoch 356/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.0553 - accuracy: 0.9861\n",
      "Epoch 357/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0556 - accuracy: 0.9791\n",
      "Epoch 358/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.0569 - accuracy: 0.9847\n",
      "Epoch 359/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.0615 - accuracy: 0.9805\n",
      "Epoch 360/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9707\n",
      "Epoch 361/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.0647 - accuracy: 0.9791\n",
      "Epoch 362/1500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.0551 - accuracy: 0.9833\n",
      "Epoch 363/1500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.0612 - accuracy: 0.9833\n",
      "Epoch 364/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0540 - accuracy: 0.9833\n",
      "Epoch 365/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.0531 - accuracy: 0.9833\n",
      "Epoch 366/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.0538 - accuracy: 0.9861\n",
      "Epoch 367/1500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.0728 - accuracy: 0.9777\n",
      "Epoch 368/1500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.0687 - accuracy: 0.9735\n",
      "Epoch 369/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0565 - accuracy: 0.9791\n",
      "Epoch 370/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.9847\n",
      "Epoch 371/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.0644 - accuracy: 0.9805\n",
      "Epoch 372/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.0625 - accuracy: 0.9791\n",
      "Epoch 373/1500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.0864 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 343.\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9679\n",
      "Epoch 373: early stopping\n",
      "7/7 [==============================] - 0s 831us/step - loss: 1.0531 - accuracy: 0.7136\n",
      "7/7 [==============================] - 0s 613us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.69 (20/29)\n",
      "Before appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "After appending - Cat IDs: 382, Predictions: 382, Actuals: 382, Gender: 382\n",
      "Final Test Results - Loss: 1.0531020164489746, Accuracy: 0.7136363387107849, Precision: 0.7364369060507637, Recall: 0.655448717948718, F1 Score: 0.6899544692685454\n",
      "Confusion Matrix:\n",
      " [[130   0  30]\n",
      " [  2   6   0]\n",
      " [ 31   0  21]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "106A    14\n",
      "042A    14\n",
      "028A    13\n",
      "111A    13\n",
      "039A    12\n",
      "068A    11\n",
      "063A    11\n",
      "036A    11\n",
      "025A    11\n",
      "005A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "051B     9\n",
      "022A     9\n",
      "065A     9\n",
      "045A     9\n",
      "015A     9\n",
      "010A     8\n",
      "095A     8\n",
      "094A     8\n",
      "013B     8\n",
      "031A     7\n",
      "117A     7\n",
      "053A     6\n",
      "108A     6\n",
      "008A     6\n",
      "109A     6\n",
      "007A     6\n",
      "037A     6\n",
      "070A     5\n",
      "021A     5\n",
      "044A     5\n",
      "023B     5\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "062A     4\n",
      "009A     4\n",
      "104A     4\n",
      "058A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "056A     3\n",
      "113A     3\n",
      "025B     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "054A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "032A     2\n",
      "069A     2\n",
      "049A     1\n",
      "088A     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "004A     1\n",
      "092A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "073A     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "059A    14\n",
      "097B    14\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "072A     9\n",
      "033A     9\n",
      "027A     7\n",
      "099A     7\n",
      "050A     7\n",
      "023A     6\n",
      "034A     5\n",
      "025C     5\n",
      "075A     5\n",
      "052A     4\n",
      "003A     4\n",
      "012A     3\n",
      "006A     3\n",
      "018A     2\n",
      "026C     1\n",
      "019B     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    250\n",
      "F    202\n",
      "M    180\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    157\n",
      "X     98\n",
      "F     50\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 103A, 071A, 028A, 062A, 101...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 043...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 097B, 019A, 074A, 067A, 020A, 002...\n",
      "kitten                                   [047A, 050A, 115A]\n",
      "senior                       [097A, 059A, 116A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 52, 'kitten': 13, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 22, 'kitten': 3, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "632\n",
      "Length of y_train_val:\n",
      "632\n",
      "Length of groups_train_val:\n",
      "632\n",
      "No common groups found between train and test sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 374, 1: 135, 2: 123})\n",
      "Epoch 1/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 1.2309 - accuracy: 0.4652\n",
      "Epoch 2/1500\n",
      "20/20 [==============================] - 0s 975us/step - loss: 0.9535 - accuracy: 0.5570\n",
      "Epoch 3/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.8881 - accuracy: 0.5997\n",
      "Epoch 4/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.8552 - accuracy: 0.6329\n",
      "Epoch 5/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7980 - accuracy: 0.6535\n",
      "Epoch 6/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7544 - accuracy: 0.6741\n",
      "Epoch 7/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.7089 - accuracy: 0.7041\n",
      "Epoch 8/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6916 - accuracy: 0.6978\n",
      "Epoch 9/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6741 - accuracy: 0.7168\n",
      "Epoch 10/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6408 - accuracy: 0.7373\n",
      "Epoch 11/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.6097 - accuracy: 0.7516\n",
      "Epoch 12/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5901 - accuracy: 0.7674\n",
      "Epoch 13/1500\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.5809 - accuracy: 0.7658\n",
      "Epoch 14/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.7579\n",
      "Epoch 15/1500\n",
      "20/20 [==============================] - 0s 984us/step - loss: 0.5562 - accuracy: 0.7532\n",
      "Epoch 16/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5616 - accuracy: 0.7595\n",
      "Epoch 17/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5506 - accuracy: 0.7706\n",
      "Epoch 18/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5154 - accuracy: 0.8070\n",
      "Epoch 19/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5278 - accuracy: 0.7864\n",
      "Epoch 20/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5664 - accuracy: 0.7547\n",
      "Epoch 21/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4679 - accuracy: 0.8038\n",
      "Epoch 22/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5514 - accuracy: 0.7753\n",
      "Epoch 23/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5450 - accuracy: 0.7801\n",
      "Epoch 24/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4744 - accuracy: 0.8006\n",
      "Epoch 25/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4695 - accuracy: 0.8165\n",
      "Epoch 26/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4621 - accuracy: 0.8085\n",
      "Epoch 27/1500\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.4736 - accuracy: 0.7975\n",
      "Epoch 28/1500\n",
      "20/20 [==============================] - 0s 996us/step - loss: 0.4497 - accuracy: 0.8022\n",
      "Epoch 29/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4461 - accuracy: 0.8196\n",
      "Epoch 30/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4400 - accuracy: 0.8165\n",
      "Epoch 31/1500\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.4515 - accuracy: 0.8275\n",
      "Epoch 32/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4242 - accuracy: 0.8354\n",
      "Epoch 33/1500\n",
      "20/20 [==============================] - 0s 992us/step - loss: 0.4343 - accuracy: 0.8149\n",
      "Epoch 34/1500\n",
      "20/20 [==============================] - 0s 982us/step - loss: 0.4600 - accuracy: 0.8101\n",
      "Epoch 35/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4123 - accuracy: 0.8323\n",
      "Epoch 36/1500\n",
      "20/20 [==============================] - 0s 990us/step - loss: 0.4249 - accuracy: 0.8196\n",
      "Epoch 37/1500\n",
      "20/20 [==============================] - 0s 980us/step - loss: 0.4237 - accuracy: 0.8323\n",
      "Epoch 38/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3977 - accuracy: 0.8513\n",
      "Epoch 39/1500\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.3801 - accuracy: 0.8465\n",
      "Epoch 40/1500\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8402\n",
      "Epoch 41/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3885 - accuracy: 0.8544\n",
      "Epoch 42/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4051 - accuracy: 0.8180\n",
      "Epoch 43/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4061 - accuracy: 0.8259\n",
      "Epoch 44/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.8513\n",
      "Epoch 45/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8592\n",
      "Epoch 46/1500\n",
      "20/20 [==============================] - 0s 988us/step - loss: 0.3712 - accuracy: 0.8386\n",
      "Epoch 47/1500\n",
      "20/20 [==============================] - 0s 954us/step - loss: 0.4143 - accuracy: 0.8180\n",
      "Epoch 48/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3754 - accuracy: 0.8418\n",
      "Epoch 49/1500\n",
      "20/20 [==============================] - 0s 978us/step - loss: 0.3657 - accuracy: 0.8497\n",
      "Epoch 50/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3843 - accuracy: 0.8291\n",
      "Epoch 51/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3494 - accuracy: 0.8592\n",
      "Epoch 52/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3280 - accuracy: 0.8687\n",
      "Epoch 53/1500\n",
      "20/20 [==============================] - 0s 990us/step - loss: 0.3594 - accuracy: 0.8544\n",
      "Epoch 54/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3368 - accuracy: 0.8639\n",
      "Epoch 55/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3595 - accuracy: 0.8576\n",
      "Epoch 56/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3163 - accuracy: 0.8703\n",
      "Epoch 57/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3401 - accuracy: 0.8560\n",
      "Epoch 58/1500\n",
      "20/20 [==============================] - 0s 991us/step - loss: 0.3197 - accuracy: 0.8797\n",
      "Epoch 59/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3370 - accuracy: 0.8718\n",
      "Epoch 60/1500\n",
      "20/20 [==============================] - 0s 969us/step - loss: 0.3277 - accuracy: 0.8576\n",
      "Epoch 61/1500\n",
      "20/20 [==============================] - 0s 959us/step - loss: 0.3223 - accuracy: 0.8750\n",
      "Epoch 62/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3322 - accuracy: 0.8703\n",
      "Epoch 63/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3391 - accuracy: 0.8718\n",
      "Epoch 64/1500\n",
      "20/20 [==============================] - 0s 983us/step - loss: 0.3285 - accuracy: 0.8703\n",
      "Epoch 65/1500\n",
      "20/20 [==============================] - 0s 988us/step - loss: 0.3206 - accuracy: 0.8718\n",
      "Epoch 66/1500\n",
      "20/20 [==============================] - 0s 970us/step - loss: 0.2938 - accuracy: 0.8845\n",
      "Epoch 67/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.8877\n",
      "Epoch 68/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2766 - accuracy: 0.8924\n",
      "Epoch 69/1500\n",
      "20/20 [==============================] - 0s 992us/step - loss: 0.2973 - accuracy: 0.8766\n",
      "Epoch 70/1500\n",
      "20/20 [==============================] - 0s 972us/step - loss: 0.2862 - accuracy: 0.8671\n",
      "Epoch 71/1500\n",
      "20/20 [==============================] - 0s 988us/step - loss: 0.2964 - accuracy: 0.8813\n",
      "Epoch 72/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2874 - accuracy: 0.8940\n",
      "Epoch 73/1500\n",
      "20/20 [==============================] - 0s 998us/step - loss: 0.2897 - accuracy: 0.8797\n",
      "Epoch 74/1500\n",
      "20/20 [==============================] - 0s 995us/step - loss: 0.3149 - accuracy: 0.8639\n",
      "Epoch 75/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2985 - accuracy: 0.8734\n",
      "Epoch 76/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2662 - accuracy: 0.9051\n",
      "Epoch 77/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3002 - accuracy: 0.8845\n",
      "Epoch 78/1500\n",
      "20/20 [==============================] - 0s 995us/step - loss: 0.3014 - accuracy: 0.8813\n",
      "Epoch 79/1500\n",
      "20/20 [==============================] - 0s 985us/step - loss: 0.2743 - accuracy: 0.8956\n",
      "Epoch 80/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2651 - accuracy: 0.9082\n",
      "Epoch 81/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2425 - accuracy: 0.9019\n",
      "Epoch 82/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2904 - accuracy: 0.8987\n",
      "Epoch 83/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2948 - accuracy: 0.8782\n",
      "Epoch 84/1500\n",
      "20/20 [==============================] - 0s 995us/step - loss: 0.2830 - accuracy: 0.8877\n",
      "Epoch 85/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2859 - accuracy: 0.8924\n",
      "Epoch 86/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2606 - accuracy: 0.9051\n",
      "Epoch 87/1500\n",
      "20/20 [==============================] - 0s 986us/step - loss: 0.2456 - accuracy: 0.9003\n",
      "Epoch 88/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2590 - accuracy: 0.8972\n",
      "Epoch 89/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2502 - accuracy: 0.8924\n",
      "Epoch 90/1500\n",
      "20/20 [==============================] - 0s 982us/step - loss: 0.2561 - accuracy: 0.8892\n",
      "Epoch 91/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2663 - accuracy: 0.8845\n",
      "Epoch 92/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2628 - accuracy: 0.8924\n",
      "Epoch 93/1500\n",
      "20/20 [==============================] - 0s 990us/step - loss: 0.2597 - accuracy: 0.9035\n",
      "Epoch 94/1500\n",
      "20/20 [==============================] - 0s 995us/step - loss: 0.2797 - accuracy: 0.8908\n",
      "Epoch 95/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2557 - accuracy: 0.9035\n",
      "Epoch 96/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2434 - accuracy: 0.9098\n",
      "Epoch 97/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2445 - accuracy: 0.9051\n",
      "Epoch 98/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2291 - accuracy: 0.9066\n",
      "Epoch 99/1500\n",
      "20/20 [==============================] - 0s 993us/step - loss: 0.2374 - accuracy: 0.9019\n",
      "Epoch 100/1500\n",
      "20/20 [==============================] - 0s 992us/step - loss: 0.2132 - accuracy: 0.9367\n",
      "Epoch 101/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2336 - accuracy: 0.9051\n",
      "Epoch 102/1500\n",
      "20/20 [==============================] - 0s 999us/step - loss: 0.2357 - accuracy: 0.9130\n",
      "Epoch 103/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2087 - accuracy: 0.9161\n",
      "Epoch 104/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2168 - accuracy: 0.9225\n",
      "Epoch 105/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2365 - accuracy: 0.9130\n",
      "Epoch 106/1500\n",
      "20/20 [==============================] - 0s 999us/step - loss: 0.2700 - accuracy: 0.9082\n",
      "Epoch 107/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.8987\n",
      "Epoch 108/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2353 - accuracy: 0.9051\n",
      "Epoch 109/1500\n",
      "20/20 [==============================] - 0s 998us/step - loss: 0.2506 - accuracy: 0.9003\n",
      "Epoch 110/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2318 - accuracy: 0.9019\n",
      "Epoch 111/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2203 - accuracy: 0.9146\n",
      "Epoch 112/1500\n",
      "20/20 [==============================] - 0s 995us/step - loss: 0.2068 - accuracy: 0.9209\n",
      "Epoch 113/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9241\n",
      "Epoch 114/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2323 - accuracy: 0.9051\n",
      "Epoch 115/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.9415\n",
      "Epoch 116/1500\n",
      "20/20 [==============================] - 0s 989us/step - loss: 0.2269 - accuracy: 0.9003\n",
      "Epoch 117/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9225\n",
      "Epoch 118/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9130\n",
      "Epoch 119/1500\n",
      "20/20 [==============================] - 0s 994us/step - loss: 0.1978 - accuracy: 0.9304\n",
      "Epoch 120/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2126 - accuracy: 0.9241\n",
      "Epoch 121/1500\n",
      "20/20 [==============================] - 0s 961us/step - loss: 0.2307 - accuracy: 0.8987\n",
      "Epoch 122/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.8924\n",
      "Epoch 123/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.9241\n",
      "Epoch 124/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.9161\n",
      "Epoch 125/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2052 - accuracy: 0.9256\n",
      "Epoch 126/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1884 - accuracy: 0.9272\n",
      "Epoch 127/1500\n",
      "20/20 [==============================] - 0s 999us/step - loss: 0.2341 - accuracy: 0.9177\n",
      "Epoch 128/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2318 - accuracy: 0.9146\n",
      "Epoch 129/1500\n",
      "20/20 [==============================] - 0s 989us/step - loss: 0.1952 - accuracy: 0.9304\n",
      "Epoch 130/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9256\n",
      "Epoch 131/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9225\n",
      "Epoch 132/1500\n",
      "20/20 [==============================] - 0s 995us/step - loss: 0.1952 - accuracy: 0.9256\n",
      "Epoch 133/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1964 - accuracy: 0.9288\n",
      "Epoch 134/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.9383\n",
      "Epoch 135/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.9288\n",
      "Epoch 136/1500\n",
      "20/20 [==============================] - 0s 973us/step - loss: 0.1968 - accuracy: 0.9367\n",
      "Epoch 137/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1875 - accuracy: 0.9351\n",
      "Epoch 138/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9288\n",
      "Epoch 139/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2012 - accuracy: 0.9272\n",
      "Epoch 140/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9462\n",
      "Epoch 141/1500\n",
      "20/20 [==============================] - 0s 963us/step - loss: 0.1921 - accuracy: 0.9272\n",
      "Epoch 142/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9177\n",
      "Epoch 143/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1954 - accuracy: 0.9256\n",
      "Epoch 144/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.9399\n",
      "Epoch 145/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9351\n",
      "Epoch 146/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1706 - accuracy: 0.9351\n",
      "Epoch 147/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1626 - accuracy: 0.9478\n",
      "Epoch 148/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1722 - accuracy: 0.9462\n",
      "Epoch 149/1500\n",
      "20/20 [==============================] - 0s 988us/step - loss: 0.1870 - accuracy: 0.9288\n",
      "Epoch 150/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9335\n",
      "Epoch 151/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.9209\n",
      "Epoch 152/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9209\n",
      "Epoch 153/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9383\n",
      "Epoch 154/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.9383\n",
      "Epoch 155/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9335\n",
      "Epoch 156/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1626 - accuracy: 0.9415\n",
      "Epoch 157/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9462\n",
      "Epoch 158/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1890 - accuracy: 0.9399\n",
      "Epoch 159/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9225\n",
      "Epoch 160/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.9241\n",
      "Epoch 161/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.9494\n",
      "Epoch 162/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.9272\n",
      "Epoch 163/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.9256\n",
      "Epoch 164/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1493 - accuracy: 0.9399\n",
      "Epoch 165/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9509\n",
      "Epoch 166/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9478\n",
      "Epoch 167/1500\n",
      "20/20 [==============================] - 0s 981us/step - loss: 0.1561 - accuracy: 0.9509\n",
      "Epoch 168/1500\n",
      "20/20 [==============================] - 0s 999us/step - loss: 0.1478 - accuracy: 0.9446\n",
      "Epoch 169/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9415\n",
      "Epoch 170/1500\n",
      "20/20 [==============================] - 0s 981us/step - loss: 0.1552 - accuracy: 0.9462\n",
      "Epoch 171/1500\n",
      "20/20 [==============================] - 0s 941us/step - loss: 0.1728 - accuracy: 0.9383\n",
      "Epoch 172/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9541\n",
      "Epoch 173/1500\n",
      "20/20 [==============================] - 0s 999us/step - loss: 0.1464 - accuracy: 0.9509\n",
      "Epoch 174/1500\n",
      "20/20 [==============================] - 0s 953us/step - loss: 0.2095 - accuracy: 0.9146\n",
      "Epoch 175/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1700 - accuracy: 0.9320\n",
      "Epoch 176/1500\n",
      "20/20 [==============================] - 0s 982us/step - loss: 0.1772 - accuracy: 0.9351\n",
      "Epoch 177/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1702 - accuracy: 0.9446\n",
      "Epoch 178/1500\n",
      "20/20 [==============================] - 0s 983us/step - loss: 0.1587 - accuracy: 0.9399\n",
      "Epoch 179/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9304\n",
      "Epoch 180/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9383\n",
      "Epoch 181/1500\n",
      "20/20 [==============================] - 0s 984us/step - loss: 0.1514 - accuracy: 0.9446\n",
      "Epoch 182/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9525\n",
      "Epoch 183/1500\n",
      "20/20 [==============================] - 0s 979us/step - loss: 0.1566 - accuracy: 0.9462\n",
      "Epoch 184/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9415\n",
      "Epoch 185/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9399\n",
      "Epoch 186/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.9557\n",
      "Epoch 187/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.9557\n",
      "Epoch 188/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9478\n",
      "Epoch 189/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9525\n",
      "Epoch 190/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9604\n",
      "Epoch 191/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9541\n",
      "Epoch 192/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9446\n",
      "Epoch 193/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9478\n",
      "Epoch 194/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1678 - accuracy: 0.9399\n",
      "Epoch 195/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9557\n",
      "Epoch 196/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9415\n",
      "Epoch 197/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9430\n",
      "Epoch 198/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9430\n",
      "Epoch 199/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9446\n",
      "Epoch 200/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9652\n",
      "Epoch 201/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9478\n",
      "Epoch 202/1500\n",
      "20/20 [==============================] - 0s 949us/step - loss: 0.1281 - accuracy: 0.9573\n",
      "Epoch 203/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9636\n",
      "Epoch 204/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9446\n",
      "Epoch 205/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9446\n",
      "Epoch 206/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9652\n",
      "Epoch 207/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9699\n",
      "Epoch 208/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9557\n",
      "Epoch 209/1500\n",
      "20/20 [==============================] - 0s 969us/step - loss: 0.1294 - accuracy: 0.9541\n",
      "Epoch 210/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9161\n",
      "Epoch 211/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9589\n",
      "Epoch 212/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9399\n",
      "Epoch 213/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9699\n",
      "Epoch 214/1500\n",
      "20/20 [==============================] - 0s 999us/step - loss: 0.1271 - accuracy: 0.9541\n",
      "Epoch 215/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9494\n",
      "Epoch 216/1500\n",
      "20/20 [==============================] - 0s 956us/step - loss: 0.1415 - accuracy: 0.9478\n",
      "Epoch 217/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9589\n",
      "Epoch 218/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9652\n",
      "Epoch 219/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9731\n",
      "Epoch 220/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9652\n",
      "Epoch 221/1500\n",
      "20/20 [==============================] - 0s 921us/step - loss: 0.1284 - accuracy: 0.9604\n",
      "Epoch 222/1500\n",
      "20/20 [==============================] - 0s 972us/step - loss: 0.1302 - accuracy: 0.9557\n",
      "Epoch 223/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9652\n",
      "Epoch 224/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9604\n",
      "Epoch 225/1500\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.1165 - accuracy: 0.9620\n",
      "Epoch 226/1500\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.1170 - accuracy: 0.9589\n",
      "Epoch 227/1500\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1061 - accuracy: 0.9715\n",
      "Epoch 228/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9557\n",
      "Epoch 229/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9557\n",
      "Epoch 230/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9652\n",
      "Epoch 231/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9494\n",
      "Epoch 232/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9636\n",
      "Epoch 233/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9573\n",
      "Epoch 234/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9509\n",
      "Epoch 235/1500\n",
      "20/20 [==============================] - 0s 981us/step - loss: 0.1240 - accuracy: 0.9684\n",
      "Epoch 236/1500\n",
      "20/20 [==============================] - 0s 966us/step - loss: 0.1537 - accuracy: 0.9399\n",
      "Epoch 237/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9573\n",
      "Epoch 238/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9620\n",
      "Epoch 239/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9747\n",
      "Epoch 240/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9715\n",
      "Epoch 241/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9684\n",
      "Epoch 242/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9541\n",
      "Epoch 243/1500\n",
      "20/20 [==============================] - 0s 962us/step - loss: 0.1509 - accuracy: 0.9383\n",
      "Epoch 244/1500\n",
      "20/20 [==============================] - 0s 971us/step - loss: 0.1523 - accuracy: 0.9446\n",
      "Epoch 245/1500\n",
      "20/20 [==============================] - 0s 975us/step - loss: 0.1414 - accuracy: 0.9478\n",
      "Epoch 246/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9684\n",
      "Epoch 247/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9652\n",
      "Epoch 248/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9731\n",
      "Epoch 249/1500\n",
      "20/20 [==============================] - 0s 988us/step - loss: 0.1300 - accuracy: 0.9430\n",
      "Epoch 250/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9636\n",
      "Epoch 251/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9652\n",
      "Epoch 252/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9684\n",
      "Epoch 253/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9684\n",
      "Epoch 254/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9699\n",
      "Epoch 255/1500\n",
      "20/20 [==============================] - 0s 984us/step - loss: 0.0982 - accuracy: 0.9684\n",
      "Epoch 256/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9652\n",
      "Epoch 257/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9668\n",
      "Epoch 258/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9494\n",
      "Epoch 259/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.9763\n",
      "Epoch 260/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9573\n",
      "Epoch 261/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9636\n",
      "Epoch 262/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9684\n",
      "Epoch 263/1500\n",
      "20/20 [==============================] - 0s 998us/step - loss: 0.0987 - accuracy: 0.9794\n",
      "Epoch 264/1500\n",
      "20/20 [==============================] - 0s 962us/step - loss: 0.1054 - accuracy: 0.9668\n",
      "Epoch 265/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9684\n",
      "Epoch 266/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9794\n",
      "Epoch 267/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9699\n",
      "Epoch 268/1500\n",
      "20/20 [==============================] - 0s 975us/step - loss: 0.1074 - accuracy: 0.9557\n",
      "Epoch 269/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9652\n",
      "Epoch 270/1500\n",
      "20/20 [==============================] - 0s 993us/step - loss: 0.1238 - accuracy: 0.9636\n",
      "Epoch 271/1500\n",
      "20/20 [==============================] - 0s 970us/step - loss: 0.1041 - accuracy: 0.9620\n",
      "Epoch 272/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9668\n",
      "Epoch 273/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9573\n",
      "Epoch 274/1500\n",
      "20/20 [==============================] - 0s 982us/step - loss: 0.0997 - accuracy: 0.9636\n",
      "Epoch 275/1500\n",
      "20/20 [==============================] - 0s 982us/step - loss: 0.1013 - accuracy: 0.9636\n",
      "Epoch 276/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9573\n",
      "Epoch 277/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9715\n",
      "Epoch 278/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9589\n",
      "Epoch 279/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9541\n",
      "Epoch 280/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9430\n",
      "Epoch 281/1500\n",
      "20/20 [==============================] - 0s 993us/step - loss: 0.0992 - accuracy: 0.9731\n",
      "Epoch 282/1500\n",
      "20/20 [==============================] - 0s 983us/step - loss: 0.0985 - accuracy: 0.9636\n",
      "Epoch 283/1500\n",
      "20/20 [==============================] - 0s 980us/step - loss: 0.1055 - accuracy: 0.9731\n",
      "Epoch 284/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9715\n",
      "Epoch 285/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9715\n",
      "Epoch 286/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9699\n",
      "Epoch 287/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9715\n",
      "Epoch 288/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0956 - accuracy: 0.9731\n",
      "Epoch 289/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9652\n",
      "Epoch 290/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9699\n",
      "Epoch 291/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9794\n",
      "Epoch 292/1500\n",
      "20/20 [==============================] - 0s 988us/step - loss: 0.0888 - accuracy: 0.9731\n",
      "Epoch 293/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9509\n",
      "Epoch 294/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.9731\n",
      "Epoch 295/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9699\n",
      "Epoch 296/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9699\n",
      "Epoch 297/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9699\n",
      "Epoch 298/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9636\n",
      "Epoch 299/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9636\n",
      "Epoch 300/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.9842\n",
      "Epoch 301/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9684\n",
      "Epoch 302/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9604\n",
      "Epoch 303/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9684\n",
      "Epoch 304/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9652\n",
      "Epoch 305/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9747\n",
      "Epoch 306/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9573\n",
      "Epoch 307/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9731\n",
      "Epoch 308/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9699\n",
      "Epoch 309/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9715\n",
      "Epoch 310/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9763\n",
      "Epoch 311/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9794\n",
      "Epoch 312/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9636\n",
      "Epoch 313/1500\n",
      "20/20 [==============================] - 0s 986us/step - loss: 0.0980 - accuracy: 0.9684\n",
      "Epoch 314/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.9668\n",
      "Epoch 315/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.9794\n",
      "Epoch 316/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9794\n",
      "Epoch 317/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9636\n",
      "Epoch 318/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9652\n",
      "Epoch 319/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9557\n",
      "Epoch 320/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9494\n",
      "Epoch 321/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9763\n",
      "Epoch 322/1500\n",
      "20/20 [==============================] - 0s 990us/step - loss: 0.1189 - accuracy: 0.9589\n",
      "Epoch 323/1500\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.1040 - accuracy: 0.9636\n",
      "Epoch 324/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9589\n",
      "Epoch 325/1500\n",
      "20/20 [==============================] - 0s 977us/step - loss: 0.0922 - accuracy: 0.9731\n",
      "Epoch 326/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.9858\n",
      "Epoch 327/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9763\n",
      "Epoch 328/1500\n",
      "20/20 [==============================] - 0s 4ms/step - loss: 0.0951 - accuracy: 0.9731\n",
      "Epoch 329/1500\n",
      "20/20 [==============================] - 0s 2ms/step - loss: 0.1033 - accuracy: 0.9763\n",
      "Epoch 330/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9747\n",
      "Epoch 331/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9699\n",
      "Epoch 332/1500\n",
      "20/20 [==============================] - 0s 998us/step - loss: 0.1077 - accuracy: 0.9604\n",
      "Epoch 333/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9589\n",
      "Epoch 334/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9763\n",
      "Epoch 335/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9699\n",
      "Epoch 336/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9636\n",
      "Epoch 337/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9604\n",
      "Epoch 338/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9778\n",
      "Epoch 339/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9699\n",
      "Epoch 340/1500\n",
      "20/20 [==============================] - 0s 982us/step - loss: 0.0731 - accuracy: 0.9778\n",
      "Epoch 341/1500\n",
      "20/20 [==============================] - 0s 947us/step - loss: 0.0865 - accuracy: 0.9699\n",
      "Epoch 342/1500\n",
      "20/20 [==============================] - 0s 971us/step - loss: 0.0853 - accuracy: 0.9794\n",
      "Epoch 343/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9826\n",
      "Epoch 344/1500\n",
      "20/20 [==============================] - 0s 987us/step - loss: 0.0898 - accuracy: 0.9636\n",
      "Epoch 345/1500\n",
      "20/20 [==============================] - 0s 950us/step - loss: 0.0856 - accuracy: 0.9731\n",
      "Epoch 346/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9715\n",
      "Epoch 347/1500\n",
      "20/20 [==============================] - 0s 984us/step - loss: 0.0623 - accuracy: 0.9842\n",
      "Epoch 348/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9636\n",
      "Epoch 349/1500\n",
      "20/20 [==============================] - 0s 979us/step - loss: 0.0895 - accuracy: 0.9699\n",
      "Epoch 350/1500\n",
      "20/20 [==============================] - 0s 996us/step - loss: 0.0948 - accuracy: 0.9636\n",
      "Epoch 351/1500\n",
      "20/20 [==============================] - 0s 959us/step - loss: 0.0759 - accuracy: 0.9810\n",
      "Epoch 352/1500\n",
      "20/20 [==============================] - 0s 990us/step - loss: 0.0915 - accuracy: 0.9668\n",
      "Epoch 353/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.9794\n",
      "Epoch 354/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0581 - accuracy: 0.9842\n",
      "Epoch 355/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9747\n",
      "Epoch 356/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.9794\n",
      "Epoch 357/1500\n",
      "20/20 [==============================] - 0s 993us/step - loss: 0.0790 - accuracy: 0.9715\n",
      "Epoch 358/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.9668\n",
      "Epoch 359/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9763\n",
      "Epoch 360/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9652\n",
      "Epoch 361/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9699\n",
      "Epoch 362/1500\n",
      "20/20 [==============================] - 0s 986us/step - loss: 0.0928 - accuracy: 0.9636\n",
      "Epoch 363/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9794\n",
      "Epoch 364/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9747\n",
      "Epoch 365/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9778\n",
      "Epoch 366/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9810\n",
      "Epoch 367/1500\n",
      "20/20 [==============================] - 0s 992us/step - loss: 0.0835 - accuracy: 0.9747\n",
      "Epoch 368/1500\n",
      "20/20 [==============================] - 0s 989us/step - loss: 0.0683 - accuracy: 0.9826\n",
      "Epoch 369/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.9699\n",
      "Epoch 370/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0774 - accuracy: 0.9763\n",
      "Epoch 371/1500\n",
      "20/20 [==============================] - 0s 997us/step - loss: 0.0629 - accuracy: 0.9826\n",
      "Epoch 372/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0543 - accuracy: 0.9953\n",
      "Epoch 373/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9747\n",
      "Epoch 374/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9778\n",
      "Epoch 375/1500\n",
      "20/20 [==============================] - 0s 991us/step - loss: 0.0725 - accuracy: 0.9778\n",
      "Epoch 376/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.9668\n",
      "Epoch 377/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9699\n",
      "Epoch 378/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.9778\n",
      "Epoch 379/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9715\n",
      "Epoch 380/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9715\n",
      "Epoch 381/1500\n",
      "20/20 [==============================] - 0s 981us/step - loss: 0.1165 - accuracy: 0.9668\n",
      "Epoch 382/1500\n",
      "20/20 [==============================] - 0s 983us/step - loss: 0.0839 - accuracy: 0.9684\n",
      "Epoch 383/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.9747\n",
      "Epoch 384/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.9842\n",
      "Epoch 385/1500\n",
      "20/20 [==============================] - 0s 966us/step - loss: 0.0750 - accuracy: 0.9778\n",
      "Epoch 386/1500\n",
      "20/20 [==============================] - 0s 969us/step - loss: 0.0564 - accuracy: 0.9858\n",
      "Epoch 387/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.9747\n",
      "Epoch 388/1500\n",
      "20/20 [==============================] - 0s 991us/step - loss: 0.0706 - accuracy: 0.9873\n",
      "Epoch 389/1500\n",
      "20/20 [==============================] - 0s 996us/step - loss: 0.0651 - accuracy: 0.9794\n",
      "Epoch 390/1500\n",
      "20/20 [==============================] - 0s 964us/step - loss: 0.0677 - accuracy: 0.9778\n",
      "Epoch 391/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9604\n",
      "Epoch 392/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9715\n",
      "Epoch 393/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9731\n",
      "Epoch 394/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9684\n",
      "Epoch 395/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.9858\n",
      "Epoch 396/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9747\n",
      "Epoch 397/1500\n",
      "20/20 [==============================] - 0s 967us/step - loss: 0.0706 - accuracy: 0.9826\n",
      "Epoch 398/1500\n",
      "20/20 [==============================] - 0s 995us/step - loss: 0.0738 - accuracy: 0.9826\n",
      "Epoch 399/1500\n",
      "20/20 [==============================] - 0s 971us/step - loss: 0.0626 - accuracy: 0.9778\n",
      "Epoch 400/1500\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9747\n",
      "Epoch 401/1500\n",
      "20/20 [==============================] - 0s 929us/step - loss: 0.0779 - accuracy: 0.9731\n",
      "Epoch 402/1500\n",
      " 1/20 [>.............................] - ETA: 0s - loss: 0.0189 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 372.\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.9794\n",
      "Epoch 402: early stopping\n",
      "10/10 [==============================] - 0s 753us/step - loss: 0.8040 - accuracy: 0.7607\n",
      "10/10 [==============================] - 0s 551us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (25/30)\n",
      "Before appending - Cat IDs: 382, Predictions: 382, Actuals: 382, Gender: 382\n",
      "After appending - Cat IDs: 687, Predictions: 687, Actuals: 687, Gender: 687\n",
      "Final Test Results - Loss: 0.804020345211029, Accuracy: 0.7606557607650757, Precision: 0.7015729368670546, Recall: 0.6581940904370811, F1 Score: 0.6763248161955057\n",
      "Confusion Matrix:\n",
      " [[181   7  26]\n",
      " [ 15  21   0]\n",
      " [ 25   0  30]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "071A    10\n",
      "016A    10\n",
      "072A     9\n",
      "022A     9\n",
      "045A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "094A     8\n",
      "095A     8\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "037A     6\n",
      "008A     6\n",
      "023A     6\n",
      "053A     6\n",
      "025C     5\n",
      "075A     5\n",
      "044A     5\n",
      "034A     5\n",
      "070A     5\n",
      "009A     4\n",
      "052A     4\n",
      "105A     4\n",
      "104A     4\n",
      "003A     4\n",
      "060A     3\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "025B     2\n",
      "073A     1\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "048A     1\n",
      "019B     1\n",
      "088A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "065A     9\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "021A     5\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "113A     3\n",
      "064A     3\n",
      "087A     2\n",
      "038A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "043A     1\n",
      "041A     1\n",
      "066A     1\n",
      "004A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    314\n",
      "X    271\n",
      "F    164\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    88\n",
      "X    77\n",
      "M    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 097B, 028...\n",
      "kitten    [044A, 046A, 047A, 050A, 049A, 045A, 048A, 115...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 062A, 101A, 065A, 063A, 038A, 007A, 087...\n",
      "kitten           [014B, 111A, 040A, 042A, 109A, 043A, 041A]\n",
      "senior                             [113A, 108A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 9, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 7, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'072A'}\n",
      "Moved to Test Set:\n",
      "{'072A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A'\n",
      " '029A' '032A' '033A' '034A' '036A' '037A' '039A' '044A' '045A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A' '038A'\n",
      " '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '072A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "779\n",
      "Length of y_train_val:\n",
      "779\n",
      "Length of groups_train_val:\n",
      "779\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "kitten     55\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     498\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     90\n",
      "kitten    55\n",
      "senior    13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 498, 2: 165, 1: 116})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 1.1443 - accuracy: 0.4763\n",
      "Epoch 2/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.9979 - accuracy: 0.5712\n",
      "Epoch 3/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.8780 - accuracy: 0.6290\n",
      "Epoch 4/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.8955 - accuracy: 0.6329\n",
      "Epoch 5/1500\n",
      "25/25 [==============================] - 0s 972us/step - loss: 0.8217 - accuracy: 0.6560\n",
      "Epoch 6/1500\n",
      "25/25 [==============================] - 0s 966us/step - loss: 0.7578 - accuracy: 0.6727\n",
      "Epoch 7/1500\n",
      "25/25 [==============================] - 0s 941us/step - loss: 0.6918 - accuracy: 0.7356\n",
      "Epoch 8/1500\n",
      "25/25 [==============================] - 0s 944us/step - loss: 0.6704 - accuracy: 0.7304\n",
      "Epoch 9/1500\n",
      "25/25 [==============================] - 0s 974us/step - loss: 0.6660 - accuracy: 0.7240\n",
      "Epoch 10/1500\n",
      "25/25 [==============================] - 0s 939us/step - loss: 0.6702 - accuracy: 0.7343\n",
      "Epoch 11/1500\n",
      "25/25 [==============================] - 0s 949us/step - loss: 0.6047 - accuracy: 0.7561\n",
      "Epoch 12/1500\n",
      "25/25 [==============================] - 0s 965us/step - loss: 0.5968 - accuracy: 0.7741\n",
      "Epoch 13/1500\n",
      "25/25 [==============================] - 0s 981us/step - loss: 0.6278 - accuracy: 0.7612\n",
      "Epoch 14/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6110 - accuracy: 0.7599\n",
      "Epoch 15/1500\n",
      "25/25 [==============================] - 0s 966us/step - loss: 0.5591 - accuracy: 0.7728\n",
      "Epoch 16/1500\n",
      "25/25 [==============================] - 0s 959us/step - loss: 0.5652 - accuracy: 0.7677\n",
      "Epoch 17/1500\n",
      "25/25 [==============================] - 0s 960us/step - loss: 0.5508 - accuracy: 0.7728\n",
      "Epoch 18/1500\n",
      "25/25 [==============================] - 0s 966us/step - loss: 0.5228 - accuracy: 0.7959\n",
      "Epoch 19/1500\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.5375 - accuracy: 0.7805\n",
      "Epoch 20/1500\n",
      "25/25 [==============================] - 0s 985us/step - loss: 0.5260 - accuracy: 0.7920\n",
      "Epoch 21/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4804 - accuracy: 0.8023\n",
      "Epoch 22/1500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 0.5174 - accuracy: 0.7895\n",
      "Epoch 23/1500\n",
      "25/25 [==============================] - 0s 963us/step - loss: 0.4631 - accuracy: 0.8049\n",
      "Epoch 24/1500\n",
      "25/25 [==============================] - 0s 936us/step - loss: 0.4947 - accuracy: 0.8074\n",
      "Epoch 25/1500\n",
      "25/25 [==============================] - 0s 972us/step - loss: 0.4774 - accuracy: 0.7972\n",
      "Epoch 26/1500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 0.4708 - accuracy: 0.8139\n",
      "Epoch 27/1500\n",
      "25/25 [==============================] - 0s 953us/step - loss: 0.4688 - accuracy: 0.8177\n",
      "Epoch 28/1500\n",
      "25/25 [==============================] - 0s 930us/step - loss: 0.4739 - accuracy: 0.7920\n",
      "Epoch 29/1500\n",
      "25/25 [==============================] - 0s 967us/step - loss: 0.4358 - accuracy: 0.8344\n",
      "Epoch 30/1500\n",
      "25/25 [==============================] - 0s 960us/step - loss: 0.4768 - accuracy: 0.8087\n",
      "Epoch 31/1500\n",
      "25/25 [==============================] - 0s 954us/step - loss: 0.4397 - accuracy: 0.8062\n",
      "Epoch 32/1500\n",
      "25/25 [==============================] - 0s 953us/step - loss: 0.3974 - accuracy: 0.8357\n",
      "Epoch 33/1500\n",
      "25/25 [==============================] - 0s 939us/step - loss: 0.4255 - accuracy: 0.8280\n",
      "Epoch 34/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8395\n",
      "Epoch 35/1500\n",
      "25/25 [==============================] - 0s 927us/step - loss: 0.4124 - accuracy: 0.8331\n",
      "Epoch 36/1500\n",
      "25/25 [==============================] - 0s 939us/step - loss: 0.3746 - accuracy: 0.8460\n",
      "Epoch 37/1500\n",
      "25/25 [==============================] - 0s 902us/step - loss: 0.3911 - accuracy: 0.8331\n",
      "Epoch 38/1500\n",
      "25/25 [==============================] - 0s 923us/step - loss: 0.3832 - accuracy: 0.8601\n",
      "Epoch 39/1500\n",
      "25/25 [==============================] - 0s 958us/step - loss: 0.3944 - accuracy: 0.8511\n",
      "Epoch 40/1500\n",
      "25/25 [==============================] - 0s 972us/step - loss: 0.3684 - accuracy: 0.8626\n",
      "Epoch 41/1500\n",
      "25/25 [==============================] - 0s 968us/step - loss: 0.3723 - accuracy: 0.8421\n",
      "Epoch 42/1500\n",
      "25/25 [==============================] - 0s 961us/step - loss: 0.3978 - accuracy: 0.8370\n",
      "Epoch 43/1500\n",
      "25/25 [==============================] - 0s 985us/step - loss: 0.3724 - accuracy: 0.8460\n",
      "Epoch 44/1500\n",
      "25/25 [==============================] - 0s 946us/step - loss: 0.3764 - accuracy: 0.8549\n",
      "Epoch 45/1500\n",
      "25/25 [==============================] - 0s 939us/step - loss: 0.3526 - accuracy: 0.8562\n",
      "Epoch 46/1500\n",
      "25/25 [==============================] - 0s 963us/step - loss: 0.3532 - accuracy: 0.8562\n",
      "Epoch 47/1500\n",
      "25/25 [==============================] - 0s 936us/step - loss: 0.3592 - accuracy: 0.8626\n",
      "Epoch 48/1500\n",
      "25/25 [==============================] - 0s 972us/step - loss: 0.3478 - accuracy: 0.8639\n",
      "Epoch 49/1500\n",
      "25/25 [==============================] - 0s 973us/step - loss: 0.3573 - accuracy: 0.8652\n",
      "Epoch 50/1500\n",
      "25/25 [==============================] - 0s 931us/step - loss: 0.3362 - accuracy: 0.8742\n",
      "Epoch 51/1500\n",
      "25/25 [==============================] - 0s 993us/step - loss: 0.3312 - accuracy: 0.8755\n",
      "Epoch 52/1500\n",
      "25/25 [==============================] - 0s 948us/step - loss: 0.3533 - accuracy: 0.8537\n",
      "Epoch 53/1500\n",
      "25/25 [==============================] - 0s 920us/step - loss: 0.3557 - accuracy: 0.8691\n",
      "Epoch 54/1500\n",
      "25/25 [==============================] - 0s 930us/step - loss: 0.3870 - accuracy: 0.8370\n",
      "Epoch 55/1500\n",
      "25/25 [==============================] - 0s 924us/step - loss: 0.3546 - accuracy: 0.8549\n",
      "Epoch 56/1500\n",
      "25/25 [==============================] - 0s 914us/step - loss: 0.3626 - accuracy: 0.8639\n",
      "Epoch 57/1500\n",
      "25/25 [==============================] - 0s 940us/step - loss: 0.3212 - accuracy: 0.8896\n",
      "Epoch 58/1500\n",
      "25/25 [==============================] - 0s 922us/step - loss: 0.3215 - accuracy: 0.8729\n",
      "Epoch 59/1500\n",
      "25/25 [==============================] - 0s 932us/step - loss: 0.3053 - accuracy: 0.8819\n",
      "Epoch 60/1500\n",
      "25/25 [==============================] - 0s 968us/step - loss: 0.3316 - accuracy: 0.8858\n",
      "Epoch 61/1500\n",
      "25/25 [==============================] - 0s 959us/step - loss: 0.3156 - accuracy: 0.8819\n",
      "Epoch 62/1500\n",
      "25/25 [==============================] - 0s 934us/step - loss: 0.3452 - accuracy: 0.8626\n",
      "Epoch 63/1500\n",
      "25/25 [==============================] - 0s 954us/step - loss: 0.3247 - accuracy: 0.8768\n",
      "Epoch 64/1500\n",
      "25/25 [==============================] - 0s 951us/step - loss: 0.3167 - accuracy: 0.8703\n",
      "Epoch 65/1500\n",
      "25/25 [==============================] - 0s 941us/step - loss: 0.3283 - accuracy: 0.8806\n",
      "Epoch 66/1500\n",
      "25/25 [==============================] - 0s 953us/step - loss: 0.2947 - accuracy: 0.8883\n",
      "Epoch 67/1500\n",
      "25/25 [==============================] - 0s 954us/step - loss: 0.3047 - accuracy: 0.8845\n",
      "Epoch 68/1500\n",
      "25/25 [==============================] - 0s 962us/step - loss: 0.2884 - accuracy: 0.8896\n",
      "Epoch 69/1500\n",
      "25/25 [==============================] - 0s 930us/step - loss: 0.3160 - accuracy: 0.8793\n",
      "Epoch 70/1500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 0.2973 - accuracy: 0.8780\n",
      "Epoch 71/1500\n",
      "25/25 [==============================] - 0s 934us/step - loss: 0.3130 - accuracy: 0.8703\n",
      "Epoch 72/1500\n",
      "25/25 [==============================] - 0s 954us/step - loss: 0.2924 - accuracy: 0.8922\n",
      "Epoch 73/1500\n",
      "25/25 [==============================] - 0s 932us/step - loss: 0.3170 - accuracy: 0.8678\n",
      "Epoch 74/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2664 - accuracy: 0.8947\n",
      "Epoch 75/1500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 0.2903 - accuracy: 0.8819\n",
      "Epoch 76/1500\n",
      "25/25 [==============================] - 0s 906us/step - loss: 0.2755 - accuracy: 0.8909\n",
      "Epoch 77/1500\n",
      "25/25 [==============================] - 0s 952us/step - loss: 0.3021 - accuracy: 0.8729\n",
      "Epoch 78/1500\n",
      "25/25 [==============================] - 0s 979us/step - loss: 0.2863 - accuracy: 0.8819\n",
      "Epoch 79/1500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 0.2837 - accuracy: 0.8947\n",
      "Epoch 80/1500\n",
      "25/25 [==============================] - 0s 932us/step - loss: 0.2967 - accuracy: 0.8832\n",
      "Epoch 81/1500\n",
      "25/25 [==============================] - 0s 949us/step - loss: 0.2970 - accuracy: 0.8793\n",
      "Epoch 82/1500\n",
      "25/25 [==============================] - 0s 921us/step - loss: 0.2700 - accuracy: 0.8960\n",
      "Epoch 83/1500\n",
      "25/25 [==============================] - 0s 974us/step - loss: 0.2974 - accuracy: 0.8960\n",
      "Epoch 84/1500\n",
      "25/25 [==============================] - 0s 985us/step - loss: 0.2568 - accuracy: 0.9153\n",
      "Epoch 85/1500\n",
      "25/25 [==============================] - 0s 942us/step - loss: 0.2868 - accuracy: 0.8947\n",
      "Epoch 86/1500\n",
      "25/25 [==============================] - 0s 932us/step - loss: 0.2750 - accuracy: 0.8922\n",
      "Epoch 87/1500\n",
      "25/25 [==============================] - 0s 943us/step - loss: 0.2695 - accuracy: 0.8947\n",
      "Epoch 88/1500\n",
      "25/25 [==============================] - 0s 970us/step - loss: 0.2666 - accuracy: 0.9076\n",
      "Epoch 89/1500\n",
      "25/25 [==============================] - 0s 953us/step - loss: 0.2681 - accuracy: 0.8883\n",
      "Epoch 90/1500\n",
      "25/25 [==============================] - 0s 906us/step - loss: 0.2596 - accuracy: 0.8935\n",
      "Epoch 91/1500\n",
      "25/25 [==============================] - 0s 935us/step - loss: 0.2530 - accuracy: 0.8947\n",
      "Epoch 92/1500\n",
      "25/25 [==============================] - 0s 936us/step - loss: 0.2544 - accuracy: 0.9050\n",
      "Epoch 93/1500\n",
      "25/25 [==============================] - 0s 929us/step - loss: 0.2521 - accuracy: 0.9012\n",
      "Epoch 94/1500\n",
      "25/25 [==============================] - 0s 925us/step - loss: 0.2916 - accuracy: 0.8845\n",
      "Epoch 95/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.2682 - accuracy: 0.8960\n",
      "Epoch 96/1500\n",
      "25/25 [==============================] - 0s 954us/step - loss: 0.2473 - accuracy: 0.9101\n",
      "Epoch 97/1500\n",
      "25/25 [==============================] - 0s 951us/step - loss: 0.2482 - accuracy: 0.8973\n",
      "Epoch 98/1500\n",
      "25/25 [==============================] - 0s 945us/step - loss: 0.2434 - accuracy: 0.9076\n",
      "Epoch 99/1500\n",
      "25/25 [==============================] - 0s 935us/step - loss: 0.2500 - accuracy: 0.9114\n",
      "Epoch 100/1500\n",
      "25/25 [==============================] - 0s 942us/step - loss: 0.2656 - accuracy: 0.8999\n",
      "Epoch 101/1500\n",
      "25/25 [==============================] - 0s 955us/step - loss: 0.2595 - accuracy: 0.8973\n",
      "Epoch 102/1500\n",
      "25/25 [==============================] - 0s 954us/step - loss: 0.2573 - accuracy: 0.8986\n",
      "Epoch 103/1500\n",
      "25/25 [==============================] - 0s 932us/step - loss: 0.2428 - accuracy: 0.9140\n",
      "Epoch 104/1500\n",
      "25/25 [==============================] - 0s 918us/step - loss: 0.2761 - accuracy: 0.8883\n",
      "Epoch 105/1500\n",
      "25/25 [==============================] - 0s 962us/step - loss: 0.2466 - accuracy: 0.9050\n",
      "Epoch 106/1500\n",
      "25/25 [==============================] - 0s 953us/step - loss: 0.2575 - accuracy: 0.9114\n",
      "Epoch 107/1500\n",
      "25/25 [==============================] - 0s 955us/step - loss: 0.2565 - accuracy: 0.9024\n",
      "Epoch 108/1500\n",
      "25/25 [==============================] - 0s 981us/step - loss: 0.2469 - accuracy: 0.9063\n",
      "Epoch 109/1500\n",
      "25/25 [==============================] - 0s 951us/step - loss: 0.2207 - accuracy: 0.9140\n",
      "Epoch 110/1500\n",
      "25/25 [==============================] - 0s 929us/step - loss: 0.2304 - accuracy: 0.9127\n",
      "Epoch 111/1500\n",
      "25/25 [==============================] - 0s 977us/step - loss: 0.2137 - accuracy: 0.9153\n",
      "Epoch 112/1500\n",
      "25/25 [==============================] - 0s 933us/step - loss: 0.2347 - accuracy: 0.8999\n",
      "Epoch 113/1500\n",
      "25/25 [==============================] - 0s 959us/step - loss: 0.2438 - accuracy: 0.9089\n",
      "Epoch 114/1500\n",
      "25/25 [==============================] - 0s 962us/step - loss: 0.2347 - accuracy: 0.9178\n",
      "Epoch 115/1500\n",
      "25/25 [==============================] - 0s 951us/step - loss: 0.2242 - accuracy: 0.9127\n",
      "Epoch 116/1500\n",
      "25/25 [==============================] - 0s 960us/step - loss: 0.2236 - accuracy: 0.9127\n",
      "Epoch 117/1500\n",
      "25/25 [==============================] - 0s 950us/step - loss: 0.2114 - accuracy: 0.9217\n",
      "Epoch 118/1500\n",
      "25/25 [==============================] - 0s 965us/step - loss: 0.2288 - accuracy: 0.9076\n",
      "Epoch 119/1500\n",
      "25/25 [==============================] - 0s 950us/step - loss: 0.2169 - accuracy: 0.9204\n",
      "Epoch 120/1500\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.2030 - accuracy: 0.9153\n",
      "Epoch 121/1500\n",
      "25/25 [==============================] - 0s 943us/step - loss: 0.2128 - accuracy: 0.9153\n",
      "Epoch 122/1500\n",
      "25/25 [==============================] - 0s 936us/step - loss: 0.2448 - accuracy: 0.8999\n",
      "Epoch 123/1500\n",
      "25/25 [==============================] - 0s 962us/step - loss: 0.2108 - accuracy: 0.9281\n",
      "Epoch 124/1500\n",
      "25/25 [==============================] - 0s 945us/step - loss: 0.2049 - accuracy: 0.9191\n",
      "Epoch 125/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1981 - accuracy: 0.9281\n",
      "Epoch 126/1500\n",
      "25/25 [==============================] - 0s 939us/step - loss: 0.2041 - accuracy: 0.9217\n",
      "Epoch 127/1500\n",
      "25/25 [==============================] - 0s 951us/step - loss: 0.2216 - accuracy: 0.9243\n",
      "Epoch 128/1500\n",
      "25/25 [==============================] - 0s 950us/step - loss: 0.2056 - accuracy: 0.9294\n",
      "Epoch 129/1500\n",
      "25/25 [==============================] - 0s 929us/step - loss: 0.1978 - accuracy: 0.9268\n",
      "Epoch 130/1500\n",
      "25/25 [==============================] - 0s 965us/step - loss: 0.2176 - accuracy: 0.9127\n",
      "Epoch 131/1500\n",
      "25/25 [==============================] - 0s 921us/step - loss: 0.2140 - accuracy: 0.9153\n",
      "Epoch 132/1500\n",
      "25/25 [==============================] - 0s 946us/step - loss: 0.2019 - accuracy: 0.9230\n",
      "Epoch 133/1500\n",
      "25/25 [==============================] - 0s 926us/step - loss: 0.2326 - accuracy: 0.9089\n",
      "Epoch 134/1500\n",
      "25/25 [==============================] - 0s 937us/step - loss: 0.2021 - accuracy: 0.9345\n",
      "Epoch 135/1500\n",
      "25/25 [==============================] - 0s 973us/step - loss: 0.2213 - accuracy: 0.9140\n",
      "Epoch 136/1500\n",
      "25/25 [==============================] - 0s 946us/step - loss: 0.1906 - accuracy: 0.9307\n",
      "Epoch 137/1500\n",
      "25/25 [==============================] - 0s 943us/step - loss: 0.2042 - accuracy: 0.9243\n",
      "Epoch 138/1500\n",
      "25/25 [==============================] - 0s 980us/step - loss: 0.2078 - accuracy: 0.9294\n",
      "Epoch 139/1500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.1975 - accuracy: 0.9230\n",
      "Epoch 140/1500\n",
      "25/25 [==============================] - 0s 958us/step - loss: 0.1944 - accuracy: 0.9332\n",
      "Epoch 141/1500\n",
      "25/25 [==============================] - 0s 958us/step - loss: 0.1881 - accuracy: 0.9422\n",
      "Epoch 142/1500\n",
      "25/25 [==============================] - 0s 946us/step - loss: 0.2113 - accuracy: 0.9268\n",
      "Epoch 143/1500\n",
      "25/25 [==============================] - 0s 941us/step - loss: 0.1646 - accuracy: 0.9448\n",
      "Epoch 144/1500\n",
      "25/25 [==============================] - 0s 949us/step - loss: 0.2182 - accuracy: 0.9255\n",
      "Epoch 145/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.1788 - accuracy: 0.9397\n",
      "Epoch 146/1500\n",
      "25/25 [==============================] - 0s 954us/step - loss: 0.1996 - accuracy: 0.9101\n",
      "Epoch 147/1500\n",
      "25/25 [==============================] - 0s 944us/step - loss: 0.1935 - accuracy: 0.9255\n",
      "Epoch 148/1500\n",
      "25/25 [==============================] - 0s 942us/step - loss: 0.2212 - accuracy: 0.9217\n",
      "Epoch 149/1500\n",
      "25/25 [==============================] - 0s 942us/step - loss: 0.1707 - accuracy: 0.9345\n",
      "Epoch 150/1500\n",
      "25/25 [==============================] - 0s 966us/step - loss: 0.1919 - accuracy: 0.9281\n",
      "Epoch 151/1500\n",
      "25/25 [==============================] - 0s 959us/step - loss: 0.1862 - accuracy: 0.9332\n",
      "Epoch 152/1500\n",
      "25/25 [==============================] - 0s 955us/step - loss: 0.1940 - accuracy: 0.9281\n",
      "Epoch 153/1500\n",
      "25/25 [==============================] - 0s 975us/step - loss: 0.1805 - accuracy: 0.9384\n",
      "Epoch 154/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.9487\n",
      "Epoch 155/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.1619 - accuracy: 0.9435\n",
      "Epoch 156/1500\n",
      "25/25 [==============================] - 0s 923us/step - loss: 0.1907 - accuracy: 0.9422\n",
      "Epoch 157/1500\n",
      "25/25 [==============================] - 0s 979us/step - loss: 0.2065 - accuracy: 0.9230\n",
      "Epoch 158/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.9320\n",
      "Epoch 159/1500\n",
      "25/25 [==============================] - 0s 947us/step - loss: 0.1892 - accuracy: 0.9307\n",
      "Epoch 160/1500\n",
      "25/25 [==============================] - 0s 950us/step - loss: 0.1803 - accuracy: 0.9448\n",
      "Epoch 161/1500\n",
      "25/25 [==============================] - 0s 943us/step - loss: 0.1836 - accuracy: 0.9332\n",
      "Epoch 162/1500\n",
      "25/25 [==============================] - 0s 943us/step - loss: 0.1935 - accuracy: 0.9217\n",
      "Epoch 163/1500\n",
      "25/25 [==============================] - 0s 991us/step - loss: 0.1869 - accuracy: 0.9243\n",
      "Epoch 164/1500\n",
      "25/25 [==============================] - 0s 943us/step - loss: 0.1600 - accuracy: 0.9397\n",
      "Epoch 165/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.9307\n",
      "Epoch 166/1500\n",
      "25/25 [==============================] - 0s 942us/step - loss: 0.1605 - accuracy: 0.9551\n",
      "Epoch 167/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1870 - accuracy: 0.9191\n",
      "Epoch 168/1500\n",
      "25/25 [==============================] - 0s 944us/step - loss: 0.1875 - accuracy: 0.9243\n",
      "Epoch 169/1500\n",
      "25/25 [==============================] - 0s 967us/step - loss: 0.1623 - accuracy: 0.9461\n",
      "Epoch 170/1500\n",
      "25/25 [==============================] - 0s 955us/step - loss: 0.1799 - accuracy: 0.9320\n",
      "Epoch 171/1500\n",
      "25/25 [==============================] - 0s 939us/step - loss: 0.1720 - accuracy: 0.9448\n",
      "Epoch 172/1500\n",
      "25/25 [==============================] - 0s 930us/step - loss: 0.1766 - accuracy: 0.9371\n",
      "Epoch 173/1500\n",
      "25/25 [==============================] - 0s 951us/step - loss: 0.1818 - accuracy: 0.9230\n",
      "Epoch 174/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1954 - accuracy: 0.9268\n",
      "Epoch 175/1500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 0.1703 - accuracy: 0.9358\n",
      "Epoch 176/1500\n",
      "25/25 [==============================] - 0s 991us/step - loss: 0.1729 - accuracy: 0.9435\n",
      "Epoch 177/1500\n",
      "25/25 [==============================] - 0s 981us/step - loss: 0.1479 - accuracy: 0.9448\n",
      "Epoch 178/1500\n",
      "25/25 [==============================] - 0s 955us/step - loss: 0.1772 - accuracy: 0.9332\n",
      "Epoch 179/1500\n",
      "25/25 [==============================] - 0s 961us/step - loss: 0.1645 - accuracy: 0.9358\n",
      "Epoch 180/1500\n",
      "25/25 [==============================] - 0s 920us/step - loss: 0.1674 - accuracy: 0.9345\n",
      "Epoch 181/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.9525\n",
      "Epoch 182/1500\n",
      "25/25 [==============================] - 0s 941us/step - loss: 0.1896 - accuracy: 0.9230\n",
      "Epoch 183/1500\n",
      "25/25 [==============================] - 0s 954us/step - loss: 0.1507 - accuracy: 0.9512\n",
      "Epoch 184/1500\n",
      "25/25 [==============================] - 0s 961us/step - loss: 0.1754 - accuracy: 0.9384\n",
      "Epoch 185/1500\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.1572 - accuracy: 0.9499\n",
      "Epoch 186/1500\n",
      "25/25 [==============================] - 0s 886us/step - loss: 0.1550 - accuracy: 0.9499\n",
      "Epoch 187/1500\n",
      "25/25 [==============================] - 0s 952us/step - loss: 0.1679 - accuracy: 0.9371\n",
      "Epoch 188/1500\n",
      "25/25 [==============================] - 0s 979us/step - loss: 0.1894 - accuracy: 0.9204\n",
      "Epoch 189/1500\n",
      "25/25 [==============================] - 0s 962us/step - loss: 0.1499 - accuracy: 0.9487\n",
      "Epoch 190/1500\n",
      "25/25 [==============================] - 0s 973us/step - loss: 0.1457 - accuracy: 0.9487\n",
      "Epoch 191/1500\n",
      "25/25 [==============================] - 0s 969us/step - loss: 0.1488 - accuracy: 0.9474\n",
      "Epoch 192/1500\n",
      "25/25 [==============================] - 0s 975us/step - loss: 0.1428 - accuracy: 0.9474\n",
      "Epoch 193/1500\n",
      "25/25 [==============================] - 0s 959us/step - loss: 0.1691 - accuracy: 0.9294\n",
      "Epoch 194/1500\n",
      "25/25 [==============================] - 0s 949us/step - loss: 0.1441 - accuracy: 0.9499\n",
      "Epoch 195/1500\n",
      "25/25 [==============================] - 0s 960us/step - loss: 0.1493 - accuracy: 0.9499\n",
      "Epoch 196/1500\n",
      "25/25 [==============================] - 0s 978us/step - loss: 0.1365 - accuracy: 0.9487\n",
      "Epoch 197/1500\n",
      "25/25 [==============================] - 0s 925us/step - loss: 0.1269 - accuracy: 0.9499\n",
      "Epoch 198/1500\n",
      "25/25 [==============================] - 0s 979us/step - loss: 0.1176 - accuracy: 0.9641\n",
      "Epoch 199/1500\n",
      "25/25 [==============================] - 0s 980us/step - loss: 0.1394 - accuracy: 0.9512\n",
      "Epoch 200/1500\n",
      "25/25 [==============================] - 0s 963us/step - loss: 0.1574 - accuracy: 0.9422\n",
      "Epoch 201/1500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.1402 - accuracy: 0.9499\n",
      "Epoch 202/1500\n",
      "25/25 [==============================] - 0s 944us/step - loss: 0.1442 - accuracy: 0.9512\n",
      "Epoch 203/1500\n",
      "25/25 [==============================] - 0s 989us/step - loss: 0.1484 - accuracy: 0.9487\n",
      "Epoch 204/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.1356 - accuracy: 0.9499\n",
      "Epoch 205/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9435\n",
      "Epoch 206/1500\n",
      "25/25 [==============================] - 0s 1000us/step - loss: 0.1588 - accuracy: 0.9409\n",
      "Epoch 207/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.1604 - accuracy: 0.9371\n",
      "Epoch 208/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9564\n",
      "Epoch 209/1500\n",
      "25/25 [==============================] - 0s 999us/step - loss: 0.1385 - accuracy: 0.9435\n",
      "Epoch 210/1500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.1483 - accuracy: 0.9422\n",
      "Epoch 211/1500\n",
      "25/25 [==============================] - 0s 980us/step - loss: 0.1319 - accuracy: 0.9589\n",
      "Epoch 212/1500\n",
      "25/25 [==============================] - 0s 996us/step - loss: 0.1223 - accuracy: 0.9602\n",
      "Epoch 213/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9551\n",
      "Epoch 214/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9551\n",
      "Epoch 215/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9564\n",
      "Epoch 216/1500\n",
      "25/25 [==============================] - 0s 989us/step - loss: 0.1384 - accuracy: 0.9512\n",
      "Epoch 217/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9628\n",
      "Epoch 218/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9653\n",
      "Epoch 219/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9474\n",
      "Epoch 220/1500\n",
      "25/25 [==============================] - 0s 977us/step - loss: 0.1365 - accuracy: 0.9538\n",
      "Epoch 221/1500\n",
      "25/25 [==============================] - 0s 984us/step - loss: 0.1176 - accuracy: 0.9641\n",
      "Epoch 222/1500\n",
      "25/25 [==============================] - 0s 981us/step - loss: 0.1340 - accuracy: 0.9512\n",
      "Epoch 223/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9512\n",
      "Epoch 224/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9499\n",
      "Epoch 225/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9641\n",
      "Epoch 226/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9615\n",
      "Epoch 227/1500\n",
      "25/25 [==============================] - 0s 989us/step - loss: 0.1145 - accuracy: 0.9653\n",
      "Epoch 228/1500\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.1069 - accuracy: 0.9666\n",
      "Epoch 229/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1179 - accuracy: 0.9564\n",
      "Epoch 230/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9679\n",
      "Epoch 231/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9576\n",
      "Epoch 232/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9589\n",
      "Epoch 233/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9551\n",
      "Epoch 234/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1477 - accuracy: 0.9461\n",
      "Epoch 235/1500\n",
      "25/25 [==============================] - 0s 975us/step - loss: 0.1543 - accuracy: 0.9281\n",
      "Epoch 236/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9615\n",
      "Epoch 237/1500\n",
      "25/25 [==============================] - 0s 976us/step - loss: 0.1167 - accuracy: 0.9602\n",
      "Epoch 238/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9641\n",
      "Epoch 239/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9499\n",
      "Epoch 240/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9564\n",
      "Epoch 241/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9602\n",
      "Epoch 242/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9538\n",
      "Epoch 243/1500\n",
      "25/25 [==============================] - 0s 988us/step - loss: 0.1275 - accuracy: 0.9551\n",
      "Epoch 244/1500\n",
      "25/25 [==============================] - 0s 972us/step - loss: 0.1400 - accuracy: 0.9422\n",
      "Epoch 245/1500\n",
      "25/25 [==============================] - 0s 978us/step - loss: 0.1327 - accuracy: 0.9499\n",
      "Epoch 246/1500\n",
      "25/25 [==============================] - 0s 987us/step - loss: 0.1353 - accuracy: 0.9499\n",
      "Epoch 247/1500\n",
      "25/25 [==============================] - 0s 970us/step - loss: 0.1145 - accuracy: 0.9615\n",
      "Epoch 248/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9718\n",
      "Epoch 249/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9666\n",
      "Epoch 250/1500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.1253 - accuracy: 0.9589\n",
      "Epoch 251/1500\n",
      "25/25 [==============================] - 0s 960us/step - loss: 0.1144 - accuracy: 0.9602\n",
      "Epoch 252/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9551\n",
      "Epoch 253/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9653\n",
      "Epoch 254/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.1067 - accuracy: 0.9653\n",
      "Epoch 255/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9589\n",
      "Epoch 256/1500\n",
      "25/25 [==============================] - 0s 995us/step - loss: 0.1339 - accuracy: 0.9551\n",
      "Epoch 257/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9551\n",
      "Epoch 258/1500\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0825 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 228.\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9525\n",
      "Epoch 258: early stopping\n",
      "5/5 [==============================] - 0s 897us/step - loss: 0.6159 - accuracy: 0.7405\n",
      "5/5 [==============================] - 0s 702us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (22/28)\n",
      "Before appending - Cat IDs: 687, Predictions: 687, Actuals: 687, Gender: 687\n",
      "After appending - Cat IDs: 845, Predictions: 845, Actuals: 845, Gender: 845\n",
      "Final Test Results - Loss: 0.6158638596534729, Accuracy: 0.7405063509941101, Precision: 0.6398011752394752, Recall: 0.6106449106449107, F1 Score: 0.6176230165894365\n",
      "Confusion Matrix:\n",
      " [[75  3 12]\n",
      " [17 38  0]\n",
      " [ 9  0  4]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.670759839735378\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8921595513820648\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7249959111213684\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6885645705583558\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6641056788405828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[0]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ###################\n",
    "    # PREPARING MODEL #\n",
    "    ###################\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79e793-c694-4bc7-8cc6-05e124ddf71f",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2c3c239d-6215-4589-a583-b37a18ca22cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 845, Predictions: 845, Actuals: 845, Gender: 845\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "99978976-e4a9-4c04-a6b2-6b14a3111277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "189b812b-d37f-462f-a99f-6de8e4a7899f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.77 (85/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "907d957f-7340-4a76-b3fa-7cf166f43049",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "7016fa20-094f-409a-9a2f-b6d6793aed03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, adult, adult, kitten, kitten, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, adult, kitten, adult, kitten, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, senior, adult, kitten, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, kitten, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A              [adult, adult, senior, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [adult, adult, senior, senior, senior, senior,...        senior           senior                   True\n",
       "59    053A        [adult, adult, adult, adult, kitten, adult]         adult            adult                   True\n",
       "58    052A                      [adult, adult, senior, adult]         adult            adult                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A           [kitten, kitten, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [senior, senior, adult]        senior           senior                   True\n",
       "105   111A  [kitten, adult, adult, kitten, kitten, kitten,...        kitten           kitten                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, adult, adult, adult, ad...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, adult, adult, s...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "89    094A  [senior, senior, adult, senior, adult, senior,...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "25    023A        [adult, kitten, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, adult, s...         adult            adult                   True\n",
       "16    014B  [kitten, adult, kitten, adult, kitten, kitten,...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, senior, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "2     002A  [adult, adult, senior, adult, kitten, adult, a...         adult            adult                   True\n",
       "30    025C                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "109   117A  [senior, senior, senior, adult, senior, adult,...        senior           senior                   True\n",
       "34    027A  [adult, adult, senior, adult, senior, adult, s...         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "41    035A                     [senior, adult, senior, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "42    036A  [adult, senior, senior, adult, adult, adult, a...         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "103   109A         [adult, adult, adult, adult, adult, adult]         adult           kitten                  False\n",
       "101   106A  [adult, senior, senior, senior, adult, adult, ...         adult           senior                  False\n",
       "102   108A       [adult, adult, adult, senior, adult, senior]         adult           senior                  False\n",
       "104   110A                                     [adult, adult]         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "52    047A  [adult, adult, adult, adult, kitten, kitten, k...         adult           kitten                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "57    051B  [senior, adult, adult, adult, adult, adult, se...         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, senior, senior, ...         adult           senior                  False\n",
       "64    058A                             [senior, adult, adult]         adult           senior                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "65    059A  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "69    063A  [senior, senior, senior, adult, senior, senior...        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "994531fd-e6fb-4491-9eab-86e41ac1ed3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     64\n",
      "kitten    11\n",
      "senior    10\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "229af3e2-6a64-4f3d-a594-e04108fce87f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             64  87.671233\n",
      "1           kitten           15             11  73.333333\n",
      "2           senior           22             10  45.454545\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3960450a-db52-4464-a965-00f1e600bda5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbf768-64c2-48ec-80e3-3a961b0b12a6",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fe16003e-4015-4d28-ae37-ca0c94952758",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          558            456  81.720430\n",
      "1           kitten          109             74  67.889908\n",
      "2           senior          178             87  48.876404\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5b5b8766-b4bd-4dd2-92e2-6513741b03ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efa447-1e32-42df-8664-a3f1f0e0f812",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "5eecd6a6-4094-4747-8029-f795a87f8ff0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      174     81.69\n",
      "1          M    337      236     70.03\n",
      "2          X    295      207     70.17\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "115592e9-47eb-42fe-8cb5-26beeb7328ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae066af0-659b-43f0-924c-2c50be0e6c40",
   "metadata": {},
   "source": [
    "# RANDOM SEED 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cc76296a-4029-447e-b12c-41520160251a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[1]))\n",
    "np.random.seed(int(random_seeds[1]))\n",
    "tf.random.set_seed(int(random_seeds[1]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "093977a2-1813-4a02-9573-f757b3d2a567",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0c89dbfa-4c9b-4e46-a0d9-f659db30532f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a62fdf-86b5-45e7-9249-73a55985936a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "ee1b36c7-4a91-4071-a713-c904da913c3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "106A    14\n",
      "097B    14\n",
      "001A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "005A    10\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "022A     9\n",
      "051B     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "033A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "027A     7\n",
      "117A     7\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "053A     6\n",
      "008A     6\n",
      "023A     6\n",
      "037A     6\n",
      "007A     6\n",
      "034A     5\n",
      "025C     5\n",
      "070A     5\n",
      "023B     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "062A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "052A     4\n",
      "009A     4\n",
      "060A     3\n",
      "012A     3\n",
      "006A     3\n",
      "064A     3\n",
      "058A     3\n",
      "054A     2\n",
      "061A     2\n",
      "087A     2\n",
      "069A     2\n",
      "032A     2\n",
      "011A     2\n",
      "018A     2\n",
      "025B     2\n",
      "093A     2\n",
      "088A     1\n",
      "091A     1\n",
      "100A     1\n",
      "090A     1\n",
      "019B     1\n",
      "115A     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "073A     1\n",
      "026C     1\n",
      "076A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "014B    10\n",
      "015A     9\n",
      "099A     7\n",
      "050A     7\n",
      "021A     5\n",
      "003A     4\n",
      "056A     3\n",
      "014A     3\n",
      "113A     3\n",
      "038A     2\n",
      "102A     2\n",
      "096A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    233\n",
      "M    226\n",
      "F    210\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    115\n",
      "M    111\n",
      "F     42\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 040A, 046A, 109A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 059A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 103A, 074A, 002B, 101A, 038A, 099A, 014...\n",
      "kitten                 [014B, 111A, 047A, 042A, 050A, 110A]\n",
      "senior                       [097A, 113A, 056A, 051A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "669\n",
      "Length of y_train_val:\n",
      "669\n",
      "Length of groups_train_val:\n",
      "669\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 428, 2: 143, 1: 98})\n",
      "Epoch 1/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.2990 - accuracy: 0.3946\n",
      "Epoch 2/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 1.0204 - accuracy: 0.5262\n",
      "Epoch 3/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.9546 - accuracy: 0.5710\n",
      "Epoch 4/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.9075 - accuracy: 0.5785\n",
      "Epoch 5/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.8310 - accuracy: 0.6517\n",
      "Epoch 6/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.8282 - accuracy: 0.6338\n",
      "Epoch 7/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.7541 - accuracy: 0.6756\n",
      "Epoch 8/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.6956 - accuracy: 0.7220\n",
      "Epoch 9/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.6740 - accuracy: 0.7145\n",
      "Epoch 10/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.6592 - accuracy: 0.7250\n",
      "Epoch 11/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.6814 - accuracy: 0.7100\n",
      "Epoch 12/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.5934 - accuracy: 0.7608\n",
      "Epoch 13/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.5881 - accuracy: 0.7623\n",
      "Epoch 14/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.5710 - accuracy: 0.7788\n",
      "Epoch 15/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.5559 - accuracy: 0.7623\n",
      "Epoch 16/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.5903 - accuracy: 0.7638\n",
      "Epoch 17/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.5366 - accuracy: 0.7788\n",
      "Epoch 18/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.5207 - accuracy: 0.7758\n",
      "Epoch 19/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.5313 - accuracy: 0.7892\n",
      "Epoch 20/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4876 - accuracy: 0.8206\n",
      "Epoch 21/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4856 - accuracy: 0.8191\n",
      "Epoch 22/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4923 - accuracy: 0.7982\n",
      "Epoch 23/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4835 - accuracy: 0.8146\n",
      "Epoch 24/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4651 - accuracy: 0.8161\n",
      "Epoch 25/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4684 - accuracy: 0.8236\n",
      "Epoch 26/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.8326\n",
      "Epoch 27/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4314 - accuracy: 0.8341\n",
      "Epoch 28/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4408 - accuracy: 0.8161\n",
      "Epoch 29/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4340 - accuracy: 0.7952\n",
      "Epoch 30/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3832 - accuracy: 0.8460\n",
      "Epoch 31/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4078 - accuracy: 0.8580\n",
      "Epoch 32/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3999 - accuracy: 0.8460\n",
      "Epoch 33/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4134 - accuracy: 0.8386\n",
      "Epoch 34/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3842 - accuracy: 0.8625\n",
      "Epoch 35/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.4004 - accuracy: 0.8445\n",
      "Epoch 36/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3717 - accuracy: 0.8535\n",
      "Epoch 37/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3621 - accuracy: 0.8729\n",
      "Epoch 38/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3901 - accuracy: 0.8550\n",
      "Epoch 39/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3851 - accuracy: 0.8445\n",
      "Epoch 40/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3664 - accuracy: 0.8565\n",
      "Epoch 41/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3778 - accuracy: 0.8520\n",
      "Epoch 42/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3630 - accuracy: 0.8550\n",
      "Epoch 43/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3608 - accuracy: 0.8595\n",
      "Epoch 44/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3562 - accuracy: 0.8670\n",
      "Epoch 45/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3660 - accuracy: 0.8460\n",
      "Epoch 46/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3649 - accuracy: 0.8505\n",
      "Epoch 47/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3502 - accuracy: 0.8640\n",
      "Epoch 48/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3477 - accuracy: 0.8759\n",
      "Epoch 49/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3097 - accuracy: 0.8819\n",
      "Epoch 50/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3158 - accuracy: 0.8759\n",
      "Epoch 51/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3601 - accuracy: 0.8460\n",
      "Epoch 52/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3076 - accuracy: 0.8954\n",
      "Epoch 53/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3483 - accuracy: 0.8595\n",
      "Epoch 54/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.8789\n",
      "Epoch 55/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3202 - accuracy: 0.8789\n",
      "Epoch 56/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.8714\n",
      "Epoch 57/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3070 - accuracy: 0.8924\n",
      "Epoch 58/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3235 - accuracy: 0.8714\n",
      "Epoch 59/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3398 - accuracy: 0.8714\n",
      "Epoch 60/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.3031 - accuracy: 0.8879\n",
      "Epoch 61/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2813 - accuracy: 0.9058\n",
      "Epoch 62/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2890 - accuracy: 0.8909\n",
      "Epoch 63/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2864 - accuracy: 0.8924\n",
      "Epoch 64/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2904 - accuracy: 0.8864\n",
      "Epoch 65/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.8909\n",
      "Epoch 66/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2551 - accuracy: 0.9118\n",
      "Epoch 67/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2650 - accuracy: 0.9088\n",
      "Epoch 68/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2722 - accuracy: 0.8834\n",
      "Epoch 69/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8864\n",
      "Epoch 70/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2753 - accuracy: 0.8849\n",
      "Epoch 71/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2827 - accuracy: 0.8999\n",
      "Epoch 72/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2731 - accuracy: 0.8939\n",
      "Epoch 73/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2902 - accuracy: 0.8954\n",
      "Epoch 74/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2630 - accuracy: 0.8864\n",
      "Epoch 75/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2776 - accuracy: 0.8894\n",
      "Epoch 76/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2487 - accuracy: 0.8999\n",
      "Epoch 77/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2402 - accuracy: 0.9178\n",
      "Epoch 78/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2664 - accuracy: 0.8999\n",
      "Epoch 79/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2608 - accuracy: 0.9043\n",
      "Epoch 80/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2715 - accuracy: 0.8984\n",
      "Epoch 81/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2761 - accuracy: 0.8999\n",
      "Epoch 82/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9133\n",
      "Epoch 83/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2495 - accuracy: 0.9043\n",
      "Epoch 84/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2414 - accuracy: 0.9073\n",
      "Epoch 85/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.8924\n",
      "Epoch 86/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2349 - accuracy: 0.9103\n",
      "Epoch 87/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2096 - accuracy: 0.9208\n",
      "Epoch 88/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2157 - accuracy: 0.9148\n",
      "Epoch 89/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2201 - accuracy: 0.9163\n",
      "Epoch 90/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2437 - accuracy: 0.9103\n",
      "Epoch 91/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2379 - accuracy: 0.9148\n",
      "Epoch 92/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2314 - accuracy: 0.9028\n",
      "Epoch 93/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2272 - accuracy: 0.9238\n",
      "Epoch 94/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9073\n",
      "Epoch 95/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2458 - accuracy: 0.8999\n",
      "Epoch 96/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2209 - accuracy: 0.9163\n",
      "Epoch 97/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2347 - accuracy: 0.9163\n",
      "Epoch 98/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2155 - accuracy: 0.9223\n",
      "Epoch 99/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2144 - accuracy: 0.9223\n",
      "Epoch 100/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9283\n",
      "Epoch 101/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2486 - accuracy: 0.8969\n",
      "Epoch 102/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2388 - accuracy: 0.9118\n",
      "Epoch 103/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9163\n",
      "Epoch 104/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2098 - accuracy: 0.9208\n",
      "Epoch 105/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2434 - accuracy: 0.9013\n",
      "Epoch 106/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2209 - accuracy: 0.9193\n",
      "Epoch 107/1500\n",
      "21/21 [==============================] - 0s 994us/step - loss: 0.2185 - accuracy: 0.9148\n",
      "Epoch 108/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9327\n",
      "Epoch 109/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2056 - accuracy: 0.9223\n",
      "Epoch 110/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9178\n",
      "Epoch 111/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.9402\n",
      "Epoch 112/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2012 - accuracy: 0.9208\n",
      "Epoch 113/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2006 - accuracy: 0.9223\n",
      "Epoch 114/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9342\n",
      "Epoch 115/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.9178\n",
      "Epoch 116/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9148\n",
      "Epoch 117/1500\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 0.1864 - accuracy: 0.9357\n",
      "Epoch 118/1500\n",
      "21/21 [==============================] - 0s 7ms/step - loss: 0.1895 - accuracy: 0.9432\n",
      "Epoch 119/1500\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.1903 - accuracy: 0.9297\n",
      "Epoch 120/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9193\n",
      "Epoch 121/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1642 - accuracy: 0.9462\n",
      "Epoch 122/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9342\n",
      "Epoch 123/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.9372\n",
      "Epoch 124/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1721 - accuracy: 0.9447\n",
      "Epoch 125/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9297\n",
      "Epoch 126/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.9268\n",
      "Epoch 127/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.9312\n",
      "Epoch 128/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.9641\n",
      "Epoch 129/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9372\n",
      "Epoch 130/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9297\n",
      "Epoch 131/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9193\n",
      "Epoch 132/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1929 - accuracy: 0.9297\n",
      "Epoch 133/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9462\n",
      "Epoch 134/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1575 - accuracy: 0.9417\n",
      "Epoch 135/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1727 - accuracy: 0.9312\n",
      "Epoch 136/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.9223\n",
      "Epoch 137/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.9522\n",
      "Epoch 138/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9357\n",
      "Epoch 139/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1808 - accuracy: 0.9327\n",
      "Epoch 140/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9432\n",
      "Epoch 141/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9372\n",
      "Epoch 142/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1738 - accuracy: 0.9327\n",
      "Epoch 143/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1746 - accuracy: 0.9387\n",
      "Epoch 144/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.9417\n",
      "Epoch 145/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9402\n",
      "Epoch 146/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1808 - accuracy: 0.9357\n",
      "Epoch 147/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9462\n",
      "Epoch 148/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1561 - accuracy: 0.9477\n",
      "Epoch 149/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9342\n",
      "Epoch 150/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9507\n",
      "Epoch 151/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1711 - accuracy: 0.9417\n",
      "Epoch 152/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9387\n",
      "Epoch 153/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9357\n",
      "Epoch 154/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9372\n",
      "Epoch 155/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9462\n",
      "Epoch 156/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9372\n",
      "Epoch 157/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9537\n",
      "Epoch 158/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9492\n",
      "Epoch 159/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9507\n",
      "Epoch 160/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1710 - accuracy: 0.9357\n",
      "Epoch 161/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9447\n",
      "Epoch 162/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9567\n",
      "Epoch 163/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.9507\n",
      "Epoch 164/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1929 - accuracy: 0.9327\n",
      "Epoch 165/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9447\n",
      "Epoch 166/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1719 - accuracy: 0.9208\n",
      "Epoch 167/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9552\n",
      "Epoch 168/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9447\n",
      "Epoch 169/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9537\n",
      "Epoch 170/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9507\n",
      "Epoch 171/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9447\n",
      "Epoch 172/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9432\n",
      "Epoch 173/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9596\n",
      "Epoch 174/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9522\n",
      "Epoch 175/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9447\n",
      "Epoch 176/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9447\n",
      "Epoch 177/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9507\n",
      "Epoch 178/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9641\n",
      "Epoch 179/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9372\n",
      "Epoch 180/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9477\n",
      "Epoch 181/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.9462\n",
      "Epoch 182/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9686\n",
      "Epoch 183/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9432\n",
      "Epoch 184/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9492\n",
      "Epoch 185/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9656\n",
      "Epoch 186/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9567\n",
      "Epoch 187/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9567\n",
      "Epoch 188/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9567\n",
      "Epoch 189/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9626\n",
      "Epoch 190/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9522\n",
      "Epoch 191/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9731\n",
      "Epoch 192/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9537\n",
      "Epoch 193/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9492\n",
      "Epoch 194/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9507\n",
      "Epoch 195/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9656\n",
      "Epoch 196/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9507\n",
      "Epoch 197/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9537\n",
      "Epoch 198/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9581\n",
      "Epoch 199/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9641\n",
      "Epoch 200/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9626\n",
      "Epoch 201/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9567\n",
      "Epoch 202/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9432\n",
      "Epoch 203/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9596\n",
      "Epoch 204/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9417\n",
      "Epoch 205/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9596\n",
      "Epoch 206/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9716\n",
      "Epoch 207/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9641\n",
      "Epoch 208/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9611\n",
      "Epoch 209/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9641\n",
      "Epoch 210/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9581\n",
      "Epoch 211/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9611\n",
      "Epoch 212/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9522\n",
      "Epoch 213/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9596\n",
      "Epoch 214/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9507\n",
      "Epoch 215/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9641\n",
      "Epoch 216/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9731\n",
      "Epoch 217/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9716\n",
      "Epoch 218/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9432\n",
      "Epoch 219/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9567\n",
      "Epoch 220/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9596\n",
      "Epoch 221/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9581\n",
      "Epoch 222/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9522\n",
      "Epoch 223/1500\n",
      "21/21 [==============================] - 0s 8ms/step - loss: 0.1172 - accuracy: 0.9611\n",
      "Epoch 224/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9626\n",
      "Epoch 225/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9671\n",
      "Epoch 226/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9507\n",
      "Epoch 227/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9641\n",
      "Epoch 228/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9671\n",
      "Epoch 229/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.9641\n",
      "Epoch 230/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9716\n",
      "Epoch 231/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9701\n",
      "Epoch 232/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9581\n",
      "Epoch 233/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9567\n",
      "Epoch 234/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9567\n",
      "Epoch 235/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9746\n",
      "Epoch 236/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9537\n",
      "Epoch 237/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9581\n",
      "Epoch 238/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9641\n",
      "Epoch 239/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9581\n",
      "Epoch 240/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9671\n",
      "Epoch 241/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9671\n",
      "Epoch 242/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9522\n",
      "Epoch 243/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9552\n",
      "Epoch 244/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9552\n",
      "Epoch 245/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1414 - accuracy: 0.9507\n",
      "Epoch 246/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9567\n",
      "Epoch 247/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9716\n",
      "Epoch 248/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9641\n",
      "Epoch 249/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9701\n",
      "Epoch 250/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9701\n",
      "Epoch 251/1500\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.1210 - accuracy: 0.9581\n",
      "Epoch 252/1500\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.0979 - accuracy: 0.9686\n",
      "Epoch 253/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9761\n",
      "Epoch 254/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9686\n",
      "Epoch 255/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9746\n",
      "Epoch 256/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9686\n",
      "Epoch 257/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9656\n",
      "Epoch 258/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9686\n",
      "Epoch 259/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9701\n",
      "Epoch 260/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9581\n",
      "Epoch 261/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9611\n",
      "Epoch 262/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9701\n",
      "Epoch 263/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9656\n",
      "Epoch 264/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9686\n",
      "Epoch 265/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9716\n",
      "Epoch 266/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.9701\n",
      "Epoch 267/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9671\n",
      "Epoch 268/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9701\n",
      "Epoch 269/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9626\n",
      "Epoch 270/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9791\n",
      "Epoch 271/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9716\n",
      "Epoch 272/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9581\n",
      "Epoch 273/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9716\n",
      "Epoch 274/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9611\n",
      "Epoch 275/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9447\n",
      "Epoch 276/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9611\n",
      "Epoch 277/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.9731\n",
      "Epoch 278/1500\n",
      "21/21 [==============================] - 0s 992us/step - loss: 0.0906 - accuracy: 0.9656\n",
      "Epoch 279/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9656\n",
      "Epoch 280/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9656\n",
      "Epoch 281/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9686\n",
      "Epoch 282/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9716\n",
      "Epoch 283/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9626\n",
      "Epoch 284/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9701\n",
      "Epoch 285/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9716\n",
      "Epoch 286/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.9836\n",
      "Epoch 287/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.9746\n",
      "Epoch 288/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0705 - accuracy: 0.9806\n",
      "Epoch 289/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9701\n",
      "Epoch 290/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9731\n",
      "Epoch 291/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9686\n",
      "Epoch 292/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9716\n",
      "Epoch 293/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9716\n",
      "Epoch 294/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9701\n",
      "Epoch 295/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9776\n",
      "Epoch 296/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9821\n",
      "Epoch 297/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9581\n",
      "Epoch 298/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9626\n",
      "Epoch 299/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9552\n",
      "Epoch 300/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9686\n",
      "Epoch 301/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9746\n",
      "Epoch 302/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9791\n",
      "Epoch 303/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9656\n",
      "Epoch 304/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9656\n",
      "Epoch 305/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.9865\n",
      "Epoch 306/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9671\n",
      "Epoch 307/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.9746\n",
      "Epoch 308/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9761\n",
      "Epoch 309/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9791\n",
      "Epoch 310/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.9761\n",
      "Epoch 311/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.9895\n",
      "Epoch 312/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.9821\n",
      "Epoch 313/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9641\n",
      "Epoch 314/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9626\n",
      "Epoch 315/1500\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.0965 - accuracy: 0.9552\n",
      "Epoch 316/1500\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.0695 - accuracy: 0.9806\n",
      "Epoch 317/1500\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 0.0739 - accuracy: 0.9761\n",
      "Epoch 318/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9701\n",
      "Epoch 319/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0588 - accuracy: 0.9806\n",
      "Epoch 320/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.9821\n",
      "Epoch 321/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.9836\n",
      "Epoch 322/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.9746\n",
      "Epoch 323/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.9716\n",
      "Epoch 324/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9731\n",
      "Epoch 325/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.9701\n",
      "Epoch 326/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9581\n",
      "Epoch 327/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9701\n",
      "Epoch 328/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.9806\n",
      "Epoch 329/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0585 - accuracy: 0.9851\n",
      "Epoch 330/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9776\n",
      "Epoch 331/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9791\n",
      "Epoch 332/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.9776\n",
      "Epoch 333/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0637 - accuracy: 0.9806\n",
      "Epoch 334/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9791\n",
      "Epoch 335/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9746\n",
      "Epoch 336/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9686\n",
      "Epoch 337/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9686\n",
      "Epoch 338/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.9731\n",
      "Epoch 339/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.9851\n",
      "Epoch 340/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9761\n",
      "Epoch 341/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.9821\n",
      "Epoch 342/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.9791\n",
      "Epoch 343/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0542 - accuracy: 0.9910\n",
      "Epoch 344/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.9851\n",
      "Epoch 345/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0705 - accuracy: 0.9761\n",
      "Epoch 346/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0563 - accuracy: 0.9880\n",
      "Epoch 347/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.9731\n",
      "Epoch 348/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.9806\n",
      "Epoch 349/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9791\n",
      "Epoch 350/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9731\n",
      "Epoch 351/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.9821\n",
      "Epoch 352/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9776\n",
      "Epoch 353/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9716\n",
      "Epoch 354/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9671\n",
      "Epoch 355/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.9671\n",
      "Epoch 356/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9731\n",
      "Epoch 357/1500\n",
      "21/21 [==============================] - 0s 922us/step - loss: 0.0792 - accuracy: 0.9641\n",
      "Epoch 358/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9716\n",
      "Epoch 359/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9776\n",
      "Epoch 360/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.9836\n",
      "Epoch 361/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.9880\n",
      "Epoch 362/1500\n",
      "21/21 [==============================] - 0s 996us/step - loss: 0.0779 - accuracy: 0.9746\n",
      "Epoch 363/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9821\n",
      "Epoch 364/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9761\n",
      "Epoch 365/1500\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 0.0648 - accuracy: 0.9776\n",
      "Epoch 366/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.9880\n",
      "Epoch 367/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9731\n",
      "Epoch 368/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9731\n",
      "Epoch 369/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9731\n",
      "Epoch 370/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9776\n",
      "Epoch 371/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.9776\n",
      "Epoch 372/1500\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0644 - accuracy: 0.9746\n",
      "Epoch 373/1500\n",
      " 1/21 [>.............................] - ETA: 0s - loss: 0.0315 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 343.\n",
      "21/21 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9746\n",
      "Epoch 373: early stopping\n",
      "9/9 [==============================] - 0s 741us/step - loss: 0.8991 - accuracy: 0.7052\n",
      "9/9 [==============================] - 0s 627us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (19/25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "Final Test Results - Loss: 0.8991381525993347, Accuracy: 0.7052238583564758, Precision: 0.7060131971051512, Recall: 0.6542726679712981, F1 Score: 0.6503715250219457\n",
      "Confusion Matrix:\n",
      " [[128   1  31]\n",
      " [ 33  39   1]\n",
      " [ 13   0  22]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "053A     6\n",
      "108A     6\n",
      "023A     6\n",
      "007A     6\n",
      "025C     5\n",
      "044A     5\n",
      "023B     5\n",
      "021A     5\n",
      "070A     5\n",
      "034A     5\n",
      "003A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "104A     4\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "113A     3\n",
      "056A     3\n",
      "018A     2\n",
      "038A     2\n",
      "025B     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "102A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "073A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "063A    11\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "095A     8\n",
      "037A     6\n",
      "008A     6\n",
      "109A     6\n",
      "075A     5\n",
      "105A     4\n",
      "064A     3\n",
      "060A     3\n",
      "093A     2\n",
      "087A     2\n",
      "011A     2\n",
      "061A     2\n",
      "076A     1\n",
      "043A     1\n",
      "091A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    238\n",
      "F    229\n",
      "X    221\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    127\n",
      "M     99\n",
      "F     23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 015A, 103A, 071A, 097B, 028A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 050A, 049...\n",
      "senior    [097A, 057A, 106A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 001A, 019A, 067A, 022A, 029A, 095A, 072...\n",
      "kitten                             [046A, 109A, 043A, 045A]\n",
      "senior                             [093A, 051B, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '044A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '043A'\n",
      " '045A' '046A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'040A'}\n",
      "Moved to Test Set:\n",
      "{'040A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '041A' '042A'\n",
      " '044A' '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '040A'\n",
      " '043A' '045A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "Length of X_train_val:\n",
      "741\n",
      "Length of y_train_val:\n",
      "741\n",
      "Length of groups_train_val:\n",
      "741\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     155\n",
      "kitten     79\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten    145\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     155\n",
      "kitten     26\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 433, 2: 163, 1: 145})\n",
      "Epoch 1/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.1045 - accuracy: 0.5007\n",
      "Epoch 2/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9093 - accuracy: 0.5870\n",
      "Epoch 3/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8026 - accuracy: 0.6356\n",
      "Epoch 4/1500\n",
      "24/24 [==============================] - 0s 970us/step - loss: 0.7963 - accuracy: 0.6613\n",
      "Epoch 5/1500\n",
      "24/24 [==============================] - 0s 887us/step - loss: 0.7423 - accuracy: 0.6991\n",
      "Epoch 6/1500\n",
      "24/24 [==============================] - 0s 952us/step - loss: 0.6760 - accuracy: 0.7260\n",
      "Epoch 7/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6696 - accuracy: 0.7328\n",
      "Epoch 8/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6706 - accuracy: 0.7260\n",
      "Epoch 9/1500\n",
      "24/24 [==============================] - 0s 977us/step - loss: 0.6505 - accuracy: 0.7233\n",
      "Epoch 10/1500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.5843 - accuracy: 0.7800\n",
      "Epoch 11/1500\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6140 - accuracy: 0.7692\n",
      "Epoch 12/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6129 - accuracy: 0.7530\n",
      "Epoch 13/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5693 - accuracy: 0.7733\n",
      "Epoch 14/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5635 - accuracy: 0.7881\n",
      "Epoch 15/1500\n",
      "24/24 [==============================] - 0s 1000us/step - loss: 0.5521 - accuracy: 0.7841\n",
      "Epoch 16/1500\n",
      "24/24 [==============================] - 0s 997us/step - loss: 0.5439 - accuracy: 0.7868\n",
      "Epoch 17/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5098 - accuracy: 0.8070\n",
      "Epoch 18/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5502 - accuracy: 0.7895\n",
      "Epoch 19/1500\n",
      "24/24 [==============================] - 0s 979us/step - loss: 0.5167 - accuracy: 0.7868\n",
      "Epoch 20/1500\n",
      "24/24 [==============================] - 0s 996us/step - loss: 0.5167 - accuracy: 0.7976\n",
      "Epoch 21/1500\n",
      "24/24 [==============================] - 0s 996us/step - loss: 0.4810 - accuracy: 0.8219\n",
      "Epoch 22/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4913 - accuracy: 0.8070\n",
      "Epoch 23/1500\n",
      "24/24 [==============================] - 0s 952us/step - loss: 0.4827 - accuracy: 0.8178\n",
      "Epoch 24/1500\n",
      "24/24 [==============================] - 0s 945us/step - loss: 0.4774 - accuracy: 0.8003\n",
      "Epoch 25/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4749 - accuracy: 0.8057\n",
      "Epoch 26/1500\n",
      "24/24 [==============================] - 0s 971us/step - loss: 0.4895 - accuracy: 0.8138\n",
      "Epoch 27/1500\n",
      "24/24 [==============================] - 0s 987us/step - loss: 0.4668 - accuracy: 0.8232\n",
      "Epoch 28/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4506 - accuracy: 0.8219\n",
      "Epoch 29/1500\n",
      "24/24 [==============================] - 0s 992us/step - loss: 0.4363 - accuracy: 0.8246\n",
      "Epoch 30/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4257 - accuracy: 0.8300\n",
      "Epoch 31/1500\n",
      "24/24 [==============================] - 0s 947us/step - loss: 0.4567 - accuracy: 0.8138\n",
      "Epoch 32/1500\n",
      "24/24 [==============================] - 0s 960us/step - loss: 0.4295 - accuracy: 0.8205\n",
      "Epoch 33/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4215 - accuracy: 0.8259\n",
      "Epoch 34/1500\n",
      "24/24 [==============================] - 0s 991us/step - loss: 0.4485 - accuracy: 0.8111\n",
      "Epoch 35/1500\n",
      "24/24 [==============================] - 0s 989us/step - loss: 0.4148 - accuracy: 0.8219\n",
      "Epoch 36/1500\n",
      "24/24 [==============================] - 0s 968us/step - loss: 0.4502 - accuracy: 0.8354\n",
      "Epoch 37/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3922 - accuracy: 0.8664\n",
      "Epoch 38/1500\n",
      "24/24 [==============================] - 0s 992us/step - loss: 0.4197 - accuracy: 0.8462\n",
      "Epoch 39/1500\n",
      "24/24 [==============================] - 0s 996us/step - loss: 0.4030 - accuracy: 0.8367\n",
      "Epoch 40/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4020 - accuracy: 0.8367\n",
      "Epoch 41/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3791 - accuracy: 0.8462\n",
      "Epoch 42/1500\n",
      "24/24 [==============================] - 0s 955us/step - loss: 0.3979 - accuracy: 0.8448\n",
      "Epoch 43/1500\n",
      "24/24 [==============================] - 0s 975us/step - loss: 0.4170 - accuracy: 0.8340\n",
      "Epoch 44/1500\n",
      "24/24 [==============================] - 0s 948us/step - loss: 0.4082 - accuracy: 0.8313\n",
      "Epoch 45/1500\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.3907 - accuracy: 0.8489\n",
      "Epoch 46/1500\n",
      "24/24 [==============================] - 0s 973us/step - loss: 0.3893 - accuracy: 0.8448\n",
      "Epoch 47/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3372 - accuracy: 0.8637\n",
      "Epoch 48/1500\n",
      "24/24 [==============================] - 0s 964us/step - loss: 0.4034 - accuracy: 0.8435\n",
      "Epoch 49/1500\n",
      "24/24 [==============================] - 0s 994us/step - loss: 0.3609 - accuracy: 0.8637\n",
      "Epoch 50/1500\n",
      "24/24 [==============================] - 0s 916us/step - loss: 0.3547 - accuracy: 0.8596\n",
      "Epoch 51/1500\n",
      "24/24 [==============================] - 0s 983us/step - loss: 0.3724 - accuracy: 0.8475\n",
      "Epoch 52/1500\n",
      "24/24 [==============================] - 0s 977us/step - loss: 0.3492 - accuracy: 0.8570\n",
      "Epoch 53/1500\n",
      "24/24 [==============================] - 0s 976us/step - loss: 0.3505 - accuracy: 0.8704\n",
      "Epoch 54/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3326 - accuracy: 0.8745\n",
      "Epoch 55/1500\n",
      "24/24 [==============================] - 0s 971us/step - loss: 0.3318 - accuracy: 0.8758\n",
      "Epoch 56/1500\n",
      "24/24 [==============================] - 0s 976us/step - loss: 0.3463 - accuracy: 0.8583\n",
      "Epoch 57/1500\n",
      "24/24 [==============================] - 0s 951us/step - loss: 0.3163 - accuracy: 0.8785\n",
      "Epoch 58/1500\n",
      "24/24 [==============================] - 0s 947us/step - loss: 0.3430 - accuracy: 0.8691\n",
      "Epoch 59/1500\n",
      "24/24 [==============================] - 0s 968us/step - loss: 0.3623 - accuracy: 0.8677\n",
      "Epoch 60/1500\n",
      "24/24 [==============================] - 0s 973us/step - loss: 0.3040 - accuracy: 0.8826\n",
      "Epoch 61/1500\n",
      "24/24 [==============================] - 0s 963us/step - loss: 0.3283 - accuracy: 0.8664\n",
      "Epoch 62/1500\n",
      "24/24 [==============================] - 0s 979us/step - loss: 0.3110 - accuracy: 0.8812\n",
      "Epoch 63/1500\n",
      "24/24 [==============================] - 0s 959us/step - loss: 0.3211 - accuracy: 0.8785\n",
      "Epoch 64/1500\n",
      "24/24 [==============================] - 0s 964us/step - loss: 0.3541 - accuracy: 0.8610\n",
      "Epoch 65/1500\n",
      "24/24 [==============================] - 0s 973us/step - loss: 0.3257 - accuracy: 0.8650\n",
      "Epoch 66/1500\n",
      "24/24 [==============================] - 0s 975us/step - loss: 0.3188 - accuracy: 0.8893\n",
      "Epoch 67/1500\n",
      "24/24 [==============================] - 0s 982us/step - loss: 0.3125 - accuracy: 0.8704\n",
      "Epoch 68/1500\n",
      "24/24 [==============================] - 0s 989us/step - loss: 0.3311 - accuracy: 0.8799\n",
      "Epoch 69/1500\n",
      "24/24 [==============================] - 0s 993us/step - loss: 0.2997 - accuracy: 0.8880\n",
      "Epoch 70/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3185 - accuracy: 0.8772\n",
      "Epoch 71/1500\n",
      "24/24 [==============================] - 0s 966us/step - loss: 0.2998 - accuracy: 0.8745\n",
      "Epoch 72/1500\n",
      "24/24 [==============================] - 0s 948us/step - loss: 0.3185 - accuracy: 0.8785\n",
      "Epoch 73/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3181 - accuracy: 0.8650\n",
      "Epoch 74/1500\n",
      "24/24 [==============================] - 0s 966us/step - loss: 0.3180 - accuracy: 0.8772\n",
      "Epoch 75/1500\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.3120 - accuracy: 0.8758\n",
      "Epoch 76/1500\n",
      "24/24 [==============================] - 0s 972us/step - loss: 0.3014 - accuracy: 0.8907\n",
      "Epoch 77/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3202 - accuracy: 0.8758\n",
      "Epoch 78/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2928 - accuracy: 0.8718\n",
      "Epoch 79/1500\n",
      "24/24 [==============================] - 0s 963us/step - loss: 0.3125 - accuracy: 0.8907\n",
      "Epoch 80/1500\n",
      "24/24 [==============================] - 0s 980us/step - loss: 0.3020 - accuracy: 0.8758\n",
      "Epoch 81/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2724 - accuracy: 0.8947\n",
      "Epoch 82/1500\n",
      "24/24 [==============================] - 0s 972us/step - loss: 0.2728 - accuracy: 0.9001\n",
      "Epoch 83/1500\n",
      "24/24 [==============================] - 0s 957us/step - loss: 0.2715 - accuracy: 0.9069\n",
      "Epoch 84/1500\n",
      "24/24 [==============================] - 0s 975us/step - loss: 0.2956 - accuracy: 0.8826\n",
      "Epoch 85/1500\n",
      "24/24 [==============================] - 0s 978us/step - loss: 0.2926 - accuracy: 0.8974\n",
      "Epoch 86/1500\n",
      "24/24 [==============================] - 0s 919us/step - loss: 0.2799 - accuracy: 0.8907\n",
      "Epoch 87/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2933 - accuracy: 0.8826\n",
      "Epoch 88/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2675 - accuracy: 0.8880\n",
      "Epoch 89/1500\n",
      "24/24 [==============================] - 0s 988us/step - loss: 0.2730 - accuracy: 0.9001\n",
      "Epoch 90/1500\n",
      "24/24 [==============================] - 0s 998us/step - loss: 0.2682 - accuracy: 0.8880\n",
      "Epoch 91/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2602 - accuracy: 0.9042\n",
      "Epoch 92/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2631 - accuracy: 0.9055\n",
      "Epoch 93/1500\n",
      "24/24 [==============================] - 0s 979us/step - loss: 0.2658 - accuracy: 0.9001\n",
      "Epoch 94/1500\n",
      "24/24 [==============================] - 0s 947us/step - loss: 0.2616 - accuracy: 0.9028\n",
      "Epoch 95/1500\n",
      "24/24 [==============================] - 0s 995us/step - loss: 0.2467 - accuracy: 0.9015\n",
      "Epoch 96/1500\n",
      "24/24 [==============================] - 0s 950us/step - loss: 0.2503 - accuracy: 0.9001\n",
      "Epoch 97/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2689 - accuracy: 0.8934\n",
      "Epoch 98/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2730 - accuracy: 0.8947\n",
      "Epoch 99/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2727 - accuracy: 0.9042\n",
      "Epoch 100/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2775 - accuracy: 0.8880\n",
      "Epoch 101/1500\n",
      "24/24 [==============================] - 0s 974us/step - loss: 0.2758 - accuracy: 0.8799\n",
      "Epoch 102/1500\n",
      "24/24 [==============================] - 0s 985us/step - loss: 0.2565 - accuracy: 0.8988\n",
      "Epoch 103/1500\n",
      "24/24 [==============================] - 0s 975us/step - loss: 0.2675 - accuracy: 0.9082\n",
      "Epoch 104/1500\n",
      "24/24 [==============================] - 0s 977us/step - loss: 0.2717 - accuracy: 0.8934\n",
      "Epoch 105/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2385 - accuracy: 0.9163\n",
      "Epoch 106/1500\n",
      "24/24 [==============================] - 0s 982us/step - loss: 0.2827 - accuracy: 0.8961\n",
      "Epoch 107/1500\n",
      "24/24 [==============================] - 0s 995us/step - loss: 0.2618 - accuracy: 0.9055\n",
      "Epoch 108/1500\n",
      "24/24 [==============================] - 0s 996us/step - loss: 0.2441 - accuracy: 0.9082\n",
      "Epoch 109/1500\n",
      "24/24 [==============================] - 0s 998us/step - loss: 0.2494 - accuracy: 0.9082\n",
      "Epoch 110/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2464 - accuracy: 0.9123\n",
      "Epoch 111/1500\n",
      "24/24 [==============================] - 0s 989us/step - loss: 0.2442 - accuracy: 0.9069\n",
      "Epoch 112/1500\n",
      "24/24 [==============================] - 0s 983us/step - loss: 0.2528 - accuracy: 0.8974\n",
      "Epoch 113/1500\n",
      "24/24 [==============================] - 0s 971us/step - loss: 0.2612 - accuracy: 0.8988\n",
      "Epoch 114/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2262 - accuracy: 0.9123\n",
      "Epoch 115/1500\n",
      "24/24 [==============================] - 0s 987us/step - loss: 0.2519 - accuracy: 0.9055\n",
      "Epoch 116/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2491 - accuracy: 0.9015\n",
      "Epoch 117/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2196 - accuracy: 0.9366\n",
      "Epoch 118/1500\n",
      "24/24 [==============================] - 0s 989us/step - loss: 0.2538 - accuracy: 0.8961\n",
      "Epoch 119/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2296 - accuracy: 0.9136\n",
      "Epoch 120/1500\n",
      "24/24 [==============================] - 0s 998us/step - loss: 0.2348 - accuracy: 0.9055\n",
      "Epoch 121/1500\n",
      "24/24 [==============================] - 0s 995us/step - loss: 0.2311 - accuracy: 0.9163\n",
      "Epoch 122/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2366 - accuracy: 0.9028\n",
      "Epoch 123/1500\n",
      "24/24 [==============================] - 0s 993us/step - loss: 0.2421 - accuracy: 0.9082\n",
      "Epoch 124/1500\n",
      "24/24 [==============================] - 0s 986us/step - loss: 0.2182 - accuracy: 0.9177\n",
      "Epoch 125/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2348 - accuracy: 0.9082\n",
      "Epoch 126/1500\n",
      "24/24 [==============================] - 0s 987us/step - loss: 0.2157 - accuracy: 0.9271\n",
      "Epoch 127/1500\n",
      "24/24 [==============================] - 0s 983us/step - loss: 0.2020 - accuracy: 0.9393\n",
      "Epoch 128/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2291 - accuracy: 0.9136\n",
      "Epoch 129/1500\n",
      "24/24 [==============================] - 0s 993us/step - loss: 0.2349 - accuracy: 0.9015\n",
      "Epoch 130/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2249 - accuracy: 0.9163\n",
      "Epoch 131/1500\n",
      "24/24 [==============================] - 0s 999us/step - loss: 0.2303 - accuracy: 0.9136\n",
      "Epoch 132/1500\n",
      "24/24 [==============================] - 0s 981us/step - loss: 0.2321 - accuracy: 0.9096\n",
      "Epoch 133/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2192 - accuracy: 0.9109\n",
      "Epoch 134/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2132 - accuracy: 0.9285\n",
      "Epoch 135/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2036 - accuracy: 0.9298\n",
      "Epoch 136/1500\n",
      "24/24 [==============================] - 0s 984us/step - loss: 0.2124 - accuracy: 0.9150\n",
      "Epoch 137/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2190 - accuracy: 0.9271\n",
      "Epoch 138/1500\n",
      "24/24 [==============================] - 0s 984us/step - loss: 0.1989 - accuracy: 0.9312\n",
      "Epoch 139/1500\n",
      "24/24 [==============================] - 0s 982us/step - loss: 0.2142 - accuracy: 0.9163\n",
      "Epoch 140/1500\n",
      "24/24 [==============================] - 0s 981us/step - loss: 0.2120 - accuracy: 0.9244\n",
      "Epoch 141/1500\n",
      "24/24 [==============================] - 0s 997us/step - loss: 0.2159 - accuracy: 0.9204\n",
      "Epoch 142/1500\n",
      "24/24 [==============================] - 0s 972us/step - loss: 0.2375 - accuracy: 0.9123\n",
      "Epoch 143/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2122 - accuracy: 0.9150\n",
      "Epoch 144/1500\n",
      "24/24 [==============================] - 0s 987us/step - loss: 0.2409 - accuracy: 0.8988\n",
      "Epoch 145/1500\n",
      "24/24 [==============================] - 0s 989us/step - loss: 0.2138 - accuracy: 0.9231\n",
      "Epoch 146/1500\n",
      "24/24 [==============================] - 0s 935us/step - loss: 0.2176 - accuracy: 0.9177\n",
      "Epoch 147/1500\n",
      "24/24 [==============================] - 0s 955us/step - loss: 0.2279 - accuracy: 0.9082\n",
      "Epoch 148/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2158 - accuracy: 0.9258\n",
      "Epoch 149/1500\n",
      "24/24 [==============================] - 0s 986us/step - loss: 0.2178 - accuracy: 0.9136\n",
      "Epoch 150/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9204\n",
      "Epoch 151/1500\n",
      "24/24 [==============================] - 0s 968us/step - loss: 0.2129 - accuracy: 0.9177\n",
      "Epoch 152/1500\n",
      "24/24 [==============================] - 0s 981us/step - loss: 0.2178 - accuracy: 0.9136\n",
      "Epoch 153/1500\n",
      "24/24 [==============================] - 0s 999us/step - loss: 0.2305 - accuracy: 0.9177\n",
      "Epoch 154/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2235 - accuracy: 0.9123\n",
      "Epoch 155/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1871 - accuracy: 0.9366\n",
      "Epoch 156/1500\n",
      "24/24 [==============================] - 0s 943us/step - loss: 0.2007 - accuracy: 0.9204\n",
      "Epoch 157/1500\n",
      "24/24 [==============================] - 0s 990us/step - loss: 0.1996 - accuracy: 0.9217\n",
      "Epoch 158/1500\n",
      "24/24 [==============================] - 0s 985us/step - loss: 0.2087 - accuracy: 0.9204\n",
      "Epoch 159/1500\n",
      "24/24 [==============================] - 0s 6ms/step - loss: 0.2253 - accuracy: 0.9096\n",
      "Epoch 160/1500\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.2175 - accuracy: 0.9244\n",
      "Epoch 161/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2359 - accuracy: 0.9028\n",
      "Epoch 162/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2209 - accuracy: 0.9055\n",
      "Epoch 163/1500\n",
      "24/24 [==============================] - 0s 1000us/step - loss: 0.2033 - accuracy: 0.9231\n",
      "Epoch 164/1500\n",
      "24/24 [==============================] - 0s 982us/step - loss: 0.2231 - accuracy: 0.9177\n",
      "Epoch 165/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9177\n",
      "Epoch 166/1500\n",
      "24/24 [==============================] - 0s 954us/step - loss: 0.2181 - accuracy: 0.9136\n",
      "Epoch 167/1500\n",
      "24/24 [==============================] - 0s 958us/step - loss: 0.1872 - accuracy: 0.9312\n",
      "Epoch 168/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9217\n",
      "Epoch 169/1500\n",
      "24/24 [==============================] - 0s 975us/step - loss: 0.2080 - accuracy: 0.9244\n",
      "Epoch 170/1500\n",
      "24/24 [==============================] - 0s 975us/step - loss: 0.1831 - accuracy: 0.9366\n",
      "Epoch 171/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2108 - accuracy: 0.9204\n",
      "Epoch 172/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9217\n",
      "Epoch 173/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.9258\n",
      "Epoch 174/1500\n",
      "24/24 [==============================] - 0s 965us/step - loss: 0.2015 - accuracy: 0.9231\n",
      "Epoch 175/1500\n",
      "24/24 [==============================] - 0s 997us/step - loss: 0.1892 - accuracy: 0.9298\n",
      "Epoch 176/1500\n",
      "24/24 [==============================] - 0s 956us/step - loss: 0.1869 - accuracy: 0.9366\n",
      "Epoch 177/1500\n",
      "24/24 [==============================] - 0s 975us/step - loss: 0.1893 - accuracy: 0.9244\n",
      "Epoch 178/1500\n",
      "24/24 [==============================] - 0s 976us/step - loss: 0.1978 - accuracy: 0.9285\n",
      "Epoch 179/1500\n",
      "24/24 [==============================] - 0s 992us/step - loss: 0.1968 - accuracy: 0.9244\n",
      "Epoch 180/1500\n",
      "24/24 [==============================] - 0s 964us/step - loss: 0.1932 - accuracy: 0.9298\n",
      "Epoch 181/1500\n",
      "24/24 [==============================] - 0s 999us/step - loss: 0.1837 - accuracy: 0.9325\n",
      "Epoch 182/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9352\n",
      "Epoch 183/1500\n",
      "24/24 [==============================] - 0s 967us/step - loss: 0.1954 - accuracy: 0.9285\n",
      "Epoch 184/1500\n",
      "24/24 [==============================] - 0s 946us/step - loss: 0.1878 - accuracy: 0.9366\n",
      "Epoch 185/1500\n",
      "24/24 [==============================] - 0s 992us/step - loss: 0.1974 - accuracy: 0.9312\n",
      "Epoch 186/1500\n",
      "24/24 [==============================] - 0s 987us/step - loss: 0.1803 - accuracy: 0.9366\n",
      "Epoch 187/1500\n",
      "24/24 [==============================] - 0s 973us/step - loss: 0.1663 - accuracy: 0.9447\n",
      "Epoch 188/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1475 - accuracy: 0.9474\n",
      "Epoch 189/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9177\n",
      "Epoch 190/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9393\n",
      "Epoch 191/1500\n",
      "24/24 [==============================] - 0s 985us/step - loss: 0.2120 - accuracy: 0.9177\n",
      "Epoch 192/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1649 - accuracy: 0.9447\n",
      "Epoch 193/1500\n",
      "24/24 [==============================] - 0s 959us/step - loss: 0.1866 - accuracy: 0.9393\n",
      "Epoch 194/1500\n",
      "24/24 [==============================] - 0s 992us/step - loss: 0.1906 - accuracy: 0.9379\n",
      "Epoch 195/1500\n",
      "24/24 [==============================] - 0s 962us/step - loss: 0.1829 - accuracy: 0.9379\n",
      "Epoch 196/1500\n",
      "24/24 [==============================] - 0s 978us/step - loss: 0.1906 - accuracy: 0.9231\n",
      "Epoch 197/1500\n",
      "24/24 [==============================] - 0s 989us/step - loss: 0.1930 - accuracy: 0.9244\n",
      "Epoch 198/1500\n",
      "24/24 [==============================] - 0s 984us/step - loss: 0.1692 - accuracy: 0.9406\n",
      "Epoch 199/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9393\n",
      "Epoch 200/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.9393\n",
      "Epoch 201/1500\n",
      "24/24 [==============================] - 0s 976us/step - loss: 0.1508 - accuracy: 0.9487\n",
      "Epoch 202/1500\n",
      "24/24 [==============================] - 0s 963us/step - loss: 0.1910 - accuracy: 0.9325\n",
      "Epoch 203/1500\n",
      "24/24 [==============================] - 0s 987us/step - loss: 0.1611 - accuracy: 0.9501\n",
      "Epoch 204/1500\n",
      "24/24 [==============================] - 0s 973us/step - loss: 0.1554 - accuracy: 0.9460\n",
      "Epoch 205/1500\n",
      "24/24 [==============================] - 0s 967us/step - loss: 0.1783 - accuracy: 0.9393\n",
      "Epoch 206/1500\n",
      "24/24 [==============================] - 0s 966us/step - loss: 0.1591 - accuracy: 0.9514\n",
      "Epoch 207/1500\n",
      "24/24 [==============================] - 0s 949us/step - loss: 0.1748 - accuracy: 0.9393\n",
      "Epoch 208/1500\n",
      "24/24 [==============================] - 0s 989us/step - loss: 0.1765 - accuracy: 0.9393\n",
      "Epoch 209/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.9366\n",
      "Epoch 210/1500\n",
      "24/24 [==============================] - 0s 968us/step - loss: 0.1733 - accuracy: 0.9420\n",
      "Epoch 211/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1534 - accuracy: 0.9433\n",
      "Epoch 212/1500\n",
      "24/24 [==============================] - 0s 997us/step - loss: 0.1612 - accuracy: 0.9447\n",
      "Epoch 213/1500\n",
      "24/24 [==============================] - 0s 959us/step - loss: 0.1595 - accuracy: 0.9379\n",
      "Epoch 214/1500\n",
      "24/24 [==============================] - 0s 988us/step - loss: 0.1642 - accuracy: 0.9528\n",
      "Epoch 215/1500\n",
      "24/24 [==============================] - 0s 966us/step - loss: 0.1515 - accuracy: 0.9474\n",
      "Epoch 216/1500\n",
      "24/24 [==============================] - 0s 990us/step - loss: 0.1555 - accuracy: 0.9433\n",
      "Epoch 217/1500\n",
      "24/24 [==============================] - 0s 942us/step - loss: 0.1632 - accuracy: 0.9393\n",
      "Epoch 218/1500\n",
      " 1/24 [>.............................] - ETA: 0s - loss: 0.2669 - accuracy: 0.8750Restoring model weights from the end of the best epoch: 188.\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9352\n",
      "Epoch 218: early stopping\n",
      "7/7 [==============================] - 0s 753us/step - loss: 0.6852 - accuracy: 0.7092\n",
      "7/7 [==============================] - 0s 574us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (21/29)\n",
      "Before appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "After appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "Final Test Results - Loss: 0.6851666569709778, Accuracy: 0.7091836929321289, Precision: 0.569899317440301, Recall: 0.6741659773917839, F1 Score: 0.5915981871279655\n",
      "Confusion Matrix:\n",
      " [[110  13  32]\n",
      " [  4  22   0]\n",
      " [  8   0   7]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "042A    14\n",
      "097B    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "095A     8\n",
      "117A     7\n",
      "050A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "008A     6\n",
      "109A     6\n",
      "053A     6\n",
      "037A     6\n",
      "108A     6\n",
      "023A     6\n",
      "023B     5\n",
      "075A     5\n",
      "021A     5\n",
      "105A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "003A     4\n",
      "056A     3\n",
      "113A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "012A     3\n",
      "058A     3\n",
      "061A     2\n",
      "011A     2\n",
      "102A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "069A     2\n",
      "041A     1\n",
      "019B     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "096A     1\n",
      "076A     1\n",
      "088A     1\n",
      "092A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "073A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "000B    19\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "025A    11\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "010A     8\n",
      "013B     8\n",
      "007A     6\n",
      "070A     5\n",
      "044A     5\n",
      "034A     5\n",
      "025C     5\n",
      "104A     4\n",
      "009A     4\n",
      "006A     3\n",
      "018A     2\n",
      "054A     2\n",
      "025B     2\n",
      "032A     2\n",
      "049A     1\n",
      "026C     1\n",
      "066A     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    322\n",
      "M    241\n",
      "F    159\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "F    93\n",
      "X    26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 001A, 103A, 097B, 019A, 074...\n",
      "kitten    [014B, 111A, 040A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 057A, 055A, 113A, 051B, 117A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 071A, 028A, 020A, 034A, 005A, 002A, 009...\n",
      "kitten                                   [044A, 049A, 115A]\n",
      "senior           [106A, 104A, 059A, 116A, 054A, 016A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 13, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 3, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "722\n",
      "Length of y_train_val:\n",
      "722\n",
      "Length of groups_train_val:\n",
      "722\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 437, 1: 164, 2: 121})\n",
      "Epoch 1/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.2612 - accuracy: 0.4626\n",
      "Epoch 2/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.9475 - accuracy: 0.5873\n",
      "Epoch 3/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.9162 - accuracy: 0.6053\n",
      "Epoch 4/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8008 - accuracy: 0.6316\n",
      "Epoch 5/1500\n",
      "23/23 [==============================] - 0s 932us/step - loss: 0.6982 - accuracy: 0.7022\n",
      "Epoch 6/1500\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.7060 - accuracy: 0.7216\n",
      "Epoch 7/1500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.6638 - accuracy: 0.7285\n",
      "Epoch 8/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.6480 - accuracy: 0.7438\n",
      "Epoch 9/1500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.6017 - accuracy: 0.7396\n",
      "Epoch 10/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.6302 - accuracy: 0.7327\n",
      "Epoch 11/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5888 - accuracy: 0.7770\n",
      "Epoch 12/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.6093 - accuracy: 0.7881\n",
      "Epoch 13/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.5648 - accuracy: 0.7742\n",
      "Epoch 14/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.5165 - accuracy: 0.7867\n",
      "Epoch 15/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5004 - accuracy: 0.7881\n",
      "Epoch 16/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.5084 - accuracy: 0.8033\n",
      "Epoch 17/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.4914 - accuracy: 0.8144\n",
      "Epoch 18/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.4798 - accuracy: 0.8241\n",
      "Epoch 19/1500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.4766 - accuracy: 0.8144\n",
      "Epoch 20/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.4207 - accuracy: 0.8587\n",
      "Epoch 21/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.4325 - accuracy: 0.8449\n",
      "Epoch 22/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.4627 - accuracy: 0.8158\n",
      "Epoch 23/1500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.4409 - accuracy: 0.8338\n",
      "Epoch 24/1500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.4058 - accuracy: 0.8463\n",
      "Epoch 25/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.4064 - accuracy: 0.8366\n",
      "Epoch 26/1500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.3865 - accuracy: 0.8518\n",
      "Epoch 27/1500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.3961 - accuracy: 0.8421\n",
      "Epoch 28/1500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.4021 - accuracy: 0.8421\n",
      "Epoch 29/1500\n",
      "23/23 [==============================] - 0s 931us/step - loss: 0.4090 - accuracy: 0.8380\n",
      "Epoch 30/1500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.3735 - accuracy: 0.8670\n",
      "Epoch 31/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.3501 - accuracy: 0.8698\n",
      "Epoch 32/1500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.3713 - accuracy: 0.8573\n",
      "Epoch 33/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.3426 - accuracy: 0.8809\n",
      "Epoch 34/1500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.3718 - accuracy: 0.8615\n",
      "Epoch 35/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.3390 - accuracy: 0.8670\n",
      "Epoch 36/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.3551 - accuracy: 0.8573\n",
      "Epoch 37/1500\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.3487 - accuracy: 0.8615\n",
      "Epoch 38/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.3429 - accuracy: 0.8767\n",
      "Epoch 39/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.3492 - accuracy: 0.8629\n",
      "Epoch 40/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.3283 - accuracy: 0.8740\n",
      "Epoch 41/1500\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.3250 - accuracy: 0.8740\n",
      "Epoch 42/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.3388 - accuracy: 0.8629\n",
      "Epoch 43/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.3240 - accuracy: 0.8809\n",
      "Epoch 44/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.2975 - accuracy: 0.8934\n",
      "Epoch 45/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.3220 - accuracy: 0.8726\n",
      "Epoch 46/1500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.3272 - accuracy: 0.8781\n",
      "Epoch 47/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.3222 - accuracy: 0.8795\n",
      "Epoch 48/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.2753 - accuracy: 0.9072\n",
      "Epoch 49/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.3220 - accuracy: 0.8781\n",
      "Epoch 50/1500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.3008 - accuracy: 0.8878\n",
      "Epoch 51/1500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.2945 - accuracy: 0.8947\n",
      "Epoch 52/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.2869 - accuracy: 0.8795\n",
      "Epoch 53/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.2924 - accuracy: 0.8753\n",
      "Epoch 54/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.3066 - accuracy: 0.8753\n",
      "Epoch 55/1500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.2828 - accuracy: 0.8823\n",
      "Epoch 56/1500\n",
      "23/23 [==============================] - 0s 949us/step - loss: 0.2463 - accuracy: 0.9086\n",
      "Epoch 57/1500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.2792 - accuracy: 0.8934\n",
      "Epoch 58/1500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.2588 - accuracy: 0.9155\n",
      "Epoch 59/1500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.2890 - accuracy: 0.8837\n",
      "Epoch 60/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.3378 - accuracy: 0.8657\n",
      "Epoch 61/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.2724 - accuracy: 0.8850\n",
      "Epoch 62/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.2566 - accuracy: 0.9044\n",
      "Epoch 63/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.2536 - accuracy: 0.9003\n",
      "Epoch 64/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.2775 - accuracy: 0.8864\n",
      "Epoch 65/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.2487 - accuracy: 0.9155\n",
      "Epoch 66/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.2477 - accuracy: 0.9155\n",
      "Epoch 67/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.2719 - accuracy: 0.8989\n",
      "Epoch 68/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.2618 - accuracy: 0.8961\n",
      "Epoch 69/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.2586 - accuracy: 0.9086\n",
      "Epoch 70/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.2432 - accuracy: 0.9183\n",
      "Epoch 71/1500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.2424 - accuracy: 0.9072\n",
      "Epoch 72/1500\n",
      "23/23 [==============================] - 0s 925us/step - loss: 0.2500 - accuracy: 0.8989\n",
      "Epoch 73/1500\n",
      "23/23 [==============================] - 0s 942us/step - loss: 0.2452 - accuracy: 0.9086\n",
      "Epoch 74/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2083 - accuracy: 0.9211\n",
      "Epoch 75/1500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.2136 - accuracy: 0.9377\n",
      "Epoch 76/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.2305 - accuracy: 0.9114\n",
      "Epoch 77/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2658 - accuracy: 0.9003\n",
      "Epoch 78/1500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.2385 - accuracy: 0.8975\n",
      "Epoch 79/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.2458 - accuracy: 0.9114\n",
      "Epoch 80/1500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.2656 - accuracy: 0.8989\n",
      "Epoch 81/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.2193 - accuracy: 0.9252\n",
      "Epoch 82/1500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.2668 - accuracy: 0.9017\n",
      "Epoch 83/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9197\n",
      "Epoch 84/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.2479 - accuracy: 0.9058\n",
      "Epoch 85/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.2324 - accuracy: 0.9127\n",
      "Epoch 86/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.2226 - accuracy: 0.9155\n",
      "Epoch 87/1500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.2357 - accuracy: 0.9030\n",
      "Epoch 88/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.2350 - accuracy: 0.9017\n",
      "Epoch 89/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.2157 - accuracy: 0.9280\n",
      "Epoch 90/1500\n",
      "23/23 [==============================] - 0s 943us/step - loss: 0.2100 - accuracy: 0.9211\n",
      "Epoch 91/1500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.2554 - accuracy: 0.9058\n",
      "Epoch 92/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.2465 - accuracy: 0.8975\n",
      "Epoch 93/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.1983 - accuracy: 0.9197\n",
      "Epoch 94/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.2346 - accuracy: 0.9127\n",
      "Epoch 95/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.2519 - accuracy: 0.9030\n",
      "Epoch 96/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.2181 - accuracy: 0.9211\n",
      "Epoch 97/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9294\n",
      "Epoch 98/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.2279 - accuracy: 0.9100\n",
      "Epoch 99/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2167 - accuracy: 0.9197\n",
      "Epoch 100/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.1909 - accuracy: 0.9349\n",
      "Epoch 101/1500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.2296 - accuracy: 0.9114\n",
      "Epoch 102/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.2075 - accuracy: 0.9197\n",
      "Epoch 103/1500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.2332 - accuracy: 0.9155\n",
      "Epoch 104/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2037 - accuracy: 0.9266\n",
      "Epoch 105/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1962 - accuracy: 0.9224\n",
      "Epoch 106/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2503 - accuracy: 0.9114\n",
      "Epoch 107/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.9280\n",
      "Epoch 108/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2242 - accuracy: 0.9197\n",
      "Epoch 109/1500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.1909 - accuracy: 0.9307\n",
      "Epoch 110/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.2119 - accuracy: 0.9197\n",
      "Epoch 111/1500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.1885 - accuracy: 0.9266\n",
      "Epoch 112/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1655 - accuracy: 0.9280\n",
      "Epoch 113/1500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.1859 - accuracy: 0.9349\n",
      "Epoch 114/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.1913 - accuracy: 0.9363\n",
      "Epoch 115/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.2049 - accuracy: 0.9169\n",
      "Epoch 116/1500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.1980 - accuracy: 0.9294\n",
      "Epoch 117/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9418\n",
      "Epoch 118/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9460\n",
      "Epoch 119/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.2206 - accuracy: 0.9086\n",
      "Epoch 120/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1965 - accuracy: 0.9418\n",
      "Epoch 121/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.2025 - accuracy: 0.9294\n",
      "Epoch 122/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.1705 - accuracy: 0.9432\n",
      "Epoch 123/1500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.2002 - accuracy: 0.9280\n",
      "Epoch 124/1500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.1657 - accuracy: 0.9391\n",
      "Epoch 125/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.1877 - accuracy: 0.9266\n",
      "Epoch 126/1500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.2002 - accuracy: 0.9224\n",
      "Epoch 127/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.1889 - accuracy: 0.9307\n",
      "Epoch 128/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.1891 - accuracy: 0.9266\n",
      "Epoch 129/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.1758 - accuracy: 0.9280\n",
      "Epoch 130/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.1737 - accuracy: 0.9197\n",
      "Epoch 131/1500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.1765 - accuracy: 0.9377\n",
      "Epoch 132/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.1820 - accuracy: 0.9363\n",
      "Epoch 133/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.2124 - accuracy: 0.9155\n",
      "Epoch 134/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9418\n",
      "Epoch 135/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.1492 - accuracy: 0.9404\n",
      "Epoch 136/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.1557 - accuracy: 0.9432\n",
      "Epoch 137/1500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.1810 - accuracy: 0.9363\n",
      "Epoch 138/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.1901 - accuracy: 0.9252\n",
      "Epoch 139/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9294\n",
      "Epoch 140/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.1812 - accuracy: 0.9349\n",
      "Epoch 141/1500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.1699 - accuracy: 0.9321\n",
      "Epoch 142/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.1740 - accuracy: 0.9418\n",
      "Epoch 143/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.1585 - accuracy: 0.9432\n",
      "Epoch 144/1500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.1549 - accuracy: 0.9460\n",
      "Epoch 145/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9418\n",
      "Epoch 146/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.1556 - accuracy: 0.9474\n",
      "Epoch 147/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1462 - accuracy: 0.9460\n",
      "Epoch 148/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.1825 - accuracy: 0.9266\n",
      "Epoch 149/1500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.1616 - accuracy: 0.9377\n",
      "Epoch 150/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.1454 - accuracy: 0.9501\n",
      "Epoch 151/1500\n",
      "23/23 [==============================] - 0s 943us/step - loss: 0.1591 - accuracy: 0.9446\n",
      "Epoch 152/1500\n",
      "23/23 [==============================] - 0s 942us/step - loss: 0.1566 - accuracy: 0.9391\n",
      "Epoch 153/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.1704 - accuracy: 0.9363\n",
      "Epoch 154/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.1355 - accuracy: 0.9543\n",
      "Epoch 155/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.1450 - accuracy: 0.9543\n",
      "Epoch 156/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.1476 - accuracy: 0.9432\n",
      "Epoch 157/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.1545 - accuracy: 0.9446\n",
      "Epoch 158/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.1536 - accuracy: 0.9432\n",
      "Epoch 159/1500\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.1571 - accuracy: 0.9543\n",
      "Epoch 160/1500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.1570 - accuracy: 0.9377\n",
      "Epoch 161/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.1740 - accuracy: 0.9391\n",
      "Epoch 162/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.1472 - accuracy: 0.9474\n",
      "Epoch 163/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.1433 - accuracy: 0.9418\n",
      "Epoch 164/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.1572 - accuracy: 0.9460\n",
      "Epoch 165/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.1223 - accuracy: 0.9584\n",
      "Epoch 166/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.1539 - accuracy: 0.9501\n",
      "Epoch 167/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.1492 - accuracy: 0.9391\n",
      "Epoch 168/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.1236 - accuracy: 0.9626\n",
      "Epoch 169/1500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.1382 - accuracy: 0.9460\n",
      "Epoch 170/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.1414 - accuracy: 0.9515\n",
      "Epoch 171/1500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.1413 - accuracy: 0.9488\n",
      "Epoch 172/1500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.1431 - accuracy: 0.9584\n",
      "Epoch 173/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.1402 - accuracy: 0.9501\n",
      "Epoch 174/1500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.1473 - accuracy: 0.9446\n",
      "Epoch 175/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.1277 - accuracy: 0.9571\n",
      "Epoch 176/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.1634 - accuracy: 0.9391\n",
      "Epoch 177/1500\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.1219 - accuracy: 0.9543\n",
      "Epoch 178/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1300 - accuracy: 0.9488\n",
      "Epoch 179/1500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.1475 - accuracy: 0.9377\n",
      "Epoch 180/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.1306 - accuracy: 0.9557\n",
      "Epoch 181/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.1346 - accuracy: 0.9515\n",
      "Epoch 182/1500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.1517 - accuracy: 0.9515\n",
      "Epoch 183/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.9404\n",
      "Epoch 184/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9460\n",
      "Epoch 185/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9529\n",
      "Epoch 186/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9363\n",
      "Epoch 187/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.1334 - accuracy: 0.9598\n",
      "Epoch 188/1500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.1294 - accuracy: 0.9557\n",
      "Epoch 189/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9501\n",
      "Epoch 190/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.1202 - accuracy: 0.9612\n",
      "Epoch 191/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.1384 - accuracy: 0.9488\n",
      "Epoch 192/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.1119 - accuracy: 0.9668\n",
      "Epoch 193/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.1330 - accuracy: 0.9446\n",
      "Epoch 194/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.1326 - accuracy: 0.9529\n",
      "Epoch 195/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9543\n",
      "Epoch 196/1500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.1160 - accuracy: 0.9612\n",
      "Epoch 197/1500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.1226 - accuracy: 0.9598\n",
      "Epoch 198/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.1456 - accuracy: 0.9446\n",
      "Epoch 199/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.1220 - accuracy: 0.9626\n",
      "Epoch 200/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9557\n",
      "Epoch 201/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.1297 - accuracy: 0.9640\n",
      "Epoch 202/1500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.1161 - accuracy: 0.9584\n",
      "Epoch 203/1500\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.1152 - accuracy: 0.9598\n",
      "Epoch 204/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.1183 - accuracy: 0.9640\n",
      "Epoch 205/1500\n",
      "23/23 [==============================] - 0s 940us/step - loss: 0.1176 - accuracy: 0.9543\n",
      "Epoch 206/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.1326 - accuracy: 0.9571\n",
      "Epoch 207/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.0991 - accuracy: 0.9654\n",
      "Epoch 208/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9571\n",
      "Epoch 209/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.1242 - accuracy: 0.9501\n",
      "Epoch 210/1500\n",
      "23/23 [==============================] - 0s 942us/step - loss: 0.1300 - accuracy: 0.9584\n",
      "Epoch 211/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.1336 - accuracy: 0.9543\n",
      "Epoch 212/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.1129 - accuracy: 0.9557\n",
      "Epoch 213/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.1056 - accuracy: 0.9598\n",
      "Epoch 214/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.1025 - accuracy: 0.9640\n",
      "Epoch 215/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9515\n",
      "Epoch 216/1500\n",
      "23/23 [==============================] - 0s 940us/step - loss: 0.1123 - accuracy: 0.9557\n",
      "Epoch 217/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.1121 - accuracy: 0.9598\n",
      "Epoch 218/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.1081 - accuracy: 0.9737\n",
      "Epoch 219/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.1418 - accuracy: 0.9488\n",
      "Epoch 220/1500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.1288 - accuracy: 0.9432\n",
      "Epoch 221/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.1278 - accuracy: 0.9557\n",
      "Epoch 222/1500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.0944 - accuracy: 0.9737\n",
      "Epoch 223/1500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.1171 - accuracy: 0.9543\n",
      "Epoch 224/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.1251 - accuracy: 0.9543\n",
      "Epoch 225/1500\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.1153 - accuracy: 0.9543\n",
      "Epoch 226/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.1166 - accuracy: 0.9584\n",
      "Epoch 227/1500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.1002 - accuracy: 0.9695\n",
      "Epoch 228/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9723\n",
      "Epoch 229/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.0926 - accuracy: 0.9681\n",
      "Epoch 230/1500\n",
      "23/23 [==============================] - 0s 945us/step - loss: 0.1185 - accuracy: 0.9612\n",
      "Epoch 231/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.0957 - accuracy: 0.9723\n",
      "Epoch 232/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.1106 - accuracy: 0.9612\n",
      "Epoch 233/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.1160 - accuracy: 0.9557\n",
      "Epoch 234/1500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.0929 - accuracy: 0.9668\n",
      "Epoch 235/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.0989 - accuracy: 0.9709\n",
      "Epoch 236/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.1091 - accuracy: 0.9598\n",
      "Epoch 237/1500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.0908 - accuracy: 0.9668\n",
      "Epoch 238/1500\n",
      "23/23 [==============================] - 0s 911us/step - loss: 0.1166 - accuracy: 0.9598\n",
      "Epoch 239/1500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.1017 - accuracy: 0.9654\n",
      "Epoch 240/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.1093 - accuracy: 0.9640\n",
      "Epoch 241/1500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1127 - accuracy: 0.9584\n",
      "Epoch 242/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1054 - accuracy: 0.9557\n",
      "Epoch 243/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9668\n",
      "Epoch 244/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9654\n",
      "Epoch 245/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.1019 - accuracy: 0.9654\n",
      "Epoch 246/1500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.1162 - accuracy: 0.9571\n",
      "Epoch 247/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.0951 - accuracy: 0.9695\n",
      "Epoch 248/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.0951 - accuracy: 0.9709\n",
      "Epoch 249/1500\n",
      "23/23 [==============================] - 0s 926us/step - loss: 0.1066 - accuracy: 0.9640\n",
      "Epoch 250/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.0952 - accuracy: 0.9695\n",
      "Epoch 251/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.1071 - accuracy: 0.9626\n",
      "Epoch 252/1500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.1011 - accuracy: 0.9668\n",
      "Epoch 253/1500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.0978 - accuracy: 0.9668\n",
      "Epoch 254/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1126 - accuracy: 0.9640\n",
      "Epoch 255/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9640\n",
      "Epoch 256/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.0937 - accuracy: 0.9681\n",
      "Epoch 257/1500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.1012 - accuracy: 0.9723\n",
      "Epoch 258/1500\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.1212 - accuracy: 0.9612\n",
      "Epoch 259/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.1093 - accuracy: 0.9640\n",
      "Epoch 260/1500\n",
      "23/23 [==============================] - 0s 920us/step - loss: 0.1067 - accuracy: 0.9668\n",
      "Epoch 261/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.0947 - accuracy: 0.9709\n",
      "Epoch 262/1500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.0989 - accuracy: 0.9598\n",
      "Epoch 263/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1030 - accuracy: 0.9681\n",
      "Epoch 264/1500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.0784 - accuracy: 0.9792\n",
      "Epoch 265/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.0958 - accuracy: 0.9709\n",
      "Epoch 266/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9640\n",
      "Epoch 267/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9709\n",
      "Epoch 268/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.0955 - accuracy: 0.9640\n",
      "Epoch 269/1500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.0851 - accuracy: 0.9709\n",
      "Epoch 270/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.9765\n",
      "Epoch 271/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9598\n",
      "Epoch 272/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.0914 - accuracy: 0.9681\n",
      "Epoch 273/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.0928 - accuracy: 0.9723\n",
      "Epoch 274/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.0941 - accuracy: 0.9654\n",
      "Epoch 275/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.0849 - accuracy: 0.9778\n",
      "Epoch 276/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9820\n",
      "Epoch 277/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.0845 - accuracy: 0.9751\n",
      "Epoch 278/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.0861 - accuracy: 0.9681\n",
      "Epoch 279/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.0893 - accuracy: 0.9751\n",
      "Epoch 280/1500\n",
      "23/23 [==============================] - 0s 949us/step - loss: 0.0924 - accuracy: 0.9765\n",
      "Epoch 281/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9654\n",
      "Epoch 282/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.1047 - accuracy: 0.9612\n",
      "Epoch 283/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.0836 - accuracy: 0.9723\n",
      "Epoch 284/1500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.0824 - accuracy: 0.9695\n",
      "Epoch 285/1500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.1030 - accuracy: 0.9598\n",
      "Epoch 286/1500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.0956 - accuracy: 0.9668\n",
      "Epoch 287/1500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.1030 - accuracy: 0.9612\n",
      "Epoch 288/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.0910 - accuracy: 0.9695\n",
      "Epoch 289/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.1040 - accuracy: 0.9626\n",
      "Epoch 290/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.0926 - accuracy: 0.9695\n",
      "Epoch 291/1500\n",
      "23/23 [==============================] - 0s 922us/step - loss: 0.0825 - accuracy: 0.9681\n",
      "Epoch 292/1500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.0820 - accuracy: 0.9778\n",
      "Epoch 293/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.0707 - accuracy: 0.9861\n",
      "Epoch 294/1500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.1041 - accuracy: 0.9612\n",
      "Epoch 295/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.0846 - accuracy: 0.9751\n",
      "Epoch 296/1500\n",
      "23/23 [==============================] - 0s 925us/step - loss: 0.1116 - accuracy: 0.9626\n",
      "Epoch 297/1500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.0826 - accuracy: 0.9737\n",
      "Epoch 298/1500\n",
      "23/23 [==============================] - 0s 935us/step - loss: 0.0721 - accuracy: 0.9792\n",
      "Epoch 299/1500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.0789 - accuracy: 0.9709\n",
      "Epoch 300/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.0810 - accuracy: 0.9792\n",
      "Epoch 301/1500\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.0867 - accuracy: 0.9668\n",
      "Epoch 302/1500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.0838 - accuracy: 0.9681\n",
      "Epoch 303/1500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.0742 - accuracy: 0.9778\n",
      "Epoch 304/1500\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.0979 - accuracy: 0.9709\n",
      "Epoch 305/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.0979 - accuracy: 0.9654\n",
      "Epoch 306/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.0694 - accuracy: 0.9834\n",
      "Epoch 307/1500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.0681 - accuracy: 0.9834\n",
      "Epoch 308/1500\n",
      "23/23 [==============================] - 0s 895us/step - loss: 0.0888 - accuracy: 0.9723\n",
      "Epoch 309/1500\n",
      "23/23 [==============================] - 0s 935us/step - loss: 0.1043 - accuracy: 0.9695\n",
      "Epoch 310/1500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.0840 - accuracy: 0.9709\n",
      "Epoch 311/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.0755 - accuracy: 0.9778\n",
      "Epoch 312/1500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.0957 - accuracy: 0.9668\n",
      "Epoch 313/1500\n",
      "23/23 [==============================] - 0s 938us/step - loss: 0.0803 - accuracy: 0.9737\n",
      "Epoch 314/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.0779 - accuracy: 0.9737\n",
      "Epoch 315/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.0853 - accuracy: 0.9723\n",
      "Epoch 316/1500\n",
      "23/23 [==============================] - 0s 927us/step - loss: 0.0936 - accuracy: 0.9640\n",
      "Epoch 317/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.0789 - accuracy: 0.9723\n",
      "Epoch 318/1500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.0899 - accuracy: 0.9626\n",
      "Epoch 319/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9820\n",
      "Epoch 320/1500\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.0776 - accuracy: 0.9751\n",
      "Epoch 321/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0648 - accuracy: 0.9806\n",
      "Epoch 322/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.0664 - accuracy: 0.9778\n",
      "Epoch 323/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.0786 - accuracy: 0.9792\n",
      "Epoch 324/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.0582 - accuracy: 0.9889\n",
      "Epoch 325/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.0560 - accuracy: 0.9834\n",
      "Epoch 326/1500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.0784 - accuracy: 0.9751\n",
      "Epoch 327/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.0559 - accuracy: 0.9889\n",
      "Epoch 328/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.0685 - accuracy: 0.9834\n",
      "Epoch 329/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.9806\n",
      "Epoch 330/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.0656 - accuracy: 0.9820\n",
      "Epoch 331/1500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.1098 - accuracy: 0.9557\n",
      "Epoch 332/1500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.0806 - accuracy: 0.9765\n",
      "Epoch 333/1500\n",
      "23/23 [==============================] - 0s 940us/step - loss: 0.0757 - accuracy: 0.9737\n",
      "Epoch 334/1500\n",
      "23/23 [==============================] - 0s 936us/step - loss: 0.0869 - accuracy: 0.9723\n",
      "Epoch 335/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.0997 - accuracy: 0.9695\n",
      "Epoch 336/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.0992 - accuracy: 0.9681\n",
      "Epoch 337/1500\n",
      "23/23 [==============================] - 0s 919us/step - loss: 0.0757 - accuracy: 0.9792\n",
      "Epoch 338/1500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.0812 - accuracy: 0.9751\n",
      "Epoch 339/1500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.0738 - accuracy: 0.9751\n",
      "Epoch 340/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.0708 - accuracy: 0.9806\n",
      "Epoch 341/1500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0605 - accuracy: 0.9820\n",
      "Epoch 342/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0596 - accuracy: 0.9848\n",
      "Epoch 343/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.9806\n",
      "Epoch 344/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.9765\n",
      "Epoch 345/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.0789 - accuracy: 0.9765\n",
      "Epoch 346/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.9737\n",
      "Epoch 347/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.0817 - accuracy: 0.9709\n",
      "Epoch 348/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.1064 - accuracy: 0.9612\n",
      "Epoch 349/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.0702 - accuracy: 0.9806\n",
      "Epoch 350/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.0762 - accuracy: 0.9737\n",
      "Epoch 351/1500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.0797 - accuracy: 0.9737\n",
      "Epoch 352/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.0663 - accuracy: 0.9792\n",
      "Epoch 353/1500\n",
      "23/23 [==============================] - 0s 943us/step - loss: 0.0859 - accuracy: 0.9709\n",
      "Epoch 354/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.9806\n",
      "Epoch 355/1500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.0300 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 325.\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.9889\n",
      "Epoch 355: early stopping\n",
      "7/7 [==============================] - 0s 764us/step - loss: 0.9323 - accuracy: 0.7488\n",
      "7/7 [==============================] - 0s 661us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.73 (22/30)\n",
      "Before appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "After appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "Final Test Results - Loss: 0.9322829842567444, Accuracy: 0.7488372325897217, Precision: 0.7132335371424556, Recall: 0.7752217187560513, F1 Score: 0.7390159767610749\n",
      "Confusion Matrix:\n",
      " [[126   2  23]\n",
      " [  0   7   0]\n",
      " [ 29   0  28]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "001A    14\n",
      "042A    14\n",
      "059A    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "025A    11\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "005A    10\n",
      "016A    10\n",
      "014B    10\n",
      "071A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "015A     9\n",
      "045A     9\n",
      "022A     9\n",
      "065A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "050A     7\n",
      "099A     7\n",
      "109A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "070A     5\n",
      "021A     5\n",
      "075A     5\n",
      "009A     4\n",
      "104A     4\n",
      "105A     4\n",
      "003A     4\n",
      "113A     3\n",
      "056A     3\n",
      "064A     3\n",
      "060A     3\n",
      "014A     3\n",
      "006A     3\n",
      "025B     2\n",
      "102A     2\n",
      "011A     2\n",
      "061A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "018A     2\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "043A     1\n",
      "115A     1\n",
      "076A     1\n",
      "066A     1\n",
      "026C     1\n",
      "096A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "057A    27\n",
      "055A    20\n",
      "097B    14\n",
      "039A    12\n",
      "040A    10\n",
      "094A     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "023A     6\n",
      "053A     6\n",
      "108A     6\n",
      "023B     5\n",
      "026A     4\n",
      "062A     4\n",
      "052A     4\n",
      "035A     4\n",
      "058A     3\n",
      "012A     3\n",
      "069A     2\n",
      "048A     1\n",
      "088A     1\n",
      "004A     1\n",
      "073A     1\n",
      "041A     1\n",
      "092A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    306\n",
      "X    268\n",
      "F    158\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    94\n",
      "X    80\n",
      "M    31\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 106A, 104A, 059A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 097B, 062A, 039A, 023A, 027A, 069A, 026...\n",
      "kitten                                   [040A, 041A, 048A]\n",
      "senior                 [057A, 055A, 117A, 058A, 094A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 13, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 3, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '020A'\n",
      " '021A' '022A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '012A' '019B' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'026C'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to Test Set:\n",
      "{'026C'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A'\n",
      " '009A' '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A'\n",
      " '020A' '021A' '022A' '024A' '025A' '025B' '025C' '026B' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '012A' '019B' '023A' '023B' '026A' '026C' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "Length of X_train_val:\n",
      "770\n",
      "Length of y_train_val:\n",
      "770\n",
      "Length of groups_train_val:\n",
      "770\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     466\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     122\n",
      "senior     71\n",
      "kitten     12\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     504\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     84\n",
      "senior    71\n",
      "kitten    12\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 504, 1: 159, 2: 107})\n",
      "Epoch 1/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.2868 - accuracy: 0.4675\n",
      "Epoch 2/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.0124 - accuracy: 0.5584\n",
      "Epoch 3/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.9092 - accuracy: 0.6156\n",
      "Epoch 4/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.8622 - accuracy: 0.6506\n",
      "Epoch 5/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.8006 - accuracy: 0.7052\n",
      "Epoch 6/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7626 - accuracy: 0.6909\n",
      "Epoch 7/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7336 - accuracy: 0.6948\n",
      "Epoch 8/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6828 - accuracy: 0.7286\n",
      "Epoch 9/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7005 - accuracy: 0.7338\n",
      "Epoch 10/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6559 - accuracy: 0.7481\n",
      "Epoch 11/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6378 - accuracy: 0.7558\n",
      "Epoch 12/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6338 - accuracy: 0.7416\n",
      "Epoch 13/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5945 - accuracy: 0.7623\n",
      "Epoch 14/1500\n",
      "25/25 [==============================] - 0s 982us/step - loss: 0.6006 - accuracy: 0.7506\n",
      "Epoch 15/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5856 - accuracy: 0.7701\n",
      "Epoch 16/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5738 - accuracy: 0.7857\n",
      "Epoch 17/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5571 - accuracy: 0.7727\n",
      "Epoch 18/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5511 - accuracy: 0.7896\n",
      "Epoch 19/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5005 - accuracy: 0.8039\n",
      "Epoch 20/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5050 - accuracy: 0.7961\n",
      "Epoch 21/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5222 - accuracy: 0.8117\n",
      "Epoch 22/1500\n",
      "25/25 [==============================] - 0s 965us/step - loss: 0.5313 - accuracy: 0.7961\n",
      "Epoch 23/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5087 - accuracy: 0.8000\n",
      "Epoch 24/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5104 - accuracy: 0.8052\n",
      "Epoch 25/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4956 - accuracy: 0.8013\n",
      "Epoch 26/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4654 - accuracy: 0.8299\n",
      "Epoch 27/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4635 - accuracy: 0.8273\n",
      "Epoch 28/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4823 - accuracy: 0.8182\n",
      "Epoch 29/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4530 - accuracy: 0.8286\n",
      "Epoch 30/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4307 - accuracy: 0.8325\n",
      "Epoch 31/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4651 - accuracy: 0.8312\n",
      "Epoch 32/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4475 - accuracy: 0.8208\n",
      "Epoch 33/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4495 - accuracy: 0.8156\n",
      "Epoch 34/1500\n",
      "25/25 [==============================] - 0s 999us/step - loss: 0.4587 - accuracy: 0.8117\n",
      "Epoch 35/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4333 - accuracy: 0.8312\n",
      "Epoch 36/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3965 - accuracy: 0.8506\n",
      "Epoch 37/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4205 - accuracy: 0.8273\n",
      "Epoch 38/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4024 - accuracy: 0.8481\n",
      "Epoch 39/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4242 - accuracy: 0.8455\n",
      "Epoch 40/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3785 - accuracy: 0.8532\n",
      "Epoch 41/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4024 - accuracy: 0.8519\n",
      "Epoch 42/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4260 - accuracy: 0.8390\n",
      "Epoch 43/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8623\n",
      "Epoch 44/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4047 - accuracy: 0.8390\n",
      "Epoch 45/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4091 - accuracy: 0.8506\n",
      "Epoch 46/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4070 - accuracy: 0.8416\n",
      "Epoch 47/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4107 - accuracy: 0.8325\n",
      "Epoch 48/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4039 - accuracy: 0.8377\n",
      "Epoch 49/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3852 - accuracy: 0.8532\n",
      "Epoch 50/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3807 - accuracy: 0.8481\n",
      "Epoch 51/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3819 - accuracy: 0.8571\n",
      "Epoch 52/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3939 - accuracy: 0.8468\n",
      "Epoch 53/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4007 - accuracy: 0.8481\n",
      "Epoch 54/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3808 - accuracy: 0.8532\n",
      "Epoch 55/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3641 - accuracy: 0.8494\n",
      "Epoch 56/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3638 - accuracy: 0.8623\n",
      "Epoch 57/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3682 - accuracy: 0.8571\n",
      "Epoch 58/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3654 - accuracy: 0.8481\n",
      "Epoch 59/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3473 - accuracy: 0.8857\n",
      "Epoch 60/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3774 - accuracy: 0.8571\n",
      "Epoch 61/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3474 - accuracy: 0.8662\n",
      "Epoch 62/1500\n",
      "25/25 [==============================] - 0s 993us/step - loss: 0.3601 - accuracy: 0.8753\n",
      "Epoch 63/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3413 - accuracy: 0.8766\n",
      "Epoch 64/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3534 - accuracy: 0.8688\n",
      "Epoch 65/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3660 - accuracy: 0.8506\n",
      "Epoch 66/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3614 - accuracy: 0.8688\n",
      "Epoch 67/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3251 - accuracy: 0.8831\n",
      "Epoch 68/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3715 - accuracy: 0.8649\n",
      "Epoch 69/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3641 - accuracy: 0.8701\n",
      "Epoch 70/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3725 - accuracy: 0.8545\n",
      "Epoch 71/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3691 - accuracy: 0.8675\n",
      "Epoch 72/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3351 - accuracy: 0.8610\n",
      "Epoch 73/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3594 - accuracy: 0.8597\n",
      "Epoch 74/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8883\n",
      "Epoch 75/1500\n",
      "25/25 [==============================] - 0s 995us/step - loss: 0.3226 - accuracy: 0.8610\n",
      "Epoch 76/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3250 - accuracy: 0.8766\n",
      "Epoch 77/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3391 - accuracy: 0.8623\n",
      "Epoch 78/1500\n",
      "25/25 [==============================] - 0s 995us/step - loss: 0.3185 - accuracy: 0.8818\n",
      "Epoch 79/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3333 - accuracy: 0.8662\n",
      "Epoch 80/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3144 - accuracy: 0.8844\n",
      "Epoch 81/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.8831\n",
      "Epoch 82/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3231 - accuracy: 0.8779\n",
      "Epoch 83/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2989 - accuracy: 0.8948\n",
      "Epoch 84/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3110 - accuracy: 0.8740\n",
      "Epoch 85/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3408 - accuracy: 0.8610\n",
      "Epoch 86/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3027 - accuracy: 0.8844\n",
      "Epoch 87/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3332 - accuracy: 0.8688\n",
      "Epoch 88/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.9000\n",
      "Epoch 89/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3135 - accuracy: 0.8857\n",
      "Epoch 90/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3162 - accuracy: 0.8792\n",
      "Epoch 91/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2991 - accuracy: 0.8870\n",
      "Epoch 92/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3009 - accuracy: 0.8922\n",
      "Epoch 93/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3017 - accuracy: 0.8909\n",
      "Epoch 94/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3000 - accuracy: 0.8961\n",
      "Epoch 95/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3099 - accuracy: 0.8740\n",
      "Epoch 96/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3089 - accuracy: 0.8870\n",
      "Epoch 97/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2997 - accuracy: 0.8857\n",
      "Epoch 98/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3110 - accuracy: 0.8766\n",
      "Epoch 99/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2880 - accuracy: 0.8909\n",
      "Epoch 100/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2848 - accuracy: 0.8831\n",
      "Epoch 101/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2999 - accuracy: 0.8805\n",
      "Epoch 102/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3198 - accuracy: 0.8792\n",
      "Epoch 103/1500\n",
      "25/25 [==============================] - 0s 991us/step - loss: 0.2763 - accuracy: 0.9026\n",
      "Epoch 104/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2522 - accuracy: 0.9039\n",
      "Epoch 105/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.8844\n",
      "Epoch 106/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2894 - accuracy: 0.9026\n",
      "Epoch 107/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.8974\n",
      "Epoch 108/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3176 - accuracy: 0.8753\n",
      "Epoch 109/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2845 - accuracy: 0.8896\n",
      "Epoch 110/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2904 - accuracy: 0.8831\n",
      "Epoch 111/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2785 - accuracy: 0.8987\n",
      "Epoch 112/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2935 - accuracy: 0.8844\n",
      "Epoch 113/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2971 - accuracy: 0.8844\n",
      "Epoch 114/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2645 - accuracy: 0.8922\n",
      "Epoch 115/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2630 - accuracy: 0.9039\n",
      "Epoch 116/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2812 - accuracy: 0.8948\n",
      "Epoch 117/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2980 - accuracy: 0.8857\n",
      "Epoch 118/1500\n",
      "25/25 [==============================] - 0s 974us/step - loss: 0.2661 - accuracy: 0.8935\n",
      "Epoch 119/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.8987\n",
      "Epoch 120/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2781 - accuracy: 0.8935\n",
      "Epoch 121/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2741 - accuracy: 0.9039\n",
      "Epoch 122/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2552 - accuracy: 0.9091\n",
      "Epoch 123/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2859 - accuracy: 0.8935\n",
      "Epoch 124/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2716 - accuracy: 0.8922\n",
      "Epoch 125/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2918 - accuracy: 0.8948\n",
      "Epoch 126/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2723 - accuracy: 0.9104\n",
      "Epoch 127/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2578 - accuracy: 0.9052\n",
      "Epoch 128/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2840 - accuracy: 0.8974\n",
      "Epoch 129/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2682 - accuracy: 0.8987\n",
      "Epoch 130/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2914 - accuracy: 0.8948\n",
      "Epoch 131/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2753 - accuracy: 0.9013\n",
      "Epoch 132/1500\n",
      "25/25 [==============================] - 0s 998us/step - loss: 0.2589 - accuracy: 0.9000\n",
      "Epoch 133/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2659 - accuracy: 0.8974\n",
      "Epoch 134/1500\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.1359 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 104.\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.9013\n",
      "Epoch 134: early stopping\n",
      "6/6 [==============================] - 0s 862us/step - loss: 0.9690 - accuracy: 0.6287\n",
      "6/6 [==============================] - 0s 643us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "After appending - Cat IDs: 846, Predictions: 846, Actuals: 846, Gender: 846\n",
      "Final Test Results - Loss: 0.9690437316894531, Accuracy: 0.628742516040802, Precision: 0.7036458333333333, Recall: 0.6923764811088754, F1 Score: 0.6407873282087119\n",
      "Confusion Matrix:\n",
      " [[75  4  5]\n",
      " [ 1 11  0]\n",
      " [52  0 19]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6554432542799244\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8714078813791275\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.6979968249797821\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6731979712553102\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6990092113070021\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[1]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ###################\n",
    "    # PREPARING MODEL #\n",
    "    ###################\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2845ad-17c1-494a-8bd0-971aefca9f01",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3910db95-6772-4098-bef1-fb5857901e5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 846, Predictions: 846, Actuals: 846, Gender: 846\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "d543c7c2-c11b-4511-ba5a-c52969a61a62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6b6f2173-89a8-40ba-afa6-410005c2dcb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (82/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "aa8d9ba5-9d96-4aee-b3e2-ba87553d1899",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0b4ff1f7-d22d-4409-98d4-49e9a82bcb58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, senior, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, senior, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, kitten, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, senior, adult, a...         adult            adult                   True\n",
       "80    074A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, senior, senior, ...         adult            adult                   True\n",
       "70    064A                             [kitten, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, adult, senior, adult, senior, adult, ...        senior           senior                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A           [kitten, kitten, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [senior, senior, adult]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, senior, ...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, adult, ...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, senior,...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, senior, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "44    038A                                    [adult, kitten]         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, senior, senior, adult, senior, adult, ...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C              [senior, senior, adult, adult, adult]         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, senior, adult, adult, a...         adult            adult                   True\n",
       "25    023A        [adult, kitten, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                      [adult, adult, senior, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, kitten, kitten, adult, adult, k...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, se...         adult            adult                   True\n",
       "82    076A                                           [kitten]        kitten            adult                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "48    042A  [adult, kitten, kitten, adult, adult, adult, a...         adult           kitten                  False\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, s...        senior            adult                  False\n",
       "42    036A  [adult, senior, senior, adult, adult, senior, ...        senior            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "102   108A         [adult, adult, adult, adult, adult, adult]         adult           senior                  False\n",
       "101   106A  [adult, adult, senior, senior, adult, adult, a...         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "99    104A                     [adult, senior, adult, senior]         adult           senior                  False\n",
       "52    047A  [adult, adult, adult, adult, adult, kitten, ki...         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "69    063A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "33    026C                                   [kitten, kitten]        kitten            adult                  False\n",
       "12    011A                                    [senior, adult]         adult           senior                  False\n",
       "67    061A                                    [senior, adult]         adult           senior                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "61    055A  [adult, adult, senior, adult, adult, adult, ad...         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, senior, senior, senior, ...         adult           senior                  False\n",
       "65    059A  [adult, adult, adult, senior, senior, adult, a...         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, senior, senior, ...         adult           senior                  False\n",
       "109   117A  [adult, senior, senior, adult, senior, adult, ...         adult           senior                  False"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5ec47984-bc63-4f3f-a7a4-d3979dbd4c52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     64\n",
      "kitten    10\n",
      "senior     8\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "952d49d2-180d-4f36-85b0-6e2b96610559",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             64  87.671233\n",
      "1           kitten           15             10  66.666667\n",
      "2           senior           22              8  36.363636\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6a94de06-f9df-49f6-99e7-abc9024f3dc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885dd133-e156-4b01-8e84-4d94546e0eca",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "dd0127df-e0fb-4862-9ac4-2113bd346e78",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          550            439  79.818182\n",
      "1           kitten          118             79  66.949153\n",
      "2           senior          178             76  42.696629\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4a0df220-5e5c-4c46-8474-ac4ecf291071",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a7528-860f-45b0-8c00-d0ddf10a0d8f",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "2924eee7-fd8b-4cee-9e43-bb0352e4c43b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    214      157     73.36\n",
      "1          M    337      249     73.89\n",
      "2          X    295      188     63.73\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "b7b2139e-f100-44e6-aff4-807f96753499",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58f49-cac6-40b1-8422-e3d95576c453",
   "metadata": {},
   "source": [
    "# RANDOM SEED 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bf9d1b64-575e-47ca-bf37-e8916438d506",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[2]))\n",
    "np.random.seed(int(random_seeds[2]))\n",
    "tf.random.set_seed(int(random_seeds[2]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "91d5db1d-6c9f-4047-97b9-5a0eef4fbaaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "e435598c-cabc-4129-8504-68aac9cc06bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29b526-5098-4ce6-bc77-a93d1e6e1397",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "79a922e6-198c-4c43-a6f2-90dc580ae875",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "071A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "033A     9\n",
      "072A     9\n",
      "015A     9\n",
      "022A     9\n",
      "051B     9\n",
      "065A     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "108A     6\n",
      "109A     6\n",
      "037A     6\n",
      "007A     6\n",
      "008A     6\n",
      "025C     5\n",
      "070A     5\n",
      "021A     5\n",
      "034A     5\n",
      "075A     5\n",
      "023B     5\n",
      "035A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "058A     3\n",
      "014A     3\n",
      "011A     2\n",
      "061A     2\n",
      "032A     2\n",
      "093A     2\n",
      "025B     2\n",
      "087A     2\n",
      "069A     2\n",
      "073A     1\n",
      "115A     1\n",
      "088A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "019B     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "005A    10\n",
      "045A     9\n",
      "117A     7\n",
      "053A     6\n",
      "023A     6\n",
      "044A     5\n",
      "105A     4\n",
      "052A     4\n",
      "104A     4\n",
      "018A     2\n",
      "054A     2\n",
      "038A     2\n",
      "102A     2\n",
      "091A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    265\n",
      "M    256\n",
      "F    198\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    83\n",
      "M    81\n",
      "F    54\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 103A, 071A, 028A, 067...\n",
      "kitten    [014B, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [001A, 097B, 019A, 074A, 029A, 005A, 091A, 023...\n",
      "kitten                             [044A, 111A, 045A, 110A]\n",
      "senior                             [104A, 054A, 117A, 051A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "Length of X_train_val:\n",
      "719\n",
      "Length of y_train_val:\n",
      "719\n",
      "Length of groups_train_val:\n",
      "719\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 423, 2: 153, 1: 143})\n",
      "Epoch 1/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.1719 - accuracy: 0.4687\n",
      "Epoch 2/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.9354 - accuracy: 0.5994\n",
      "Epoch 3/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8605 - accuracy: 0.6426\n",
      "Epoch 4/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7626 - accuracy: 0.7065\n",
      "Epoch 5/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7365 - accuracy: 0.7149\n",
      "Epoch 6/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7045 - accuracy: 0.7232\n",
      "Epoch 7/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6546 - accuracy: 0.7524\n",
      "Epoch 8/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6608 - accuracy: 0.7385\n",
      "Epoch 9/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6057 - accuracy: 0.7622\n",
      "Epoch 10/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5980 - accuracy: 0.7552\n",
      "Epoch 11/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5244 - accuracy: 0.7858\n",
      "Epoch 12/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5667 - accuracy: 0.7858\n",
      "Epoch 13/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5456 - accuracy: 0.7872\n",
      "Epoch 14/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5239 - accuracy: 0.7942\n",
      "Epoch 15/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5282 - accuracy: 0.8025\n",
      "Epoch 16/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5060 - accuracy: 0.8025\n",
      "Epoch 17/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4992 - accuracy: 0.8081\n",
      "Epoch 18/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.5089 - accuracy: 0.8122\n",
      "Epoch 19/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4771 - accuracy: 0.8234\n",
      "Epoch 20/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4874 - accuracy: 0.8136\n",
      "Epoch 21/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4735 - accuracy: 0.8095\n",
      "Epoch 22/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4602 - accuracy: 0.8234\n",
      "Epoch 23/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4172 - accuracy: 0.8401\n",
      "Epoch 24/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4084 - accuracy: 0.8554\n",
      "Epoch 25/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3948 - accuracy: 0.8498\n",
      "Epoch 26/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4436 - accuracy: 0.8289\n",
      "Epoch 27/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4069 - accuracy: 0.8484\n",
      "Epoch 28/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4023 - accuracy: 0.8428\n",
      "Epoch 29/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8637\n",
      "Epoch 30/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3614 - accuracy: 0.8554\n",
      "Epoch 31/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4235 - accuracy: 0.8401\n",
      "Epoch 32/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3948 - accuracy: 0.8554\n",
      "Epoch 33/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3874 - accuracy: 0.8595\n",
      "Epoch 34/1500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3863 - accuracy: 0.8512\n",
      "Epoch 35/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.3409 - accuracy: 0.8651\n",
      "Epoch 36/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8512\n",
      "Epoch 37/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3701 - accuracy: 0.8623\n",
      "Epoch 38/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3386 - accuracy: 0.8707\n",
      "Epoch 39/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3453 - accuracy: 0.8595\n",
      "Epoch 40/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3660 - accuracy: 0.8595\n",
      "Epoch 41/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3528 - accuracy: 0.8832\n",
      "Epoch 42/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.8567\n",
      "Epoch 43/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3540 - accuracy: 0.8609\n",
      "Epoch 44/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3284 - accuracy: 0.8790\n",
      "Epoch 45/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.8832\n",
      "Epoch 46/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2976 - accuracy: 0.8860\n",
      "Epoch 47/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3130 - accuracy: 0.8846\n",
      "Epoch 48/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3236 - accuracy: 0.8720\n",
      "Epoch 49/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2905 - accuracy: 0.8901\n",
      "Epoch 50/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2694 - accuracy: 0.9026\n",
      "Epoch 51/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3047 - accuracy: 0.8804\n",
      "Epoch 52/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2916 - accuracy: 0.9152\n",
      "Epoch 53/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2976 - accuracy: 0.8873\n",
      "Epoch 54/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2951 - accuracy: 0.8887\n",
      "Epoch 55/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2934 - accuracy: 0.8943\n",
      "Epoch 56/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2790 - accuracy: 0.8915\n",
      "Epoch 57/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2871 - accuracy: 0.8971\n",
      "Epoch 58/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9068\n",
      "Epoch 59/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2571 - accuracy: 0.8999\n",
      "Epoch 60/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2834 - accuracy: 0.8943\n",
      "Epoch 61/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2826 - accuracy: 0.8943\n",
      "Epoch 62/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2999 - accuracy: 0.8804\n",
      "Epoch 63/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3067 - accuracy: 0.8762\n",
      "Epoch 64/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2691 - accuracy: 0.9040\n",
      "Epoch 65/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2685 - accuracy: 0.8999\n",
      "Epoch 66/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2800 - accuracy: 0.8887\n",
      "Epoch 67/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2962 - accuracy: 0.8929\n",
      "Epoch 68/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.9054\n",
      "Epoch 69/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3017 - accuracy: 0.8804\n",
      "Epoch 70/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2560 - accuracy: 0.9068\n",
      "Epoch 71/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2484 - accuracy: 0.9096\n",
      "Epoch 72/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3031 - accuracy: 0.8846\n",
      "Epoch 73/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2496 - accuracy: 0.9026\n",
      "Epoch 74/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.9054\n",
      "Epoch 75/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2417 - accuracy: 0.9013\n",
      "Epoch 76/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2345 - accuracy: 0.9026\n",
      "Epoch 77/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2548 - accuracy: 0.8971\n",
      "Epoch 78/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2256 - accuracy: 0.9124\n",
      "Epoch 79/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2351 - accuracy: 0.9138\n",
      "Epoch 80/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2252 - accuracy: 0.9305\n",
      "Epoch 81/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2359 - accuracy: 0.9138\n",
      "Epoch 82/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2549 - accuracy: 0.9138\n",
      "Epoch 83/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2270 - accuracy: 0.9179\n",
      "Epoch 84/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2304 - accuracy: 0.9207\n",
      "Epoch 85/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9138\n",
      "Epoch 86/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2442 - accuracy: 0.9068\n",
      "Epoch 87/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2440 - accuracy: 0.9040\n",
      "Epoch 88/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9346\n",
      "Epoch 89/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2222 - accuracy: 0.9110\n",
      "Epoch 90/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2175 - accuracy: 0.9193\n",
      "Epoch 91/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2527 - accuracy: 0.8901\n",
      "Epoch 92/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2153 - accuracy: 0.9221\n",
      "Epoch 93/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9332\n",
      "Epoch 94/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2057 - accuracy: 0.9207\n",
      "Epoch 95/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2052 - accuracy: 0.9263\n",
      "Epoch 96/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1920 - accuracy: 0.9318\n",
      "Epoch 97/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9207\n",
      "Epoch 98/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2165 - accuracy: 0.9138\n",
      "Epoch 99/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2315 - accuracy: 0.9110\n",
      "Epoch 100/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9179\n",
      "Epoch 101/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2285 - accuracy: 0.9068\n",
      "Epoch 102/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1947 - accuracy: 0.9305\n",
      "Epoch 103/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1901 - accuracy: 0.9291\n",
      "Epoch 104/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9277\n",
      "Epoch 105/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9318\n",
      "Epoch 106/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9374\n",
      "Epoch 107/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9388\n",
      "Epoch 108/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9305\n",
      "Epoch 109/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.9305\n",
      "Epoch 110/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9193\n",
      "Epoch 111/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1640 - accuracy: 0.9430\n",
      "Epoch 112/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1695 - accuracy: 0.9360\n",
      "Epoch 113/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9332\n",
      "Epoch 114/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9263\n",
      "Epoch 115/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9360\n",
      "Epoch 116/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1826 - accuracy: 0.9388\n",
      "Epoch 117/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.9291\n",
      "Epoch 118/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1903 - accuracy: 0.9430\n",
      "Epoch 119/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1693 - accuracy: 0.9388\n",
      "Epoch 120/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1909 - accuracy: 0.9318\n",
      "Epoch 121/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1678 - accuracy: 0.9430\n",
      "Epoch 122/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.9291\n",
      "Epoch 123/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1871 - accuracy: 0.9263\n",
      "Epoch 124/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1871 - accuracy: 0.9332\n",
      "Epoch 125/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.9374\n",
      "Epoch 126/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9332\n",
      "Epoch 127/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9458\n",
      "Epoch 128/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9471\n",
      "Epoch 129/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9513\n",
      "Epoch 130/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9471\n",
      "Epoch 131/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9374\n",
      "Epoch 132/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9318\n",
      "Epoch 133/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9569\n",
      "Epoch 134/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1659 - accuracy: 0.9346\n",
      "Epoch 135/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9346\n",
      "Epoch 136/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9485\n",
      "Epoch 137/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1640 - accuracy: 0.9374\n",
      "Epoch 138/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1634 - accuracy: 0.9513\n",
      "Epoch 139/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9332\n",
      "Epoch 140/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9499\n",
      "Epoch 141/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1742 - accuracy: 0.9291\n",
      "Epoch 142/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9388\n",
      "Epoch 143/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9430\n",
      "Epoch 144/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9332\n",
      "Epoch 145/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9402\n",
      "Epoch 146/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9416\n",
      "Epoch 147/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1380 - accuracy: 0.9597\n",
      "Epoch 148/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9499\n",
      "Epoch 149/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9499\n",
      "Epoch 150/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9444\n",
      "Epoch 151/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1535 - accuracy: 0.9416\n",
      "Epoch 152/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9569\n",
      "Epoch 153/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9569\n",
      "Epoch 154/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9471\n",
      "Epoch 155/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9332\n",
      "Epoch 156/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9583\n",
      "Epoch 157/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9430\n",
      "Epoch 158/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9569\n",
      "Epoch 159/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9416\n",
      "Epoch 160/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9430\n",
      "Epoch 161/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9513\n",
      "Epoch 162/1500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1420 - accuracy: 0.9499\n",
      "Epoch 163/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1505 - accuracy: 0.9444\n",
      "Epoch 164/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9416\n",
      "Epoch 165/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9611\n",
      "Epoch 166/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9583\n",
      "Epoch 167/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1375 - accuracy: 0.9555\n",
      "Epoch 168/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9583\n",
      "Epoch 169/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9624\n",
      "Epoch 170/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9569\n",
      "Epoch 171/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9458\n",
      "Epoch 172/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.9402\n",
      "Epoch 173/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9471\n",
      "Epoch 174/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9499\n",
      "Epoch 175/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9638\n",
      "Epoch 176/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9485\n",
      "Epoch 177/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9569\n",
      "Epoch 178/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9611\n",
      "Epoch 179/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9555\n",
      "Epoch 180/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9499\n",
      "Epoch 181/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1584 - accuracy: 0.9346\n",
      "Epoch 182/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9527\n",
      "Epoch 183/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9611\n",
      "Epoch 184/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9555\n",
      "Epoch 185/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9597\n",
      "Epoch 186/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9624\n",
      "Epoch 187/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9416\n",
      "Epoch 188/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9555\n",
      "Epoch 189/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9430\n",
      "Epoch 190/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9583\n",
      "Epoch 191/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9583\n",
      "Epoch 192/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9611\n",
      "Epoch 193/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9513\n",
      "Epoch 194/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9583\n",
      "Epoch 195/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9694\n",
      "Epoch 196/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9583\n",
      "Epoch 197/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9708\n",
      "Epoch 198/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9624\n",
      "Epoch 199/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9666\n",
      "Epoch 200/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9583\n",
      "Epoch 201/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.9374\n",
      "Epoch 202/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9652\n",
      "Epoch 203/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9638\n",
      "Epoch 204/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9555\n",
      "Epoch 205/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1135 - accuracy: 0.9638\n",
      "Epoch 206/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9638\n",
      "Epoch 207/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9513\n",
      "Epoch 208/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9638\n",
      "Epoch 209/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9624\n",
      "Epoch 210/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9597\n",
      "Epoch 211/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9583\n",
      "Epoch 212/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9750\n",
      "Epoch 213/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9583\n",
      "Epoch 214/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.9708\n",
      "Epoch 215/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9652\n",
      "Epoch 216/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9694\n",
      "Epoch 217/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9624\n",
      "Epoch 218/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9611\n",
      "Epoch 219/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9694\n",
      "Epoch 220/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9471\n",
      "Epoch 221/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9624\n",
      "Epoch 222/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9666\n",
      "Epoch 223/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9680\n",
      "Epoch 224/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9736\n",
      "Epoch 225/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9597\n",
      "Epoch 226/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9652\n",
      "Epoch 227/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9750\n",
      "Epoch 228/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9777\n",
      "Epoch 229/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9666\n",
      "Epoch 230/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9694\n",
      "Epoch 231/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9652\n",
      "Epoch 232/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9638\n",
      "Epoch 233/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9597\n",
      "Epoch 234/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9569\n",
      "Epoch 235/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.9722\n",
      "Epoch 236/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9750\n",
      "Epoch 237/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9652\n",
      "Epoch 238/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9583\n",
      "Epoch 239/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9680\n",
      "Epoch 240/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9638\n",
      "Epoch 241/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9680\n",
      "Epoch 242/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9666\n",
      "Epoch 243/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9722\n",
      "Epoch 244/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9638\n",
      "Epoch 245/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9666\n",
      "Epoch 246/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0994 - accuracy: 0.9666\n",
      "Epoch 247/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9638\n",
      "Epoch 248/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.9722\n",
      "Epoch 249/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9708\n",
      "Epoch 250/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.9833\n",
      "Epoch 251/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.9652\n",
      "Epoch 252/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.9708\n",
      "Epoch 253/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9652\n",
      "Epoch 254/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9638\n",
      "Epoch 255/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9764\n",
      "Epoch 256/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9708\n",
      "Epoch 257/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9652\n",
      "Epoch 258/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9638\n",
      "Epoch 259/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.9819\n",
      "Epoch 260/1500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1089 - accuracy: 0.9680\n",
      "Epoch 261/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9541\n",
      "Epoch 262/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1115 - accuracy: 0.9652\n",
      "Epoch 263/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9652\n",
      "Epoch 264/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9722\n",
      "Epoch 265/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9764\n",
      "Epoch 266/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9680\n",
      "Epoch 267/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.9722\n",
      "Epoch 268/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9805\n",
      "Epoch 269/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9750\n",
      "Epoch 270/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9847\n",
      "Epoch 271/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9694\n",
      "Epoch 272/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9652\n",
      "Epoch 273/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9666\n",
      "Epoch 274/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9708\n",
      "Epoch 275/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9680\n",
      "Epoch 276/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9750\n",
      "Epoch 277/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9791\n",
      "Epoch 278/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9736\n",
      "Epoch 279/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.9764\n",
      "Epoch 280/1500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.1867 - accuracy: 0.9062Restoring model weights from the end of the best epoch: 250.\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.9708\n",
      "Epoch 280: early stopping\n",
      "7/7 [==============================] - 0s 904us/step - loss: 0.8593 - accuracy: 0.7661\n",
      "7/7 [==============================] - 0s 629us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (18/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "Final Test Results - Loss: 0.8592875599861145, Accuracy: 0.7660550475120544, Precision: 0.6784090909090909, Recall: 0.7301298701298702, F1 Score: 0.6931097558017107\n",
      "Confusion Matrix:\n",
      " [[129   4  32]\n",
      " [  2  26   0]\n",
      " [ 13   0  12]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "072A     9\n",
      "033A     9\n",
      "045A     9\n",
      "022A     9\n",
      "094A     8\n",
      "013B     8\n",
      "010A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "007A     6\n",
      "037A     6\n",
      "053A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "021A     5\n",
      "035A     4\n",
      "003A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "009A     4\n",
      "105A     4\n",
      "058A     3\n",
      "064A     3\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "014A     3\n",
      "093A     2\n",
      "025B     2\n",
      "038A     2\n",
      "087A     2\n",
      "102A     2\n",
      "032A     2\n",
      "054A     2\n",
      "018A     2\n",
      "115A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "110A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "088A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "042A    14\n",
      "106A    14\n",
      "116A    12\n",
      "039A    12\n",
      "063A    11\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "014B    10\n",
      "065A     9\n",
      "015A     9\n",
      "051B     9\n",
      "095A     8\n",
      "070A     5\n",
      "075A     5\n",
      "023B     5\n",
      "062A     4\n",
      "060A     3\n",
      "011A     2\n",
      "069A     2\n",
      "061A     2\n",
      "043A     1\n",
      "092A     1\n",
      "076A     1\n",
      "004A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    241\n",
      "F    181\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "X    75\n",
      "F    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 001A, 103A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 111A, 046A, 047A, 109A, 050A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 104A, 055A, 059A, 113A, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 071A, 062A, 002B, 095A, 065A, 039...\n",
      "kitten                             [014B, 040A, 042A, 043A]\n",
      "senior                 [106A, 116A, 051B, 016A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 12, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 4, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '018A' '019A' '020A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '004A' '011A' '014B' '015A' '016A' '019B' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'026B'}\n",
      "Moved to Test Set:\n",
      "{'026B'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '018A' '019A' '020A' '021A' '022A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '011A' '014B' '015A' '016A' '019B' '023B' '026B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "Length of X_train_val:\n",
      "733\n",
      "Length of y_train_val:\n",
      "733\n",
      "Length of groups_train_val:\n",
      "733\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     430\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     158\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     468\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     120\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 468, 1: 136, 2: 129})\n",
      "Epoch 1/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.2090 - accuracy: 0.5007\n",
      "Epoch 2/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.9844 - accuracy: 0.5866\n",
      "Epoch 3/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.9069 - accuracy: 0.6057\n",
      "Epoch 4/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8373 - accuracy: 0.6589\n",
      "Epoch 5/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7854 - accuracy: 0.6685\n",
      "Epoch 6/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7634 - accuracy: 0.6876\n",
      "Epoch 7/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6967 - accuracy: 0.7203\n",
      "Epoch 8/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7010 - accuracy: 0.7244\n",
      "Epoch 9/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6744 - accuracy: 0.7162\n",
      "Epoch 10/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6332 - accuracy: 0.7599\n",
      "Epoch 11/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6081 - accuracy: 0.7544\n",
      "Epoch 12/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5769 - accuracy: 0.7572\n",
      "Epoch 13/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5478 - accuracy: 0.7749\n",
      "Epoch 14/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5680 - accuracy: 0.7844\n",
      "Epoch 15/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5398 - accuracy: 0.7735\n",
      "Epoch 16/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5184 - accuracy: 0.7981\n",
      "Epoch 17/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5002 - accuracy: 0.7913\n",
      "Epoch 18/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4808 - accuracy: 0.8022\n",
      "Epoch 19/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4130 - accuracy: 0.8417\n",
      "Epoch 20/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4701 - accuracy: 0.7844\n",
      "Epoch 21/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4482 - accuracy: 0.8117\n",
      "Epoch 22/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8322\n",
      "Epoch 23/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4380 - accuracy: 0.8267\n",
      "Epoch 24/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4836 - accuracy: 0.8131\n",
      "Epoch 25/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4110 - accuracy: 0.8417\n",
      "Epoch 26/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4106 - accuracy: 0.8308\n",
      "Epoch 27/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3987 - accuracy: 0.8363\n",
      "Epoch 28/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8472\n",
      "Epoch 29/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3977 - accuracy: 0.8336\n",
      "Epoch 30/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3819 - accuracy: 0.8527\n",
      "Epoch 31/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3822 - accuracy: 0.8595\n",
      "Epoch 32/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3679 - accuracy: 0.8581\n",
      "Epoch 33/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3547 - accuracy: 0.8581\n",
      "Epoch 34/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3562 - accuracy: 0.8663\n",
      "Epoch 35/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3791 - accuracy: 0.8540\n",
      "Epoch 36/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3277 - accuracy: 0.8813\n",
      "Epoch 37/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3618 - accuracy: 0.8540\n",
      "Epoch 38/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3268 - accuracy: 0.8663\n",
      "Epoch 39/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3505 - accuracy: 0.8622\n",
      "Epoch 40/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3095 - accuracy: 0.8827\n",
      "Epoch 41/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.8731\n",
      "Epoch 42/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3389 - accuracy: 0.8677\n",
      "Epoch 43/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3094 - accuracy: 0.8718\n",
      "Epoch 44/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3295 - accuracy: 0.8622\n",
      "Epoch 45/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8622\n",
      "Epoch 46/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.8649\n",
      "Epoch 47/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3073 - accuracy: 0.8881\n",
      "Epoch 48/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2698 - accuracy: 0.9045\n",
      "Epoch 49/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2917 - accuracy: 0.8731\n",
      "Epoch 50/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3224 - accuracy: 0.8690\n",
      "Epoch 51/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3111 - accuracy: 0.8827\n",
      "Epoch 52/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2893 - accuracy: 0.8909\n",
      "Epoch 53/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3152 - accuracy: 0.8813\n",
      "Epoch 54/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2958 - accuracy: 0.8799\n",
      "Epoch 55/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2924 - accuracy: 0.8786\n",
      "Epoch 56/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2752 - accuracy: 0.9018\n",
      "Epoch 57/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2515 - accuracy: 0.9072\n",
      "Epoch 58/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2748 - accuracy: 0.8895\n",
      "Epoch 59/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2865 - accuracy: 0.9004\n",
      "Epoch 60/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2609 - accuracy: 0.8950\n",
      "Epoch 61/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.9086\n",
      "Epoch 62/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.8963\n",
      "Epoch 63/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2366 - accuracy: 0.9086\n",
      "Epoch 64/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2463 - accuracy: 0.8963\n",
      "Epoch 65/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2521 - accuracy: 0.9059\n",
      "Epoch 66/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2587 - accuracy: 0.9031\n",
      "Epoch 67/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2549 - accuracy: 0.9018\n",
      "Epoch 68/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2199 - accuracy: 0.9154\n",
      "Epoch 69/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2379 - accuracy: 0.9072\n",
      "Epoch 70/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2442 - accuracy: 0.9072\n",
      "Epoch 71/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9236\n",
      "Epoch 72/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2287 - accuracy: 0.9168\n",
      "Epoch 73/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2256 - accuracy: 0.9209\n",
      "Epoch 74/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.8990\n",
      "Epoch 75/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.9045\n",
      "Epoch 76/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.9004\n",
      "Epoch 77/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9086\n",
      "Epoch 78/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9291\n",
      "Epoch 79/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.9154\n",
      "Epoch 80/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2196 - accuracy: 0.9250\n",
      "Epoch 81/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.9181\n",
      "Epoch 82/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2104 - accuracy: 0.9209\n",
      "Epoch 83/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2068 - accuracy: 0.9209\n",
      "Epoch 84/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9372\n",
      "Epoch 85/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2160 - accuracy: 0.9127\n",
      "Epoch 86/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9100\n",
      "Epoch 87/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2131 - accuracy: 0.9181\n",
      "Epoch 88/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1989 - accuracy: 0.9181\n",
      "Epoch 89/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2210 - accuracy: 0.9291\n",
      "Epoch 90/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2129 - accuracy: 0.9168\n",
      "Epoch 91/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2019 - accuracy: 0.9291\n",
      "Epoch 92/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2216 - accuracy: 0.9154\n",
      "Epoch 93/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.9168\n",
      "Epoch 94/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2345 - accuracy: 0.9127\n",
      "Epoch 95/1500\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2266 - accuracy: 0.9141\n",
      "Epoch 96/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9222\n",
      "Epoch 97/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2086 - accuracy: 0.9263\n",
      "Epoch 98/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1893 - accuracy: 0.9263\n",
      "Epoch 99/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.9400\n",
      "Epoch 100/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1917 - accuracy: 0.9209\n",
      "Epoch 101/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9523\n",
      "Epoch 102/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9222\n",
      "Epoch 103/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1893 - accuracy: 0.9250\n",
      "Epoch 104/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9277\n",
      "Epoch 105/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.9400\n",
      "Epoch 106/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9222\n",
      "Epoch 107/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.9441\n",
      "Epoch 108/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.9304\n",
      "Epoch 109/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1738 - accuracy: 0.9427\n",
      "Epoch 110/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.9304\n",
      "Epoch 111/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.9359\n",
      "Epoch 112/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9236\n",
      "Epoch 113/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1738 - accuracy: 0.9386\n",
      "Epoch 114/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9250\n",
      "Epoch 115/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1785 - accuracy: 0.9359\n",
      "Epoch 116/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1944 - accuracy: 0.9277\n",
      "Epoch 117/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9482\n",
      "Epoch 118/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9454\n",
      "Epoch 119/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9468\n",
      "Epoch 120/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9454\n",
      "Epoch 121/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9427\n",
      "Epoch 122/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1913 - accuracy: 0.9195\n",
      "Epoch 123/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1926 - accuracy: 0.9181\n",
      "Epoch 124/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9482\n",
      "Epoch 125/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9427\n",
      "Epoch 126/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9468\n",
      "Epoch 127/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9454\n",
      "Epoch 128/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9495\n",
      "Epoch 129/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9413\n",
      "Epoch 130/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9454\n",
      "Epoch 131/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9509\n",
      "Epoch 132/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9495\n",
      "Epoch 133/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.9291\n",
      "Epoch 134/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1477 - accuracy: 0.9454\n",
      "Epoch 135/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9536\n",
      "Epoch 136/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9536\n",
      "Epoch 137/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9400\n",
      "Epoch 138/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.9509\n",
      "Epoch 139/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9413\n",
      "Epoch 140/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9495\n",
      "Epoch 141/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9468\n",
      "Epoch 142/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9591\n",
      "Epoch 143/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9495\n",
      "Epoch 144/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9577\n",
      "Epoch 145/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9482\n",
      "Epoch 146/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9550\n",
      "Epoch 147/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.9659\n",
      "Epoch 148/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9441\n",
      "Epoch 149/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9509\n",
      "Epoch 150/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9495\n",
      "Epoch 151/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9536\n",
      "Epoch 152/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9591\n",
      "Epoch 153/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9523\n",
      "Epoch 154/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9577\n",
      "Epoch 155/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9550\n",
      "Epoch 156/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9618\n",
      "Epoch 157/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1521 - accuracy: 0.9345\n",
      "Epoch 158/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9454\n",
      "Epoch 159/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9482\n",
      "Epoch 160/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9550\n",
      "Epoch 161/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9536\n",
      "Epoch 162/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9577\n",
      "Epoch 163/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9563\n",
      "Epoch 164/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9468\n",
      "Epoch 165/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9591\n",
      "Epoch 166/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9632\n",
      "Epoch 167/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9632\n",
      "Epoch 168/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9591\n",
      "Epoch 169/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9523\n",
      "Epoch 170/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9618\n",
      "Epoch 171/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9468\n",
      "Epoch 172/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9563\n",
      "Epoch 173/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9604\n",
      "Epoch 174/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9741\n",
      "Epoch 175/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9604\n",
      "Epoch 176/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9673\n",
      "Epoch 177/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9591\n",
      "Epoch 178/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9591\n",
      "Epoch 179/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9727\n",
      "Epoch 180/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9550\n",
      "Epoch 181/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9645\n",
      "Epoch 182/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9632\n",
      "Epoch 183/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9550\n",
      "Epoch 184/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9632\n",
      "Epoch 185/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9700\n",
      "Epoch 186/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9659\n",
      "Epoch 187/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9659\n",
      "Epoch 188/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9618\n",
      "Epoch 189/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9686\n",
      "Epoch 190/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9604\n",
      "Epoch 191/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9591\n",
      "Epoch 192/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9563\n",
      "Epoch 193/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9523\n",
      "Epoch 194/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9645\n",
      "Epoch 195/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9686\n",
      "Epoch 196/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9591\n",
      "Epoch 197/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9604\n",
      "Epoch 198/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9741\n",
      "Epoch 199/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9700\n",
      "Epoch 200/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9604\n",
      "Epoch 201/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9714\n",
      "Epoch 202/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9563\n",
      "Epoch 203/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9591\n",
      "Epoch 204/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9591\n",
      "Epoch 205/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9768\n",
      "Epoch 206/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9632\n",
      "Epoch 207/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9673\n",
      "Epoch 208/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9604\n",
      "Epoch 209/1500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.0969 - accuracy: 0.9673\n",
      "Epoch 210/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1343 - accuracy: 0.9563\n",
      "Epoch 211/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1320 - accuracy: 0.9536\n",
      "Epoch 212/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9632\n",
      "Epoch 213/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.9754\n",
      "Epoch 214/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9659\n",
      "Epoch 215/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9618\n",
      "Epoch 216/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9618\n",
      "Epoch 217/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9754\n",
      "Epoch 218/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9754\n",
      "Epoch 219/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9768\n",
      "Epoch 220/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9700\n",
      "Epoch 221/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9659\n",
      "Epoch 222/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9727\n",
      "Epoch 223/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9645\n",
      "Epoch 224/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9741\n",
      "Epoch 225/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9618\n",
      "Epoch 226/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.9714\n",
      "Epoch 227/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9714\n",
      "Epoch 228/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.1017 - accuracy: 0.9591\n",
      "Epoch 229/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9645\n",
      "Epoch 230/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9741\n",
      "Epoch 231/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.9836\n",
      "Epoch 232/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.9795\n",
      "Epoch 233/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9714\n",
      "Epoch 234/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.9768\n",
      "Epoch 235/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.9795\n",
      "Epoch 236/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9768\n",
      "Epoch 237/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9795\n",
      "Epoch 238/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9741\n",
      "Epoch 239/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9754\n",
      "Epoch 240/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9618\n",
      "Epoch 241/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9741\n",
      "Epoch 242/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9782\n",
      "Epoch 243/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.9877\n",
      "Epoch 244/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.9673\n",
      "Epoch 245/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9823\n",
      "Epoch 246/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.9782\n",
      "Epoch 247/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.9659\n",
      "Epoch 248/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9850\n",
      "Epoch 249/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9686\n",
      "Epoch 250/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9591\n",
      "Epoch 251/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9700\n",
      "Epoch 252/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.9659\n",
      "Epoch 253/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9754\n",
      "Epoch 254/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.0706 - accuracy: 0.9795\n",
      "Epoch 255/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9727\n",
      "Epoch 256/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9604\n",
      "Epoch 257/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9659\n",
      "Epoch 258/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.9836\n",
      "Epoch 259/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9714\n",
      "Epoch 260/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9754\n",
      "Epoch 261/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9714\n",
      "Epoch 262/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.9782\n",
      "Epoch 263/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9714\n",
      "Epoch 264/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.9850\n",
      "Epoch 265/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9714\n",
      "Epoch 266/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9714\n",
      "Epoch 267/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9714\n",
      "Epoch 268/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.9686\n",
      "Epoch 269/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9741\n",
      "Epoch 270/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.9727\n",
      "Epoch 271/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.9782\n",
      "Epoch 272/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9795\n",
      "Epoch 273/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9700\n",
      "Epoch 274/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9836\n",
      "Epoch 275/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.9877\n",
      "Epoch 276/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9782\n",
      "Epoch 277/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9768\n",
      "Epoch 278/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.9809\n",
      "Epoch 279/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.9700\n",
      "Epoch 280/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9727\n",
      "Epoch 281/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9618\n",
      "Epoch 282/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9877\n",
      "Epoch 283/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9782\n",
      "Epoch 284/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.9809\n",
      "Epoch 285/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.9823\n",
      "Epoch 286/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.9795\n",
      "Epoch 287/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.9836\n",
      "Epoch 288/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0705 - accuracy: 0.9768\n",
      "Epoch 289/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.9823\n",
      "Epoch 290/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.9795\n",
      "Epoch 291/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.9768\n",
      "Epoch 292/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9727\n",
      "Epoch 293/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9741\n",
      "Epoch 294/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.9809\n",
      "Epoch 295/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.9823\n",
      "Epoch 296/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9700\n",
      "Epoch 297/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.9823\n",
      "Epoch 298/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.9782\n",
      "Epoch 299/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.9727\n",
      "Epoch 300/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.9836\n",
      "Epoch 301/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0585 - accuracy: 0.9836\n",
      "Epoch 302/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0542 - accuracy: 0.9836\n",
      "Epoch 303/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.9741\n",
      "Epoch 304/1500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0860 - accuracy: 0.9714\n",
      "Epoch 305/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0631 - accuracy: 0.9782\n",
      "Epoch 306/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9754\n",
      "Epoch 307/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9741\n",
      "Epoch 308/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.9836\n",
      "Epoch 309/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0509 - accuracy: 0.9864\n",
      "Epoch 310/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9727\n",
      "Epoch 311/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9795\n",
      "Epoch 312/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9836\n",
      "Epoch 313/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0502 - accuracy: 0.9864\n",
      "Epoch 314/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9782\n",
      "Epoch 315/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.9768\n",
      "Epoch 316/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0498 - accuracy: 0.9905\n",
      "Epoch 317/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0484 - accuracy: 0.9864\n",
      "Epoch 318/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0526 - accuracy: 0.9877\n",
      "Epoch 319/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0537 - accuracy: 0.9809\n",
      "Epoch 320/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9768\n",
      "Epoch 321/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9754\n",
      "Epoch 322/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9782\n",
      "Epoch 323/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0537 - accuracy: 0.9864\n",
      "Epoch 324/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9823\n",
      "Epoch 325/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9850\n",
      "Epoch 326/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0555 - accuracy: 0.9768\n",
      "Epoch 327/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9741\n",
      "Epoch 328/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9809\n",
      "Epoch 329/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0441 - accuracy: 0.9905\n",
      "Epoch 330/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9823\n",
      "Epoch 331/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0586 - accuracy: 0.9877\n",
      "Epoch 332/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.0557 - accuracy: 0.9864\n",
      "Epoch 333/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.9754\n",
      "Epoch 334/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9768\n",
      "Epoch 335/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0582 - accuracy: 0.9836\n",
      "Epoch 336/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0558 - accuracy: 0.9864\n",
      "Epoch 337/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9782\n",
      "Epoch 338/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0485 - accuracy: 0.9891\n",
      "Epoch 339/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0501 - accuracy: 0.9850\n",
      "Epoch 340/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.9823\n",
      "Epoch 341/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0474 - accuracy: 0.9918\n",
      "Epoch 342/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0464 - accuracy: 0.9836\n",
      "Epoch 343/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0568 - accuracy: 0.9850\n",
      "Epoch 344/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9768\n",
      "Epoch 345/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0589 - accuracy: 0.9850\n",
      "Epoch 346/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0591 - accuracy: 0.9823\n",
      "Epoch 347/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0562 - accuracy: 0.9864\n",
      "Epoch 348/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9864\n",
      "Epoch 349/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0558 - accuracy: 0.9864\n",
      "Epoch 350/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0551 - accuracy: 0.9850\n",
      "Epoch 351/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0485 - accuracy: 0.9905\n",
      "Epoch 352/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0575 - accuracy: 0.9795\n",
      "Epoch 353/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0446 - accuracy: 0.9864\n",
      "Epoch 354/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0420 - accuracy: 0.9877\n",
      "Epoch 355/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0493 - accuracy: 0.9864\n",
      "Epoch 356/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0454 - accuracy: 0.9877\n",
      "Epoch 357/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9809\n",
      "Epoch 358/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0554 - accuracy: 0.9823\n",
      "Epoch 359/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9795\n",
      "Epoch 360/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0490 - accuracy: 0.9877\n",
      "Epoch 361/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0452 - accuracy: 0.9905\n",
      "Epoch 362/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0530 - accuracy: 0.9795\n",
      "Epoch 363/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0480 - accuracy: 0.9850\n",
      "Epoch 364/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9741\n",
      "Epoch 365/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9850\n",
      "Epoch 366/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.9809\n",
      "Epoch 367/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0590 - accuracy: 0.9809\n",
      "Epoch 368/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0561 - accuracy: 0.9809\n",
      "Epoch 369/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0449 - accuracy: 0.9905\n",
      "Epoch 370/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0577 - accuracy: 0.9795\n",
      "Epoch 371/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0626 - accuracy: 0.9782\n",
      "Epoch 372/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0460 - accuracy: 0.9918\n",
      "Epoch 373/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9795\n",
      "Epoch 374/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0581 - accuracy: 0.9823\n",
      "Epoch 375/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.0585 - accuracy: 0.9836\n",
      "Epoch 376/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.9809\n",
      "Epoch 377/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0457 - accuracy: 0.9809\n",
      "Epoch 378/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0486 - accuracy: 0.9918\n",
      "Epoch 379/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0385 - accuracy: 0.9945\n",
      "Epoch 380/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.0420 - accuracy: 0.9850\n",
      "Epoch 381/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0556 - accuracy: 0.9795\n",
      "Epoch 382/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0501 - accuracy: 0.9864\n",
      "Epoch 383/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9741\n",
      "Epoch 384/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0470 - accuracy: 0.9877\n",
      "Epoch 385/1500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.0476 - accuracy: 0.9850\n",
      "Epoch 386/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0611 - accuracy: 0.9768\n",
      "Epoch 387/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.9768\n",
      "Epoch 388/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.9836\n",
      "Epoch 389/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0430 - accuracy: 0.9932\n",
      "Epoch 390/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0572 - accuracy: 0.9795\n",
      "Epoch 391/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0488 - accuracy: 0.9877\n",
      "Epoch 392/1500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.0419 - accuracy: 0.9850\n",
      "Epoch 393/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9727\n",
      "Epoch 394/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0509 - accuracy: 0.9809\n",
      "Epoch 395/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0447 - accuracy: 0.9850\n",
      "Epoch 396/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0384 - accuracy: 0.9918\n",
      "Epoch 397/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0470 - accuracy: 0.9836\n",
      "Epoch 398/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0422 - accuracy: 0.9864\n",
      "Epoch 399/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0408 - accuracy: 0.9918\n",
      "Epoch 400/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.9782\n",
      "Epoch 401/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0411 - accuracy: 0.9905\n",
      "Epoch 402/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0506 - accuracy: 0.9823\n",
      "Epoch 403/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0453 - accuracy: 0.9918\n",
      "Epoch 404/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0434 - accuracy: 0.9905\n",
      "Epoch 405/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0368 - accuracy: 0.9932\n",
      "Epoch 406/1500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.0449 - accuracy: 0.9905\n",
      "Epoch 407/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0476 - accuracy: 0.9850\n",
      "Epoch 408/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.0413 - accuracy: 0.9877\n",
      "Epoch 409/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0438 - accuracy: 0.9877\n",
      "Epoch 410/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0512 - accuracy: 0.9836\n",
      "Epoch 411/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0382 - accuracy: 0.9932\n",
      "Epoch 412/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0443 - accuracy: 0.9891\n",
      "Epoch 413/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9795\n",
      "Epoch 414/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0432 - accuracy: 0.9891\n",
      "Epoch 415/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0434 - accuracy: 0.9877\n",
      "Epoch 416/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0454 - accuracy: 0.9877\n",
      "Epoch 417/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0387 - accuracy: 0.9918\n",
      "Epoch 418/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0303 - accuracy: 0.9918\n",
      "Epoch 419/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0394 - accuracy: 0.9918\n",
      "Epoch 420/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0425 - accuracy: 0.9891\n",
      "Epoch 421/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9741\n",
      "Epoch 422/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0391 - accuracy: 0.9850\n",
      "Epoch 423/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0374 - accuracy: 0.9918\n",
      "Epoch 424/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0433 - accuracy: 0.9932\n",
      "Epoch 425/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.9877\n",
      "Epoch 426/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0445 - accuracy: 0.9836\n",
      "Epoch 427/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0473 - accuracy: 0.9823\n",
      "Epoch 428/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0528 - accuracy: 0.9864\n",
      "Epoch 429/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0603 - accuracy: 0.9795\n",
      "Epoch 430/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0449 - accuracy: 0.9891\n",
      "Epoch 431/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0487 - accuracy: 0.9877\n",
      "Epoch 432/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0473 - accuracy: 0.9850\n",
      "Epoch 433/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.9809\n",
      "Epoch 434/1500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.0471 - accuracy: 0.9864\n",
      "Epoch 435/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0370 - accuracy: 0.9932\n",
      "Epoch 436/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.0492 - accuracy: 0.9877\n",
      "Epoch 437/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.0474 - accuracy: 0.9850\n",
      "Epoch 438/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0401 - accuracy: 0.9891\n",
      "Epoch 439/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0365 - accuracy: 0.9959\n",
      "Epoch 440/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0431 - accuracy: 0.9864\n",
      "Epoch 441/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0461 - accuracy: 0.9877\n",
      "Epoch 442/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0371 - accuracy: 0.9932\n",
      "Epoch 443/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0454 - accuracy: 0.9864\n",
      "Epoch 444/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0623 - accuracy: 0.9809\n",
      "Epoch 445/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0390 - accuracy: 0.9864\n",
      "Epoch 446/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0477 - accuracy: 0.9891\n",
      "Epoch 447/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0415 - accuracy: 0.9864\n",
      "Epoch 448/1500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.0212 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 418.\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0432 - accuracy: 0.9836\n",
      "Epoch 448: early stopping\n",
      "7/7 [==============================] - 0s 827us/step - loss: 1.0506 - accuracy: 0.7108\n",
      "7/7 [==============================] - 0s 679us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.64 (18/28)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "After appending - Cat IDs: 422, Predictions: 422, Actuals: 422, Gender: 422\n",
      "Final Test Results - Loss: 1.050641655921936, Accuracy: 0.7107843160629272, Precision: 0.6933407433407434, Recall: 0.680328798185941, F1 Score: 0.6836347975882858\n",
      "Confusion Matrix:\n",
      " [[97  4 19]\n",
      " [ 3 31  1]\n",
      " [32  0 17]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "042A    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "040A    10\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "015A     9\n",
      "045A     9\n",
      "095A     8\n",
      "117A     7\n",
      "099A     7\n",
      "031A     7\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "012A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "060A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "038A     2\n",
      "093A     2\n",
      "018A     2\n",
      "054A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "091A     1\n",
      "019B     1\n",
      "092A     1\n",
      "004A     1\n",
      "049A     1\n",
      "076A     1\n",
      "043A     1\n",
      "026C     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "020A    23\n",
      "067A    19\n",
      "097A    16\n",
      "059A    14\n",
      "022A     9\n",
      "072A     9\n",
      "010A     8\n",
      "094A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "037A     6\n",
      "109A     6\n",
      "008A     6\n",
      "009A     4\n",
      "003A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "025B     2\n",
      "087A     2\n",
      "032A     2\n",
      "066A     1\n",
      "048A     1\n",
      "041A     1\n",
      "115A     1\n",
      "096A     1\n",
      "100A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    243\n",
      "X    229\n",
      "F    226\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    119\n",
      "M     94\n",
      "F     26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 043A, 049...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [067A, 020A, 022A, 072A, 009A, 027A, 013B, 014...\n",
      "kitten                 [046A, 109A, 050A, 041A, 048A, 115A]\n",
      "senior                       [097A, 059A, 058A, 094A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '044A' '045A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '046A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'042A'}\n",
      "Moved to Test Set:\n",
      "{'042A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '043A' '044A' '045A' '046A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '042A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "Length of X_train_val:\n",
      "747\n",
      "Length of y_train_val:\n",
      "747\n",
      "Length of groups_train_val:\n",
      "747\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     470\n",
      "senior    136\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     118\n",
      "kitten     79\n",
      "senior     42\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     470\n",
      "kitten    141\n",
      "senior    136\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     118\n",
      "senior     42\n",
      "kitten     30\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 470, 1: 141, 2: 136})\n",
      "Epoch 1/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 1.2592 - accuracy: 0.4632\n",
      "Epoch 2/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.9675 - accuracy: 0.5823\n",
      "Epoch 3/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8870 - accuracy: 0.6439\n",
      "Epoch 4/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.8278 - accuracy: 0.6600\n",
      "Epoch 5/1500\n",
      "24/24 [==============================] - 0s 985us/step - loss: 0.7893 - accuracy: 0.6760\n",
      "Epoch 6/1500\n",
      "24/24 [==============================] - 0s 986us/step - loss: 0.7841 - accuracy: 0.6667\n",
      "Epoch 7/1500\n",
      "24/24 [==============================] - 0s 981us/step - loss: 0.7026 - accuracy: 0.7095\n",
      "Epoch 8/1500\n",
      "24/24 [==============================] - 0s 962us/step - loss: 0.6598 - accuracy: 0.7390\n",
      "Epoch 9/1500\n",
      "24/24 [==============================] - 0s 946us/step - loss: 0.6670 - accuracy: 0.7296\n",
      "Epoch 10/1500\n",
      "24/24 [==============================] - 0s 979us/step - loss: 0.6174 - accuracy: 0.7550\n",
      "Epoch 11/1500\n",
      "24/24 [==============================] - 0s 953us/step - loss: 0.6069 - accuracy: 0.7363\n",
      "Epoch 12/1500\n",
      "24/24 [==============================] - 0s 7ms/step - loss: 0.5802 - accuracy: 0.7483\n",
      "Epoch 13/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5894 - accuracy: 0.7537\n",
      "Epoch 14/1500\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.6021 - accuracy: 0.7483\n",
      "Epoch 15/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.6085 - accuracy: 0.7497\n",
      "Epoch 16/1500\n",
      "24/24 [==============================] - 0s 945us/step - loss: 0.5482 - accuracy: 0.7671\n",
      "Epoch 17/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5535 - accuracy: 0.7764\n",
      "Epoch 18/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5114 - accuracy: 0.7738\n",
      "Epoch 19/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.5204 - accuracy: 0.7778\n",
      "Epoch 20/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4932 - accuracy: 0.7979\n",
      "Epoch 21/1500\n",
      "24/24 [==============================] - 0s 991us/step - loss: 0.4564 - accuracy: 0.8260\n",
      "Epoch 22/1500\n",
      "24/24 [==============================] - 0s 943us/step - loss: 0.4942 - accuracy: 0.7871\n",
      "Epoch 23/1500\n",
      "24/24 [==============================] - 0s 975us/step - loss: 0.4591 - accuracy: 0.8153\n",
      "Epoch 24/1500\n",
      "24/24 [==============================] - 0s 967us/step - loss: 0.4684 - accuracy: 0.8072\n",
      "Epoch 25/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4486 - accuracy: 0.8233\n",
      "Epoch 26/1500\n",
      "24/24 [==============================] - 0s 979us/step - loss: 0.4519 - accuracy: 0.8059\n",
      "Epoch 27/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4402 - accuracy: 0.8206\n",
      "Epoch 28/1500\n",
      "24/24 [==============================] - 0s 946us/step - loss: 0.4432 - accuracy: 0.8086\n",
      "Epoch 29/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4322 - accuracy: 0.8193\n",
      "Epoch 30/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.4062 - accuracy: 0.8340\n",
      "Epoch 31/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3969 - accuracy: 0.8394\n",
      "Epoch 32/1500\n",
      "24/24 [==============================] - 0s 935us/step - loss: 0.4328 - accuracy: 0.8126\n",
      "Epoch 33/1500\n",
      "24/24 [==============================] - 0s 943us/step - loss: 0.4165 - accuracy: 0.8461\n",
      "Epoch 34/1500\n",
      "24/24 [==============================] - 0s 965us/step - loss: 0.4329 - accuracy: 0.8233\n",
      "Epoch 35/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3839 - accuracy: 0.8461\n",
      "Epoch 36/1500\n",
      "24/24 [==============================] - 0s 991us/step - loss: 0.4052 - accuracy: 0.8394\n",
      "Epoch 37/1500\n",
      "24/24 [==============================] - 0s 960us/step - loss: 0.3874 - accuracy: 0.8461\n",
      "Epoch 38/1500\n",
      "24/24 [==============================] - 0s 977us/step - loss: 0.3750 - accuracy: 0.8527\n",
      "Epoch 39/1500\n",
      "24/24 [==============================] - 0s 956us/step - loss: 0.3808 - accuracy: 0.8407\n",
      "Epoch 40/1500\n",
      "24/24 [==============================] - 0s 946us/step - loss: 0.3792 - accuracy: 0.8461\n",
      "Epoch 41/1500\n",
      "24/24 [==============================] - 0s 970us/step - loss: 0.4101 - accuracy: 0.8327\n",
      "Epoch 42/1500\n",
      "24/24 [==============================] - 0s 976us/step - loss: 0.3698 - accuracy: 0.8594\n",
      "Epoch 43/1500\n",
      "24/24 [==============================] - 0s 986us/step - loss: 0.3536 - accuracy: 0.8635\n",
      "Epoch 44/1500\n",
      "24/24 [==============================] - 0s 946us/step - loss: 0.3728 - accuracy: 0.8407\n",
      "Epoch 45/1500\n",
      "24/24 [==============================] - 0s 999us/step - loss: 0.3404 - accuracy: 0.8568\n",
      "Epoch 46/1500\n",
      "24/24 [==============================] - 0s 956us/step - loss: 0.3513 - accuracy: 0.8568\n",
      "Epoch 47/1500\n",
      "24/24 [==============================] - 0s 924us/step - loss: 0.3969 - accuracy: 0.8353\n",
      "Epoch 48/1500\n",
      "24/24 [==============================] - 0s 955us/step - loss: 0.3817 - accuracy: 0.8447\n",
      "Epoch 49/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.8581\n",
      "Epoch 50/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3584 - accuracy: 0.8474\n",
      "Epoch 51/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3540 - accuracy: 0.8594\n",
      "Epoch 52/1500\n",
      "24/24 [==============================] - 0s 922us/step - loss: 0.3446 - accuracy: 0.8608\n",
      "Epoch 53/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3568 - accuracy: 0.8487\n",
      "Epoch 54/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3633 - accuracy: 0.8501\n",
      "Epoch 55/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3574 - accuracy: 0.8541\n",
      "Epoch 56/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3384 - accuracy: 0.8661\n",
      "Epoch 57/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.8554\n",
      "Epoch 58/1500\n",
      "24/24 [==============================] - 0s 963us/step - loss: 0.3494 - accuracy: 0.8581\n",
      "Epoch 59/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3067 - accuracy: 0.8768\n",
      "Epoch 60/1500\n",
      "24/24 [==============================] - 0s 980us/step - loss: 0.3100 - accuracy: 0.8822\n",
      "Epoch 61/1500\n",
      "24/24 [==============================] - 0s 982us/step - loss: 0.3517 - accuracy: 0.8635\n",
      "Epoch 62/1500\n",
      "24/24 [==============================] - 0s 973us/step - loss: 0.3389 - accuracy: 0.8608\n",
      "Epoch 63/1500\n",
      "24/24 [==============================] - 0s 965us/step - loss: 0.3134 - accuracy: 0.8768\n",
      "Epoch 64/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2761 - accuracy: 0.8835\n",
      "Epoch 65/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3100 - accuracy: 0.8795\n",
      "Epoch 66/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3129 - accuracy: 0.8768\n",
      "Epoch 67/1500\n",
      "24/24 [==============================] - 0s 985us/step - loss: 0.3257 - accuracy: 0.8728\n",
      "Epoch 68/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3100 - accuracy: 0.8768\n",
      "Epoch 69/1500\n",
      "24/24 [==============================] - 0s 980us/step - loss: 0.3117 - accuracy: 0.8822\n",
      "Epoch 70/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3033 - accuracy: 0.8768\n",
      "Epoch 71/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2943 - accuracy: 0.8876\n",
      "Epoch 72/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2954 - accuracy: 0.8701\n",
      "Epoch 73/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2787 - accuracy: 0.8942\n",
      "Epoch 74/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.8822\n",
      "Epoch 75/1500\n",
      "24/24 [==============================] - 0s 985us/step - loss: 0.2902 - accuracy: 0.8956\n",
      "Epoch 76/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2672 - accuracy: 0.9050\n",
      "Epoch 77/1500\n",
      "24/24 [==============================] - 0s 942us/step - loss: 0.2894 - accuracy: 0.8835\n",
      "Epoch 78/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2832 - accuracy: 0.8916\n",
      "Epoch 79/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2985 - accuracy: 0.8795\n",
      "Epoch 80/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2920 - accuracy: 0.8889\n",
      "Epoch 81/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2741 - accuracy: 0.8902\n",
      "Epoch 82/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2765 - accuracy: 0.8862\n",
      "Epoch 83/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2799 - accuracy: 0.8902\n",
      "Epoch 84/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2840 - accuracy: 0.8862\n",
      "Epoch 85/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2650 - accuracy: 0.8956\n",
      "Epoch 86/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.8956\n",
      "Epoch 87/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.9063\n",
      "Epoch 88/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2448 - accuracy: 0.9143\n",
      "Epoch 89/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2565 - accuracy: 0.8942\n",
      "Epoch 90/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2623 - accuracy: 0.8969\n",
      "Epoch 91/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2453 - accuracy: 0.9063\n",
      "Epoch 92/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.9090\n",
      "Epoch 93/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2539 - accuracy: 0.9157\n",
      "Epoch 94/1500\n",
      "24/24 [==============================] - 0s 985us/step - loss: 0.2284 - accuracy: 0.9237\n",
      "Epoch 95/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2352 - accuracy: 0.9130\n",
      "Epoch 96/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2320 - accuracy: 0.9023\n",
      "Epoch 97/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2434 - accuracy: 0.9143\n",
      "Epoch 98/1500\n",
      "24/24 [==============================] - 0s 972us/step - loss: 0.2506 - accuracy: 0.9090\n",
      "Epoch 99/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2368 - accuracy: 0.9170\n",
      "Epoch 100/1500\n",
      "24/24 [==============================] - 0s 942us/step - loss: 0.2664 - accuracy: 0.8969\n",
      "Epoch 101/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2791 - accuracy: 0.8902\n",
      "Epoch 102/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.9036\n",
      "Epoch 103/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2383 - accuracy: 0.9090\n",
      "Epoch 104/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2562 - accuracy: 0.8969\n",
      "Epoch 105/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2512 - accuracy: 0.9009\n",
      "Epoch 106/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2297 - accuracy: 0.9197\n",
      "Epoch 107/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9183\n",
      "Epoch 108/1500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.2299 - accuracy: 0.9170\n",
      "Epoch 109/1500\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.2149 - accuracy: 0.9197\n",
      "Epoch 110/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1952 - accuracy: 0.9317\n",
      "Epoch 111/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2011 - accuracy: 0.9290\n",
      "Epoch 112/1500\n",
      "24/24 [==============================] - 0s 989us/step - loss: 0.2312 - accuracy: 0.9036\n",
      "Epoch 113/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2381 - accuracy: 0.9170\n",
      "Epoch 114/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2257 - accuracy: 0.9224\n",
      "Epoch 115/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.9023\n",
      "Epoch 116/1500\n",
      "24/24 [==============================] - 0s 977us/step - loss: 0.2125 - accuracy: 0.9250\n",
      "Epoch 117/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2322 - accuracy: 0.9130\n",
      "Epoch 118/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.9076\n",
      "Epoch 119/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2175 - accuracy: 0.9157\n",
      "Epoch 120/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2202 - accuracy: 0.9210\n",
      "Epoch 121/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9076\n",
      "Epoch 122/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2461 - accuracy: 0.8983\n",
      "Epoch 123/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2291 - accuracy: 0.9063\n",
      "Epoch 124/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2209 - accuracy: 0.9103\n",
      "Epoch 125/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2154 - accuracy: 0.9210\n",
      "Epoch 126/1500\n",
      "24/24 [==============================] - 0s 992us/step - loss: 0.2106 - accuracy: 0.9157\n",
      "Epoch 127/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9197\n",
      "Epoch 128/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.9424\n",
      "Epoch 129/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9157\n",
      "Epoch 130/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2359 - accuracy: 0.9076\n",
      "Epoch 131/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2050 - accuracy: 0.9317\n",
      "Epoch 132/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2226 - accuracy: 0.9023\n",
      "Epoch 133/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9224\n",
      "Epoch 134/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1933 - accuracy: 0.9317\n",
      "Epoch 135/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9250\n",
      "Epoch 136/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.9290\n",
      "Epoch 137/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9090\n",
      "Epoch 138/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.9304\n",
      "Epoch 139/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.9371\n",
      "Epoch 140/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2032 - accuracy: 0.9224\n",
      "Epoch 141/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9424\n",
      "Epoch 142/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2179 - accuracy: 0.9210\n",
      "Epoch 143/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9371\n",
      "Epoch 144/1500\n",
      "24/24 [==============================] - 0s 997us/step - loss: 0.2099 - accuracy: 0.9224\n",
      "Epoch 145/1500\n",
      "24/24 [==============================] - 0s 991us/step - loss: 0.1899 - accuracy: 0.9331\n",
      "Epoch 146/1500\n",
      "24/24 [==============================] - 0s 978us/step - loss: 0.1962 - accuracy: 0.9277\n",
      "Epoch 147/1500\n",
      "24/24 [==============================] - 0s 944us/step - loss: 0.1802 - accuracy: 0.9384\n",
      "Epoch 148/1500\n",
      "24/24 [==============================] - 0s 979us/step - loss: 0.1721 - accuracy: 0.9331\n",
      "Epoch 149/1500\n",
      "24/24 [==============================] - 0s 990us/step - loss: 0.1863 - accuracy: 0.9250\n",
      "Epoch 150/1500\n",
      "24/24 [==============================] - 0s 997us/step - loss: 0.1864 - accuracy: 0.9438\n",
      "Epoch 151/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9250\n",
      "Epoch 152/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.9357\n",
      "Epoch 153/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.9317\n",
      "Epoch 154/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1738 - accuracy: 0.9331\n",
      "Epoch 155/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.9411\n",
      "Epoch 156/1500\n",
      "24/24 [==============================] - 0s 991us/step - loss: 0.1711 - accuracy: 0.9344\n",
      "Epoch 157/1500\n",
      "24/24 [==============================] - 0s 998us/step - loss: 0.1862 - accuracy: 0.9224\n",
      "Epoch 158/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9277\n",
      "Epoch 159/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9290\n",
      "Epoch 160/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9277\n",
      "Epoch 161/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2183 - accuracy: 0.9090\n",
      "Epoch 162/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1954 - accuracy: 0.9304\n",
      "Epoch 163/1500\n",
      "24/24 [==============================] - 0s 964us/step - loss: 0.1725 - accuracy: 0.9317\n",
      "Epoch 164/1500\n",
      "24/24 [==============================] - 0s 976us/step - loss: 0.1609 - accuracy: 0.9478\n",
      "Epoch 165/1500\n",
      "24/24 [==============================] - 0s 920us/step - loss: 0.1599 - accuracy: 0.9384\n",
      "Epoch 166/1500\n",
      "24/24 [==============================] - 0s 975us/step - loss: 0.1575 - accuracy: 0.9451\n",
      "Epoch 167/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1862 - accuracy: 0.9183\n",
      "Epoch 168/1500\n",
      "24/24 [==============================] - 0s 928us/step - loss: 0.1683 - accuracy: 0.9384\n",
      "Epoch 169/1500\n",
      "24/24 [==============================] - 0s 991us/step - loss: 0.1755 - accuracy: 0.9384\n",
      "Epoch 170/1500\n",
      "24/24 [==============================] - 0s 930us/step - loss: 0.1546 - accuracy: 0.9465\n",
      "Epoch 171/1500\n",
      "24/24 [==============================] - 0s 998us/step - loss: 0.1524 - accuracy: 0.9371\n",
      "Epoch 172/1500\n",
      "24/24 [==============================] - 0s 953us/step - loss: 0.1767 - accuracy: 0.9344\n",
      "Epoch 173/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9398\n",
      "Epoch 174/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.9290\n",
      "Epoch 175/1500\n",
      "24/24 [==============================] - 0s 981us/step - loss: 0.1614 - accuracy: 0.9451\n",
      "Epoch 176/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9465\n",
      "Epoch 177/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1721 - accuracy: 0.9264\n",
      "Epoch 178/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9438\n",
      "Epoch 179/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9451\n",
      "Epoch 180/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9424\n",
      "Epoch 181/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9451\n",
      "Epoch 182/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.9384\n",
      "Epoch 183/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9277\n",
      "Epoch 184/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9371\n",
      "Epoch 185/1500\n",
      "24/24 [==============================] - 0s 973us/step - loss: 0.1897 - accuracy: 0.9264\n",
      "Epoch 186/1500\n",
      "24/24 [==============================] - 0s 946us/step - loss: 0.1542 - accuracy: 0.9384\n",
      "Epoch 187/1500\n",
      "24/24 [==============================] - 0s 933us/step - loss: 0.1737 - accuracy: 0.9357\n",
      "Epoch 188/1500\n",
      "24/24 [==============================] - 0s 968us/step - loss: 0.1767 - accuracy: 0.9357\n",
      "Epoch 189/1500\n",
      "24/24 [==============================] - 0s 987us/step - loss: 0.1689 - accuracy: 0.9398\n",
      "Epoch 190/1500\n",
      "24/24 [==============================] - 0s 955us/step - loss: 0.1695 - accuracy: 0.9331\n",
      "Epoch 191/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9398\n",
      "Epoch 192/1500\n",
      "24/24 [==============================] - 0s 980us/step - loss: 0.1491 - accuracy: 0.9465\n",
      "Epoch 193/1500\n",
      "24/24 [==============================] - 0s 993us/step - loss: 0.1344 - accuracy: 0.9572\n",
      "Epoch 194/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1575 - accuracy: 0.9478\n",
      "Epoch 195/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9531\n",
      "Epoch 196/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9478\n",
      "Epoch 197/1500\n",
      "24/24 [==============================] - 0s 994us/step - loss: 0.1558 - accuracy: 0.9424\n",
      "Epoch 198/1500\n",
      "24/24 [==============================] - 0s 975us/step - loss: 0.1393 - accuracy: 0.9478\n",
      "Epoch 199/1500\n",
      "24/24 [==============================] - 0s 996us/step - loss: 0.1753 - accuracy: 0.9224\n",
      "Epoch 200/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.9371\n",
      "Epoch 201/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9572\n",
      "Epoch 202/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9331\n",
      "Epoch 203/1500\n",
      "24/24 [==============================] - 0s 979us/step - loss: 0.1672 - accuracy: 0.9424\n",
      "Epoch 204/1500\n",
      "24/24 [==============================] - 0s 991us/step - loss: 0.1621 - accuracy: 0.9438\n",
      "Epoch 205/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9652\n",
      "Epoch 206/1500\n",
      "24/24 [==============================] - 0s 977us/step - loss: 0.1394 - accuracy: 0.9531\n",
      "Epoch 207/1500\n",
      "24/24 [==============================] - 0s 963us/step - loss: 0.1433 - accuracy: 0.9465\n",
      "Epoch 208/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9518\n",
      "Epoch 209/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9558\n",
      "Epoch 210/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9598\n",
      "Epoch 211/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9625\n",
      "Epoch 212/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9585\n",
      "Epoch 213/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9585\n",
      "Epoch 214/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9545\n",
      "Epoch 215/1500\n",
      "24/24 [==============================] - 0s 8ms/step - loss: 0.1429 - accuracy: 0.9465\n",
      "Epoch 216/1500\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.1355 - accuracy: 0.9545\n",
      "Epoch 217/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9531\n",
      "Epoch 218/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.9398\n",
      "Epoch 219/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9491\n",
      "Epoch 220/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9612\n",
      "Epoch 221/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9598\n",
      "Epoch 222/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9478\n",
      "Epoch 223/1500\n",
      "24/24 [==============================] - 0s 968us/step - loss: 0.1314 - accuracy: 0.9545\n",
      "Epoch 224/1500\n",
      "24/24 [==============================] - 0s 959us/step - loss: 0.1441 - accuracy: 0.9491\n",
      "Epoch 225/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1689 - accuracy: 0.9451\n",
      "Epoch 226/1500\n",
      "24/24 [==============================] - 0s 948us/step - loss: 0.1184 - accuracy: 0.9531\n",
      "Epoch 227/1500\n",
      "24/24 [==============================] - 0s 939us/step - loss: 0.1246 - accuracy: 0.9545\n",
      "Epoch 228/1500\n",
      "24/24 [==============================] - 0s 933us/step - loss: 0.1313 - accuracy: 0.9518\n",
      "Epoch 229/1500\n",
      "24/24 [==============================] - 0s 965us/step - loss: 0.1243 - accuracy: 0.9491\n",
      "Epoch 230/1500\n",
      "24/24 [==============================] - 0s 961us/step - loss: 0.1416 - accuracy: 0.9438\n",
      "Epoch 231/1500\n",
      "24/24 [==============================] - 0s 980us/step - loss: 0.1261 - accuracy: 0.9558\n",
      "Epoch 232/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9625\n",
      "Epoch 233/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9572\n",
      "Epoch 234/1500\n",
      "24/24 [==============================] - 0s 974us/step - loss: 0.1300 - accuracy: 0.9518\n",
      "Epoch 235/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.9465\n",
      "Epoch 236/1500\n",
      "24/24 [==============================] - 0s 996us/step - loss: 0.1400 - accuracy: 0.9545\n",
      "Epoch 237/1500\n",
      "24/24 [==============================] - 0s 987us/step - loss: 0.1275 - accuracy: 0.9505\n",
      "Epoch 238/1500\n",
      "24/24 [==============================] - 0s 999us/step - loss: 0.1156 - accuracy: 0.9625\n",
      "Epoch 239/1500\n",
      "24/24 [==============================] - 0s 962us/step - loss: 0.1285 - accuracy: 0.9558\n",
      "Epoch 240/1500\n",
      "24/24 [==============================] - 0s 989us/step - loss: 0.1379 - accuracy: 0.9451\n",
      "Epoch 241/1500\n",
      "24/24 [==============================] - 0s 968us/step - loss: 0.1437 - accuracy: 0.9438\n",
      "Epoch 242/1500\n",
      "24/24 [==============================] - 0s 925us/step - loss: 0.1564 - accuracy: 0.9411\n",
      "Epoch 243/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9478\n",
      "Epoch 244/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9612\n",
      "Epoch 245/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9585\n",
      "Epoch 246/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9625\n",
      "Epoch 247/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9585\n",
      "Epoch 248/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9598\n",
      "Epoch 249/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9625\n",
      "Epoch 250/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1497 - accuracy: 0.9424\n",
      "Epoch 251/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9531\n",
      "Epoch 252/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9585\n",
      "Epoch 253/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9572\n",
      "Epoch 254/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9531\n",
      "Epoch 255/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9518\n",
      "Epoch 256/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9531\n",
      "Epoch 257/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9558\n",
      "Epoch 258/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.9625\n",
      "Epoch 259/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9545\n",
      "Epoch 260/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9585\n",
      "Epoch 261/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1351 - accuracy: 0.9518\n",
      "Epoch 262/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9679\n",
      "Epoch 263/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9612\n",
      "Epoch 264/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9572\n",
      "Epoch 265/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9692\n",
      "Epoch 266/1500\n",
      "24/24 [==============================] - 0s 986us/step - loss: 0.1136 - accuracy: 0.9625\n",
      "Epoch 267/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9679\n",
      "Epoch 268/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9639\n",
      "Epoch 269/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9625\n",
      "Epoch 270/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9545\n",
      "Epoch 271/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.9451\n",
      "Epoch 272/1500\n",
      "24/24 [==============================] - 0s 988us/step - loss: 0.1172 - accuracy: 0.9598\n",
      "Epoch 273/1500\n",
      "24/24 [==============================] - 0s 948us/step - loss: 0.1392 - accuracy: 0.9451\n",
      "Epoch 274/1500\n",
      "24/24 [==============================] - 0s 986us/step - loss: 0.1113 - accuracy: 0.9531\n",
      "Epoch 275/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9625\n",
      "Epoch 276/1500\n",
      "24/24 [==============================] - 0s 966us/step - loss: 0.1419 - accuracy: 0.9505\n",
      "Epoch 277/1500\n",
      "24/24 [==============================] - 0s 978us/step - loss: 0.1100 - accuracy: 0.9612\n",
      "Epoch 278/1500\n",
      "24/24 [==============================] - 0s 963us/step - loss: 0.1119 - accuracy: 0.9558\n",
      "Epoch 279/1500\n",
      "24/24 [==============================] - 0s 969us/step - loss: 0.0942 - accuracy: 0.9732\n",
      "Epoch 280/1500\n",
      "24/24 [==============================] - 0s 955us/step - loss: 0.1146 - accuracy: 0.9531\n",
      "Epoch 281/1500\n",
      "24/24 [==============================] - 0s 963us/step - loss: 0.1093 - accuracy: 0.9598\n",
      "Epoch 282/1500\n",
      "24/24 [==============================] - 0s 968us/step - loss: 0.1004 - accuracy: 0.9665\n",
      "Epoch 283/1500\n",
      "24/24 [==============================] - 0s 971us/step - loss: 0.0888 - accuracy: 0.9786\n",
      "Epoch 284/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9639\n",
      "Epoch 285/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9679\n",
      "Epoch 286/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9679\n",
      "Epoch 287/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9612\n",
      "Epoch 288/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9612\n",
      "Epoch 289/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9652\n",
      "Epoch 290/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9639\n",
      "Epoch 291/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9612\n",
      "Epoch 292/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9853\n",
      "Epoch 293/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9639\n",
      "Epoch 294/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9692\n",
      "Epoch 295/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9572\n",
      "Epoch 296/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9585\n",
      "Epoch 297/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.9813\n",
      "Epoch 298/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9719\n",
      "Epoch 299/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9679\n",
      "Epoch 300/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9652\n",
      "Epoch 301/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9746\n",
      "Epoch 302/1500\n",
      "24/24 [==============================] - 0s 973us/step - loss: 0.1361 - accuracy: 0.9585\n",
      "Epoch 303/1500\n",
      "24/24 [==============================] - 0s 992us/step - loss: 0.1053 - accuracy: 0.9679\n",
      "Epoch 304/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9719\n",
      "Epoch 305/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0977 - accuracy: 0.9746\n",
      "Epoch 306/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9692\n",
      "Epoch 307/1500\n",
      "24/24 [==============================] - 0s 5ms/step - loss: 0.1122 - accuracy: 0.9639\n",
      "Epoch 308/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9652\n",
      "Epoch 309/1500\n",
      "24/24 [==============================] - 0s 2ms/step - loss: 0.0794 - accuracy: 0.9772\n",
      "Epoch 310/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9692\n",
      "Epoch 311/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9625\n",
      "Epoch 312/1500\n",
      "24/24 [==============================] - 0s 931us/step - loss: 0.0936 - accuracy: 0.9692\n",
      "Epoch 313/1500\n",
      "24/24 [==============================] - 0s 964us/step - loss: 0.0926 - accuracy: 0.9705\n",
      "Epoch 314/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9665\n",
      "Epoch 315/1500\n",
      "24/24 [==============================] - 0s 969us/step - loss: 0.0970 - accuracy: 0.9692\n",
      "Epoch 316/1500\n",
      "24/24 [==============================] - 0s 999us/step - loss: 0.0827 - accuracy: 0.9799\n",
      "Epoch 317/1500\n",
      "24/24 [==============================] - 0s 936us/step - loss: 0.0832 - accuracy: 0.9746\n",
      "Epoch 318/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9558\n",
      "Epoch 319/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9545\n",
      "Epoch 320/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9679\n",
      "Epoch 321/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9598\n",
      "Epoch 322/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9665\n",
      "Epoch 323/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9625\n",
      "Epoch 324/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9719\n",
      "Epoch 325/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9732\n",
      "Epoch 326/1500\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9692\n",
      "Epoch 327/1500\n",
      " 1/24 [>.............................] - ETA: 0s - loss: 0.0177 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 297.\n",
      "24/24 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9705\n",
      "Epoch 327: early stopping\n",
      "6/6 [==============================] - 0s 962us/step - loss: 0.5328 - accuracy: 0.7947\n",
      "6/6 [==============================] - 0s 716us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (24/30)\n",
      "Before appending - Cat IDs: 422, Predictions: 422, Actuals: 422, Gender: 422\n",
      "After appending - Cat IDs: 612, Predictions: 612, Actuals: 612, Gender: 612\n",
      "Final Test Results - Loss: 0.5327648520469666, Accuracy: 0.7947368621826172, Precision: 0.8388381668456857, Recall: 0.6938122141511972, F1 Score: 0.7321617730732073\n",
      "Confusion Matrix:\n",
      " [[113   1   4]\n",
      " [  7  23   0]\n",
      " [ 27   0  15]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "116A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "016A    10\n",
      "005A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "050A     7\n",
      "117A     7\n",
      "037A     6\n",
      "053A     6\n",
      "008A     6\n",
      "109A     6\n",
      "023A     6\n",
      "044A     5\n",
      "023B     5\n",
      "070A     5\n",
      "075A     5\n",
      "009A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "060A     3\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "025B     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "032A     2\n",
      "018A     2\n",
      "069A     2\n",
      "092A     1\n",
      "100A     1\n",
      "096A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "041A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "066A     1\n",
      "091A     1\n",
      "076A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "055A    20\n",
      "101A    15\n",
      "028A    13\n",
      "002A    13\n",
      "025A    11\n",
      "033A     9\n",
      "031A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "034A     5\n",
      "025C     5\n",
      "021A     5\n",
      "026A     4\n",
      "035A     4\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "093A     2\n",
      "049A     1\n",
      "026C     1\n",
      "073A     1\n",
      "088A     1\n",
      "090A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    277\n",
      "M    271\n",
      "F    151\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    101\n",
      "X     71\n",
      "M     66\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 071A, 097B, 019A, 074A, 067...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 050...\n",
      "senior    [097A, 106A, 104A, 059A, 116A, 051B, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 103A, 028A, 101A, 034A, 002A, 099...\n",
      "kitten                                         [047A, 049A]\n",
      "senior           [093A, 057A, 055A, 113A, 056A, 108A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 14, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 2, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "Length of X_train_val:\n",
      "699\n",
      "Length of y_train_val:\n",
      "699\n",
      "Length of groups_train_val:\n",
      "699\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 441, 1: 142, 2: 116})\n",
      "Epoch 1/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.2822 - accuracy: 0.4578\n",
      "Epoch 2/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9987 - accuracy: 0.5637\n",
      "Epoch 3/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8768 - accuracy: 0.6237\n",
      "Epoch 4/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8290 - accuracy: 0.6352\n",
      "Epoch 5/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7806 - accuracy: 0.6595\n",
      "Epoch 6/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7599 - accuracy: 0.6781\n",
      "Epoch 7/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7377 - accuracy: 0.6996\n",
      "Epoch 8/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7024 - accuracy: 0.7182\n",
      "Epoch 9/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6325 - accuracy: 0.7353\n",
      "Epoch 10/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5944 - accuracy: 0.7625\n",
      "Epoch 11/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6211 - accuracy: 0.7482\n",
      "Epoch 12/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5876 - accuracy: 0.7654\n",
      "Epoch 13/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5704 - accuracy: 0.7840\n",
      "Epoch 14/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5598 - accuracy: 0.7740\n",
      "Epoch 15/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5233 - accuracy: 0.7969\n",
      "Epoch 16/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5485 - accuracy: 0.7969\n",
      "Epoch 17/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4660 - accuracy: 0.8083\n",
      "Epoch 18/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4852 - accuracy: 0.8197\n",
      "Epoch 19/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4859 - accuracy: 0.8026\n",
      "Epoch 20/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4533 - accuracy: 0.8298\n",
      "Epoch 21/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4387 - accuracy: 0.8240\n",
      "Epoch 22/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4554 - accuracy: 0.8283\n",
      "Epoch 23/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8240\n",
      "Epoch 24/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4356 - accuracy: 0.8269\n",
      "Epoch 25/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4141 - accuracy: 0.8340\n",
      "Epoch 26/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4065 - accuracy: 0.8283\n",
      "Epoch 27/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4321 - accuracy: 0.8240\n",
      "Epoch 28/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4226 - accuracy: 0.8326\n",
      "Epoch 29/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4012 - accuracy: 0.8398\n",
      "Epoch 30/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3907 - accuracy: 0.8441\n",
      "Epoch 31/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3695 - accuracy: 0.8541\n",
      "Epoch 32/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3744 - accuracy: 0.8569\n",
      "Epoch 33/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3854 - accuracy: 0.8312\n",
      "Epoch 34/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3670 - accuracy: 0.8541\n",
      "Epoch 35/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3198 - accuracy: 0.8770\n",
      "Epoch 36/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3514 - accuracy: 0.8612\n",
      "Epoch 37/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3645 - accuracy: 0.8541\n",
      "Epoch 38/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3778 - accuracy: 0.8498\n",
      "Epoch 39/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3586 - accuracy: 0.8612\n",
      "Epoch 40/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3462 - accuracy: 0.8612\n",
      "Epoch 41/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3432 - accuracy: 0.8712\n",
      "Epoch 42/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3349 - accuracy: 0.8684\n",
      "Epoch 43/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3123 - accuracy: 0.8712\n",
      "Epoch 44/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3253 - accuracy: 0.8670\n",
      "Epoch 45/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3241 - accuracy: 0.8755\n",
      "Epoch 46/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3341 - accuracy: 0.8741\n",
      "Epoch 47/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3451 - accuracy: 0.8684\n",
      "Epoch 48/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3070 - accuracy: 0.8856\n",
      "Epoch 49/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3349 - accuracy: 0.8655\n",
      "Epoch 50/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.8641\n",
      "Epoch 51/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2874 - accuracy: 0.8856\n",
      "Epoch 52/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2823 - accuracy: 0.8956\n",
      "Epoch 53/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3291 - accuracy: 0.8784\n",
      "Epoch 54/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3204 - accuracy: 0.8798\n",
      "Epoch 55/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.8827\n",
      "Epoch 56/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3171 - accuracy: 0.8741\n",
      "Epoch 57/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.8956\n",
      "Epoch 58/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2916 - accuracy: 0.8770\n",
      "Epoch 59/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2687 - accuracy: 0.8856\n",
      "Epoch 60/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2967 - accuracy: 0.8941\n",
      "Epoch 61/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3003 - accuracy: 0.8784\n",
      "Epoch 62/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2748 - accuracy: 0.8870\n",
      "Epoch 63/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2990 - accuracy: 0.8870\n",
      "Epoch 64/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2803 - accuracy: 0.8913\n",
      "Epoch 65/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3008 - accuracy: 0.8784\n",
      "Epoch 66/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.9084\n",
      "Epoch 67/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2507 - accuracy: 0.9070\n",
      "Epoch 68/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2791 - accuracy: 0.8884\n",
      "Epoch 69/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2567 - accuracy: 0.9027\n",
      "Epoch 70/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2604 - accuracy: 0.8927\n",
      "Epoch 71/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2672 - accuracy: 0.8898\n",
      "Epoch 72/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2682 - accuracy: 0.8984\n",
      "Epoch 73/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2601 - accuracy: 0.8827\n",
      "Epoch 74/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2491 - accuracy: 0.9070\n",
      "Epoch 75/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2695 - accuracy: 0.8984\n",
      "Epoch 76/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2315 - accuracy: 0.9127\n",
      "Epoch 77/1500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.2477 - accuracy: 0.9056\n",
      "Epoch 78/1500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2298 - accuracy: 0.9113\n",
      "Epoch 79/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2449 - accuracy: 0.9056\n",
      "Epoch 80/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2324 - accuracy: 0.9084\n",
      "Epoch 81/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2136 - accuracy: 0.9285\n",
      "Epoch 82/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2497 - accuracy: 0.8970\n",
      "Epoch 83/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2733 - accuracy: 0.8984\n",
      "Epoch 84/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2581 - accuracy: 0.9084\n",
      "Epoch 85/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.9084\n",
      "Epoch 86/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9142\n",
      "Epoch 87/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2234 - accuracy: 0.9113\n",
      "Epoch 88/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2341 - accuracy: 0.9142\n",
      "Epoch 89/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2555 - accuracy: 0.9013\n",
      "Epoch 90/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.9170\n",
      "Epoch 91/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2167 - accuracy: 0.9170\n",
      "Epoch 92/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2197 - accuracy: 0.9127\n",
      "Epoch 93/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9256\n",
      "Epoch 94/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9256\n",
      "Epoch 95/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.9299\n",
      "Epoch 96/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2088 - accuracy: 0.9270\n",
      "Epoch 97/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2221 - accuracy: 0.9213\n",
      "Epoch 98/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2290 - accuracy: 0.9127\n",
      "Epoch 99/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2052 - accuracy: 0.9242\n",
      "Epoch 100/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1806 - accuracy: 0.9342\n",
      "Epoch 101/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9084\n",
      "Epoch 102/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1972 - accuracy: 0.9199\n",
      "Epoch 103/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1921 - accuracy: 0.9270\n",
      "Epoch 104/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2228 - accuracy: 0.9084\n",
      "Epoch 105/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9256\n",
      "Epoch 106/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1793 - accuracy: 0.9385\n",
      "Epoch 107/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2259 - accuracy: 0.9156\n",
      "Epoch 108/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2095 - accuracy: 0.9213\n",
      "Epoch 109/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9256\n",
      "Epoch 110/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2061 - accuracy: 0.9227\n",
      "Epoch 111/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.9299\n",
      "Epoch 112/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9213\n",
      "Epoch 113/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9299\n",
      "Epoch 114/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9356\n",
      "Epoch 115/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9413\n",
      "Epoch 116/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2030 - accuracy: 0.9256\n",
      "Epoch 117/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2150 - accuracy: 0.9127\n",
      "Epoch 118/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2026 - accuracy: 0.9213\n",
      "Epoch 119/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1656 - accuracy: 0.9399\n",
      "Epoch 120/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9385\n",
      "Epoch 121/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9399\n",
      "Epoch 122/1500\n",
      "22/22 [==============================] - 0s 1000us/step - loss: 0.1807 - accuracy: 0.9242\n",
      "Epoch 123/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.9227\n",
      "Epoch 124/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9485\n",
      "Epoch 125/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.9328\n",
      "Epoch 126/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.9299\n",
      "Epoch 127/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9428\n",
      "Epoch 128/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.9371\n",
      "Epoch 129/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1996 - accuracy: 0.9142\n",
      "Epoch 130/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9142\n",
      "Epoch 131/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1930 - accuracy: 0.9285\n",
      "Epoch 132/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1626 - accuracy: 0.9342\n",
      "Epoch 133/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2048 - accuracy: 0.9227\n",
      "Epoch 134/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.9356\n",
      "Epoch 135/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9299\n",
      "Epoch 136/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1628 - accuracy: 0.9428\n",
      "Epoch 137/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9399\n",
      "Epoch 138/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.9242\n",
      "Epoch 139/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9399\n",
      "Epoch 140/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9542\n",
      "Epoch 141/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9399\n",
      "Epoch 142/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.9328\n",
      "Epoch 143/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.9399\n",
      "Epoch 144/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9428\n",
      "Epoch 145/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1551 - accuracy: 0.9456\n",
      "Epoch 146/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1906 - accuracy: 0.9371\n",
      "Epoch 147/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9342\n",
      "Epoch 148/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9313\n",
      "Epoch 149/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1404 - accuracy: 0.9557\n",
      "Epoch 150/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9471\n",
      "Epoch 151/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9499\n",
      "Epoch 152/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.9471\n",
      "Epoch 153/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9471\n",
      "Epoch 154/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1561 - accuracy: 0.9428\n",
      "Epoch 155/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9285\n",
      "Epoch 156/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9514\n",
      "Epoch 157/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.9499\n",
      "Epoch 158/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9428\n",
      "Epoch 159/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9442\n",
      "Epoch 160/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.9428\n",
      "Epoch 161/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9456\n",
      "Epoch 162/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9514\n",
      "Epoch 163/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9571\n",
      "Epoch 164/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9528\n",
      "Epoch 165/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9514\n",
      "Epoch 166/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9413\n",
      "Epoch 167/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9399\n",
      "Epoch 168/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9442\n",
      "Epoch 169/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9471\n",
      "Epoch 170/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9499\n",
      "Epoch 171/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.9313\n",
      "Epoch 172/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1558 - accuracy: 0.9428\n",
      "Epoch 173/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.9499\n",
      "Epoch 174/1500\n",
      "22/22 [==============================] - 0s 992us/step - loss: 0.1317 - accuracy: 0.9485\n",
      "Epoch 175/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9471\n",
      "Epoch 176/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1592 - accuracy: 0.9413\n",
      "Epoch 177/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9671\n",
      "Epoch 178/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9413\n",
      "Epoch 179/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9413\n",
      "Epoch 180/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1546 - accuracy: 0.9371\n",
      "Epoch 181/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9585\n",
      "Epoch 182/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9728\n",
      "Epoch 183/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9471\n",
      "Epoch 184/1500\n",
      "22/22 [==============================] - 0s 987us/step - loss: 0.1315 - accuracy: 0.9557\n",
      "Epoch 185/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9671\n",
      "Epoch 186/1500\n",
      "22/22 [==============================] - 0s 999us/step - loss: 0.1113 - accuracy: 0.9657\n",
      "Epoch 187/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9671\n",
      "Epoch 188/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9485\n",
      "Epoch 189/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9499\n",
      "Epoch 190/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1477 - accuracy: 0.9542\n",
      "Epoch 191/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9471\n",
      "Epoch 192/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9571\n",
      "Epoch 193/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9557\n",
      "Epoch 194/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9642\n",
      "Epoch 195/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9528\n",
      "Epoch 196/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9471\n",
      "Epoch 197/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9499\n",
      "Epoch 198/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9557\n",
      "Epoch 199/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9571\n",
      "Epoch 200/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9585\n",
      "Epoch 201/1500\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.1449 - accuracy: 0.9514\n",
      "Epoch 202/1500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1283 - accuracy: 0.9485\n",
      "Epoch 203/1500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1250 - accuracy: 0.9485\n",
      "Epoch 204/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9585\n",
      "Epoch 205/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9614\n",
      "Epoch 206/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9728\n",
      "Epoch 207/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9557\n",
      "Epoch 208/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9557\n",
      "Epoch 209/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9485\n",
      "Epoch 210/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9571\n",
      "Epoch 211/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9599\n",
      "Epoch 212/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9685\n",
      "Epoch 213/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9628\n",
      "Epoch 214/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9599\n",
      "Epoch 215/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9528\n",
      "Epoch 216/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9599\n",
      "Epoch 217/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9456\n",
      "Epoch 218/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.9700\n",
      "Epoch 219/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9571\n",
      "Epoch 220/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9557\n",
      "Epoch 221/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9514\n",
      "Epoch 222/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9499\n",
      "Epoch 223/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9428\n",
      "Epoch 224/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9700\n",
      "Epoch 225/1500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1164 - accuracy: 0.9599\n",
      "Epoch 226/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9485\n",
      "Epoch 227/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9657\n",
      "Epoch 228/1500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.1063 - accuracy: 0.9671\n",
      "Epoch 229/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9528\n",
      "Epoch 230/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9599\n",
      "Epoch 231/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9642\n",
      "Epoch 232/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9700\n",
      "Epoch 233/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9614\n",
      "Epoch 234/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9800\n",
      "Epoch 235/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9557\n",
      "Epoch 236/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9685\n",
      "Epoch 237/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9700\n",
      "Epoch 238/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9657\n",
      "Epoch 239/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.9728\n",
      "Epoch 240/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9671\n",
      "Epoch 241/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9728\n",
      "Epoch 242/1500\n",
      "22/22 [==============================] - 0s 989us/step - loss: 0.0882 - accuracy: 0.9757\n",
      "Epoch 243/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9714\n",
      "Epoch 244/1500\n",
      "22/22 [==============================] - 0s 1000us/step - loss: 0.0906 - accuracy: 0.9714\n",
      "Epoch 245/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9671\n",
      "Epoch 246/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9542\n",
      "Epoch 247/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9742\n",
      "Epoch 248/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9614\n",
      "Epoch 249/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9599\n",
      "Epoch 250/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9585\n",
      "Epoch 251/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9642\n",
      "Epoch 252/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9657\n",
      "Epoch 253/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9614\n",
      "Epoch 254/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9685\n",
      "Epoch 255/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9642\n",
      "Epoch 256/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9671\n",
      "Epoch 257/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9657\n",
      "Epoch 258/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9599\n",
      "Epoch 259/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9657\n",
      "Epoch 260/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9800\n",
      "Epoch 261/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9728\n",
      "Epoch 262/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9542\n",
      "Epoch 263/1500\n",
      "22/22 [==============================] - 0s 995us/step - loss: 0.0863 - accuracy: 0.9700\n",
      "Epoch 264/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9700\n",
      "Epoch 265/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9657\n",
      "Epoch 266/1500\n",
      "22/22 [==============================] - 0s 999us/step - loss: 0.1076 - accuracy: 0.9585\n",
      "Epoch 267/1500\n",
      "22/22 [==============================] - 0s 990us/step - loss: 0.1216 - accuracy: 0.9542\n",
      "Epoch 268/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9599\n",
      "Epoch 269/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9785\n",
      "Epoch 270/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9671\n",
      "Epoch 271/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9771\n",
      "Epoch 272/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9685\n",
      "Epoch 273/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9657\n",
      "Epoch 274/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9657\n",
      "Epoch 275/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9685\n",
      "Epoch 276/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9728\n",
      "Epoch 277/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9771\n",
      "Epoch 278/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9685\n",
      "Epoch 279/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9671\n",
      "Epoch 280/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.9614\n",
      "Epoch 281/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9542\n",
      "Epoch 282/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9685\n",
      "Epoch 283/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9542\n",
      "Epoch 284/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9700\n",
      "Epoch 285/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9714\n",
      "Epoch 286/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9742\n",
      "Epoch 287/1500\n",
      "22/22 [==============================] - 0s 1000us/step - loss: 0.0972 - accuracy: 0.9685\n",
      "Epoch 288/1500\n",
      "22/22 [==============================] - 0s 992us/step - loss: 0.0842 - accuracy: 0.9771\n",
      "Epoch 289/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.9728\n",
      "Epoch 290/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9742\n",
      "Epoch 291/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9742\n",
      "Epoch 292/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9714\n",
      "Epoch 293/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9742\n",
      "Epoch 294/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9628\n",
      "Epoch 295/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9742\n",
      "Epoch 296/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9742\n",
      "Epoch 297/1500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.0579 - accuracy: 0.9900\n",
      "Epoch 298/1500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.0747 - accuracy: 0.9728\n",
      "Epoch 299/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9785\n",
      "Epoch 300/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9700\n",
      "Epoch 301/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9714\n",
      "Epoch 302/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9628\n",
      "Epoch 303/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9614\n",
      "Epoch 304/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9742\n",
      "Epoch 305/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9700\n",
      "Epoch 306/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9685\n",
      "Epoch 307/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9685\n",
      "Epoch 308/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9714\n",
      "Epoch 309/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9714\n",
      "Epoch 310/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9671\n",
      "Epoch 311/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9657\n",
      "Epoch 312/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.9657\n",
      "Epoch 313/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9771\n",
      "Epoch 314/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9742\n",
      "Epoch 315/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9685\n",
      "Epoch 316/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.9814\n",
      "Epoch 317/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9685\n",
      "Epoch 318/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9785\n",
      "Epoch 319/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.9828\n",
      "Epoch 320/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9685\n",
      "Epoch 321/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.9642\n",
      "Epoch 322/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.9857\n",
      "Epoch 323/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9728\n",
      "Epoch 324/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9742\n",
      "Epoch 325/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9714\n",
      "Epoch 326/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9771\n",
      "Epoch 327/1500\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.0172 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 297.\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9728\n",
      "Epoch 327: early stopping\n",
      "8/8 [==============================] - 0s 928us/step - loss: 1.0628 - accuracy: 0.6597\n",
      "8/8 [==============================] - 0s 732us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (23/29)\n",
      "Before appending - Cat IDs: 612, Predictions: 612, Actuals: 612, Gender: 612\n",
      "After appending - Cat IDs: 850, Predictions: 850, Actuals: 850, Gender: 850\n",
      "Final Test Results - Loss: 1.0628337860107422, Accuracy: 0.6596638560295105, Precision: 0.6653021442495126, Recall: 0.544049952201867, F1 Score: 0.5743678921319693\n",
      "Confusion Matrix:\n",
      " [[128   2  17]\n",
      " [ 13  16   0]\n",
      " [ 49   0  13]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6708185546487933\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8763819634914398\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7328100204467773\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.718972536336258\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6620802086672188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[2]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ###################\n",
    "    # PREPARING MODEL #\n",
    "    ###################\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be692ae4-6d3f-4353-b5bf-554d20da4df3",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8da9a092-ed2e-4397-a6c8-2c4888735265",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 850, Predictions: 850, Actuals: 850, Gender: 850\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "51cf386a-c49e-4716-ba15-aa3b7930419a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "a8d43ac5-d50e-430d-98a1-ff4f45006bae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (82/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ccc9acb7-bb1b-42a6-bb25-cdf5a3356315",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "95e69b27-cae1-4a3a-ba70-5244a11aadf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, kitten, k...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[senior, senior, senior, adult, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, senior, adult, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, senior, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[senior, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A              [adult, adult, senior, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, adult, adult, adult, kitten, kitten, k...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A             [adult, adult, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [senior, senior, adult]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A    [senior, senior, senior, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "92    097A  [senior, senior, senior, adult, adult, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "29    025B                                    [adult, senior]         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "43    037A        [adult, adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "22    020A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "17    015A  [adult, adult, adult, adult, senior, adult, se...         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "32    026B                                    [senior, adult]         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "42    036A  [adult, senior, senior, adult, adult, adult, a...         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "41    035A                      [kitten, adult, adult, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "89    094A  [adult, senior, adult, adult, adult, senior, a...         adult           senior                  False\n",
       "99    104A                     [adult, senior, adult, senior]         adult           senior                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "103   109A      [adult, adult, kitten, kitten, kitten, adult]         adult           kitten                  False\n",
       "101   106A  [adult, senior, adult, senior, adult, adult, a...         adult           senior                  False\n",
       "90    095A  [senior, senior, adult, senior, senior, senior...        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "87    092A                                           [senior]        senior            adult                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "57    051B  [senior, adult, senior, adult, senior, adult, ...         adult           senior                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "60    054A                                    [adult, senior]         adult           senior                  False\n",
       "61    055A  [adult, adult, senior, adult, adult, adult, ad...         adult           senior                  False\n",
       "62    056A                              [adult, adult, adult]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "65    059A  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "66    060A                           [senior, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "71    065A  [senior, adult, adult, adult, senior, senior, ...        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, adult, senior, adult, ad...         adult           senior                  False\n",
       "85    090A                                            [adult]         adult           senior                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "109   117A  [senior, senior, adult, adult, adult, adult, a...         adult           senior                  False"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d36b3c54-3377-4249-a774-6d31557e36da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     64\n",
      "kitten    13\n",
      "senior     5\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "b36eb8a4-57f3-48c0-b92c-4a8e5a52c59e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             64  87.671233\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22              5  22.727273\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1750e2da-df8c-4f00-b860-539dd822864f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6b34e-2368-4956-8241-8e6ab6e8cf81",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "fdefd9be-6e6d-4903-a0ec-7824da4313d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          550            467  84.909091\n",
      "1           kitten          122             96  78.688525\n",
      "2           senior          178             57  32.022472\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "57576e25-349c-46e0-b57b-8bf3cee3b5de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b69785-d4c1-49ab-ba75-4a733326cdc1",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "15c5b6e5-d7b0-4089-ae78-3dafe1cead3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      148     69.48\n",
      "1          M    338      252     74.56\n",
      "2          X    299      220     73.58\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b6001f1b-3f01-42e9-8761-4253acbed5e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      148     69.48\n",
      "1          M    338      252     74.56\n",
      "2          X    299      220     73.58\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b5d44-f489-4de0-8785-4bfa52d09797",
   "metadata": {},
   "source": [
    "# RANDOM SEED 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "c4439fc4-670c-4009-8ca9-23c419ee99ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[3])) \n",
    "np.random.seed(int(random_seeds[3]))\n",
    "tf.random.set_seed(int(random_seeds[3]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5fe04e3d-fd90-43c1-97af-5eac75e35f85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "2ed5261d-2aeb-412e-a017-65b461a2cf5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70aa6-1716-400a-be48-cccb3511542e",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "fb9d0248-481e-4807-85dd-407fa88963f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "072A     9\n",
      "015A     9\n",
      "051B     9\n",
      "022A     9\n",
      "033A     9\n",
      "045A     9\n",
      "095A     8\n",
      "013B     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "109A     6\n",
      "023A     6\n",
      "037A     6\n",
      "075A     5\n",
      "070A     5\n",
      "025C     5\n",
      "021A     5\n",
      "034A     5\n",
      "044A     5\n",
      "023B     5\n",
      "003A     4\n",
      "105A     4\n",
      "035A     4\n",
      "026A     4\n",
      "052A     4\n",
      "062A     4\n",
      "012A     3\n",
      "113A     3\n",
      "058A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "025B     2\n",
      "032A     2\n",
      "093A     2\n",
      "054A     2\n",
      "069A     2\n",
      "087A     2\n",
      "038A     2\n",
      "073A     1\n",
      "004A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "091A     1\n",
      "019B     1\n",
      "066A     1\n",
      "048A     1\n",
      "092A     1\n",
      "026C     1\n",
      "076A     1\n",
      "043A     1\n",
      "041A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "019A    17\n",
      "101A    15\n",
      "039A    12\n",
      "063A    11\n",
      "071A    10\n",
      "005A    10\n",
      "065A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "099A     7\n",
      "008A     6\n",
      "009A     4\n",
      "104A     4\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "096A     1\n",
      "049A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    296\n",
      "X    286\n",
      "F    208\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    62\n",
      "F    44\n",
      "M    41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 028...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 019A, 101A, 005A, 065A, 039A, 009A, 063...\n",
      "kitten                                         [050A, 049A]\n",
      "senior                       [104A, 056A, 094A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 14, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 2, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "Length of X_train_val:\n",
      "790\n",
      "Length of y_train_val:\n",
      "790\n",
      "Length of groups_train_val:\n",
      "790\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 468, 1: 163, 2: 159})\n",
      "Epoch 1/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 1.2292 - accuracy: 0.4899\n",
      "Epoch 2/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.9566 - accuracy: 0.6101\n",
      "Epoch 3/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.8329 - accuracy: 0.6620\n",
      "Epoch 4/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7927 - accuracy: 0.6570\n",
      "Epoch 5/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7433 - accuracy: 0.6785\n",
      "Epoch 6/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7304 - accuracy: 0.7228\n",
      "Epoch 7/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.6900 - accuracy: 0.7291\n",
      "Epoch 8/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6679 - accuracy: 0.7127\n",
      "Epoch 9/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5925 - accuracy: 0.7608\n",
      "Epoch 10/1500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.6154 - accuracy: 0.7595\n",
      "Epoch 11/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.6112 - accuracy: 0.7696\n",
      "Epoch 12/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5952 - accuracy: 0.7747\n",
      "Epoch 13/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5572 - accuracy: 0.7646\n",
      "Epoch 14/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5187 - accuracy: 0.7848\n",
      "Epoch 15/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5615 - accuracy: 0.7797\n",
      "Epoch 16/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5579 - accuracy: 0.7823\n",
      "Epoch 17/1500\n",
      "25/25 [==============================] - 0s 996us/step - loss: 0.4959 - accuracy: 0.7924\n",
      "Epoch 18/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4920 - accuracy: 0.7911\n",
      "Epoch 19/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4810 - accuracy: 0.7949\n",
      "Epoch 20/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4774 - accuracy: 0.8076\n",
      "Epoch 21/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4646 - accuracy: 0.8228\n",
      "Epoch 22/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4720 - accuracy: 0.8139\n",
      "Epoch 23/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.4481 - accuracy: 0.8203\n",
      "Epoch 24/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4424 - accuracy: 0.8228\n",
      "Epoch 25/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4811 - accuracy: 0.8051\n",
      "Epoch 26/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4672 - accuracy: 0.8139\n",
      "Epoch 27/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4251 - accuracy: 0.8367\n",
      "Epoch 28/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4221 - accuracy: 0.8342\n",
      "Epoch 29/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4623 - accuracy: 0.8215\n",
      "Epoch 30/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3779 - accuracy: 0.8506\n",
      "Epoch 31/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4117 - accuracy: 0.8380\n",
      "Epoch 32/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3997 - accuracy: 0.8380\n",
      "Epoch 33/1500\n",
      "25/25 [==============================] - 0s 978us/step - loss: 0.4036 - accuracy: 0.8342\n",
      "Epoch 34/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4049 - accuracy: 0.8418\n",
      "Epoch 35/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.3977 - accuracy: 0.8367\n",
      "Epoch 36/1500\n",
      "25/25 [==============================] - 0s 980us/step - loss: 0.4006 - accuracy: 0.8405\n",
      "Epoch 37/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3718 - accuracy: 0.8570\n",
      "Epoch 38/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4190 - accuracy: 0.8304\n",
      "Epoch 39/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4454 - accuracy: 0.8165\n",
      "Epoch 40/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3757 - accuracy: 0.8304\n",
      "Epoch 41/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3889 - accuracy: 0.8506\n",
      "Epoch 42/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3735 - accuracy: 0.8418\n",
      "Epoch 43/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3647 - accuracy: 0.8468\n",
      "Epoch 44/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3563 - accuracy: 0.8532\n",
      "Epoch 45/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3432 - accuracy: 0.8620\n",
      "Epoch 46/1500\n",
      "25/25 [==============================] - 0s 964us/step - loss: 0.3676 - accuracy: 0.8519\n",
      "Epoch 47/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8848\n",
      "Epoch 48/1500\n",
      "25/25 [==============================] - 0s 945us/step - loss: 0.3351 - accuracy: 0.8734\n",
      "Epoch 49/1500\n",
      "25/25 [==============================] - 0s 988us/step - loss: 0.3178 - accuracy: 0.8848\n",
      "Epoch 50/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3552 - accuracy: 0.8722\n",
      "Epoch 51/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3591 - accuracy: 0.8532\n",
      "Epoch 52/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8608\n",
      "Epoch 53/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3208 - accuracy: 0.8709\n",
      "Epoch 54/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3170 - accuracy: 0.8835\n",
      "Epoch 55/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3143 - accuracy: 0.8797\n",
      "Epoch 56/1500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.3079 - accuracy: 0.8747\n",
      "Epoch 57/1500\n",
      "25/25 [==============================] - 0s 3ms/step - loss: 0.3086 - accuracy: 0.8772\n",
      "Epoch 58/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3037 - accuracy: 0.8886\n",
      "Epoch 59/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3140 - accuracy: 0.8747\n",
      "Epoch 60/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3156 - accuracy: 0.8633\n",
      "Epoch 61/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3231 - accuracy: 0.8759\n",
      "Epoch 62/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8873\n",
      "Epoch 63/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3143 - accuracy: 0.8722\n",
      "Epoch 64/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2903 - accuracy: 0.8861\n",
      "Epoch 65/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.8861\n",
      "Epoch 66/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3006 - accuracy: 0.8949\n",
      "Epoch 67/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2848 - accuracy: 0.8911\n",
      "Epoch 68/1500\n",
      "25/25 [==============================] - 0s 976us/step - loss: 0.2994 - accuracy: 0.8772\n",
      "Epoch 69/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2828 - accuracy: 0.8873\n",
      "Epoch 70/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2828 - accuracy: 0.8759\n",
      "Epoch 71/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2879 - accuracy: 0.8823\n",
      "Epoch 72/1500\n",
      "25/25 [==============================] - 0s 894us/step - loss: 0.2757 - accuracy: 0.8962\n",
      "Epoch 73/1500\n",
      "25/25 [==============================] - 0s 980us/step - loss: 0.3082 - accuracy: 0.8785\n",
      "Epoch 74/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.2616 - accuracy: 0.8975\n",
      "Epoch 75/1500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.3049 - accuracy: 0.8899\n",
      "Epoch 76/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2624 - accuracy: 0.9013\n",
      "Epoch 77/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2780 - accuracy: 0.8975\n",
      "Epoch 78/1500\n",
      "25/25 [==============================] - 0s 992us/step - loss: 0.2907 - accuracy: 0.8684\n",
      "Epoch 79/1500\n",
      "25/25 [==============================] - 0s 943us/step - loss: 0.3057 - accuracy: 0.8785\n",
      "Epoch 80/1500\n",
      "25/25 [==============================] - 0s 985us/step - loss: 0.2923 - accuracy: 0.8835\n",
      "Epoch 81/1500\n",
      "25/25 [==============================] - 0s 962us/step - loss: 0.2610 - accuracy: 0.8924\n",
      "Epoch 82/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.2587 - accuracy: 0.8975\n",
      "Epoch 83/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2734 - accuracy: 0.8886\n",
      "Epoch 84/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2552 - accuracy: 0.8987\n",
      "Epoch 85/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2416 - accuracy: 0.9025\n",
      "Epoch 86/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.9127\n",
      "Epoch 87/1500\n",
      "25/25 [==============================] - 0s 985us/step - loss: 0.2445 - accuracy: 0.9063\n",
      "Epoch 88/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2518 - accuracy: 0.9000\n",
      "Epoch 89/1500\n",
      "25/25 [==============================] - 0s 988us/step - loss: 0.2520 - accuracy: 0.8975\n",
      "Epoch 90/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.2483 - accuracy: 0.9139\n",
      "Epoch 91/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2599 - accuracy: 0.9038\n",
      "Epoch 92/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2300 - accuracy: 0.9114\n",
      "Epoch 93/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2588 - accuracy: 0.8937\n",
      "Epoch 94/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2636 - accuracy: 0.8949\n",
      "Epoch 95/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2844 - accuracy: 0.8949\n",
      "Epoch 96/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2431 - accuracy: 0.9051\n",
      "Epoch 97/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2427 - accuracy: 0.9063\n",
      "Epoch 98/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2231 - accuracy: 0.9152\n",
      "Epoch 99/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2248 - accuracy: 0.9177\n",
      "Epoch 100/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2165 - accuracy: 0.9266\n",
      "Epoch 101/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9165\n",
      "Epoch 102/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2095 - accuracy: 0.9253\n",
      "Epoch 103/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9076\n",
      "Epoch 104/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2304 - accuracy: 0.9165\n",
      "Epoch 105/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2291 - accuracy: 0.9152\n",
      "Epoch 106/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2609 - accuracy: 0.8975\n",
      "Epoch 107/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.9013\n",
      "Epoch 108/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2249 - accuracy: 0.9127\n",
      "Epoch 109/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.9038\n",
      "Epoch 110/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2332 - accuracy: 0.9152\n",
      "Epoch 111/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2144 - accuracy: 0.9228\n",
      "Epoch 112/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2263 - accuracy: 0.9101\n",
      "Epoch 113/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2316 - accuracy: 0.9190\n",
      "Epoch 114/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9139\n",
      "Epoch 115/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9228\n",
      "Epoch 116/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9228\n",
      "Epoch 117/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9215\n",
      "Epoch 118/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.2043 - accuracy: 0.9241\n",
      "Epoch 119/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2132 - accuracy: 0.9253\n",
      "Epoch 120/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.2060 - accuracy: 0.9203\n",
      "Epoch 121/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2178 - accuracy: 0.9190\n",
      "Epoch 122/1500\n",
      "25/25 [==============================] - 0s 998us/step - loss: 0.2233 - accuracy: 0.9127\n",
      "Epoch 123/1500\n",
      "25/25 [==============================] - 0s 996us/step - loss: 0.2177 - accuracy: 0.9177\n",
      "Epoch 124/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9101\n",
      "Epoch 125/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1927 - accuracy: 0.9354\n",
      "Epoch 126/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1933 - accuracy: 0.9329\n",
      "Epoch 127/1500\n",
      "25/25 [==============================] - 0s 993us/step - loss: 0.2121 - accuracy: 0.9241\n",
      "Epoch 128/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1869 - accuracy: 0.9291\n",
      "Epoch 129/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1779 - accuracy: 0.9291\n",
      "Epoch 130/1500\n",
      "25/25 [==============================] - 0s 979us/step - loss: 0.2110 - accuracy: 0.9177\n",
      "Epoch 131/1500\n",
      "25/25 [==============================] - 0s 983us/step - loss: 0.1814 - accuracy: 0.9329\n",
      "Epoch 132/1500\n",
      "25/25 [==============================] - 0s 961us/step - loss: 0.2059 - accuracy: 0.9152\n",
      "Epoch 133/1500\n",
      "25/25 [==============================] - 0s 964us/step - loss: 0.1963 - accuracy: 0.9316\n",
      "Epoch 134/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9278\n",
      "Epoch 135/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9405\n",
      "Epoch 136/1500\n",
      "25/25 [==============================] - 0s 992us/step - loss: 0.1697 - accuracy: 0.9367\n",
      "Epoch 137/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2048 - accuracy: 0.9241\n",
      "Epoch 138/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9304\n",
      "Epoch 139/1500\n",
      "25/25 [==============================] - 0s 985us/step - loss: 0.1684 - accuracy: 0.9430\n",
      "Epoch 140/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.9304\n",
      "Epoch 141/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.9342\n",
      "Epoch 142/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.2094 - accuracy: 0.9165\n",
      "Epoch 143/1500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.1779 - accuracy: 0.9342\n",
      "Epoch 144/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9253\n",
      "Epoch 145/1500\n",
      "25/25 [==============================] - 0s 982us/step - loss: 0.1926 - accuracy: 0.9253\n",
      "Epoch 146/1500\n",
      "25/25 [==============================] - 0s 996us/step - loss: 0.1723 - accuracy: 0.9304\n",
      "Epoch 147/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.9253\n",
      "Epoch 148/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9291\n",
      "Epoch 149/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1717 - accuracy: 0.9354\n",
      "Epoch 150/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.9544\n",
      "Epoch 151/1500\n",
      "25/25 [==============================] - 0s 998us/step - loss: 0.1840 - accuracy: 0.9405\n",
      "Epoch 152/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1809 - accuracy: 0.9253\n",
      "Epoch 153/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9443\n",
      "Epoch 154/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9291\n",
      "Epoch 155/1500\n",
      "25/25 [==============================] - 0s 1000us/step - loss: 0.1807 - accuracy: 0.9342\n",
      "Epoch 156/1500\n",
      "25/25 [==============================] - 0s 971us/step - loss: 0.1705 - accuracy: 0.9430\n",
      "Epoch 157/1500\n",
      "25/25 [==============================] - 0s 977us/step - loss: 0.1664 - accuracy: 0.9342\n",
      "Epoch 158/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.1857 - accuracy: 0.9316\n",
      "Epoch 159/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9481\n",
      "Epoch 160/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9494\n",
      "Epoch 161/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1738 - accuracy: 0.9253\n",
      "Epoch 162/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9278\n",
      "Epoch 163/1500\n",
      "25/25 [==============================] - 0s 988us/step - loss: 0.1530 - accuracy: 0.9481\n",
      "Epoch 164/1500\n",
      "25/25 [==============================] - 0s 998us/step - loss: 0.1619 - accuracy: 0.9392\n",
      "Epoch 165/1500\n",
      "25/25 [==============================] - 0s 999us/step - loss: 0.1510 - accuracy: 0.9532\n",
      "Epoch 166/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.9367\n",
      "Epoch 167/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9380\n",
      "Epoch 168/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1617 - accuracy: 0.9278\n",
      "Epoch 169/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9620\n",
      "Epoch 170/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9329\n",
      "Epoch 171/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.9468\n",
      "Epoch 172/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1809 - accuracy: 0.9342\n",
      "Epoch 173/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9342\n",
      "Epoch 174/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9405\n",
      "Epoch 175/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9354\n",
      "Epoch 176/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9506\n",
      "Epoch 177/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9418\n",
      "Epoch 178/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1446 - accuracy: 0.9544\n",
      "Epoch 179/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9392\n",
      "Epoch 180/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9481\n",
      "Epoch 181/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9456\n",
      "Epoch 182/1500\n",
      "25/25 [==============================] - 0s 9ms/step - loss: 0.1625 - accuracy: 0.9329\n",
      "Epoch 183/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1413 - accuracy: 0.9468\n",
      "Epoch 184/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9443\n",
      "Epoch 185/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.9494\n",
      "Epoch 186/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9570\n",
      "Epoch 187/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9506\n",
      "Epoch 188/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9595\n",
      "Epoch 189/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.9304\n",
      "Epoch 190/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.9253\n",
      "Epoch 191/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9519\n",
      "Epoch 192/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.1602 - accuracy: 0.9342\n",
      "Epoch 193/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9532\n",
      "Epoch 194/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9405\n",
      "Epoch 195/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1643 - accuracy: 0.9342\n",
      "Epoch 196/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9329\n",
      "Epoch 197/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.9494\n",
      "Epoch 198/1500\n",
      "25/25 [==============================] - 0s 974us/step - loss: 0.1532 - accuracy: 0.9392\n",
      "Epoch 199/1500\n",
      "25/25 [==============================] - 0s 982us/step - loss: 0.1668 - accuracy: 0.9354\n",
      "Epoch 200/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.9468\n",
      "Epoch 201/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9380\n",
      "Epoch 202/1500\n",
      "25/25 [==============================] - 0s 961us/step - loss: 0.1419 - accuracy: 0.9570\n",
      "Epoch 203/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9456\n",
      "Epoch 204/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9329\n",
      "Epoch 205/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9544\n",
      "Epoch 206/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9684\n",
      "Epoch 207/1500\n",
      "25/25 [==============================] - 0s 989us/step - loss: 0.1703 - accuracy: 0.9342\n",
      "Epoch 208/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1351 - accuracy: 0.9494\n",
      "Epoch 209/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9570\n",
      "Epoch 210/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9456\n",
      "Epoch 211/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9506\n",
      "Epoch 212/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9481\n",
      "Epoch 213/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9506\n",
      "Epoch 214/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9532\n",
      "Epoch 215/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9595\n",
      "Epoch 216/1500\n",
      "25/25 [==============================] - 0s 984us/step - loss: 0.1239 - accuracy: 0.9557\n",
      "Epoch 217/1500\n",
      "25/25 [==============================] - 0s 995us/step - loss: 0.1480 - accuracy: 0.9430\n",
      "Epoch 218/1500\n",
      "25/25 [==============================] - 0s 968us/step - loss: 0.1374 - accuracy: 0.9519\n",
      "Epoch 219/1500\n",
      "25/25 [==============================] - 0s 996us/step - loss: 0.1402 - accuracy: 0.9456\n",
      "Epoch 220/1500\n",
      "25/25 [==============================] - 0s 992us/step - loss: 0.1404 - accuracy: 0.9494\n",
      "Epoch 221/1500\n",
      "25/25 [==============================] - 0s 987us/step - loss: 0.1388 - accuracy: 0.9443\n",
      "Epoch 222/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9671\n",
      "Epoch 223/1500\n",
      "25/25 [==============================] - 0s 990us/step - loss: 0.1167 - accuracy: 0.9658\n",
      "Epoch 224/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9506\n",
      "Epoch 225/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9620\n",
      "Epoch 226/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9570\n",
      "Epoch 227/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9582\n",
      "Epoch 228/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9658\n",
      "Epoch 229/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9709\n",
      "Epoch 230/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9582\n",
      "Epoch 231/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9506\n",
      "Epoch 232/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9696\n",
      "Epoch 233/1500\n",
      "25/25 [==============================] - 0s 999us/step - loss: 0.1229 - accuracy: 0.9519\n",
      "Epoch 234/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9506\n",
      "Epoch 235/1500\n",
      "25/25 [==============================] - 0s 995us/step - loss: 0.1219 - accuracy: 0.9633\n",
      "Epoch 236/1500\n",
      "25/25 [==============================] - 0s 970us/step - loss: 0.1425 - accuracy: 0.9456\n",
      "Epoch 237/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9506\n",
      "Epoch 238/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9570\n",
      "Epoch 239/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9405\n",
      "Epoch 240/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9481\n",
      "Epoch 241/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9620\n",
      "Epoch 242/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9696\n",
      "Epoch 243/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9608\n",
      "Epoch 244/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9456\n",
      "Epoch 245/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9709\n",
      "Epoch 246/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.1080 - accuracy: 0.9671\n",
      "Epoch 247/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9658\n",
      "Epoch 248/1500\n",
      "25/25 [==============================] - 0s 977us/step - loss: 0.1324 - accuracy: 0.9544\n",
      "Epoch 249/1500\n",
      "25/25 [==============================] - 0s 996us/step - loss: 0.1332 - accuracy: 0.9494\n",
      "Epoch 250/1500\n",
      "25/25 [==============================] - 0s 972us/step - loss: 0.1234 - accuracy: 0.9557\n",
      "Epoch 251/1500\n",
      "25/25 [==============================] - 0s 969us/step - loss: 0.1081 - accuracy: 0.9658\n",
      "Epoch 252/1500\n",
      "25/25 [==============================] - 0s 993us/step - loss: 0.1238 - accuracy: 0.9582\n",
      "Epoch 253/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9468\n",
      "Epoch 254/1500\n",
      "25/25 [==============================] - 0s 979us/step - loss: 0.1284 - accuracy: 0.9494\n",
      "Epoch 255/1500\n",
      "25/25 [==============================] - 0s 993us/step - loss: 0.1249 - accuracy: 0.9582\n",
      "Epoch 256/1500\n",
      "25/25 [==============================] - 0s 996us/step - loss: 0.1250 - accuracy: 0.9582\n",
      "Epoch 257/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9557\n",
      "Epoch 258/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9582\n",
      "Epoch 259/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9481\n",
      "Epoch 260/1500\n",
      "25/25 [==============================] - 0s 5ms/step - loss: 0.1395 - accuracy: 0.9582\n",
      "Epoch 261/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1136 - accuracy: 0.9633\n",
      "Epoch 262/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.9494\n",
      "Epoch 263/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9557\n",
      "Epoch 264/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9582\n",
      "Epoch 265/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9557\n",
      "Epoch 266/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9658\n",
      "Epoch 267/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9646\n",
      "Epoch 268/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1388 - accuracy: 0.9380\n",
      "Epoch 269/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9633\n",
      "Epoch 270/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9557\n",
      "Epoch 271/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9709\n",
      "Epoch 272/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9595\n",
      "Epoch 273/1500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0992 - accuracy: 0.9658\n",
      "Epoch 274/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1112 - accuracy: 0.9595\n",
      "Epoch 275/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9671\n",
      "Epoch 276/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9494\n",
      "Epoch 277/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9544\n",
      "Epoch 278/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9595\n",
      "Epoch 279/1500\n",
      "25/25 [==============================] - 0s 992us/step - loss: 0.1131 - accuracy: 0.9633\n",
      "Epoch 280/1500\n",
      "25/25 [==============================] - 0s 992us/step - loss: 0.1167 - accuracy: 0.9506\n",
      "Epoch 281/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9709\n",
      "Epoch 282/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9658\n",
      "Epoch 283/1500\n",
      "25/25 [==============================] - 0s 976us/step - loss: 0.1066 - accuracy: 0.9671\n",
      "Epoch 284/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9506\n",
      "Epoch 285/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9747\n",
      "Epoch 286/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9747\n",
      "Epoch 287/1500\n",
      "25/25 [==============================] - 0s 976us/step - loss: 0.1196 - accuracy: 0.9570\n",
      "Epoch 288/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9418\n",
      "Epoch 289/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9684\n",
      "Epoch 290/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9671\n",
      "Epoch 291/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9709\n",
      "Epoch 292/1500\n",
      "25/25 [==============================] - 0s 972us/step - loss: 0.0963 - accuracy: 0.9696\n",
      "Epoch 293/1500\n",
      "25/25 [==============================] - 0s 978us/step - loss: 0.0981 - accuracy: 0.9684\n",
      "Epoch 294/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9696\n",
      "Epoch 295/1500\n",
      "25/25 [==============================] - 0s 946us/step - loss: 0.0856 - accuracy: 0.9722\n",
      "Epoch 296/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9582\n",
      "Epoch 297/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.1182 - accuracy: 0.9582\n",
      "Epoch 298/1500\n",
      "25/25 [==============================] - 0s 962us/step - loss: 0.0871 - accuracy: 0.9772\n",
      "Epoch 299/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9671\n",
      "Epoch 300/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9608\n",
      "Epoch 301/1500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.0887 - accuracy: 0.9734\n",
      "Epoch 302/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9734\n",
      "Epoch 303/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0994 - accuracy: 0.9696\n",
      "Epoch 304/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9620\n",
      "Epoch 305/1500\n",
      "25/25 [==============================] - 0s 995us/step - loss: 0.1075 - accuracy: 0.9608\n",
      "Epoch 306/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9671\n",
      "Epoch 307/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9671\n",
      "Epoch 308/1500\n",
      "25/25 [==============================] - 0s 968us/step - loss: 0.1100 - accuracy: 0.9582\n",
      "Epoch 309/1500\n",
      "25/25 [==============================] - 0s 988us/step - loss: 0.0935 - accuracy: 0.9734\n",
      "Epoch 310/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.1044 - accuracy: 0.9696\n",
      "Epoch 311/1500\n",
      "25/25 [==============================] - 0s 961us/step - loss: 0.0882 - accuracy: 0.9759\n",
      "Epoch 312/1500\n",
      "25/25 [==============================] - 0s 970us/step - loss: 0.0848 - accuracy: 0.9734\n",
      "Epoch 313/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9797\n",
      "Epoch 314/1500\n",
      "25/25 [==============================] - 0s 989us/step - loss: 0.0766 - accuracy: 0.9772\n",
      "Epoch 315/1500\n",
      "25/25 [==============================] - 0s 999us/step - loss: 0.0879 - accuracy: 0.9722\n",
      "Epoch 316/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.0838 - accuracy: 0.9772\n",
      "Epoch 317/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9709\n",
      "Epoch 318/1500\n",
      "25/25 [==============================] - 0s 989us/step - loss: 0.1076 - accuracy: 0.9684\n",
      "Epoch 319/1500\n",
      "25/25 [==============================] - 0s 991us/step - loss: 0.1020 - accuracy: 0.9620\n",
      "Epoch 320/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9709\n",
      "Epoch 321/1500\n",
      "25/25 [==============================] - 0s 968us/step - loss: 0.0927 - accuracy: 0.9595\n",
      "Epoch 322/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.0738 - accuracy: 0.9772\n",
      "Epoch 323/1500\n",
      "25/25 [==============================] - 0s 985us/step - loss: 0.0772 - accuracy: 0.9772\n",
      "Epoch 324/1500\n",
      "25/25 [==============================] - 0s 984us/step - loss: 0.0979 - accuracy: 0.9646\n",
      "Epoch 325/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.9658\n",
      "Epoch 326/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9772\n",
      "Epoch 327/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9810\n",
      "Epoch 328/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9620\n",
      "Epoch 329/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9684\n",
      "Epoch 330/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9658\n",
      "Epoch 331/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9696\n",
      "Epoch 332/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9557\n",
      "Epoch 333/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9709\n",
      "Epoch 334/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9633\n",
      "Epoch 335/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9633\n",
      "Epoch 336/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9595\n",
      "Epoch 337/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9633\n",
      "Epoch 338/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9671\n",
      "Epoch 339/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9658\n",
      "Epoch 340/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9810\n",
      "Epoch 341/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9684\n",
      "Epoch 342/1500\n",
      "25/25 [==============================] - 0s 977us/step - loss: 0.1128 - accuracy: 0.9608\n",
      "Epoch 343/1500\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.1427 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 313.\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9722\n",
      "Epoch 343: early stopping\n",
      "5/5 [==============================] - 0s 908us/step - loss: 0.5863 - accuracy: 0.8095\n",
      "5/5 [==============================] - 0s 725us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (19/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "Final Test Results - Loss: 0.5863332152366638, Accuracy: 0.8095238208770752, Precision: 0.6966726084373143, Recall: 0.848391812865497, F1 Score: 0.7480936746080737\n",
      "Confusion Matrix:\n",
      " [[97  3 20]\n",
      " [ 0  8  0]\n",
      " [ 5  0 14]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "042A    14\n",
      "059A    14\n",
      "001A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "025A    11\n",
      "036A    11\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "065A     9\n",
      "051B     9\n",
      "072A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "099A     7\n",
      "008A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "026A     4\n",
      "009A     4\n",
      "105A     4\n",
      "035A     4\n",
      "003A     4\n",
      "104A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "014A     3\n",
      "012A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "018A     2\n",
      "032A     2\n",
      "093A     2\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "048A     1\n",
      "096A     1\n",
      "088A     1\n",
      "076A     1\n",
      "091A     1\n",
      "115A     1\n",
      "110A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "097B    14\n",
      "068A    11\n",
      "015A     9\n",
      "045A     9\n",
      "033A     9\n",
      "109A     6\n",
      "053A     6\n",
      "075A     5\n",
      "025C     5\n",
      "062A     4\n",
      "060A     3\n",
      "025B     2\n",
      "087A     2\n",
      "038A     2\n",
      "054A     2\n",
      "041A     1\n",
      "043A     1\n",
      "026C     1\n",
      "066A     1\n",
      "004A     1\n",
      "019B     1\n",
      "090A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    299\n",
      "F    216\n",
      "M    214\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    123\n",
      "X     49\n",
      "F     36\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 071A, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 050...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 097B, 067A, 020A, 062A, 002B, 029...\n",
      "kitten                             [109A, 043A, 041A, 045A]\n",
      "senior                             [055A, 054A, 090A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "Length of X_train_val:\n",
      "729\n",
      "Length of y_train_val:\n",
      "729\n",
      "Length of groups_train_val:\n",
      "729\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 421, 1: 154, 2: 154})\n",
      "Epoch 1/1500\n",
      "23/23 [==============================] - 1s 2ms/step - loss: 1.1311 - accuracy: 0.4787\n",
      "Epoch 2/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.9948 - accuracy: 0.5720\n",
      "Epoch 3/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8155 - accuracy: 0.6708\n",
      "Epoch 4/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7854 - accuracy: 0.6639\n",
      "Epoch 5/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7525 - accuracy: 0.6763\n",
      "Epoch 6/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6972 - accuracy: 0.7133\n",
      "Epoch 7/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6311 - accuracy: 0.7449\n",
      "Epoch 8/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6843 - accuracy: 0.7339\n",
      "Epoch 9/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6512 - accuracy: 0.7270\n",
      "Epoch 10/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6231 - accuracy: 0.7421\n",
      "Epoch 11/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5820 - accuracy: 0.7613\n",
      "Epoch 12/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5714 - accuracy: 0.7668\n",
      "Epoch 13/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5710 - accuracy: 0.7723\n",
      "Epoch 14/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5923 - accuracy: 0.7558\n",
      "Epoch 15/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5293 - accuracy: 0.7915\n",
      "Epoch 16/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5286 - accuracy: 0.7764\n",
      "Epoch 17/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5017 - accuracy: 0.7860\n",
      "Epoch 18/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5128 - accuracy: 0.7929\n",
      "Epoch 19/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4923 - accuracy: 0.8121\n",
      "Epoch 20/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4771 - accuracy: 0.8093\n",
      "Epoch 21/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4822 - accuracy: 0.8093\n",
      "Epoch 22/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4834 - accuracy: 0.7956\n",
      "Epoch 23/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4558 - accuracy: 0.8258\n",
      "Epoch 24/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4193 - accuracy: 0.8285\n",
      "Epoch 25/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4424 - accuracy: 0.8189\n",
      "Epoch 26/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4252 - accuracy: 0.8258\n",
      "Epoch 27/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3994 - accuracy: 0.8450\n",
      "Epoch 28/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4456 - accuracy: 0.8272\n",
      "Epoch 29/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4165 - accuracy: 0.8258\n",
      "Epoch 30/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4224 - accuracy: 0.8203\n",
      "Epoch 31/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8436\n",
      "Epoch 32/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4059 - accuracy: 0.8326\n",
      "Epoch 33/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4099 - accuracy: 0.8299\n",
      "Epoch 34/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3990 - accuracy: 0.8436\n",
      "Epoch 35/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3670 - accuracy: 0.8505\n",
      "Epoch 36/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4039 - accuracy: 0.8450\n",
      "Epoch 37/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3572 - accuracy: 0.8615\n",
      "Epoch 38/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3887 - accuracy: 0.8368\n",
      "Epoch 39/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3646 - accuracy: 0.8642\n",
      "Epoch 40/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3973 - accuracy: 0.8368\n",
      "Epoch 41/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3997 - accuracy: 0.8354\n",
      "Epoch 42/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3405 - accuracy: 0.8615\n",
      "Epoch 43/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3246 - accuracy: 0.8669\n",
      "Epoch 44/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3785 - accuracy: 0.8436\n",
      "Epoch 45/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3427 - accuracy: 0.8615\n",
      "Epoch 46/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3688 - accuracy: 0.8519\n",
      "Epoch 47/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3657 - accuracy: 0.8477\n",
      "Epoch 48/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3396 - accuracy: 0.8642\n",
      "Epoch 49/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3180 - accuracy: 0.8765\n",
      "Epoch 50/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3437 - accuracy: 0.8711\n",
      "Epoch 51/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3599 - accuracy: 0.8642\n",
      "Epoch 52/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8656\n",
      "Epoch 53/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3060 - accuracy: 0.8793\n",
      "Epoch 54/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3271 - accuracy: 0.8656\n",
      "Epoch 55/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3306 - accuracy: 0.8642\n",
      "Epoch 56/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2994 - accuracy: 0.8820\n",
      "Epoch 57/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2936 - accuracy: 0.8889\n",
      "Epoch 58/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3020 - accuracy: 0.8752\n",
      "Epoch 59/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2816 - accuracy: 0.8944\n",
      "Epoch 60/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2888 - accuracy: 0.8807\n",
      "Epoch 61/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3028 - accuracy: 0.8697\n",
      "Epoch 62/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2863 - accuracy: 0.8889\n",
      "Epoch 63/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.8848\n",
      "Epoch 64/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2859 - accuracy: 0.8820\n",
      "Epoch 65/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2716 - accuracy: 0.8889\n",
      "Epoch 66/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3071 - accuracy: 0.8724\n",
      "Epoch 67/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3153 - accuracy: 0.8793\n",
      "Epoch 68/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2995 - accuracy: 0.8779\n",
      "Epoch 69/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2941 - accuracy: 0.8889\n",
      "Epoch 70/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2766 - accuracy: 0.8971\n",
      "Epoch 71/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2932 - accuracy: 0.8848\n",
      "Epoch 72/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2666 - accuracy: 0.8916\n",
      "Epoch 73/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2901 - accuracy: 0.8820\n",
      "Epoch 74/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2866 - accuracy: 0.8820\n",
      "Epoch 75/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2656 - accuracy: 0.8861\n",
      "Epoch 76/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2673 - accuracy: 0.8957\n",
      "Epoch 77/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2808 - accuracy: 0.8999\n",
      "Epoch 78/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2709 - accuracy: 0.9053\n",
      "Epoch 79/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.8957\n",
      "Epoch 80/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2656 - accuracy: 0.9012\n",
      "Epoch 81/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2547 - accuracy: 0.8903\n",
      "Epoch 82/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2333 - accuracy: 0.9067\n",
      "Epoch 83/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2552 - accuracy: 0.9040\n",
      "Epoch 84/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.8985\n",
      "Epoch 85/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9053\n",
      "Epoch 86/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2281 - accuracy: 0.9150\n",
      "Epoch 87/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2421 - accuracy: 0.9136\n",
      "Epoch 88/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.9095\n",
      "Epoch 89/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2474 - accuracy: 0.9012\n",
      "Epoch 90/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2434 - accuracy: 0.9067\n",
      "Epoch 91/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2346 - accuracy: 0.8944\n",
      "Epoch 92/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2491 - accuracy: 0.9067\n",
      "Epoch 93/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2370 - accuracy: 0.9122\n",
      "Epoch 94/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9095\n",
      "Epoch 95/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2330 - accuracy: 0.9081\n",
      "Epoch 96/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2749 - accuracy: 0.8971\n",
      "Epoch 97/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9108\n",
      "Epoch 98/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2406 - accuracy: 0.9053\n",
      "Epoch 99/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2835 - accuracy: 0.8820\n",
      "Epoch 100/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9204\n",
      "Epoch 101/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2056 - accuracy: 0.9191\n",
      "Epoch 102/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9163\n",
      "Epoch 103/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2069 - accuracy: 0.9287\n",
      "Epoch 104/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2140 - accuracy: 0.9328\n",
      "Epoch 105/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2029 - accuracy: 0.9273\n",
      "Epoch 106/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2364 - accuracy: 0.9026\n",
      "Epoch 107/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2144 - accuracy: 0.9150\n",
      "Epoch 108/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.9342\n",
      "Epoch 109/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.8985\n",
      "Epoch 110/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1955 - accuracy: 0.9273\n",
      "Epoch 111/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2323 - accuracy: 0.9163\n",
      "Epoch 112/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2336 - accuracy: 0.9040\n",
      "Epoch 113/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2104 - accuracy: 0.9150\n",
      "Epoch 114/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2475 - accuracy: 0.9095\n",
      "Epoch 115/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2141 - accuracy: 0.9163\n",
      "Epoch 116/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2065 - accuracy: 0.9218\n",
      "Epoch 117/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2120 - accuracy: 0.9163\n",
      "Epoch 118/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2010 - accuracy: 0.9355\n",
      "Epoch 119/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1988 - accuracy: 0.9259\n",
      "Epoch 120/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.9204\n",
      "Epoch 121/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9218\n",
      "Epoch 122/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1888 - accuracy: 0.9232\n",
      "Epoch 123/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2136 - accuracy: 0.9204\n",
      "Epoch 124/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.9287\n",
      "Epoch 125/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9328\n",
      "Epoch 126/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9300\n",
      "Epoch 127/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9369\n",
      "Epoch 128/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9246\n",
      "Epoch 129/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1952 - accuracy: 0.9246\n",
      "Epoch 130/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9218\n",
      "Epoch 131/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9204\n",
      "Epoch 132/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1928 - accuracy: 0.9191\n",
      "Epoch 133/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9300\n",
      "Epoch 134/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9287\n",
      "Epoch 135/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2047 - accuracy: 0.9232\n",
      "Epoch 136/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9342\n",
      "Epoch 137/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1963 - accuracy: 0.9204\n",
      "Epoch 138/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9081\n",
      "Epoch 139/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.9300\n",
      "Epoch 140/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2177 - accuracy: 0.9081\n",
      "Epoch 141/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1964 - accuracy: 0.9177\n",
      "Epoch 142/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1699 - accuracy: 0.9342\n",
      "Epoch 143/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9355\n",
      "Epoch 144/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.9273\n",
      "Epoch 145/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9424\n",
      "Epoch 146/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9369\n",
      "Epoch 147/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9520\n",
      "Epoch 148/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9383\n",
      "Epoch 149/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.9287\n",
      "Epoch 150/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1657 - accuracy: 0.9396\n",
      "Epoch 151/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1786 - accuracy: 0.9314\n",
      "Epoch 152/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1935 - accuracy: 0.9383\n",
      "Epoch 153/1500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1662 - accuracy: 0.9369\n",
      "Epoch 154/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1611 - accuracy: 0.9424\n",
      "Epoch 155/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9396\n",
      "Epoch 156/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.9369\n",
      "Epoch 157/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9506\n",
      "Epoch 158/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9204\n",
      "Epoch 159/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9465\n",
      "Epoch 160/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.9287\n",
      "Epoch 161/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9218\n",
      "Epoch 162/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9383\n",
      "Epoch 163/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1648 - accuracy: 0.9355\n",
      "Epoch 164/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9355\n",
      "Epoch 165/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1746 - accuracy: 0.9287\n",
      "Epoch 166/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9438\n",
      "Epoch 167/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.9246\n",
      "Epoch 168/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9506\n",
      "Epoch 169/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.9246\n",
      "Epoch 170/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9287\n",
      "Epoch 171/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9424\n",
      "Epoch 172/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9396\n",
      "Epoch 173/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1659 - accuracy: 0.9410\n",
      "Epoch 174/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9383\n",
      "Epoch 175/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9383\n",
      "Epoch 176/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.9369\n",
      "Epoch 177/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9451\n",
      "Epoch 178/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9506\n",
      "Epoch 179/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9547\n",
      "Epoch 180/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9369\n",
      "Epoch 181/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1592 - accuracy: 0.9451\n",
      "Epoch 182/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9396\n",
      "Epoch 183/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9465\n",
      "Epoch 184/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9424\n",
      "Epoch 185/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.9438\n",
      "Epoch 186/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9465\n",
      "Epoch 187/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9520\n",
      "Epoch 188/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9300\n",
      "Epoch 189/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9465\n",
      "Epoch 190/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1586 - accuracy: 0.9396\n",
      "Epoch 191/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9369\n",
      "Epoch 192/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9438\n",
      "Epoch 193/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1558 - accuracy: 0.9355\n",
      "Epoch 194/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9520\n",
      "Epoch 195/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9506\n",
      "Epoch 196/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9479\n",
      "Epoch 197/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9492\n",
      "Epoch 198/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9465\n",
      "Epoch 199/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.9520\n",
      "Epoch 200/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9479\n",
      "Epoch 201/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.9534\n",
      "Epoch 202/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.9287\n",
      "Epoch 203/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9506\n",
      "Epoch 204/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.9520\n",
      "Epoch 205/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1518 - accuracy: 0.9451\n",
      "Epoch 206/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9479\n",
      "Epoch 207/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9438\n",
      "Epoch 208/1500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.1449 - accuracy: 0.9375Restoring model weights from the end of the best epoch: 178.\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9547\n",
      "Epoch 208: early stopping\n",
      "7/7 [==============================] - 0s 960us/step - loss: 0.5088 - accuracy: 0.7837\n",
      "7/7 [==============================] - 0s 720us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "After appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "Final Test Results - Loss: 0.5087935924530029, Accuracy: 0.7836538553237915, Precision: 0.654054054054054, Recall: 0.7384153262103245, F1 Score: 0.6837998712998713\n",
      "Confusion Matrix:\n",
      " [[135   6  26]\n",
      " [  3  14   0]\n",
      " [ 10   0  14]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "016A    10\n",
      "071A    10\n",
      "065A     9\n",
      "045A     9\n",
      "022A     9\n",
      "015A     9\n",
      "033A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "031A     7\n",
      "117A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "053A     6\n",
      "075A     5\n",
      "044A     5\n",
      "021A     5\n",
      "025C     5\n",
      "034A     5\n",
      "023B     5\n",
      "009A     4\n",
      "003A     4\n",
      "104A     4\n",
      "062A     4\n",
      "113A     3\n",
      "014A     3\n",
      "056A     3\n",
      "060A     3\n",
      "064A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "018A     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "011A     2\n",
      "100A     1\n",
      "090A     1\n",
      "115A     1\n",
      "088A     1\n",
      "024A     1\n",
      "019B     1\n",
      "096A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "043A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "116A    12\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "037A     6\n",
      "023A     6\n",
      "070A     5\n",
      "052A     4\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "012A     3\n",
      "006A     3\n",
      "058A     3\n",
      "032A     2\n",
      "076A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    235\n",
      "X    186\n",
      "F    169\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    162\n",
      "M    102\n",
      "F     83\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 071A, 097B, 028A, 019A, 074A, 067...\n",
      "kitten    [044A, 047A, 109A, 050A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 054A, 117...\n",
      "Name: cat_id, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 095A, 072A, 023A, 027...\n",
      "kitten                 [014B, 111A, 040A, 046A, 042A, 110A]\n",
      "senior                       [057A, 106A, 116A, 051B, 058A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A' '010A'\n",
      " '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '031A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '047A' '048A'\n",
      " '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A' '073A'\n",
      " '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A' '096A'\n",
      " '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '109A' '113A'\n",
      " '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A'\n",
      " '032A' '035A' '037A' '040A' '042A' '046A' '051B' '052A' '057A' '058A'\n",
      " '070A' '072A' '076A' '095A' '103A' '105A' '106A' '110A' '111A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A', '046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'038A', '109A'}\n",
      "Moved to Test Set:\n",
      "{'038A', '109A'}\n",
      "Removed from Test Set\n",
      "{'000A', '046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A'\n",
      " '010A' '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '031A'\n",
      " '033A' '034A' '036A' '039A' '041A' '043A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A'\n",
      " '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A'\n",
      " '073A' '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '096A' '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '113A'\n",
      " '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A' '032A'\n",
      " '035A' '037A' '038A' '040A' '042A' '051B' '052A' '057A' '058A' '070A'\n",
      " '072A' '076A' '095A' '103A' '105A' '106A' '109A' '110A' '111A' '116A']\n",
      "Length of X_train_val:\n",
      "684\n",
      "Length of y_train_val:\n",
      "684\n",
      "Length of groups_train_val:\n",
      "684\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     417\n",
      "senior    113\n",
      "kitten     60\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     171\n",
      "kitten    111\n",
      "senior     65\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     454\n",
      "kitten    117\n",
      "senior    113\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     134\n",
      "senior     65\n",
      "kitten     54\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 454, 1: 117, 2: 113})\n",
      "Epoch 1/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.3387 - accuracy: 0.4313\n",
      "Epoch 2/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0782 - accuracy: 0.5570\n",
      "Epoch 3/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9405 - accuracy: 0.6009\n",
      "Epoch 4/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8620 - accuracy: 0.6477\n",
      "Epoch 5/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8130 - accuracy: 0.6623\n",
      "Epoch 6/1500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.7592 - accuracy: 0.6886\n",
      "Epoch 7/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7308 - accuracy: 0.6930\n",
      "Epoch 8/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6751 - accuracy: 0.7412\n",
      "Epoch 9/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6756 - accuracy: 0.7222\n",
      "Epoch 10/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6346 - accuracy: 0.7471\n",
      "Epoch 11/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6643 - accuracy: 0.7412\n",
      "Epoch 12/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5927 - accuracy: 0.7529\n",
      "Epoch 13/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5619 - accuracy: 0.7939\n",
      "Epoch 14/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5678 - accuracy: 0.7734\n",
      "Epoch 15/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5230 - accuracy: 0.7924\n",
      "Epoch 16/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5087 - accuracy: 0.8085\n",
      "Epoch 17/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5137 - accuracy: 0.8129\n",
      "Epoch 18/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5271 - accuracy: 0.7865\n",
      "Epoch 19/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4958 - accuracy: 0.8085\n",
      "Epoch 20/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4532 - accuracy: 0.8216\n",
      "Epoch 21/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.8231\n",
      "Epoch 22/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5025 - accuracy: 0.8143\n",
      "Epoch 23/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4472 - accuracy: 0.8158\n",
      "Epoch 24/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4234 - accuracy: 0.8348\n",
      "Epoch 25/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4126 - accuracy: 0.8523\n",
      "Epoch 26/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4027 - accuracy: 0.8421\n",
      "Epoch 27/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3970 - accuracy: 0.8421\n",
      "Epoch 28/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4432 - accuracy: 0.8260\n",
      "Epoch 29/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3896 - accuracy: 0.8421\n",
      "Epoch 30/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4036 - accuracy: 0.8553\n",
      "Epoch 31/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4001 - accuracy: 0.8450\n",
      "Epoch 32/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3777 - accuracy: 0.8494\n",
      "Epoch 33/1500\n",
      "22/22 [==============================] - 0s 1000us/step - loss: 0.4258 - accuracy: 0.8333\n",
      "Epoch 34/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3554 - accuracy: 0.8772\n",
      "Epoch 35/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8684\n",
      "Epoch 36/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3925 - accuracy: 0.8538\n",
      "Epoch 37/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3704 - accuracy: 0.8538\n",
      "Epoch 38/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3466 - accuracy: 0.8480\n",
      "Epoch 39/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3346 - accuracy: 0.8743\n",
      "Epoch 40/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3705 - accuracy: 0.8465\n",
      "Epoch 41/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3356 - accuracy: 0.8743\n",
      "Epoch 42/1500\n",
      "22/22 [==============================] - 0s 964us/step - loss: 0.3928 - accuracy: 0.8494\n",
      "Epoch 43/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3610 - accuracy: 0.8596\n",
      "Epoch 44/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3111 - accuracy: 0.8801\n",
      "Epoch 45/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3160 - accuracy: 0.8801\n",
      "Epoch 46/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3097 - accuracy: 0.8874\n",
      "Epoch 47/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3447 - accuracy: 0.8699\n",
      "Epoch 48/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3222 - accuracy: 0.8801\n",
      "Epoch 49/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3032 - accuracy: 0.8933\n",
      "Epoch 50/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3226 - accuracy: 0.8757\n",
      "Epoch 51/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3000 - accuracy: 0.8787\n",
      "Epoch 52/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3019 - accuracy: 0.8845\n",
      "Epoch 53/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2972 - accuracy: 0.9020\n",
      "Epoch 54/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2837 - accuracy: 0.8933\n",
      "Epoch 55/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2905 - accuracy: 0.8904\n",
      "Epoch 56/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3043 - accuracy: 0.8860\n",
      "Epoch 57/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3079 - accuracy: 0.8772\n",
      "Epoch 58/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3073 - accuracy: 0.8801\n",
      "Epoch 59/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2833 - accuracy: 0.8977\n",
      "Epoch 60/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2894 - accuracy: 0.8947\n",
      "Epoch 61/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2752 - accuracy: 0.8977\n",
      "Epoch 62/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2979 - accuracy: 0.8977\n",
      "Epoch 63/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2827 - accuracy: 0.8962\n",
      "Epoch 64/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2929 - accuracy: 0.8860\n",
      "Epoch 65/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.9006\n",
      "Epoch 66/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2602 - accuracy: 0.9006\n",
      "Epoch 67/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2726 - accuracy: 0.9006\n",
      "Epoch 68/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2906 - accuracy: 0.8787\n",
      "Epoch 69/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3010 - accuracy: 0.8933\n",
      "Epoch 70/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2377 - accuracy: 0.9094\n",
      "Epoch 71/1500\n",
      "22/22 [==============================] - 0s 993us/step - loss: 0.2571 - accuracy: 0.8918\n",
      "Epoch 72/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2787 - accuracy: 0.9035\n",
      "Epoch 73/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2518 - accuracy: 0.9064\n",
      "Epoch 74/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2636 - accuracy: 0.9108\n",
      "Epoch 75/1500\n",
      "22/22 [==============================] - 0s 973us/step - loss: 0.2624 - accuracy: 0.9064\n",
      "Epoch 76/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2851 - accuracy: 0.8728\n",
      "Epoch 77/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2828 - accuracy: 0.8947\n",
      "Epoch 78/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2381 - accuracy: 0.9137\n",
      "Epoch 79/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2247 - accuracy: 0.9167\n",
      "Epoch 80/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2473 - accuracy: 0.9152\n",
      "Epoch 81/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2265 - accuracy: 0.9123\n",
      "Epoch 82/1500\n",
      "22/22 [==============================] - 0s 987us/step - loss: 0.2338 - accuracy: 0.9137\n",
      "Epoch 83/1500\n",
      "22/22 [==============================] - 0s 985us/step - loss: 0.2301 - accuracy: 0.9035\n",
      "Epoch 84/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9094\n",
      "Epoch 85/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9137\n",
      "Epoch 86/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2200 - accuracy: 0.9167\n",
      "Epoch 87/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2351 - accuracy: 0.9094\n",
      "Epoch 88/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2384 - accuracy: 0.9123\n",
      "Epoch 89/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2451 - accuracy: 0.9020\n",
      "Epoch 90/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2183 - accuracy: 0.9269\n",
      "Epoch 91/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9123\n",
      "Epoch 92/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2395 - accuracy: 0.9240\n",
      "Epoch 93/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2519 - accuracy: 0.9050\n",
      "Epoch 94/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2477 - accuracy: 0.9006\n",
      "Epoch 95/1500\n",
      "22/22 [==============================] - 0s 998us/step - loss: 0.2175 - accuracy: 0.9167\n",
      "Epoch 96/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9298\n",
      "Epoch 97/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9357\n",
      "Epoch 98/1500\n",
      "22/22 [==============================] - 0s 982us/step - loss: 0.2211 - accuracy: 0.9298\n",
      "Epoch 99/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2075 - accuracy: 0.9327\n",
      "Epoch 100/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1942 - accuracy: 0.9269\n",
      "Epoch 101/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9181\n",
      "Epoch 102/1500\n",
      "22/22 [==============================] - 0s 999us/step - loss: 0.1979 - accuracy: 0.9269\n",
      "Epoch 103/1500\n",
      "22/22 [==============================] - 0s 988us/step - loss: 0.2115 - accuracy: 0.9181\n",
      "Epoch 104/1500\n",
      "22/22 [==============================] - 0s 976us/step - loss: 0.1967 - accuracy: 0.9313\n",
      "Epoch 105/1500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.1762 - accuracy: 0.9430\n",
      "Epoch 106/1500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1952 - accuracy: 0.9211\n",
      "Epoch 107/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9327\n",
      "Epoch 108/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2104 - accuracy: 0.9181\n",
      "Epoch 109/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.9254\n",
      "Epoch 110/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9386\n",
      "Epoch 111/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1957 - accuracy: 0.9254\n",
      "Epoch 112/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2051 - accuracy: 0.9313\n",
      "Epoch 113/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9298\n",
      "Epoch 114/1500\n",
      "22/22 [==============================] - 0s 988us/step - loss: 0.2193 - accuracy: 0.9181\n",
      "Epoch 115/1500\n",
      "22/22 [==============================] - 0s 968us/step - loss: 0.1878 - accuracy: 0.9342\n",
      "Epoch 116/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.9444\n",
      "Epoch 117/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9371\n",
      "Epoch 118/1500\n",
      "22/22 [==============================] - 0s 953us/step - loss: 0.1870 - accuracy: 0.9342\n",
      "Epoch 119/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1912 - accuracy: 0.9386\n",
      "Epoch 120/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9357\n",
      "Epoch 121/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1761 - accuracy: 0.9430\n",
      "Epoch 122/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9459\n",
      "Epoch 123/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9576\n",
      "Epoch 124/1500\n",
      "22/22 [==============================] - 0s 979us/step - loss: 0.1773 - accuracy: 0.9371\n",
      "Epoch 125/1500\n",
      "22/22 [==============================] - 0s 951us/step - loss: 0.1684 - accuracy: 0.9401\n",
      "Epoch 126/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1779 - accuracy: 0.9371\n",
      "Epoch 127/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9415\n",
      "Epoch 128/1500\n",
      "22/22 [==============================] - 0s 976us/step - loss: 0.1860 - accuracy: 0.9342\n",
      "Epoch 129/1500\n",
      "22/22 [==============================] - 0s 979us/step - loss: 0.2061 - accuracy: 0.9225\n",
      "Epoch 130/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1623 - accuracy: 0.9488\n",
      "Epoch 131/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1676 - accuracy: 0.9313\n",
      "Epoch 132/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1549 - accuracy: 0.9430\n",
      "Epoch 133/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.9386\n",
      "Epoch 134/1500\n",
      "22/22 [==============================] - 0s 991us/step - loss: 0.1696 - accuracy: 0.9430\n",
      "Epoch 135/1500\n",
      "22/22 [==============================] - 0s 977us/step - loss: 0.1872 - accuracy: 0.9371\n",
      "Epoch 136/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9459\n",
      "Epoch 137/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9532\n",
      "Epoch 138/1500\n",
      "22/22 [==============================] - 0s 990us/step - loss: 0.1557 - accuracy: 0.9444\n",
      "Epoch 139/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9459\n",
      "Epoch 140/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.9313\n",
      "Epoch 141/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9284\n",
      "Epoch 142/1500\n",
      "22/22 [==============================] - 0s 957us/step - loss: 0.1665 - accuracy: 0.9444\n",
      "Epoch 143/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9357\n",
      "Epoch 144/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.9430\n",
      "Epoch 145/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.9225\n",
      "Epoch 146/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.9313\n",
      "Epoch 147/1500\n",
      "22/22 [==============================] - 0s 988us/step - loss: 0.1676 - accuracy: 0.9415\n",
      "Epoch 148/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9459\n",
      "Epoch 149/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9532\n",
      "Epoch 150/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1627 - accuracy: 0.9518\n",
      "Epoch 151/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.9386\n",
      "Epoch 152/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9547\n",
      "Epoch 153/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9474\n",
      "Epoch 154/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9591\n",
      "Epoch 155/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.9591\n",
      "Epoch 156/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9547\n",
      "Epoch 157/1500\n",
      "22/22 [==============================] - 0s 983us/step - loss: 0.1442 - accuracy: 0.9532\n",
      "Epoch 158/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9547\n",
      "Epoch 159/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9488\n",
      "Epoch 160/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.9415\n",
      "Epoch 161/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9561\n",
      "Epoch 162/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9576\n",
      "Epoch 163/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9503\n",
      "Epoch 164/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9591\n",
      "Epoch 165/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9576\n",
      "Epoch 166/1500\n",
      "22/22 [==============================] - 0s 983us/step - loss: 0.1562 - accuracy: 0.9415\n",
      "Epoch 167/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9518\n",
      "Epoch 168/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9518\n",
      "Epoch 169/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1640 - accuracy: 0.9342\n",
      "Epoch 170/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9430\n",
      "Epoch 171/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9474\n",
      "Epoch 172/1500\n",
      "22/22 [==============================] - 0s 983us/step - loss: 0.1372 - accuracy: 0.9518\n",
      "Epoch 173/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9591\n",
      "Epoch 174/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9342\n",
      "Epoch 175/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9415\n",
      "Epoch 176/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1397 - accuracy: 0.9561\n",
      "Epoch 177/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9444\n",
      "Epoch 178/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9561\n",
      "Epoch 179/1500\n",
      "22/22 [==============================] - 0s 974us/step - loss: 0.1241 - accuracy: 0.9635\n",
      "Epoch 180/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9591\n",
      "Epoch 181/1500\n",
      "22/22 [==============================] - 0s 983us/step - loss: 0.1569 - accuracy: 0.9430\n",
      "Epoch 182/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9693\n",
      "Epoch 183/1500\n",
      "22/22 [==============================] - 0s 970us/step - loss: 0.1305 - accuracy: 0.9576\n",
      "Epoch 184/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9547\n",
      "Epoch 185/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9620\n",
      "Epoch 186/1500\n",
      "22/22 [==============================] - 0s 998us/step - loss: 0.1247 - accuracy: 0.9620\n",
      "Epoch 187/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9620\n",
      "Epoch 188/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9459\n",
      "Epoch 189/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9576\n",
      "Epoch 190/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9444\n",
      "Epoch 191/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9503\n",
      "Epoch 192/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9664\n",
      "Epoch 193/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9635\n",
      "Epoch 194/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9547\n",
      "Epoch 195/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9503\n",
      "Epoch 196/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.9459\n",
      "Epoch 197/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9503\n",
      "Epoch 198/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1204 - accuracy: 0.9635\n",
      "Epoch 199/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9561\n",
      "Epoch 200/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9737\n",
      "Epoch 201/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9722\n",
      "Epoch 202/1500\n",
      "22/22 [==============================] - 0s 972us/step - loss: 0.1091 - accuracy: 0.9664\n",
      "Epoch 203/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9503\n",
      "Epoch 204/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9532\n",
      "Epoch 205/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9605\n",
      "Epoch 206/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9605\n",
      "Epoch 207/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9488\n",
      "Epoch 208/1500\n",
      "22/22 [==============================] - 0s 983us/step - loss: 0.1180 - accuracy: 0.9635\n",
      "Epoch 209/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9664\n",
      "Epoch 210/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9722\n",
      "Epoch 211/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9766\n",
      "Epoch 212/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9664\n",
      "Epoch 213/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9708\n",
      "Epoch 214/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9620\n",
      "Epoch 215/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9708\n",
      "Epoch 216/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.9649\n",
      "Epoch 217/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9722\n",
      "Epoch 218/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9576\n",
      "Epoch 219/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9576\n",
      "Epoch 220/1500\n",
      "22/22 [==============================] - 0s 963us/step - loss: 0.1061 - accuracy: 0.9664\n",
      "Epoch 221/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9649\n",
      "Epoch 222/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9635\n",
      "Epoch 223/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.9591\n",
      "Epoch 224/1500\n",
      "22/22 [==============================] - 0s 982us/step - loss: 0.0868 - accuracy: 0.9751\n",
      "Epoch 225/1500\n",
      "22/22 [==============================] - 0s 983us/step - loss: 0.0931 - accuracy: 0.9766\n",
      "Epoch 226/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9766\n",
      "Epoch 227/1500\n",
      "22/22 [==============================] - 0s 970us/step - loss: 0.0928 - accuracy: 0.9649\n",
      "Epoch 228/1500\n",
      "22/22 [==============================] - 0s 939us/step - loss: 0.1018 - accuracy: 0.9649\n",
      "Epoch 229/1500\n",
      "22/22 [==============================] - 0s 932us/step - loss: 0.1090 - accuracy: 0.9532\n",
      "Epoch 230/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9737\n",
      "Epoch 231/1500\n",
      "22/22 [==============================] - 0s 976us/step - loss: 0.0830 - accuracy: 0.9737\n",
      "Epoch 232/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9576\n",
      "Epoch 233/1500\n",
      "22/22 [==============================] - 0s 983us/step - loss: 0.0799 - accuracy: 0.9795\n",
      "Epoch 234/1500\n",
      "22/22 [==============================] - 0s 937us/step - loss: 0.0928 - accuracy: 0.9751\n",
      "Epoch 235/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9620\n",
      "Epoch 236/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9576\n",
      "Epoch 237/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.9678\n",
      "Epoch 238/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9708\n",
      "Epoch 239/1500\n",
      "22/22 [==============================] - 0s 994us/step - loss: 0.0812 - accuracy: 0.9751\n",
      "Epoch 240/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9737\n",
      "Epoch 241/1500\n",
      "22/22 [==============================] - 0s 1000us/step - loss: 0.1006 - accuracy: 0.9722\n",
      "Epoch 242/1500\n",
      "22/22 [==============================] - 0s 996us/step - loss: 0.1064 - accuracy: 0.9678\n",
      "Epoch 243/1500\n",
      "22/22 [==============================] - 0s 995us/step - loss: 0.0835 - accuracy: 0.9810\n",
      "Epoch 244/1500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0920 - accuracy: 0.9678\n",
      "Epoch 245/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9737\n",
      "Epoch 246/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9605\n",
      "Epoch 247/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9737\n",
      "Epoch 248/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0816 - accuracy: 0.9708\n",
      "Epoch 249/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.9635\n",
      "Epoch 250/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9591\n",
      "Epoch 251/1500\n",
      "22/22 [==============================] - 0s 973us/step - loss: 0.0792 - accuracy: 0.9781\n",
      "Epoch 252/1500\n",
      "22/22 [==============================] - 0s 995us/step - loss: 0.0863 - accuracy: 0.9664\n",
      "Epoch 253/1500\n",
      "22/22 [==============================] - 0s 985us/step - loss: 0.0926 - accuracy: 0.9678\n",
      "Epoch 254/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9781\n",
      "Epoch 255/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9693\n",
      "Epoch 256/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.9693\n",
      "Epoch 257/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9635\n",
      "Epoch 258/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9795\n",
      "Epoch 259/1500\n",
      "22/22 [==============================] - 0s 991us/step - loss: 0.1095 - accuracy: 0.9649\n",
      "Epoch 260/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9737\n",
      "Epoch 261/1500\n",
      "22/22 [==============================] - 0s 998us/step - loss: 0.0734 - accuracy: 0.9810\n",
      "Epoch 262/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9781\n",
      "Epoch 263/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9737\n",
      "Epoch 264/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.9664\n",
      "Epoch 265/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9737\n",
      "Epoch 266/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9591\n",
      "Epoch 267/1500\n",
      "22/22 [==============================] - 0s 994us/step - loss: 0.0854 - accuracy: 0.9766\n",
      "Epoch 268/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9737\n",
      "Epoch 269/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9781\n",
      "Epoch 270/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9664\n",
      "Epoch 271/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9678\n",
      "Epoch 272/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9693\n",
      "Epoch 273/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9795\n",
      "Epoch 274/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9751\n",
      "Epoch 275/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9635\n",
      "Epoch 276/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9737\n",
      "Epoch 277/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.9751\n",
      "Epoch 278/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.9883\n",
      "Epoch 279/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9649\n",
      "Epoch 280/1500\n",
      "22/22 [==============================] - 0s 895us/step - loss: 0.1029 - accuracy: 0.9693\n",
      "Epoch 281/1500\n",
      "22/22 [==============================] - 0s 969us/step - loss: 0.0864 - accuracy: 0.9708\n",
      "Epoch 282/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9766\n",
      "Epoch 283/1500\n",
      "22/22 [==============================] - 0s 958us/step - loss: 0.0801 - accuracy: 0.9781\n",
      "Epoch 284/1500\n",
      "22/22 [==============================] - 0s 892us/step - loss: 0.0669 - accuracy: 0.9839\n",
      "Epoch 285/1500\n",
      "22/22 [==============================] - 0s 975us/step - loss: 0.0926 - accuracy: 0.9693\n",
      "Epoch 286/1500\n",
      "22/22 [==============================] - 0s 961us/step - loss: 0.0922 - accuracy: 0.9708\n",
      "Epoch 287/1500\n",
      "22/22 [==============================] - 0s 975us/step - loss: 0.0753 - accuracy: 0.9766\n",
      "Epoch 288/1500\n",
      "22/22 [==============================] - 0s 976us/step - loss: 0.0917 - accuracy: 0.9678\n",
      "Epoch 289/1500\n",
      "22/22 [==============================] - 0s 971us/step - loss: 0.0770 - accuracy: 0.9795\n",
      "Epoch 290/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9781\n",
      "Epoch 291/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9751\n",
      "Epoch 292/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9635\n",
      "Epoch 293/1500\n",
      "22/22 [==============================] - 0s 893us/step - loss: 0.0844 - accuracy: 0.9795\n",
      "Epoch 294/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9722\n",
      "Epoch 295/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.9751\n",
      "Epoch 296/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.9810\n",
      "Epoch 297/1500\n",
      "22/22 [==============================] - 0s 978us/step - loss: 0.0808 - accuracy: 0.9781\n",
      "Epoch 298/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.9795\n",
      "Epoch 299/1500\n",
      "22/22 [==============================] - 0s 994us/step - loss: 0.0922 - accuracy: 0.9693\n",
      "Epoch 300/1500\n",
      "22/22 [==============================] - 0s 977us/step - loss: 0.0852 - accuracy: 0.9708\n",
      "Epoch 301/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9751\n",
      "Epoch 302/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9737\n",
      "Epoch 303/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.9781\n",
      "Epoch 304/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9678\n",
      "Epoch 305/1500\n",
      "22/22 [==============================] - 0s 976us/step - loss: 0.0898 - accuracy: 0.9635\n",
      "Epoch 306/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9664\n",
      "Epoch 307/1500\n",
      "22/22 [==============================] - 0s 943us/step - loss: 0.0838 - accuracy: 0.9751\n",
      "Epoch 308/1500\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.0517 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 278.\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.9781\n",
      "Epoch 308: early stopping\n",
      "8/8 [==============================] - 0s 723us/step - loss: 0.8381 - accuracy: 0.7391\n",
      "8/8 [==============================] - 0s 659us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (25/30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "After appending - Cat IDs: 608, Predictions: 608, Actuals: 608, Gender: 608\n",
      "Final Test Results - Loss: 0.8381426334381104, Accuracy: 0.739130437374115, Precision: 0.7708020637898687, Recall: 0.6976371702739862, F1 Score: 0.7246002674574105\n",
      "Confusion Matrix:\n",
      " [[114   3  17]\n",
      " [ 16  38   0]\n",
      " [ 30   0  35]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "097B    14\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "068A    11\n",
      "025A    11\n",
      "063A    11\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "005A    10\n",
      "072A     9\n",
      "065A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "099A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "062A     4\n",
      "009A     4\n",
      "012A     3\n",
      "058A     3\n",
      "060A     3\n",
      "006A     3\n",
      "056A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "032A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "066A     1\n",
      "026C     1\n",
      "076A     1\n",
      "096A     1\n",
      "041A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "047A    28\n",
      "074A    25\n",
      "000B    19\n",
      "097A    16\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "036A    11\n",
      "016A    10\n",
      "022A     9\n",
      "117A     7\n",
      "031A     7\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "003A     4\n",
      "064A     3\n",
      "113A     3\n",
      "093A     2\n",
      "069A     2\n",
      "073A     1\n",
      "091A     1\n",
      "092A     1\n",
      "048A     1\n",
      "115A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    266\n",
      "F    163\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    89\n",
      "X    75\n",
      "M    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [014B, 111A, 040A, 046A, 042A, 109A, 050A, 043...\n",
      "senior    [057A, 106A, 104A, 055A, 116A, 051B, 054A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [028A, 074A, 022A, 034A, 091A, 002A, 007A, 069...\n",
      "kitten                             [044A, 047A, 048A, 115A]\n",
      "senior     [093A, 097A, 059A, 113A, 117A, 051A, 016A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "Length of X_train_val:\n",
      "702\n",
      "Length of y_train_val:\n",
      "702\n",
      "Length of groups_train_val:\n",
      "702\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 458, 1: 136, 2: 108})\n",
      "Epoch 1/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.2005 - accuracy: 0.4601\n",
      "Epoch 2/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.0191 - accuracy: 0.5613\n",
      "Epoch 3/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8774 - accuracy: 0.6168\n",
      "Epoch 4/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8200 - accuracy: 0.6396\n",
      "Epoch 5/1500\n",
      "22/22 [==============================] - 0s 995us/step - loss: 0.8009 - accuracy: 0.6638\n",
      "Epoch 6/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7244 - accuracy: 0.6781\n",
      "Epoch 7/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7196 - accuracy: 0.6952\n",
      "Epoch 8/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7057 - accuracy: 0.7151\n",
      "Epoch 9/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6714 - accuracy: 0.7265\n",
      "Epoch 10/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6467 - accuracy: 0.7507\n",
      "Epoch 11/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6361 - accuracy: 0.7350\n",
      "Epoch 12/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5932 - accuracy: 0.7721\n",
      "Epoch 13/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5559 - accuracy: 0.7835\n",
      "Epoch 14/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5312 - accuracy: 0.7821\n",
      "Epoch 15/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5554 - accuracy: 0.7835\n",
      "Epoch 16/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5168 - accuracy: 0.7977\n",
      "Epoch 17/1500\n",
      "22/22 [==============================] - 0s 995us/step - loss: 0.4941 - accuracy: 0.8105\n",
      "Epoch 18/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4714 - accuracy: 0.8091\n",
      "Epoch 19/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5109 - accuracy: 0.7906\n",
      "Epoch 20/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4461 - accuracy: 0.8134\n",
      "Epoch 21/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4774 - accuracy: 0.7849\n",
      "Epoch 22/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4559 - accuracy: 0.8177\n",
      "Epoch 23/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4384 - accuracy: 0.8162\n",
      "Epoch 24/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4599 - accuracy: 0.7977\n",
      "Epoch 25/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4311 - accuracy: 0.8348\n",
      "Epoch 26/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4045 - accuracy: 0.8504\n",
      "Epoch 27/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4244 - accuracy: 0.8305\n",
      "Epoch 28/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4263 - accuracy: 0.8476\n",
      "Epoch 29/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3961 - accuracy: 0.8490\n",
      "Epoch 30/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4055 - accuracy: 0.8547\n",
      "Epoch 31/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3821 - accuracy: 0.8362\n",
      "Epoch 32/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3861 - accuracy: 0.8476\n",
      "Epoch 33/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3762 - accuracy: 0.8590\n",
      "Epoch 34/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3961 - accuracy: 0.8476\n",
      "Epoch 35/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3741 - accuracy: 0.8519\n",
      "Epoch 36/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3508 - accuracy: 0.8718\n",
      "Epoch 37/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3351 - accuracy: 0.8647\n",
      "Epoch 38/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3523 - accuracy: 0.8590\n",
      "Epoch 39/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3822 - accuracy: 0.8689\n",
      "Epoch 40/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3333 - accuracy: 0.8647\n",
      "Epoch 41/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3474 - accuracy: 0.8718\n",
      "Epoch 42/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3436 - accuracy: 0.8675\n",
      "Epoch 43/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3050 - accuracy: 0.8818\n",
      "Epoch 44/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3644 - accuracy: 0.8689\n",
      "Epoch 45/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3650 - accuracy: 0.8462\n",
      "Epoch 46/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3616 - accuracy: 0.8647\n",
      "Epoch 47/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3275 - accuracy: 0.8689\n",
      "Epoch 48/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3080 - accuracy: 0.8647\n",
      "Epoch 49/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8718\n",
      "Epoch 50/1500\n",
      "22/22 [==============================] - 0s 3ms/step - loss: 0.3023 - accuracy: 0.8818\n",
      "Epoch 51/1500\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.3073 - accuracy: 0.8818\n",
      "Epoch 52/1500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.3001 - accuracy: 0.8761\n",
      "Epoch 53/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2726 - accuracy: 0.8960\n",
      "Epoch 54/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2813 - accuracy: 0.8903\n",
      "Epoch 55/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2903 - accuracy: 0.8889\n",
      "Epoch 56/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2933 - accuracy: 0.8846\n",
      "Epoch 57/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2940 - accuracy: 0.8917\n",
      "Epoch 58/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2851 - accuracy: 0.8903\n",
      "Epoch 59/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2860 - accuracy: 0.8803\n",
      "Epoch 60/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2692 - accuracy: 0.8960\n",
      "Epoch 61/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2744 - accuracy: 0.8960\n",
      "Epoch 62/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2930 - accuracy: 0.8775\n",
      "Epoch 63/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2696 - accuracy: 0.9060\n",
      "Epoch 64/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2690 - accuracy: 0.8989\n",
      "Epoch 65/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2444 - accuracy: 0.9031\n",
      "Epoch 66/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2922 - accuracy: 0.8775\n",
      "Epoch 67/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2492 - accuracy: 0.9074\n",
      "Epoch 68/1500\n",
      "22/22 [==============================] - 0s 991us/step - loss: 0.2474 - accuracy: 0.9174\n",
      "Epoch 69/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2375 - accuracy: 0.9160\n",
      "Epoch 70/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9103\n",
      "Epoch 71/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2337 - accuracy: 0.9088\n",
      "Epoch 72/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2592 - accuracy: 0.8917\n",
      "Epoch 73/1500\n",
      "22/22 [==============================] - 0s 997us/step - loss: 0.2410 - accuracy: 0.9017\n",
      "Epoch 74/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2580 - accuracy: 0.8860\n",
      "Epoch 75/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2421 - accuracy: 0.9160\n",
      "Epoch 76/1500\n",
      "22/22 [==============================] - 0s 989us/step - loss: 0.2341 - accuracy: 0.9188\n",
      "Epoch 77/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2482 - accuracy: 0.9188\n",
      "Epoch 78/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2443 - accuracy: 0.9088\n",
      "Epoch 79/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2535 - accuracy: 0.8960\n",
      "Epoch 80/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.8932\n",
      "Epoch 81/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2301 - accuracy: 0.9117\n",
      "Epoch 82/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2179 - accuracy: 0.9231\n",
      "Epoch 83/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2278 - accuracy: 0.9074\n",
      "Epoch 84/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9274\n",
      "Epoch 85/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2373 - accuracy: 0.9046\n",
      "Epoch 86/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2423 - accuracy: 0.9074\n",
      "Epoch 87/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2178 - accuracy: 0.9074\n",
      "Epoch 88/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9259\n",
      "Epoch 89/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2328 - accuracy: 0.8989\n",
      "Epoch 90/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9031\n",
      "Epoch 91/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9245\n",
      "Epoch 92/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2295 - accuracy: 0.9174\n",
      "Epoch 93/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9345\n",
      "Epoch 94/1500\n",
      "22/22 [==============================] - 0s 991us/step - loss: 0.2033 - accuracy: 0.9245\n",
      "Epoch 95/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1965 - accuracy: 0.9288\n",
      "Epoch 96/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9145\n",
      "Epoch 97/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9444\n",
      "Epoch 98/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1989 - accuracy: 0.9245\n",
      "Epoch 99/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1921 - accuracy: 0.9202\n",
      "Epoch 100/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9231\n",
      "Epoch 101/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2010 - accuracy: 0.9217\n",
      "Epoch 102/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9160\n",
      "Epoch 103/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9202\n",
      "Epoch 104/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9373\n",
      "Epoch 105/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9202\n",
      "Epoch 106/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.9359\n",
      "Epoch 107/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9288\n",
      "Epoch 108/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9316\n",
      "Epoch 109/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.9302\n",
      "Epoch 110/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1930 - accuracy: 0.9302\n",
      "Epoch 111/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9345\n",
      "Epoch 112/1500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.1953 - accuracy: 0.9302\n",
      "Epoch 113/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9387\n",
      "Epoch 114/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9359\n",
      "Epoch 115/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.9274\n",
      "Epoch 116/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9373\n",
      "Epoch 117/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.9330\n",
      "Epoch 118/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9302\n",
      "Epoch 119/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9359\n",
      "Epoch 120/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1888 - accuracy: 0.9416\n",
      "Epoch 121/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.9487\n",
      "Epoch 122/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.9302\n",
      "Epoch 123/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9359\n",
      "Epoch 124/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9416\n",
      "Epoch 125/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1923 - accuracy: 0.9288\n",
      "Epoch 126/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9373\n",
      "Epoch 127/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1689 - accuracy: 0.9430\n",
      "Epoch 128/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1663 - accuracy: 0.9444\n",
      "Epoch 129/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9430\n",
      "Epoch 130/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9402\n",
      "Epoch 131/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9402\n",
      "Epoch 132/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.9345\n",
      "Epoch 133/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1677 - accuracy: 0.9330\n",
      "Epoch 134/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9487\n",
      "Epoch 135/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.9373\n",
      "Epoch 136/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9259\n",
      "Epoch 137/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1683 - accuracy: 0.9345\n",
      "Epoch 138/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9473\n",
      "Epoch 139/1500\n",
      "22/22 [==============================] - 0s 974us/step - loss: 0.1439 - accuracy: 0.9487\n",
      "Epoch 140/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9416\n",
      "Epoch 141/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9501\n",
      "Epoch 142/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1365 - accuracy: 0.9601\n",
      "Epoch 143/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9473\n",
      "Epoch 144/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9544\n",
      "Epoch 145/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.9501\n",
      "Epoch 146/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9459\n",
      "Epoch 147/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9672\n",
      "Epoch 148/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9302\n",
      "Epoch 149/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9530\n",
      "Epoch 150/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9487\n",
      "Epoch 151/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9530\n",
      "Epoch 152/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9459\n",
      "Epoch 153/1500\n",
      "22/22 [==============================] - 0s 990us/step - loss: 0.1310 - accuracy: 0.9530\n",
      "Epoch 154/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9544\n",
      "Epoch 155/1500\n",
      "22/22 [==============================] - 0s 963us/step - loss: 0.1473 - accuracy: 0.9558\n",
      "Epoch 156/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9430\n",
      "Epoch 157/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9259\n",
      "Epoch 158/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9402\n",
      "Epoch 159/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9459\n",
      "Epoch 160/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9530\n",
      "Epoch 161/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9573\n",
      "Epoch 162/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9501\n",
      "Epoch 163/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9516\n",
      "Epoch 164/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9288\n",
      "Epoch 165/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9487\n",
      "Epoch 166/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9516\n",
      "Epoch 167/1500\n",
      "22/22 [==============================] - 0s 998us/step - loss: 0.1457 - accuracy: 0.9444\n",
      "Epoch 168/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9444\n",
      "Epoch 169/1500\n",
      "22/22 [==============================] - 0s 988us/step - loss: 0.1309 - accuracy: 0.9530\n",
      "Epoch 170/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9487\n",
      "Epoch 171/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9444\n",
      "Epoch 172/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9516\n",
      "Epoch 173/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9516\n",
      "Epoch 174/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.9316\n",
      "Epoch 175/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9573\n",
      "Epoch 176/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9601\n",
      "Epoch 177/1500\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.1003 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 147.\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9587\n",
      "Epoch 177: early stopping\n",
      "8/8 [==============================] - 0s 898us/step - loss: 0.7727 - accuracy: 0.7064\n",
      "8/8 [==============================] - 0s 1ms/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.69 (20/29)\n",
      "Before appending - Cat IDs: 608, Predictions: 608, Actuals: 608, Gender: 608\n",
      "After appending - Cat IDs: 843, Predictions: 843, Actuals: 843, Gender: 843\n",
      "Final Test Results - Loss: 0.7727013826370239, Accuracy: 0.7063829898834229, Precision: 0.8065411144695033, Recall: 0.6227106227106227, F1 Score: 0.6540025686973879\n",
      "Confusion Matrix:\n",
      " [[124   1   5]\n",
      " [ 13  22   0]\n",
      " [ 50   0  20]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.7026240955156859\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.6764927059412003\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7596727758646011\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.7320174601876851\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7267887330151076\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[3]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ###################\n",
    "    # PREPARING MODEL #\n",
    "    ###################\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0522ad-89f9-479c-b073-c5e720fc6221",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "ecac477e-680c-4a82-abcb-2b3a2dbfbea4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 843, Predictions: 843, Actuals: 843, Gender: 843\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ce33a298-d7a3-40ec-8f78-00c109118230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "23d083a1-d61a-4fce-9b56-667a9930bfe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.77 (85/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "431e9ecf-be90-466a-a1b7-dd98f7fb2a9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "42e4a5c0-56dd-4771-820f-bcecaf391f0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, kitten, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, senior, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[kitten, adult, adult, adult, kitten, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, adult, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, adult, kitten, adult, kitten, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, adult, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, kitten, adult, senior, adult, adult, a...         adult            adult                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, k...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "63    057A  [senior, adult, adult, senior, adult, adult, s...        senior           senior                   True\n",
       "61    055A  [adult, adult, senior, senior, senior, adult, ...        senior           senior                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "59    053A      [kitten, adult, adult, adult, kitten, senior]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, adult, senior, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, adult, kitten, kitten, kitten, kitten,...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A             [kitten, adult, adult, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "100   105A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, adult, senior, senior, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, adult, kitten,...        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                     [adult, adult]         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, adult, a...         adult            adult                   True\n",
       "16    014B  [kitten, adult, kitten, adult, kitten, kitten,...        kitten           kitten                   True\n",
       "14    013B  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "29    025B                                    [adult, senior]         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, se...         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                      [adult, adult, senior, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "42    036A  [senior, senior, senior, adult, adult, adult, ...         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "102   108A       [senior, adult, adult, adult, adult, senior]         adult           senior                  False\n",
       "103   109A  [adult, adult, kitten, kitten, kitten, adult, ...         adult           kitten                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "106   113A                              [adult, adult, adult]         adult           senior                  False\n",
       "99    104A                     [adult, senior, adult, senior]         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "44    038A                   [kitten, kitten, kitten, kitten]        kitten            adult                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "101   106A  [adult, senior, adult, senior, adult, adult, a...         adult           senior                  False\n",
       "12    011A                                    [senior, adult]         adult           senior                  False\n",
       "56    051A  [adult, adult, adult, adult, adult, senior, ad...         adult           senior                  False\n",
       "92    097A  [adult, senior, senior, adult, senior, adult, ...         adult           senior                  False\n",
       "74    068A  [senior, adult, senior, senior, senior, adult,...        senior            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "60    054A                                    [senior, adult]         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, senior, senior, adult, a...         adult           senior                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "65    059A  [adult, adult, adult, adult, senior, senior, a...         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "69    063A  [senior, senior, senior, adult, senior, senior...        senior            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, senior, adult, a...         adult           senior                  False"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "bc2e5ff6-b761-4b71-b7cd-9eb24aae0ebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     65\n",
      "kitten    12\n",
      "senior     8\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ff925f9d-efd5-46a0-89a0-1fb3da66a46a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             65  89.041096\n",
      "1           kitten           15             12  80.000000\n",
      "2           senior           22              8  36.363636\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d8f08817-c78c-4654-b195-911f2a9ccb2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6f284-fe35-4071-8054-5e5a086b4ed9",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "4394c6ec-6da8-42de-aaaa-bc1b17bbf74b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          551            470  85.299456\n",
      "1           kitten          114             82  71.929825\n",
      "2           senior          178             83  46.629213\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8d541c77-590d-4b79-b526-85520ea5c4a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c98d75-f483-4819-b292-975bd229cfd2",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "c7325b11-ab06-45bd-8e77-d498737b7552",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    215      161     74.88\n",
      "1          M    337      269     79.82\n",
      "2          X    291      205     70.45\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "777ae0eb-dab5-47c0-86a7-0d3c6b01b9c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40270db-853d-46a1-acb4-8dab8a4f9c81",
   "metadata": {},
   "source": [
    "# RANDOM SEED 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "588e6cfb-6ee5-4b51-a9ee-79dcbfbb208c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[4]))\n",
    "np.random.seed(int(random_seeds[4]))\n",
    "tf.random.set_seed(int(random_seeds[4]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "432a9515-e7f1-482c-bdaf-73cb07a9132e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "8231e441-4472-4d9b-8ddf-dc285c07923b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9ed6b-6d2f-491e-95d8-73d33edeef6a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6cd63258-8a3b-4bc0-9ac6-ae8aa4b515b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "106A    14\n",
      "097B    14\n",
      "028A    13\n",
      "111A    13\n",
      "051A    12\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "068A    11\n",
      "014B    10\n",
      "016A    10\n",
      "071A    10\n",
      "005A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "045A     9\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "095A     8\n",
      "010A     8\n",
      "050A     7\n",
      "027A     7\n",
      "031A     7\n",
      "099A     7\n",
      "117A     7\n",
      "053A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "075A     5\n",
      "021A     5\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "062A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "009A     4\n",
      "035A     4\n",
      "056A     3\n",
      "014A     3\n",
      "058A     3\n",
      "060A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "032A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "073A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "041A     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "000B    19\n",
      "001A    14\n",
      "002A    13\n",
      "040A    10\n",
      "022A     9\n",
      "065A     9\n",
      "109A     6\n",
      "108A     6\n",
      "044A     5\n",
      "026A     4\n",
      "113A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "011A     2\n",
      "054A     2\n",
      "018A     2\n",
      "092A     1\n",
      "049A     1\n",
      "004A     1\n",
      "019B     1\n",
      "115A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    280\n",
      "X    256\n",
      "F    186\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    92\n",
      "F    66\n",
      "M    57\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [014B, 111A, 047A, 042A, 050A, 043A, 041A, 045...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 001A, 103A, 022A, 065A, 002A, 000B, 026...\n",
      "kitten                 [044A, 040A, 046A, 109A, 049A, 115A]\n",
      "senior                             [113A, 054A, 108A, 011A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 59, 'kitten': 10, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 15, 'kitten': 6, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '041A' '042A' '043A' '045A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '044A' '046A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'041A'}\n",
      "Moved to Test Set:\n",
      "{'041A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '042A' '043A' '045A' '046A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '041A' '044A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "Length of X_train_val:\n",
      "784\n",
      "Length of y_train_val:\n",
      "784\n",
      "Length of groups_train_val:\n",
      "784\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten     85\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     116\n",
      "kitten     86\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten    147\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     116\n",
      "kitten     24\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 472, 2: 165, 1: 147})\n",
      "Epoch 1/1500\n",
      "25/25 [==============================] - 1s 1ms/step - loss: 1.2067 - accuracy: 0.4758\n",
      "Epoch 2/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.9649 - accuracy: 0.5778\n",
      "Epoch 3/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.8710 - accuracy: 0.6314\n",
      "Epoch 4/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.8273 - accuracy: 0.6760\n",
      "Epoch 5/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7442 - accuracy: 0.6977\n",
      "Epoch 6/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7381 - accuracy: 0.7041\n",
      "Epoch 7/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7351 - accuracy: 0.6952\n",
      "Epoch 8/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7021 - accuracy: 0.7296\n",
      "Epoch 9/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6236 - accuracy: 0.7526\n",
      "Epoch 10/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6166 - accuracy: 0.7551\n",
      "Epoch 11/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5482 - accuracy: 0.7781\n",
      "Epoch 12/1500\n",
      "25/25 [==============================] - 0s 970us/step - loss: 0.6139 - accuracy: 0.7615\n",
      "Epoch 13/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5441 - accuracy: 0.7959\n",
      "Epoch 14/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5053 - accuracy: 0.7985\n",
      "Epoch 15/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5247 - accuracy: 0.7819\n",
      "Epoch 16/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4913 - accuracy: 0.8036\n",
      "Epoch 17/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.5730 - accuracy: 0.7793\n",
      "Epoch 18/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4877 - accuracy: 0.8048\n",
      "Epoch 19/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4980 - accuracy: 0.7908\n",
      "Epoch 20/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4721 - accuracy: 0.8087\n",
      "Epoch 21/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4401 - accuracy: 0.8278\n",
      "Epoch 22/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4683 - accuracy: 0.8291\n",
      "Epoch 23/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4496 - accuracy: 0.8367\n",
      "Epoch 24/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4611 - accuracy: 0.8316\n",
      "Epoch 25/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4399 - accuracy: 0.8291\n",
      "Epoch 26/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4419 - accuracy: 0.8329\n",
      "Epoch 27/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4496 - accuracy: 0.8202\n",
      "Epoch 28/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4276 - accuracy: 0.8163\n",
      "Epoch 29/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4105 - accuracy: 0.8444\n",
      "Epoch 30/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4289 - accuracy: 0.8367\n",
      "Epoch 31/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4115 - accuracy: 0.8418\n",
      "Epoch 32/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4240 - accuracy: 0.8278\n",
      "Epoch 33/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3897 - accuracy: 0.8431\n",
      "Epoch 34/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3955 - accuracy: 0.8495\n",
      "Epoch 35/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3893 - accuracy: 0.8393\n",
      "Epoch 36/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4011 - accuracy: 0.8508\n",
      "Epoch 37/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4037 - accuracy: 0.8508\n",
      "Epoch 38/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3859 - accuracy: 0.8533\n",
      "Epoch 39/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4044 - accuracy: 0.8520\n",
      "Epoch 40/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3934 - accuracy: 0.8559\n",
      "Epoch 41/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3674 - accuracy: 0.8520\n",
      "Epoch 42/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.4073 - accuracy: 0.8329\n",
      "Epoch 43/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3650 - accuracy: 0.8508\n",
      "Epoch 44/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3605 - accuracy: 0.8610\n",
      "Epoch 45/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3563 - accuracy: 0.8686\n",
      "Epoch 46/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3580 - accuracy: 0.8559\n",
      "Epoch 47/1500\n",
      "25/25 [==============================] - 0s 993us/step - loss: 0.3603 - accuracy: 0.8559\n",
      "Epoch 48/1500\n",
      "25/25 [==============================] - 0s 981us/step - loss: 0.3447 - accuracy: 0.8533\n",
      "Epoch 49/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3534 - accuracy: 0.8584\n",
      "Epoch 50/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3108 - accuracy: 0.8903\n",
      "Epoch 51/1500\n",
      "25/25 [==============================] - 0s 983us/step - loss: 0.3138 - accuracy: 0.8763\n",
      "Epoch 52/1500\n",
      "25/25 [==============================] - 0s 980us/step - loss: 0.3045 - accuracy: 0.8916\n",
      "Epoch 53/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3380 - accuracy: 0.8699\n",
      "Epoch 54/1500\n",
      "25/25 [==============================] - 0s 996us/step - loss: 0.3173 - accuracy: 0.8635\n",
      "Epoch 55/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3333 - accuracy: 0.8686\n",
      "Epoch 56/1500\n",
      "25/25 [==============================] - 0s 1000us/step - loss: 0.3346 - accuracy: 0.8673\n",
      "Epoch 57/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3479 - accuracy: 0.8559\n",
      "Epoch 58/1500\n",
      "25/25 [==============================] - 0s 993us/step - loss: 0.3156 - accuracy: 0.8737\n",
      "Epoch 59/1500\n",
      "25/25 [==============================] - 0s 987us/step - loss: 0.3059 - accuracy: 0.8878\n",
      "Epoch 60/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.3311 - accuracy: 0.8673\n",
      "Epoch 61/1500\n",
      "25/25 [==============================] - 0s 988us/step - loss: 0.3161 - accuracy: 0.8673\n",
      "Epoch 62/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2963 - accuracy: 0.8712\n",
      "Epoch 63/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2803 - accuracy: 0.8941\n",
      "Epoch 64/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.3304 - accuracy: 0.8712\n",
      "Epoch 65/1500\n",
      "25/25 [==============================] - 0s 959us/step - loss: 0.3049 - accuracy: 0.8737\n",
      "Epoch 66/1500\n",
      "25/25 [==============================] - 0s 980us/step - loss: 0.3390 - accuracy: 0.8622\n",
      "Epoch 67/1500\n",
      "25/25 [==============================] - 0s 998us/step - loss: 0.3089 - accuracy: 0.8839\n",
      "Epoch 68/1500\n",
      "25/25 [==============================] - 0s 1000us/step - loss: 0.3142 - accuracy: 0.8763\n",
      "Epoch 69/1500\n",
      "25/25 [==============================] - 0s 982us/step - loss: 0.3227 - accuracy: 0.8712\n",
      "Epoch 70/1500\n",
      "25/25 [==============================] - 0s 970us/step - loss: 0.3012 - accuracy: 0.8865\n",
      "Epoch 71/1500\n",
      "25/25 [==============================] - 0s 969us/step - loss: 0.2976 - accuracy: 0.8890\n",
      "Epoch 72/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2757 - accuracy: 0.8954\n",
      "Epoch 73/1500\n",
      "25/25 [==============================] - 0s 988us/step - loss: 0.2697 - accuracy: 0.8941\n",
      "Epoch 74/1500\n",
      "25/25 [==============================] - 0s 990us/step - loss: 0.2887 - accuracy: 0.8992\n",
      "Epoch 75/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2785 - accuracy: 0.8916\n",
      "Epoch 76/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2697 - accuracy: 0.8980\n",
      "Epoch 77/1500\n",
      "25/25 [==============================] - 0s 981us/step - loss: 0.2707 - accuracy: 0.9107\n",
      "Epoch 78/1500\n",
      "25/25 [==============================] - 0s 965us/step - loss: 0.3005 - accuracy: 0.8827\n",
      "Epoch 79/1500\n",
      "25/25 [==============================] - 0s 999us/step - loss: 0.2761 - accuracy: 0.8980\n",
      "Epoch 80/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2667 - accuracy: 0.8992\n",
      "Epoch 81/1500\n",
      "25/25 [==============================] - 0s 974us/step - loss: 0.3062 - accuracy: 0.8750\n",
      "Epoch 82/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2503 - accuracy: 0.9069\n",
      "Epoch 83/1500\n",
      "25/25 [==============================] - 0s 984us/step - loss: 0.2636 - accuracy: 0.8916\n",
      "Epoch 84/1500\n",
      "25/25 [==============================] - 0s 962us/step - loss: 0.2837 - accuracy: 0.8967\n",
      "Epoch 85/1500\n",
      "25/25 [==============================] - 0s 945us/step - loss: 0.2542 - accuracy: 0.9056\n",
      "Epoch 86/1500\n",
      "25/25 [==============================] - 0s 968us/step - loss: 0.2667 - accuracy: 0.8801\n",
      "Epoch 87/1500\n",
      "25/25 [==============================] - 0s 973us/step - loss: 0.2426 - accuracy: 0.9082\n",
      "Epoch 88/1500\n",
      "25/25 [==============================] - 0s 969us/step - loss: 0.2737 - accuracy: 0.8967\n",
      "Epoch 89/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2477 - accuracy: 0.8929\n",
      "Epoch 90/1500\n",
      "25/25 [==============================] - 0s 982us/step - loss: 0.2622 - accuracy: 0.9082\n",
      "Epoch 91/1500\n",
      "25/25 [==============================] - 0s 979us/step - loss: 0.2524 - accuracy: 0.9018\n",
      "Epoch 92/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.2515 - accuracy: 0.9043\n",
      "Epoch 93/1500\n",
      "25/25 [==============================] - 0s 940us/step - loss: 0.2466 - accuracy: 0.9056\n",
      "Epoch 94/1500\n",
      "25/25 [==============================] - 0s 954us/step - loss: 0.2316 - accuracy: 0.9133\n",
      "Epoch 95/1500\n",
      "25/25 [==============================] - 0s 973us/step - loss: 0.2658 - accuracy: 0.9069\n",
      "Epoch 96/1500\n",
      "25/25 [==============================] - 0s 946us/step - loss: 0.2707 - accuracy: 0.8992\n",
      "Epoch 97/1500\n",
      "25/25 [==============================] - 0s 959us/step - loss: 0.2041 - accuracy: 0.9286\n",
      "Epoch 98/1500\n",
      "25/25 [==============================] - 0s 933us/step - loss: 0.2434 - accuracy: 0.9107\n",
      "Epoch 99/1500\n",
      "25/25 [==============================] - 0s 957us/step - loss: 0.2984 - accuracy: 0.8852\n",
      "Epoch 100/1500\n",
      "25/25 [==============================] - 0s 969us/step - loss: 0.2482 - accuracy: 0.8954\n",
      "Epoch 101/1500\n",
      "25/25 [==============================] - 0s 944us/step - loss: 0.2261 - accuracy: 0.9158\n",
      "Epoch 102/1500\n",
      "25/25 [==============================] - 0s 931us/step - loss: 0.2125 - accuracy: 0.9235\n",
      "Epoch 103/1500\n",
      "25/25 [==============================] - 0s 964us/step - loss: 0.2372 - accuracy: 0.9120\n",
      "Epoch 104/1500\n",
      "25/25 [==============================] - 0s 952us/step - loss: 0.2351 - accuracy: 0.9145\n",
      "Epoch 105/1500\n",
      "25/25 [==============================] - 0s 987us/step - loss: 0.2391 - accuracy: 0.8941\n",
      "Epoch 106/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2314 - accuracy: 0.9069\n",
      "Epoch 107/1500\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.2369 - accuracy: 0.8980\n",
      "Epoch 108/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2638 - accuracy: 0.9069\n",
      "Epoch 109/1500\n",
      "25/25 [==============================] - 0s 973us/step - loss: 0.2326 - accuracy: 0.9171\n",
      "Epoch 110/1500\n",
      "25/25 [==============================] - 0s 963us/step - loss: 0.2011 - accuracy: 0.9247\n",
      "Epoch 111/1500\n",
      "25/25 [==============================] - 0s 978us/step - loss: 0.2501 - accuracy: 0.9120\n",
      "Epoch 112/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2162 - accuracy: 0.9171\n",
      "Epoch 113/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2221 - accuracy: 0.9145\n",
      "Epoch 114/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9196\n",
      "Epoch 115/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2087 - accuracy: 0.9145\n",
      "Epoch 116/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9311\n",
      "Epoch 117/1500\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.2089 - accuracy: 0.9158\n",
      "Epoch 118/1500\n",
      "25/25 [==============================] - 0s 976us/step - loss: 0.1982 - accuracy: 0.9222\n",
      "Epoch 119/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2554 - accuracy: 0.8992\n",
      "Epoch 120/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.2032 - accuracy: 0.9184\n",
      "Epoch 121/1500\n",
      "25/25 [==============================] - 0s 992us/step - loss: 0.2346 - accuracy: 0.9031\n",
      "Epoch 122/1500\n",
      "25/25 [==============================] - 0s 959us/step - loss: 0.2219 - accuracy: 0.9247\n",
      "Epoch 123/1500\n",
      "25/25 [==============================] - 0s 947us/step - loss: 0.2304 - accuracy: 0.9145\n",
      "Epoch 124/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2103 - accuracy: 0.9209\n",
      "Epoch 125/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9247\n",
      "Epoch 126/1500\n",
      "25/25 [==============================] - 0s 973us/step - loss: 0.2110 - accuracy: 0.9171\n",
      "Epoch 127/1500\n",
      "25/25 [==============================] - 0s 993us/step - loss: 0.2107 - accuracy: 0.9235\n",
      "Epoch 128/1500\n",
      "25/25 [==============================] - 0s 968us/step - loss: 0.1985 - accuracy: 0.9235\n",
      "Epoch 129/1500\n",
      "25/25 [==============================] - 0s 998us/step - loss: 0.2069 - accuracy: 0.9235\n",
      "Epoch 130/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.9324\n",
      "Epoch 131/1500\n",
      "25/25 [==============================] - 0s 990us/step - loss: 0.1962 - accuracy: 0.9196\n",
      "Epoch 132/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.1952 - accuracy: 0.9286\n",
      "Epoch 133/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9286\n",
      "Epoch 134/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2009 - accuracy: 0.9273\n",
      "Epoch 135/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9082\n",
      "Epoch 136/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1913 - accuracy: 0.9209\n",
      "Epoch 137/1500\n",
      "25/25 [==============================] - 0s 973us/step - loss: 0.1941 - accuracy: 0.9337\n",
      "Epoch 138/1500\n",
      "25/25 [==============================] - 0s 969us/step - loss: 0.1864 - accuracy: 0.9273\n",
      "Epoch 139/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1824 - accuracy: 0.9426\n",
      "Epoch 140/1500\n",
      "25/25 [==============================] - 0s 976us/step - loss: 0.2160 - accuracy: 0.9158\n",
      "Epoch 141/1500\n",
      "25/25 [==============================] - 0s 995us/step - loss: 0.1931 - accuracy: 0.9286\n",
      "Epoch 142/1500\n",
      "25/25 [==============================] - 0s 989us/step - loss: 0.1900 - accuracy: 0.9247\n",
      "Epoch 143/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.9171\n",
      "Epoch 144/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.9298\n",
      "Epoch 145/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9209\n",
      "Epoch 146/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9235\n",
      "Epoch 147/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9503\n",
      "Epoch 148/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1787 - accuracy: 0.9413\n",
      "Epoch 149/1500\n",
      "25/25 [==============================] - 0s 960us/step - loss: 0.1885 - accuracy: 0.9196\n",
      "Epoch 150/1500\n",
      "25/25 [==============================] - 0s 954us/step - loss: 0.1613 - accuracy: 0.9439\n",
      "Epoch 151/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9413\n",
      "Epoch 152/1500\n",
      "25/25 [==============================] - 0s 966us/step - loss: 0.1729 - accuracy: 0.9349\n",
      "Epoch 153/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9349\n",
      "Epoch 154/1500\n",
      "25/25 [==============================] - 0s 983us/step - loss: 0.1886 - accuracy: 0.9337\n",
      "Epoch 155/1500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.1647 - accuracy: 0.9503\n",
      "Epoch 156/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.9426\n",
      "Epoch 157/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9349\n",
      "Epoch 158/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9477\n",
      "Epoch 159/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.9375\n",
      "Epoch 160/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.9388\n",
      "Epoch 161/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1754 - accuracy: 0.9349\n",
      "Epoch 162/1500\n",
      "25/25 [==============================] - 0s 981us/step - loss: 0.1854 - accuracy: 0.9375\n",
      "Epoch 163/1500\n",
      "25/25 [==============================] - 0s 956us/step - loss: 0.1783 - accuracy: 0.9349\n",
      "Epoch 164/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.9286\n",
      "Epoch 165/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9349\n",
      "Epoch 166/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.9375\n",
      "Epoch 167/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9298\n",
      "Epoch 168/1500\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.1621 - accuracy: 0.9477\n",
      "Epoch 169/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1849 - accuracy: 0.9286\n",
      "Epoch 170/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.9413\n",
      "Epoch 171/1500\n",
      "25/25 [==============================] - 0s 953us/step - loss: 0.1679 - accuracy: 0.9324\n",
      "Epoch 172/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.9375\n",
      "Epoch 173/1500\n",
      "25/25 [==============================] - 0s 983us/step - loss: 0.1849 - accuracy: 0.9349\n",
      "Epoch 174/1500\n",
      "25/25 [==============================] - 0s 983us/step - loss: 0.1514 - accuracy: 0.9426\n",
      "Epoch 175/1500\n",
      "25/25 [==============================] - 0s 980us/step - loss: 0.1766 - accuracy: 0.9452\n",
      "Epoch 176/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.9247\n",
      "Epoch 177/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9311\n",
      "Epoch 178/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9298\n",
      "Epoch 179/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9464\n",
      "Epoch 180/1500\n",
      "25/25 [==============================] - 0s 970us/step - loss: 0.1593 - accuracy: 0.9388\n",
      "Epoch 181/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9286\n",
      "Epoch 182/1500\n",
      "25/25 [==============================] - 0s 976us/step - loss: 0.1583 - accuracy: 0.9426\n",
      "Epoch 183/1500\n",
      "25/25 [==============================] - 0s 961us/step - loss: 0.1665 - accuracy: 0.9401\n",
      "Epoch 184/1500\n",
      "25/25 [==============================] - 0s 988us/step - loss: 0.1853 - accuracy: 0.9362\n",
      "Epoch 185/1500\n",
      "25/25 [==============================] - 0s 991us/step - loss: 0.1642 - accuracy: 0.9401\n",
      "Epoch 186/1500\n",
      "25/25 [==============================] - 0s 990us/step - loss: 0.1842 - accuracy: 0.9260\n",
      "Epoch 187/1500\n",
      "25/25 [==============================] - 0s 975us/step - loss: 0.1600 - accuracy: 0.9362\n",
      "Epoch 188/1500\n",
      "25/25 [==============================] - 0s 992us/step - loss: 0.1383 - accuracy: 0.9528\n",
      "Epoch 189/1500\n",
      "25/25 [==============================] - 0s 981us/step - loss: 0.1666 - accuracy: 0.9324\n",
      "Epoch 190/1500\n",
      "25/25 [==============================] - 0s 955us/step - loss: 0.1667 - accuracy: 0.9452\n",
      "Epoch 191/1500\n",
      "25/25 [==============================] - 0s 982us/step - loss: 0.1638 - accuracy: 0.9413\n",
      "Epoch 192/1500\n",
      "25/25 [==============================] - 0s 968us/step - loss: 0.1763 - accuracy: 0.9311\n",
      "Epoch 193/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9388\n",
      "Epoch 194/1500\n",
      "25/25 [==============================] - 0s 977us/step - loss: 0.1608 - accuracy: 0.9388\n",
      "Epoch 195/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9452\n",
      "Epoch 196/1500\n",
      "25/25 [==============================] - 0s 990us/step - loss: 0.1275 - accuracy: 0.9541\n",
      "Epoch 197/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9528\n",
      "Epoch 198/1500\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.1319 - accuracy: 0.9566\n",
      "Epoch 199/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9337\n",
      "Epoch 200/1500\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.1588 - accuracy: 0.9401\n",
      "Epoch 201/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9464\n",
      "Epoch 202/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9503\n",
      "Epoch 203/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9490\n",
      "Epoch 204/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9413\n",
      "Epoch 205/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1663 - accuracy: 0.9426\n",
      "Epoch 206/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9477\n",
      "Epoch 207/1500\n",
      "25/25 [==============================] - 0s 989us/step - loss: 0.1541 - accuracy: 0.9413\n",
      "Epoch 208/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9605\n",
      "Epoch 209/1500\n",
      "25/25 [==============================] - 0s 995us/step - loss: 0.1419 - accuracy: 0.9554\n",
      "Epoch 210/1500\n",
      "25/25 [==============================] - 0s 986us/step - loss: 0.1228 - accuracy: 0.9554\n",
      "Epoch 211/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9477\n",
      "Epoch 212/1500\n",
      "25/25 [==============================] - 0s 981us/step - loss: 0.1412 - accuracy: 0.9490\n",
      "Epoch 213/1500\n",
      "25/25 [==============================] - 0s 990us/step - loss: 0.1311 - accuracy: 0.9490\n",
      "Epoch 214/1500\n",
      "25/25 [==============================] - 0s 964us/step - loss: 0.1230 - accuracy: 0.9528\n",
      "Epoch 215/1500\n",
      "25/25 [==============================] - 0s 952us/step - loss: 0.1305 - accuracy: 0.9503\n",
      "Epoch 216/1500\n",
      "25/25 [==============================] - 0s 965us/step - loss: 0.1552 - accuracy: 0.9337\n",
      "Epoch 217/1500\n",
      "25/25 [==============================] - 0s 966us/step - loss: 0.1333 - accuracy: 0.9477\n",
      "Epoch 218/1500\n",
      "25/25 [==============================] - 0s 972us/step - loss: 0.1131 - accuracy: 0.9605\n",
      "Epoch 219/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9401\n",
      "Epoch 220/1500\n",
      "25/25 [==============================] - 0s 996us/step - loss: 0.1534 - accuracy: 0.9401\n",
      "Epoch 221/1500\n",
      "25/25 [==============================] - 0s 974us/step - loss: 0.1367 - accuracy: 0.9503\n",
      "Epoch 222/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9668\n",
      "Epoch 223/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9388\n",
      "Epoch 224/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9554\n",
      "Epoch 225/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1477 - accuracy: 0.9426\n",
      "Epoch 226/1500\n",
      "25/25 [==============================] - 0s 953us/step - loss: 0.1440 - accuracy: 0.9439\n",
      "Epoch 227/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9579\n",
      "Epoch 228/1500\n",
      "25/25 [==============================] - 0s 998us/step - loss: 0.1364 - accuracy: 0.9452\n",
      "Epoch 229/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9452\n",
      "Epoch 230/1500\n",
      "25/25 [==============================] - 0s 989us/step - loss: 0.1275 - accuracy: 0.9452\n",
      "Epoch 231/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9579\n",
      "Epoch 232/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9452\n",
      "Epoch 233/1500\n",
      "25/25 [==============================] - 0s 990us/step - loss: 0.1313 - accuracy: 0.9528\n",
      "Epoch 234/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9515\n",
      "Epoch 235/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9490\n",
      "Epoch 236/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9503\n",
      "Epoch 237/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9592\n",
      "Epoch 238/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1569 - accuracy: 0.9413\n",
      "Epoch 239/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9477\n",
      "Epoch 240/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.9566\n",
      "Epoch 241/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9426\n",
      "Epoch 242/1500\n",
      "25/25 [==============================] - 0s 967us/step - loss: 0.1273 - accuracy: 0.9541\n",
      "Epoch 243/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9554\n",
      "Epoch 244/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9503\n",
      "Epoch 245/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9656\n",
      "Epoch 246/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9592\n",
      "Epoch 247/1500\n",
      "25/25 [==============================] - 0s 985us/step - loss: 0.1310 - accuracy: 0.9566\n",
      "Epoch 248/1500\n",
      "25/25 [==============================] - 0s 970us/step - loss: 0.1261 - accuracy: 0.9592\n",
      "Epoch 249/1500\n",
      "25/25 [==============================] - 0s 984us/step - loss: 0.1193 - accuracy: 0.9566\n",
      "Epoch 250/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9541\n",
      "Epoch 251/1500\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9592\n",
      "Epoch 252/1500\n",
      " 1/25 [>.............................] - ETA: 0s - loss: 0.0520 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 222.\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9490\n",
      "Epoch 252: early stopping\n",
      "5/5 [==============================] - 0s 935us/step - loss: 0.6014 - accuracy: 0.7647\n",
      "5/5 [==============================] - 0s 621us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (19/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "Final Test Results - Loss: 0.6014047861099243, Accuracy: 0.7647058963775635, Precision: 0.6683926798127393, Recall: 0.6945918066607722, F1 Score: 0.6574225955961635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[93  3 20]\n",
      " [ 8 16  0]\n",
      " [ 5  0  8]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "005A    10\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "027A     7\n",
      "008A     6\n",
      "108A     6\n",
      "109A     6\n",
      "053A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "003A     4\n",
      "105A     4\n",
      "009A     4\n",
      "026A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "012A     3\n",
      "113A     3\n",
      "014A     3\n",
      "060A     3\n",
      "011A     2\n",
      "102A     2\n",
      "032A     2\n",
      "069A     2\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "038A     2\n",
      "019B     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "092A     1\n",
      "004A     1\n",
      "041A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "073A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "067A    19\n",
      "042A    14\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "063A    11\n",
      "016A    10\n",
      "033A     9\n",
      "051B     9\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "117A     7\n",
      "050A     7\n",
      "007A     6\n",
      "075A     5\n",
      "025C     5\n",
      "087A     2\n",
      "061A     2\n",
      "025B     2\n",
      "043A     1\n",
      "066A     1\n",
      "048A     1\n",
      "088A     1\n",
      "091A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    263\n",
      "M    226\n",
      "F    177\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    111\n",
      "X     85\n",
      "F     75\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 015A, 001A, 103A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 109A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 113...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 067A, 002B, 091A, 039A, 063A, 013...\n",
      "kitten                       [047A, 042A, 050A, 043A, 048A]\n",
      "senior                 [116A, 051B, 117A, 016A, 061A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 11, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 5, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A' '011A'\n",
      " '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A' '045A'\n",
      " '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '062A' '064A' '065A' '069A' '070A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A'\n",
      " '039A' '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A'\n",
      " '067A' '068A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'002A'}\n",
      "Moved to Test Set:\n",
      "{'002A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '003A' '004A' '005A' '006A' '008A' '009A' '011A'\n",
      " '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A' '045A'\n",
      " '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '062A' '064A' '065A' '069A' '070A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A'\n",
      " '039A' '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A'\n",
      " '067A' '068A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "Length of X_train_val:\n",
      "692\n",
      "Length of y_train_val:\n",
      "692\n",
      "Length of groups_train_val:\n",
      "692\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     409\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     179\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     435\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     153\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 435, 2: 137, 1: 120})\n",
      "Epoch 1/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 1.2082 - accuracy: 0.4653\n",
      "Epoch 2/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9682 - accuracy: 0.5882\n",
      "Epoch 3/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.9079 - accuracy: 0.6199\n",
      "Epoch 4/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.8337 - accuracy: 0.6416\n",
      "Epoch 5/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7492 - accuracy: 0.6705\n",
      "Epoch 6/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.7718 - accuracy: 0.6705\n",
      "Epoch 7/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6781 - accuracy: 0.7197\n",
      "Epoch 8/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6823 - accuracy: 0.7211\n",
      "Epoch 9/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6363 - accuracy: 0.7153\n",
      "Epoch 10/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6128 - accuracy: 0.7514\n",
      "Epoch 11/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.6493 - accuracy: 0.7384\n",
      "Epoch 12/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5617 - accuracy: 0.7645\n",
      "Epoch 13/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5789 - accuracy: 0.7616\n",
      "Epoch 14/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5878 - accuracy: 0.7673\n",
      "Epoch 15/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5254 - accuracy: 0.7832\n",
      "Epoch 16/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5250 - accuracy: 0.7832\n",
      "Epoch 17/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.5012 - accuracy: 0.7746\n",
      "Epoch 18/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4864 - accuracy: 0.8049\n",
      "Epoch 19/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4903 - accuracy: 0.7991\n",
      "Epoch 20/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4871 - accuracy: 0.8049\n",
      "Epoch 21/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4714 - accuracy: 0.8020\n",
      "Epoch 22/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4626 - accuracy: 0.8165\n",
      "Epoch 23/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4275 - accuracy: 0.8338\n",
      "Epoch 24/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4465 - accuracy: 0.8280\n",
      "Epoch 25/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4367 - accuracy: 0.8454\n",
      "Epoch 26/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4530 - accuracy: 0.8165\n",
      "Epoch 27/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.8266\n",
      "Epoch 28/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4033 - accuracy: 0.8454\n",
      "Epoch 29/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3921 - accuracy: 0.8613\n",
      "Epoch 30/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4006 - accuracy: 0.8251\n",
      "Epoch 31/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4211 - accuracy: 0.8266\n",
      "Epoch 32/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4184 - accuracy: 0.8208\n",
      "Epoch 33/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.4087 - accuracy: 0.8338\n",
      "Epoch 34/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3795 - accuracy: 0.8353\n",
      "Epoch 35/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3802 - accuracy: 0.8540\n",
      "Epoch 36/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3745 - accuracy: 0.8483\n",
      "Epoch 37/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3936 - accuracy: 0.8338\n",
      "Epoch 38/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3726 - accuracy: 0.8396\n",
      "Epoch 39/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3919 - accuracy: 0.8367\n",
      "Epoch 40/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3536 - accuracy: 0.8512\n",
      "Epoch 41/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3707 - accuracy: 0.8555\n",
      "Epoch 42/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3516 - accuracy: 0.8526\n",
      "Epoch 43/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3524 - accuracy: 0.8569\n",
      "Epoch 44/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3459 - accuracy: 0.8627\n",
      "Epoch 45/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3408 - accuracy: 0.8569\n",
      "Epoch 46/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3157 - accuracy: 0.8772\n",
      "Epoch 47/1500\n",
      "22/22 [==============================] - 0s 993us/step - loss: 0.3483 - accuracy: 0.8699\n",
      "Epoch 48/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3156 - accuracy: 0.8757\n",
      "Epoch 49/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3245 - accuracy: 0.8728\n",
      "Epoch 50/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3168 - accuracy: 0.8743\n",
      "Epoch 51/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3232 - accuracy: 0.8772\n",
      "Epoch 52/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.8844\n",
      "Epoch 53/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.8714\n",
      "Epoch 54/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3239 - accuracy: 0.8714\n",
      "Epoch 55/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3297 - accuracy: 0.8642\n",
      "Epoch 56/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2993 - accuracy: 0.9017\n",
      "Epoch 57/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3101 - accuracy: 0.8642\n",
      "Epoch 58/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3003 - accuracy: 0.8642\n",
      "Epoch 59/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3013 - accuracy: 0.8916\n",
      "Epoch 60/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2883 - accuracy: 0.8743\n",
      "Epoch 61/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2684 - accuracy: 0.9003\n",
      "Epoch 62/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3022 - accuracy: 0.8757\n",
      "Epoch 63/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2960 - accuracy: 0.8757\n",
      "Epoch 64/1500\n",
      "22/22 [==============================] - 0s 975us/step - loss: 0.2984 - accuracy: 0.8801\n",
      "Epoch 65/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2735 - accuracy: 0.8974\n",
      "Epoch 66/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2927 - accuracy: 0.8873\n",
      "Epoch 67/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.8829\n",
      "Epoch 68/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2690 - accuracy: 0.9003\n",
      "Epoch 69/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2767 - accuracy: 0.8902\n",
      "Epoch 70/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.3045 - accuracy: 0.8728\n",
      "Epoch 71/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2693 - accuracy: 0.8844\n",
      "Epoch 72/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2865 - accuracy: 0.8801\n",
      "Epoch 73/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2510 - accuracy: 0.9075\n",
      "Epoch 74/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2518 - accuracy: 0.9003\n",
      "Epoch 75/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2935 - accuracy: 0.8902\n",
      "Epoch 76/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2408 - accuracy: 0.9090\n",
      "Epoch 77/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2799 - accuracy: 0.8945\n",
      "Epoch 78/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.9046\n",
      "Epoch 79/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9075\n",
      "Epoch 80/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2501 - accuracy: 0.9075\n",
      "Epoch 81/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2937 - accuracy: 0.8815\n",
      "Epoch 82/1500\n",
      "22/22 [==============================] - 0s 995us/step - loss: 0.2674 - accuracy: 0.8945\n",
      "Epoch 83/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2735 - accuracy: 0.8844\n",
      "Epoch 84/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.9032\n",
      "Epoch 85/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2644 - accuracy: 0.8945\n",
      "Epoch 86/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.9292\n",
      "Epoch 87/1500\n",
      "22/22 [==============================] - 0s 986us/step - loss: 0.2223 - accuracy: 0.9205\n",
      "Epoch 88/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.9090\n",
      "Epoch 89/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9176\n",
      "Epoch 90/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.9003\n",
      "Epoch 91/1500\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.2296 - accuracy: 0.9220\n",
      "Epoch 92/1500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.2373 - accuracy: 0.9147\n",
      "Epoch 93/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2450 - accuracy: 0.9017\n",
      "Epoch 94/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2343 - accuracy: 0.9090\n",
      "Epoch 95/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2000 - accuracy: 0.9220\n",
      "Epoch 96/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2215 - accuracy: 0.9191\n",
      "Epoch 97/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2346 - accuracy: 0.9118\n",
      "Epoch 98/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2344 - accuracy: 0.9191\n",
      "Epoch 99/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9249\n",
      "Epoch 100/1500\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.2263 - accuracy: 0.9147\n",
      "Epoch 101/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2248 - accuracy: 0.9263\n",
      "Epoch 102/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2313 - accuracy: 0.9032\n",
      "Epoch 103/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2100 - accuracy: 0.9263\n",
      "Epoch 104/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2068 - accuracy: 0.9220\n",
      "Epoch 105/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2244 - accuracy: 0.9205\n",
      "Epoch 106/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2034 - accuracy: 0.9379\n",
      "Epoch 107/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2045 - accuracy: 0.9249\n",
      "Epoch 108/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2161 - accuracy: 0.9090\n",
      "Epoch 109/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2114 - accuracy: 0.9162\n",
      "Epoch 110/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9263\n",
      "Epoch 111/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9118\n",
      "Epoch 112/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1989 - accuracy: 0.9321\n",
      "Epoch 113/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.9364\n",
      "Epoch 114/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2117 - accuracy: 0.9090\n",
      "Epoch 115/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.9436\n",
      "Epoch 116/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.9335\n",
      "Epoch 117/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1793 - accuracy: 0.9335\n",
      "Epoch 118/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9335\n",
      "Epoch 119/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9379\n",
      "Epoch 120/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.9306\n",
      "Epoch 121/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.9306\n",
      "Epoch 122/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.9408\n",
      "Epoch 123/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9335\n",
      "Epoch 124/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2124 - accuracy: 0.9147\n",
      "Epoch 125/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2229 - accuracy: 0.9292\n",
      "Epoch 126/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2057 - accuracy: 0.9191\n",
      "Epoch 127/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2091 - accuracy: 0.9147\n",
      "Epoch 128/1500\n",
      "22/22 [==============================] - 0s 988us/step - loss: 0.1853 - accuracy: 0.9277\n",
      "Epoch 129/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1942 - accuracy: 0.9162\n",
      "Epoch 130/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9451\n",
      "Epoch 131/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9350\n",
      "Epoch 132/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9335\n",
      "Epoch 133/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9263\n",
      "Epoch 134/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.9335\n",
      "Epoch 135/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.2125 - accuracy: 0.9147\n",
      "Epoch 136/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9364\n",
      "Epoch 137/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.9379\n",
      "Epoch 138/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9509\n",
      "Epoch 139/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9451\n",
      "Epoch 140/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9379\n",
      "Epoch 141/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1546 - accuracy: 0.9393\n",
      "Epoch 142/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.9350\n",
      "Epoch 143/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9364\n",
      "Epoch 144/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.9393\n",
      "Epoch 145/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9364\n",
      "Epoch 146/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.9321\n",
      "Epoch 147/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9436\n",
      "Epoch 148/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9277\n",
      "Epoch 149/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9422\n",
      "Epoch 150/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9494\n",
      "Epoch 151/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9465\n",
      "Epoch 152/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9220\n",
      "Epoch 153/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9321\n",
      "Epoch 154/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9538\n",
      "Epoch 155/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9379\n",
      "Epoch 156/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9408\n",
      "Epoch 157/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9335\n",
      "Epoch 158/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9393\n",
      "Epoch 159/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9538\n",
      "Epoch 160/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9509\n",
      "Epoch 161/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9509\n",
      "Epoch 162/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9480\n",
      "Epoch 163/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9566\n",
      "Epoch 164/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9422\n",
      "Epoch 165/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9292\n",
      "Epoch 166/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9523\n",
      "Epoch 167/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.9465\n",
      "Epoch 168/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9523\n",
      "Epoch 169/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9465\n",
      "Epoch 170/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.9480\n",
      "Epoch 171/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9393\n",
      "Epoch 172/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9653\n",
      "Epoch 173/1500\n",
      "22/22 [==============================] - 0s 990us/step - loss: 0.1356 - accuracy: 0.9451\n",
      "Epoch 174/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9566\n",
      "Epoch 175/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9393\n",
      "Epoch 176/1500\n",
      "22/22 [==============================] - 0s 988us/step - loss: 0.1270 - accuracy: 0.9624\n",
      "Epoch 177/1500\n",
      "22/22 [==============================] - 0s 981us/step - loss: 0.1227 - accuracy: 0.9682\n",
      "Epoch 178/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9538\n",
      "Epoch 179/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9581\n",
      "Epoch 180/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9451\n",
      "Epoch 181/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9509\n",
      "Epoch 182/1500\n",
      "22/22 [==============================] - 0s 997us/step - loss: 0.1390 - accuracy: 0.9581\n",
      "Epoch 183/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9408\n",
      "Epoch 184/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9552\n",
      "Epoch 185/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9668\n",
      "Epoch 186/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9595\n",
      "Epoch 187/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9581\n",
      "Epoch 188/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9711\n",
      "Epoch 189/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9465\n",
      "Epoch 190/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9566\n",
      "Epoch 191/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9566\n",
      "Epoch 192/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9436\n",
      "Epoch 193/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9523\n",
      "Epoch 194/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.9465\n",
      "Epoch 195/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9566\n",
      "Epoch 196/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9465\n",
      "Epoch 197/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9581\n",
      "Epoch 198/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9624\n",
      "Epoch 199/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9682\n",
      "Epoch 200/1500\n",
      "22/22 [==============================] - 0s 995us/step - loss: 0.1153 - accuracy: 0.9639\n",
      "Epoch 201/1500\n",
      "22/22 [==============================] - 0s 989us/step - loss: 0.1058 - accuracy: 0.9697\n",
      "Epoch 202/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9624\n",
      "Epoch 203/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9610\n",
      "Epoch 204/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9624\n",
      "Epoch 205/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9566\n",
      "Epoch 206/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9624\n",
      "Epoch 207/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9494\n",
      "Epoch 208/1500\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.1239 - accuracy: 0.9653\n",
      "Epoch 209/1500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.1060 - accuracy: 0.9653\n",
      "Epoch 210/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9624\n",
      "Epoch 211/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9639\n",
      "Epoch 212/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9610\n",
      "Epoch 213/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9523\n",
      "Epoch 214/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9711\n",
      "Epoch 215/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1244 - accuracy: 0.9610\n",
      "Epoch 216/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9566\n",
      "Epoch 217/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9595\n",
      "Epoch 218/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9682\n",
      "Epoch 219/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9697\n",
      "Epoch 220/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9595\n",
      "Epoch 221/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9682\n",
      "Epoch 222/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9552\n",
      "Epoch 223/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9480\n",
      "Epoch 224/1500\n",
      "22/22 [==============================] - 0s 974us/step - loss: 0.1325 - accuracy: 0.9538\n",
      "Epoch 225/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9566\n",
      "Epoch 226/1500\n",
      "22/22 [==============================] - 0s 987us/step - loss: 0.1134 - accuracy: 0.9552\n",
      "Epoch 227/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9581\n",
      "Epoch 228/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9552\n",
      "Epoch 229/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9595\n",
      "Epoch 230/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9697\n",
      "Epoch 231/1500\n",
      "22/22 [==============================] - 0s 999us/step - loss: 0.0906 - accuracy: 0.9769\n",
      "Epoch 232/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9624\n",
      "Epoch 233/1500\n",
      "22/22 [==============================] - 0s 984us/step - loss: 0.0958 - accuracy: 0.9725\n",
      "Epoch 234/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9595\n",
      "Epoch 235/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9697\n",
      "Epoch 236/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9697\n",
      "Epoch 237/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9682\n",
      "Epoch 238/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9668\n",
      "Epoch 239/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9668\n",
      "Epoch 240/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9480\n",
      "Epoch 241/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0956 - accuracy: 0.9740\n",
      "Epoch 242/1500\n",
      "22/22 [==============================] - 0s 994us/step - loss: 0.1080 - accuracy: 0.9682\n",
      "Epoch 243/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9740\n",
      "Epoch 244/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9653\n",
      "Epoch 245/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9509\n",
      "Epoch 246/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9610\n",
      "Epoch 247/1500\n",
      "22/22 [==============================] - 0s 992us/step - loss: 0.0926 - accuracy: 0.9682\n",
      "Epoch 248/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9798\n",
      "Epoch 249/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9711\n",
      "Epoch 250/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9711\n",
      "Epoch 251/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9595\n",
      "Epoch 252/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9711\n",
      "Epoch 253/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9494\n",
      "Epoch 254/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9783\n",
      "Epoch 255/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9812\n",
      "Epoch 256/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.9769\n",
      "Epoch 257/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9697\n",
      "Epoch 258/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9697\n",
      "Epoch 259/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9798\n",
      "Epoch 260/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.9740\n",
      "Epoch 261/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.9783\n",
      "Epoch 262/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9697\n",
      "Epoch 263/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9769\n",
      "Epoch 264/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9581\n",
      "Epoch 265/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9711\n",
      "Epoch 266/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9725\n",
      "Epoch 267/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9624\n",
      "Epoch 268/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9740\n",
      "Epoch 269/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0761 - accuracy: 0.9812\n",
      "Epoch 270/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9769\n",
      "Epoch 271/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9711\n",
      "Epoch 272/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9682\n",
      "Epoch 273/1500\n",
      "22/22 [==============================] - 0s 994us/step - loss: 0.1124 - accuracy: 0.9595\n",
      "Epoch 274/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.9711\n",
      "Epoch 275/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9610\n",
      "Epoch 276/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9725\n",
      "Epoch 277/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9668\n",
      "Epoch 278/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9769\n",
      "Epoch 279/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.9754\n",
      "Epoch 280/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9697\n",
      "Epoch 281/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9754\n",
      "Epoch 282/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9798\n",
      "Epoch 283/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9827\n",
      "Epoch 284/1500\n",
      "22/22 [==============================] - 0s 981us/step - loss: 0.0782 - accuracy: 0.9769\n",
      "Epoch 285/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9769\n",
      "Epoch 286/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9783\n",
      "Epoch 287/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.9697\n",
      "Epoch 288/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9682\n",
      "Epoch 289/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9639\n",
      "Epoch 290/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.9725\n",
      "Epoch 291/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9841\n",
      "Epoch 292/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.9653\n",
      "Epoch 293/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.9769\n",
      "Epoch 294/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9783\n",
      "Epoch 295/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9697\n",
      "Epoch 296/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9610\n",
      "Epoch 297/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9769\n",
      "Epoch 298/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9682\n",
      "Epoch 299/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9798\n",
      "Epoch 300/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9769\n",
      "Epoch 301/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9552\n",
      "Epoch 302/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9653\n",
      "Epoch 303/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9725\n",
      "Epoch 304/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9624\n",
      "Epoch 305/1500\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.0809 - accuracy: 0.9769\n",
      "Epoch 306/1500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0763 - accuracy: 0.9769\n",
      "Epoch 307/1500\n",
      "22/22 [==============================] - 0s 2ms/step - loss: 0.0802 - accuracy: 0.9740\n",
      "Epoch 308/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9711\n",
      "Epoch 309/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9711\n",
      "Epoch 310/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.9725\n",
      "Epoch 311/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.9769\n",
      "Epoch 312/1500\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9682\n",
      "Epoch 313/1500\n",
      " 1/22 [>.............................] - ETA: 0s - loss: 0.0508 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 283.\n",
      "22/22 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9682\n",
      "Epoch 313: early stopping\n",
      "8/8 [==============================] - 0s 809us/step - loss: 0.9752 - accuracy: 0.7224\n",
      "8/8 [==============================] - 0s 766us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (22/29)\n",
      "Before appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "After appending - Cat IDs: 398, Predictions: 398, Actuals: 398, Gender: 398\n",
      "Final Test Results - Loss: 0.9751534461975098, Accuracy: 0.722449004650116, Precision: 0.78311986863711, Recall: 0.5549179021201976, F1 Score: 0.5979457496311429\n",
      "Confusion Matrix:\n",
      " [[144   0   9]\n",
      " [ 34  17   0]\n",
      " [ 25   0  16]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "001A    14\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "071A    10\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "007A     6\n",
      "109A     6\n",
      "108A     6\n",
      "037A     6\n",
      "008A     6\n",
      "044A     5\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "034A     5\n",
      "023B     5\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "060A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "054A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "018A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "091A     1\n",
      "110A     1\n",
      "115A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "057A    27\n",
      "074A    25\n",
      "029A    17\n",
      "097A    16\n",
      "106A    14\n",
      "028A    13\n",
      "025A    11\n",
      "005A    10\n",
      "015A     9\n",
      "045A     9\n",
      "072A     9\n",
      "094A     8\n",
      "023A     6\n",
      "053A     6\n",
      "021A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "003A     4\n",
      "058A     3\n",
      "056A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "038A     2\n",
      "073A     1\n",
      "076A     1\n",
      "026C     1\n",
      "100A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    266\n",
      "M    237\n",
      "F    211\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    100\n",
      "X     82\n",
      "F     41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 103A, 071A, 097B, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [055A, 059A, 113A, 116A, 051B, 054A, 117A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 028A, 074A, 062A, 029A, 005A, 072A, 009...\n",
      "kitten                                               [045A]\n",
      "senior     [093A, 097A, 057A, 106A, 104A, 056A, 058A, 094A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 15, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 1, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "Length of X_train_val:\n",
      "714\n",
      "Length of y_train_val:\n",
      "714\n",
      "Length of groups_train_val:\n",
      "714\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 451, 1: 162, 2: 101})\n",
      "Epoch 1/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.1251 - accuracy: 0.5070\n",
      "Epoch 2/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.9237 - accuracy: 0.6008\n",
      "Epoch 3/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8147 - accuracy: 0.6513\n",
      "Epoch 4/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8072 - accuracy: 0.6583\n",
      "Epoch 5/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7276 - accuracy: 0.7115\n",
      "Epoch 6/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6900 - accuracy: 0.7143\n",
      "Epoch 7/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6362 - accuracy: 0.7409\n",
      "Epoch 8/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6249 - accuracy: 0.7549\n",
      "Epoch 9/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5819 - accuracy: 0.7493\n",
      "Epoch 10/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5653 - accuracy: 0.7717\n",
      "Epoch 11/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5358 - accuracy: 0.7815\n",
      "Epoch 12/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.5420 - accuracy: 0.7843\n",
      "Epoch 13/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5249 - accuracy: 0.8053\n",
      "Epoch 14/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4863 - accuracy: 0.8193\n",
      "Epoch 15/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5156 - accuracy: 0.8053\n",
      "Epoch 16/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4674 - accuracy: 0.8305\n",
      "Epoch 17/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4667 - accuracy: 0.8235\n",
      "Epoch 18/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4078 - accuracy: 0.8375\n",
      "Epoch 19/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4225 - accuracy: 0.8277\n",
      "Epoch 20/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4352 - accuracy: 0.8375\n",
      "Epoch 21/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3990 - accuracy: 0.8473\n",
      "Epoch 22/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4137 - accuracy: 0.8445\n",
      "Epoch 23/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3917 - accuracy: 0.8515\n",
      "Epoch 24/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4040 - accuracy: 0.8431\n",
      "Epoch 25/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3855 - accuracy: 0.8613\n",
      "Epoch 26/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3826 - accuracy: 0.8585\n",
      "Epoch 27/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8361\n",
      "Epoch 28/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4122 - accuracy: 0.8501\n",
      "Epoch 29/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3551 - accuracy: 0.8697\n",
      "Epoch 30/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3571 - accuracy: 0.8529\n",
      "Epoch 31/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3776 - accuracy: 0.8669\n",
      "Epoch 32/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3394 - accuracy: 0.8683\n",
      "Epoch 33/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3522 - accuracy: 0.8683\n",
      "Epoch 34/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3065 - accuracy: 0.8824\n",
      "Epoch 35/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3385 - accuracy: 0.8754\n",
      "Epoch 36/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8711\n",
      "Epoch 37/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3547 - accuracy: 0.8697\n",
      "Epoch 38/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3443 - accuracy: 0.8655\n",
      "Epoch 39/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8810\n",
      "Epoch 40/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3201 - accuracy: 0.8782\n",
      "Epoch 41/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2901 - accuracy: 0.9020\n",
      "Epoch 42/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.3022 - accuracy: 0.8824\n",
      "Epoch 43/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.2986 - accuracy: 0.8796\n",
      "Epoch 44/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3054 - accuracy: 0.8978\n",
      "Epoch 45/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.2868 - accuracy: 0.9020\n",
      "Epoch 46/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.3355 - accuracy: 0.8613\n",
      "Epoch 47/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.2985 - accuracy: 0.8838\n",
      "Epoch 48/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.2875 - accuracy: 0.9020\n",
      "Epoch 49/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.2985 - accuracy: 0.8866\n",
      "Epoch 50/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3186 - accuracy: 0.8697\n",
      "Epoch 51/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3033 - accuracy: 0.8894\n",
      "Epoch 52/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2764 - accuracy: 0.8880\n",
      "Epoch 53/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.3030 - accuracy: 0.8894\n",
      "Epoch 54/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.2823 - accuracy: 0.8894\n",
      "Epoch 55/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2765 - accuracy: 0.8880\n",
      "Epoch 56/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.2960 - accuracy: 0.8866\n",
      "Epoch 57/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.8810\n",
      "Epoch 58/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2797 - accuracy: 0.8908\n",
      "Epoch 59/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8978\n",
      "Epoch 60/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2637 - accuracy: 0.8908\n",
      "Epoch 61/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2495 - accuracy: 0.9076\n",
      "Epoch 62/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2683 - accuracy: 0.8964\n",
      "Epoch 63/1500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.2541 - accuracy: 0.9048\n",
      "Epoch 64/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2825 - accuracy: 0.8824\n",
      "Epoch 65/1500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.2905 - accuracy: 0.8936\n",
      "Epoch 66/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.2537 - accuracy: 0.8950\n",
      "Epoch 67/1500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.2222 - accuracy: 0.9202\n",
      "Epoch 68/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2650 - accuracy: 0.8922\n",
      "Epoch 69/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.2485 - accuracy: 0.8992\n",
      "Epoch 70/1500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.2507 - accuracy: 0.9034\n",
      "Epoch 71/1500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.2435 - accuracy: 0.9146\n",
      "Epoch 72/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2263 - accuracy: 0.9062\n",
      "Epoch 73/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2647 - accuracy: 0.8936\n",
      "Epoch 74/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2536 - accuracy: 0.9104\n",
      "Epoch 75/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2539 - accuracy: 0.9062\n",
      "Epoch 76/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.2172 - accuracy: 0.9230\n",
      "Epoch 77/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1959 - accuracy: 0.9286\n",
      "Epoch 78/1500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.2188 - accuracy: 0.9132\n",
      "Epoch 79/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.2287 - accuracy: 0.9146\n",
      "Epoch 80/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.2773 - accuracy: 0.8964\n",
      "Epoch 81/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.2245 - accuracy: 0.9146\n",
      "Epoch 82/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2241 - accuracy: 0.9034\n",
      "Epoch 83/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2250 - accuracy: 0.9202\n",
      "Epoch 84/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2262 - accuracy: 0.9006\n",
      "Epoch 85/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2253 - accuracy: 0.9118\n",
      "Epoch 86/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.2187 - accuracy: 0.9272\n",
      "Epoch 87/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.2101 - accuracy: 0.9286\n",
      "Epoch 88/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9314\n",
      "Epoch 89/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9328\n",
      "Epoch 90/1500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.2305 - accuracy: 0.9188\n",
      "Epoch 91/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.2179 - accuracy: 0.9104\n",
      "Epoch 92/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.2053 - accuracy: 0.9272\n",
      "Epoch 93/1500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.2436 - accuracy: 0.8992\n",
      "Epoch 94/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.2043 - accuracy: 0.9272\n",
      "Epoch 95/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.2111 - accuracy: 0.9202\n",
      "Epoch 96/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.1957 - accuracy: 0.9328\n",
      "Epoch 97/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.2082 - accuracy: 0.9202\n",
      "Epoch 98/1500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.2070 - accuracy: 0.9202\n",
      "Epoch 99/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.9146\n",
      "Epoch 100/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.9188\n",
      "Epoch 101/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9328\n",
      "Epoch 102/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.1956 - accuracy: 0.9244\n",
      "Epoch 103/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9286\n",
      "Epoch 104/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9384\n",
      "Epoch 105/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2032 - accuracy: 0.9202\n",
      "Epoch 106/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.2241 - accuracy: 0.9188\n",
      "Epoch 107/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9398\n",
      "Epoch 108/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1706 - accuracy: 0.9384\n",
      "Epoch 109/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9398\n",
      "Epoch 110/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.1996 - accuracy: 0.9328\n",
      "Epoch 111/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.2088 - accuracy: 0.9188\n",
      "Epoch 112/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.9384\n",
      "Epoch 113/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1809 - accuracy: 0.9356\n",
      "Epoch 114/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1656 - accuracy: 0.9426\n",
      "Epoch 115/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.2082 - accuracy: 0.9188\n",
      "Epoch 116/1500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.1756 - accuracy: 0.9286\n",
      "Epoch 117/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9272\n",
      "Epoch 118/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1721 - accuracy: 0.9370\n",
      "Epoch 119/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.1809 - accuracy: 0.9314\n",
      "Epoch 120/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.1716 - accuracy: 0.9468\n",
      "Epoch 121/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1740 - accuracy: 0.9356\n",
      "Epoch 122/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.9440\n",
      "Epoch 123/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1561 - accuracy: 0.9510\n",
      "Epoch 124/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1795 - accuracy: 0.9258\n",
      "Epoch 125/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1920 - accuracy: 0.9244\n",
      "Epoch 126/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1663 - accuracy: 0.9426\n",
      "Epoch 127/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9398\n",
      "Epoch 128/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9370\n",
      "Epoch 129/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9342\n",
      "Epoch 130/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9328\n",
      "Epoch 131/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9426\n",
      "Epoch 132/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1429 - accuracy: 0.9538\n",
      "Epoch 133/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9426\n",
      "Epoch 134/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9482\n",
      "Epoch 135/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9384\n",
      "Epoch 136/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.2043 - accuracy: 0.9118\n",
      "Epoch 137/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1787 - accuracy: 0.9412\n",
      "Epoch 138/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9440\n",
      "Epoch 139/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9412\n",
      "Epoch 140/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9426\n",
      "Epoch 141/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9398\n",
      "Epoch 142/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1623 - accuracy: 0.9426\n",
      "Epoch 143/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9440\n",
      "Epoch 144/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9510\n",
      "Epoch 145/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9426\n",
      "Epoch 146/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9356\n",
      "Epoch 147/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9496\n",
      "Epoch 148/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9370\n",
      "Epoch 149/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9426\n",
      "Epoch 150/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9412\n",
      "Epoch 151/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.9440\n",
      "Epoch 152/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9314\n",
      "Epoch 153/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1711 - accuracy: 0.9342\n",
      "Epoch 154/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.9314\n",
      "Epoch 155/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1747 - accuracy: 0.9314\n",
      "Epoch 156/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.9496\n",
      "Epoch 157/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.9328\n",
      "Epoch 158/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1379 - accuracy: 0.9510\n",
      "Epoch 159/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1414 - accuracy: 0.9440\n",
      "Epoch 160/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.1452 - accuracy: 0.9552\n",
      "Epoch 161/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9482\n",
      "Epoch 162/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9426\n",
      "Epoch 163/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9524\n",
      "Epoch 164/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.1472 - accuracy: 0.9482\n",
      "Epoch 165/1500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.1318 - accuracy: 0.9468\n",
      "Epoch 166/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9538\n",
      "Epoch 167/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9454\n",
      "Epoch 168/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9552\n",
      "Epoch 169/1500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.1189 - accuracy: 0.9650\n",
      "Epoch 170/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1439 - accuracy: 0.9538\n",
      "Epoch 171/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9454\n",
      "Epoch 172/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9538\n",
      "Epoch 173/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9622\n",
      "Epoch 174/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9468\n",
      "Epoch 175/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9398\n",
      "Epoch 176/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9482\n",
      "Epoch 177/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9552\n",
      "Epoch 178/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9580\n",
      "Epoch 179/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9692\n",
      "Epoch 180/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9566\n",
      "Epoch 181/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.1298 - accuracy: 0.9678\n",
      "Epoch 182/1500\n",
      "23/23 [==============================] - 0s 959us/step - loss: 0.1301 - accuracy: 0.9482\n",
      "Epoch 183/1500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1194 - accuracy: 0.9552\n",
      "Epoch 184/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9524\n",
      "Epoch 185/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.1218 - accuracy: 0.9664\n",
      "Epoch 186/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.1133 - accuracy: 0.9636\n",
      "Epoch 187/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9608\n",
      "Epoch 188/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.1214 - accuracy: 0.9608\n",
      "Epoch 189/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.1215 - accuracy: 0.9594\n",
      "Epoch 190/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.1281 - accuracy: 0.9496\n",
      "Epoch 191/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.1097 - accuracy: 0.9594\n",
      "Epoch 192/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.1227 - accuracy: 0.9552\n",
      "Epoch 193/1500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.1310 - accuracy: 0.9538\n",
      "Epoch 194/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.1156 - accuracy: 0.9608\n",
      "Epoch 195/1500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.1275 - accuracy: 0.9566\n",
      "Epoch 196/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9496\n",
      "Epoch 197/1500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.1171 - accuracy: 0.9650\n",
      "Epoch 198/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9468\n",
      "Epoch 199/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9496\n",
      "Epoch 200/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9524\n",
      "Epoch 201/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9734\n",
      "Epoch 202/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.1066 - accuracy: 0.9706\n",
      "Epoch 203/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.1111 - accuracy: 0.9636\n",
      "Epoch 204/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.1133 - accuracy: 0.9650\n",
      "Epoch 205/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9664\n",
      "Epoch 206/1500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.1361 - accuracy: 0.9468\n",
      "Epoch 207/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9720\n",
      "Epoch 208/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9636\n",
      "Epoch 209/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9552\n",
      "Epoch 210/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9510\n",
      "Epoch 211/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9566\n",
      "Epoch 212/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9496\n",
      "Epoch 213/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9384\n",
      "Epoch 214/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9650\n",
      "Epoch 215/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9496\n",
      "Epoch 216/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9692\n",
      "Epoch 217/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9510\n",
      "Epoch 218/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9678\n",
      "Epoch 219/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9706\n",
      "Epoch 220/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9482\n",
      "Epoch 221/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.9650\n",
      "Epoch 222/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9650\n",
      "Epoch 223/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9692\n",
      "Epoch 224/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9636\n",
      "Epoch 225/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9720\n",
      "Epoch 226/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9552\n",
      "Epoch 227/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9608\n",
      "Epoch 228/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9650\n",
      "Epoch 229/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9678\n",
      "Epoch 230/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0884 - accuracy: 0.9720\n",
      "Epoch 231/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1111 - accuracy: 0.9580\n",
      "Epoch 232/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9790\n",
      "Epoch 233/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9594\n",
      "Epoch 234/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.9692\n",
      "Epoch 235/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9720\n",
      "Epoch 236/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9720\n",
      "Epoch 237/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9692\n",
      "Epoch 238/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9594\n",
      "Epoch 239/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9748\n",
      "Epoch 240/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9776\n",
      "Epoch 241/1500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.0817 - accuracy: 0.9748\n",
      "Epoch 242/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.1042 - accuracy: 0.9664\n",
      "Epoch 243/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9748\n",
      "Epoch 244/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.1224 - accuracy: 0.9552\n",
      "Epoch 245/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.0955 - accuracy: 0.9706\n",
      "Epoch 246/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9678\n",
      "Epoch 247/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9580\n",
      "Epoch 248/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.0978 - accuracy: 0.9706\n",
      "Epoch 249/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.9650\n",
      "Epoch 250/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9664\n",
      "Epoch 251/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9706\n",
      "Epoch 252/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9762\n",
      "Epoch 253/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9650\n",
      "Epoch 254/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9692\n",
      "Epoch 255/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9566\n",
      "Epoch 256/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9636\n",
      "Epoch 257/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9608\n",
      "Epoch 258/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9566\n",
      "Epoch 259/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9608\n",
      "Epoch 260/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9664\n",
      "Epoch 261/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9776\n",
      "Epoch 262/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.9748\n",
      "Epoch 263/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9790\n",
      "Epoch 264/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9762\n",
      "Epoch 265/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9720\n",
      "Epoch 266/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9734\n",
      "Epoch 267/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9636\n",
      "Epoch 268/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9650\n",
      "Epoch 269/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9664\n",
      "Epoch 270/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9818\n",
      "Epoch 271/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9594\n",
      "Epoch 272/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.9748\n",
      "Epoch 273/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9608\n",
      "Epoch 274/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9734\n",
      "Epoch 275/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.9846\n",
      "Epoch 276/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9734\n",
      "Epoch 277/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9762\n",
      "Epoch 278/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.9818\n",
      "Epoch 279/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.9804\n",
      "Epoch 280/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9678\n",
      "Epoch 281/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.0906 - accuracy: 0.9720\n",
      "Epoch 282/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.0910 - accuracy: 0.9706\n",
      "Epoch 283/1500\n",
      "23/23 [==============================] - 0s 8ms/step - loss: 0.0836 - accuracy: 0.9762\n",
      "Epoch 284/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.0807 - accuracy: 0.9706\n",
      "Epoch 285/1500\n",
      "23/23 [==============================] - 0s 945us/step - loss: 0.0877 - accuracy: 0.9692\n",
      "Epoch 286/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9734\n",
      "Epoch 287/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.9734\n",
      "Epoch 288/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9804\n",
      "Epoch 289/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.0802 - accuracy: 0.9734\n",
      "Epoch 290/1500\n",
      "23/23 [==============================] - 0s 947us/step - loss: 0.0815 - accuracy: 0.9734\n",
      "Epoch 291/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.0933 - accuracy: 0.9636\n",
      "Epoch 292/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9706\n",
      "Epoch 293/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.9776\n",
      "Epoch 294/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.0873 - accuracy: 0.9692\n",
      "Epoch 295/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9790\n",
      "Epoch 296/1500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.0772 - accuracy: 0.9776\n",
      "Epoch 297/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9720\n",
      "Epoch 298/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.9860\n",
      "Epoch 299/1500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.0733 - accuracy: 0.9776\n",
      "Epoch 300/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.0799 - accuracy: 0.9804\n",
      "Epoch 301/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.9664\n",
      "Epoch 302/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.9762\n",
      "Epoch 303/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9678\n",
      "Epoch 304/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9538\n",
      "Epoch 305/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.9776\n",
      "Epoch 306/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9622\n",
      "Epoch 307/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9678\n",
      "Epoch 308/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.9790\n",
      "Epoch 309/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9678\n",
      "Epoch 310/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9636\n",
      "Epoch 311/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.0980 - accuracy: 0.9692\n",
      "Epoch 312/1500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.0937 - accuracy: 0.9762\n",
      "Epoch 313/1500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.0729 - accuracy: 0.9720\n",
      "Epoch 314/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.0792 - accuracy: 0.9790\n",
      "Epoch 315/1500\n",
      "23/23 [==============================] - 0s 935us/step - loss: 0.0676 - accuracy: 0.9776\n",
      "Epoch 316/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9706\n",
      "Epoch 317/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9762\n",
      "Epoch 318/1500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.0708 - accuracy: 0.9776\n",
      "Epoch 319/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.0688 - accuracy: 0.9818\n",
      "Epoch 320/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.0792 - accuracy: 0.9776\n",
      "Epoch 321/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0867 - accuracy: 0.9650\n",
      "Epoch 322/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9720\n",
      "Epoch 323/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.0744 - accuracy: 0.9832\n",
      "Epoch 324/1500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.0642 - accuracy: 0.9818\n",
      "Epoch 325/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.0547 - accuracy: 0.9804\n",
      "Epoch 326/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.0712 - accuracy: 0.9720\n",
      "Epoch 327/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.0674 - accuracy: 0.9776\n",
      "Epoch 328/1500\n",
      "23/23 [==============================] - 0s 959us/step - loss: 0.0757 - accuracy: 0.9706\n",
      "Epoch 329/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.0836 - accuracy: 0.9706\n",
      "Epoch 330/1500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.0843 - accuracy: 0.9734\n",
      "Epoch 331/1500\n",
      "23/23 [==============================] - 0s 960us/step - loss: 0.0768 - accuracy: 0.9776\n",
      "Epoch 332/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.0687 - accuracy: 0.9804\n",
      "Epoch 333/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.0629 - accuracy: 0.9846\n",
      "Epoch 334/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9692\n",
      "Epoch 335/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.9762\n",
      "Epoch 336/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0561 - accuracy: 0.9860\n",
      "Epoch 337/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.9776\n",
      "Epoch 338/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.9902\n",
      "Epoch 339/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.9790\n",
      "Epoch 340/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.9790\n",
      "Epoch 341/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.9818\n",
      "Epoch 342/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.9734\n",
      "Epoch 343/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0576 - accuracy: 0.9846\n",
      "Epoch 344/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.9832\n",
      "Epoch 345/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.0688 - accuracy: 0.9734\n",
      "Epoch 346/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9734\n",
      "Epoch 347/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.0810 - accuracy: 0.9692\n",
      "Epoch 348/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0605 - accuracy: 0.9846\n",
      "Epoch 349/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.0610 - accuracy: 0.9804\n",
      "Epoch 350/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.0666 - accuracy: 0.9832\n",
      "Epoch 351/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.0850 - accuracy: 0.9706\n",
      "Epoch 352/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.9762\n",
      "Epoch 353/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9860\n",
      "Epoch 354/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.9776\n",
      "Epoch 355/1500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.0305 - accuracy: 1.0000Restoring model weights from the end of the best epoch: 325.\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.9846\n",
      "Epoch 355: early stopping\n",
      "7/7 [==============================] - 0s 986us/step - loss: 1.1519 - accuracy: 0.6682\n",
      "7/7 [==============================] - 0s 800us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.73 (22/30)\n",
      "Before appending - Cat IDs: 398, Predictions: 398, Actuals: 398, Gender: 398\n",
      "After appending - Cat IDs: 621, Predictions: 621, Actuals: 621, Gender: 621\n",
      "Final Test Results - Loss: 1.1518937349319458, Accuracy: 0.6681614518165588, Precision: 0.6672891072891073, Recall: 0.6810721044297686, F1 Score: 0.6495951417004049\n",
      "Confusion Matrix:\n",
      " [[119   3  15]\n",
      " [  1   8   0]\n",
      " [ 55   0  22]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "063A    11\n",
      "068A    11\n",
      "040A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "022A     9\n",
      "045A     9\n",
      "065A     9\n",
      "072A     9\n",
      "033A     9\n",
      "015A     9\n",
      "094A     8\n",
      "010A     8\n",
      "013B     8\n",
      "117A     7\n",
      "099A     7\n",
      "050A     7\n",
      "007A     6\n",
      "053A     6\n",
      "108A     6\n",
      "109A     6\n",
      "023A     6\n",
      "021A     5\n",
      "025C     5\n",
      "044A     5\n",
      "075A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "056A     3\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "113A     3\n",
      "069A     2\n",
      "025B     2\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "087A     2\n",
      "011A     2\n",
      "093A     2\n",
      "032A     2\n",
      "054A     2\n",
      "088A     1\n",
      "115A     1\n",
      "100A     1\n",
      "024A     1\n",
      "019B     1\n",
      "043A     1\n",
      "091A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "055A    20\n",
      "019A    17\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "111A    13\n",
      "051A    12\n",
      "036A    11\n",
      "014B    10\n",
      "071A    10\n",
      "095A     8\n",
      "027A     7\n",
      "031A     7\n",
      "037A     6\n",
      "008A     6\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "105A     4\n",
      "052A     4\n",
      "014A     3\n",
      "060A     3\n",
      "102A     2\n",
      "110A     1\n",
      "096A     1\n",
      "041A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    268\n",
      "X    259\n",
      "F    182\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    89\n",
      "F    70\n",
      "M    69\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 028A, 074...\n",
      "kitten    [044A, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 097B, 019A, 020A, 101A, 095A, 034A, 027...\n",
      "kitten                             [014B, 111A, 041A, 110A]\n",
      "senior                             [055A, 059A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "Length of X_train_val:\n",
      "709\n",
      "Length of y_train_val:\n",
      "709\n",
      "Length of groups_train_val:\n",
      "709\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 432, 1: 146, 2: 131})\n",
      "Epoch 1/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 1.1861 - accuracy: 0.4739\n",
      "Epoch 2/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.9749 - accuracy: 0.5670\n",
      "Epoch 3/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8681 - accuracy: 0.6121\n",
      "Epoch 4/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.8247 - accuracy: 0.6347\n",
      "Epoch 5/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7754 - accuracy: 0.6587\n",
      "Epoch 6/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7566 - accuracy: 0.6544\n",
      "Epoch 7/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.6856 - accuracy: 0.7052\n",
      "Epoch 8/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.7407 - accuracy: 0.6982\n",
      "Epoch 9/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.6881 - accuracy: 0.7151\n",
      "Epoch 10/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.6460 - accuracy: 0.7320\n",
      "Epoch 11/1500\n",
      "23/23 [==============================] - 0s 939us/step - loss: 0.6263 - accuracy: 0.7518\n",
      "Epoch 12/1500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.6658 - accuracy: 0.7151\n",
      "Epoch 13/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.6274 - accuracy: 0.7560\n",
      "Epoch 14/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.5903 - accuracy: 0.7489\n",
      "Epoch 15/1500\n",
      "23/23 [==============================] - 0s 965us/step - loss: 0.6203 - accuracy: 0.7602\n",
      "Epoch 16/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5802 - accuracy: 0.7800\n",
      "Epoch 17/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.5602 - accuracy: 0.7884\n",
      "Epoch 18/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.5890 - accuracy: 0.7391\n",
      "Epoch 19/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5457 - accuracy: 0.7828\n",
      "Epoch 20/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5317 - accuracy: 0.7969\n",
      "Epoch 21/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5462 - accuracy: 0.7729\n",
      "Epoch 22/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5225 - accuracy: 0.7800\n",
      "Epoch 23/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5131 - accuracy: 0.7913\n",
      "Epoch 24/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4790 - accuracy: 0.8039\n",
      "Epoch 25/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4977 - accuracy: 0.7884\n",
      "Epoch 26/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.5181 - accuracy: 0.7870\n",
      "Epoch 27/1500\n",
      "23/23 [==============================] - 0s 949us/step - loss: 0.4960 - accuracy: 0.7969\n",
      "Epoch 28/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.4583 - accuracy: 0.8209\n",
      "Epoch 29/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4784 - accuracy: 0.7884\n",
      "Epoch 30/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.4769 - accuracy: 0.8166\n",
      "Epoch 31/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4682 - accuracy: 0.7997\n",
      "Epoch 32/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4509 - accuracy: 0.8011\n",
      "Epoch 33/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4248 - accuracy: 0.8209\n",
      "Epoch 34/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.8223\n",
      "Epoch 35/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4627 - accuracy: 0.8025\n",
      "Epoch 36/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4385 - accuracy: 0.8209\n",
      "Epoch 37/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4317 - accuracy: 0.8406\n",
      "Epoch 38/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4591 - accuracy: 0.8166\n",
      "Epoch 39/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4327 - accuracy: 0.8096\n",
      "Epoch 40/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4245 - accuracy: 0.8293\n",
      "Epoch 41/1500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.3915 - accuracy: 0.8519\n",
      "Epoch 42/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.4181 - accuracy: 0.8336\n",
      "Epoch 43/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4145 - accuracy: 0.8251\n",
      "Epoch 44/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4228 - accuracy: 0.8209\n",
      "Epoch 45/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4183 - accuracy: 0.8237\n",
      "Epoch 46/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4008 - accuracy: 0.8505\n",
      "Epoch 47/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3846 - accuracy: 0.8378\n",
      "Epoch 48/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3902 - accuracy: 0.8279\n",
      "Epoch 49/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3820 - accuracy: 0.8364\n",
      "Epoch 50/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3610 - accuracy: 0.8505\n",
      "Epoch 51/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3729 - accuracy: 0.8477\n",
      "Epoch 52/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3670 - accuracy: 0.8406\n",
      "Epoch 53/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3764 - accuracy: 0.8505\n",
      "Epoch 54/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.4112 - accuracy: 0.8364\n",
      "Epoch 55/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8463\n",
      "Epoch 56/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3824 - accuracy: 0.8378\n",
      "Epoch 57/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3702 - accuracy: 0.8477\n",
      "Epoch 58/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3509 - accuracy: 0.8618\n",
      "Epoch 59/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.3759 - accuracy: 0.8449\n",
      "Epoch 60/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.3750 - accuracy: 0.8547\n",
      "Epoch 61/1500\n",
      "23/23 [==============================] - 0s 996us/step - loss: 0.3672 - accuracy: 0.8646\n",
      "Epoch 62/1500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.3588 - accuracy: 0.8519\n",
      "Epoch 63/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.3244 - accuracy: 0.8731\n",
      "Epoch 64/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.3434 - accuracy: 0.8717\n",
      "Epoch 65/1500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.3540 - accuracy: 0.8505\n",
      "Epoch 66/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.3402 - accuracy: 0.8547\n",
      "Epoch 67/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.3123 - accuracy: 0.8858\n",
      "Epoch 68/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.3411 - accuracy: 0.8590\n",
      "Epoch 69/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.3540 - accuracy: 0.8449\n",
      "Epoch 70/1500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.3670 - accuracy: 0.8533\n",
      "Epoch 71/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3337 - accuracy: 0.8717\n",
      "Epoch 72/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3166 - accuracy: 0.8688\n",
      "Epoch 73/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3252 - accuracy: 0.8674\n",
      "Epoch 74/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3161 - accuracy: 0.8688\n",
      "Epoch 75/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3123 - accuracy: 0.8787\n",
      "Epoch 76/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.8886\n",
      "Epoch 77/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.3237 - accuracy: 0.8759\n",
      "Epoch 78/1500\n",
      "23/23 [==============================] - 0s 946us/step - loss: 0.3529 - accuracy: 0.8561\n",
      "Epoch 79/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.3031 - accuracy: 0.8815\n",
      "Epoch 80/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3049 - accuracy: 0.8843\n",
      "Epoch 81/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.3331 - accuracy: 0.8632\n",
      "Epoch 82/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3354 - accuracy: 0.8688\n",
      "Epoch 83/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3091 - accuracy: 0.8872\n",
      "Epoch 84/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.3434 - accuracy: 0.8449\n",
      "Epoch 85/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.3087 - accuracy: 0.8829\n",
      "Epoch 86/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.2927 - accuracy: 0.8928\n",
      "Epoch 87/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.3072 - accuracy: 0.8787\n",
      "Epoch 88/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.2770 - accuracy: 0.9027\n",
      "Epoch 89/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2822 - accuracy: 0.8970\n",
      "Epoch 90/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3034 - accuracy: 0.8900\n",
      "Epoch 91/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3007 - accuracy: 0.8843\n",
      "Epoch 92/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3104 - accuracy: 0.8801\n",
      "Epoch 93/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.3087 - accuracy: 0.8745\n",
      "Epoch 94/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.3003 - accuracy: 0.8815\n",
      "Epoch 95/1500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.2790 - accuracy: 0.8914\n",
      "Epoch 96/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.2950 - accuracy: 0.8815\n",
      "Epoch 97/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.2967 - accuracy: 0.8872\n",
      "Epoch 98/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2839 - accuracy: 0.9041\n",
      "Epoch 99/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2747 - accuracy: 0.8914\n",
      "Epoch 100/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.8773\n",
      "Epoch 101/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2911 - accuracy: 0.8872\n",
      "Epoch 102/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2605 - accuracy: 0.8970\n",
      "Epoch 103/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3000 - accuracy: 0.8717\n",
      "Epoch 104/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2881 - accuracy: 0.8872\n",
      "Epoch 105/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2730 - accuracy: 0.8942\n",
      "Epoch 106/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8970\n",
      "Epoch 107/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2974 - accuracy: 0.8886\n",
      "Epoch 108/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2753 - accuracy: 0.8956\n",
      "Epoch 109/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2917 - accuracy: 0.8787\n",
      "Epoch 110/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2574 - accuracy: 0.9013\n",
      "Epoch 111/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.3134 - accuracy: 0.8787\n",
      "Epoch 112/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2742 - accuracy: 0.8829\n",
      "Epoch 113/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2813 - accuracy: 0.8956\n",
      "Epoch 114/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2346 - accuracy: 0.9097\n",
      "Epoch 115/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.9111\n",
      "Epoch 116/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2768 - accuracy: 0.8886\n",
      "Epoch 117/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2282 - accuracy: 0.9154\n",
      "Epoch 118/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2419 - accuracy: 0.9069\n",
      "Epoch 119/1500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.2583 - accuracy: 0.8872\n",
      "Epoch 120/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.2409 - accuracy: 0.8970\n",
      "Epoch 121/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2673 - accuracy: 0.9013\n",
      "Epoch 122/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2464 - accuracy: 0.9041\n",
      "Epoch 123/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.9083\n",
      "Epoch 124/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.2621 - accuracy: 0.8914\n",
      "Epoch 125/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2384 - accuracy: 0.9069\n",
      "Epoch 126/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2362 - accuracy: 0.9111\n",
      "Epoch 127/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2529 - accuracy: 0.9097\n",
      "Epoch 128/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2573 - accuracy: 0.8942\n",
      "Epoch 129/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.2423 - accuracy: 0.9126\n",
      "Epoch 130/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.2397 - accuracy: 0.9069\n",
      "Epoch 131/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.2371 - accuracy: 0.9097\n",
      "Epoch 132/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.2308 - accuracy: 0.9168\n",
      "Epoch 133/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.2556 - accuracy: 0.8956\n",
      "Epoch 134/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2445 - accuracy: 0.9055\n",
      "Epoch 135/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.2325 - accuracy: 0.9083\n",
      "Epoch 136/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2263 - accuracy: 0.9041\n",
      "Epoch 137/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9041\n",
      "Epoch 138/1500\n",
      "23/23 [==============================] - 0s 987us/step - loss: 0.2312 - accuracy: 0.9097\n",
      "Epoch 139/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2240 - accuracy: 0.9140\n",
      "Epoch 140/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2366 - accuracy: 0.9111\n",
      "Epoch 141/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2521 - accuracy: 0.9027\n",
      "Epoch 142/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2323 - accuracy: 0.9069\n",
      "Epoch 143/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.2241 - accuracy: 0.9097\n",
      "Epoch 144/1500\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.2433 - accuracy: 0.8999\n",
      "Epoch 145/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.2289 - accuracy: 0.8970\n",
      "Epoch 146/1500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.2426 - accuracy: 0.9154\n",
      "Epoch 147/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2213 - accuracy: 0.9083\n",
      "Epoch 148/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.9013\n",
      "Epoch 149/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.2419 - accuracy: 0.8984\n",
      "Epoch 150/1500\n",
      "23/23 [==============================] - 0s 1000us/step - loss: 0.2319 - accuracy: 0.9140\n",
      "Epoch 151/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.2517 - accuracy: 0.9140\n",
      "Epoch 152/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.2001 - accuracy: 0.9238\n",
      "Epoch 153/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.2150 - accuracy: 0.9210\n",
      "Epoch 154/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.2399 - accuracy: 0.9126\n",
      "Epoch 155/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.2300 - accuracy: 0.9140\n",
      "Epoch 156/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.2275 - accuracy: 0.9140\n",
      "Epoch 157/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.9126\n",
      "Epoch 158/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2257 - accuracy: 0.9097\n",
      "Epoch 159/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2359 - accuracy: 0.9069\n",
      "Epoch 160/1500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.2054 - accuracy: 0.9224\n",
      "Epoch 161/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2045 - accuracy: 0.9252\n",
      "Epoch 162/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9267\n",
      "Epoch 163/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.2070 - accuracy: 0.9140\n",
      "Epoch 164/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2204 - accuracy: 0.9069\n",
      "Epoch 165/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9252\n",
      "Epoch 166/1500\n",
      "23/23 [==============================] - 0s 968us/step - loss: 0.2299 - accuracy: 0.9097\n",
      "Epoch 167/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2014 - accuracy: 0.9337\n",
      "Epoch 168/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2145 - accuracy: 0.9140\n",
      "Epoch 169/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.2096 - accuracy: 0.9224\n",
      "Epoch 170/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9323\n",
      "Epoch 171/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9224\n",
      "Epoch 172/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2183 - accuracy: 0.9267\n",
      "Epoch 173/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2158 - accuracy: 0.9182\n",
      "Epoch 174/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2096 - accuracy: 0.9196\n",
      "Epoch 175/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9281\n",
      "Epoch 176/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.9196\n",
      "Epoch 177/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2075 - accuracy: 0.9154\n",
      "Epoch 178/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2162 - accuracy: 0.9126\n",
      "Epoch 179/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9252\n",
      "Epoch 180/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1951 - accuracy: 0.9379\n",
      "Epoch 181/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9295\n",
      "Epoch 182/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.9379\n",
      "Epoch 183/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9379\n",
      "Epoch 184/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9323\n",
      "Epoch 185/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1947 - accuracy: 0.9267\n",
      "Epoch 186/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9309\n",
      "Epoch 187/1500\n",
      "23/23 [==============================] - 0s 999us/step - loss: 0.1964 - accuracy: 0.9210\n",
      "Epoch 188/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1912 - accuracy: 0.9337\n",
      "Epoch 189/1500\n",
      "23/23 [==============================] - 0s 961us/step - loss: 0.1975 - accuracy: 0.9323\n",
      "Epoch 190/1500\n",
      "23/23 [==============================] - 0s 955us/step - loss: 0.1983 - accuracy: 0.9267\n",
      "Epoch 191/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9408\n",
      "Epoch 192/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.1990 - accuracy: 0.9196\n",
      "Epoch 193/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.9267\n",
      "Epoch 194/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1940 - accuracy: 0.9224\n",
      "Epoch 195/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.9365\n",
      "Epoch 196/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1881 - accuracy: 0.9295\n",
      "Epoch 197/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2097 - accuracy: 0.9224\n",
      "Epoch 198/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.9281\n",
      "Epoch 199/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9436\n",
      "Epoch 200/1500\n",
      "23/23 [==============================] - 0s 941us/step - loss: 0.1645 - accuracy: 0.9492\n",
      "Epoch 201/1500\n",
      "23/23 [==============================] - 0s 944us/step - loss: 0.1719 - accuracy: 0.9394\n",
      "Epoch 202/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1875 - accuracy: 0.9295\n",
      "Epoch 203/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9069\n",
      "Epoch 204/1500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.1862 - accuracy: 0.9224\n",
      "Epoch 205/1500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.1652 - accuracy: 0.9436\n",
      "Epoch 206/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.9408\n",
      "Epoch 207/1500\n",
      "23/23 [==============================] - 0s 971us/step - loss: 0.1692 - accuracy: 0.9379\n",
      "Epoch 208/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.1863 - accuracy: 0.9267\n",
      "Epoch 209/1500\n",
      "23/23 [==============================] - 0s 945us/step - loss: 0.1686 - accuracy: 0.9464\n",
      "Epoch 210/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.1736 - accuracy: 0.9492\n",
      "Epoch 211/1500\n",
      "23/23 [==============================] - 0s 950us/step - loss: 0.1675 - accuracy: 0.9394\n",
      "Epoch 212/1500\n",
      "23/23 [==============================] - 0s 951us/step - loss: 0.1845 - accuracy: 0.9323\n",
      "Epoch 213/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.1762 - accuracy: 0.9337\n",
      "Epoch 214/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.1871 - accuracy: 0.9295\n",
      "Epoch 215/1500\n",
      "23/23 [==============================] - 0s 994us/step - loss: 0.1493 - accuracy: 0.9520\n",
      "Epoch 216/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.9394\n",
      "Epoch 217/1500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1751 - accuracy: 0.9351\n",
      "Epoch 218/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1748 - accuracy: 0.9408\n",
      "Epoch 219/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9365\n",
      "Epoch 220/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.9478\n",
      "Epoch 221/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9464\n",
      "Epoch 222/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9478\n",
      "Epoch 223/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9351\n",
      "Epoch 224/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.1768 - accuracy: 0.9464\n",
      "Epoch 225/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9478\n",
      "Epoch 226/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.1497 - accuracy: 0.9450\n",
      "Epoch 227/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.1569 - accuracy: 0.9450\n",
      "Epoch 228/1500\n",
      "23/23 [==============================] - 0s 967us/step - loss: 0.1706 - accuracy: 0.9394\n",
      "Epoch 229/1500\n",
      "23/23 [==============================] - 0s 986us/step - loss: 0.1478 - accuracy: 0.9563\n",
      "Epoch 230/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.1828 - accuracy: 0.9323\n",
      "Epoch 231/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.1748 - accuracy: 0.9337\n",
      "Epoch 232/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.1750 - accuracy: 0.9252\n",
      "Epoch 233/1500\n",
      "23/23 [==============================] - 0s 969us/step - loss: 0.1780 - accuracy: 0.9323\n",
      "Epoch 234/1500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.1639 - accuracy: 0.9394\n",
      "Epoch 235/1500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.1543 - accuracy: 0.9520\n",
      "Epoch 236/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.1508 - accuracy: 0.9506\n",
      "Epoch 237/1500\n",
      "23/23 [==============================] - 0s 942us/step - loss: 0.1524 - accuracy: 0.9379\n",
      "Epoch 238/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9422\n",
      "Epoch 239/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9464\n",
      "Epoch 240/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9422\n",
      "Epoch 241/1500\n",
      "23/23 [==============================] - 0s 933us/step - loss: 0.1584 - accuracy: 0.9365\n",
      "Epoch 242/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9577\n",
      "Epoch 243/1500\n",
      "23/23 [==============================] - 0s 985us/step - loss: 0.1471 - accuracy: 0.9535\n",
      "Epoch 244/1500\n",
      "23/23 [==============================] - 0s 970us/step - loss: 0.1495 - accuracy: 0.9492\n",
      "Epoch 245/1500\n",
      "23/23 [==============================] - 0s 949us/step - loss: 0.1494 - accuracy: 0.9464\n",
      "Epoch 246/1500\n",
      "23/23 [==============================] - 0s 997us/step - loss: 0.1417 - accuracy: 0.9563\n",
      "Epoch 247/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.9535\n",
      "Epoch 248/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.1401 - accuracy: 0.9478\n",
      "Epoch 249/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.1568 - accuracy: 0.9365\n",
      "Epoch 250/1500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.1242 - accuracy: 0.9661\n",
      "Epoch 251/1500\n",
      "23/23 [==============================] - 0s 952us/step - loss: 0.1364 - accuracy: 0.9591\n",
      "Epoch 252/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9450\n",
      "Epoch 253/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9563\n",
      "Epoch 254/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9478\n",
      "Epoch 255/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9591\n",
      "Epoch 256/1500\n",
      "23/23 [==============================] - 0s 981us/step - loss: 0.1538 - accuracy: 0.9492\n",
      "Epoch 257/1500\n",
      "23/23 [==============================] - 0s 958us/step - loss: 0.1795 - accuracy: 0.9394\n",
      "Epoch 258/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.1544 - accuracy: 0.9365\n",
      "Epoch 259/1500\n",
      "23/23 [==============================] - 0s 972us/step - loss: 0.1541 - accuracy: 0.9436\n",
      "Epoch 260/1500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.1592 - accuracy: 0.9379\n",
      "Epoch 261/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9422\n",
      "Epoch 262/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9436\n",
      "Epoch 263/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9337\n",
      "Epoch 264/1500\n",
      "23/23 [==============================] - 0s 977us/step - loss: 0.1536 - accuracy: 0.9408\n",
      "Epoch 265/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.1563 - accuracy: 0.9337\n",
      "Epoch 266/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9450\n",
      "Epoch 267/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.1709 - accuracy: 0.9309\n",
      "Epoch 268/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9506\n",
      "Epoch 269/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.1355 - accuracy: 0.9492\n",
      "Epoch 270/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9436\n",
      "Epoch 271/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9379\n",
      "Epoch 272/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9408\n",
      "Epoch 273/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9422\n",
      "Epoch 274/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9549\n",
      "Epoch 275/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.1554 - accuracy: 0.9450\n",
      "Epoch 276/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9563\n",
      "Epoch 277/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9478\n",
      "Epoch 278/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.1468 - accuracy: 0.9577\n",
      "Epoch 279/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.9563\n",
      "Epoch 280/1500\n",
      "23/23 [==============================] - 0s 983us/step - loss: 0.1332 - accuracy: 0.9591\n",
      "Epoch 281/1500\n",
      "23/23 [==============================] - 0s 953us/step - loss: 0.1383 - accuracy: 0.9506\n",
      "Epoch 282/1500\n",
      "23/23 [==============================] - 0s 959us/step - loss: 0.1406 - accuracy: 0.9535\n",
      "Epoch 283/1500\n",
      "23/23 [==============================] - 0s 956us/step - loss: 0.1196 - accuracy: 0.9661\n",
      "Epoch 284/1500\n",
      "23/23 [==============================] - 0s 954us/step - loss: 0.1244 - accuracy: 0.9647\n",
      "Epoch 285/1500\n",
      "23/23 [==============================] - 0s 992us/step - loss: 0.1583 - accuracy: 0.9450\n",
      "Epoch 286/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.1469 - accuracy: 0.9520\n",
      "Epoch 287/1500\n",
      "23/23 [==============================] - 0s 963us/step - loss: 0.1392 - accuracy: 0.9535\n",
      "Epoch 288/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.1469 - accuracy: 0.9506\n",
      "Epoch 289/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.1310 - accuracy: 0.9520\n",
      "Epoch 290/1500\n",
      "23/23 [==============================] - 0s 948us/step - loss: 0.1371 - accuracy: 0.9535\n",
      "Epoch 291/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.1310 - accuracy: 0.9520\n",
      "Epoch 292/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9520\n",
      "Epoch 293/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9676\n",
      "Epoch 294/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9577\n",
      "Epoch 295/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.1294 - accuracy: 0.9520\n",
      "Epoch 296/1500\n",
      "23/23 [==============================] - 0s 957us/step - loss: 0.1129 - accuracy: 0.9619\n",
      "Epoch 297/1500\n",
      "23/23 [==============================] - 0s 975us/step - loss: 0.1290 - accuracy: 0.9549\n",
      "Epoch 298/1500\n",
      "23/23 [==============================] - 0s 959us/step - loss: 0.1197 - accuracy: 0.9619\n",
      "Epoch 299/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.1208 - accuracy: 0.9690\n",
      "Epoch 300/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.1142 - accuracy: 0.9647\n",
      "Epoch 301/1500\n",
      "23/23 [==============================] - 0s 991us/step - loss: 0.1329 - accuracy: 0.9563\n",
      "Epoch 302/1500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.1416 - accuracy: 0.9464\n",
      "Epoch 303/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9563\n",
      "Epoch 304/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.1294 - accuracy: 0.9577\n",
      "Epoch 305/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9619\n",
      "Epoch 306/1500\n",
      "23/23 [==============================] - 0s 990us/step - loss: 0.1212 - accuracy: 0.9647\n",
      "Epoch 307/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9633\n",
      "Epoch 308/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9535\n",
      "Epoch 309/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9337\n",
      "Epoch 310/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9520\n",
      "Epoch 311/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.1364 - accuracy: 0.9549\n",
      "Epoch 312/1500\n",
      "23/23 [==============================] - 0s 4ms/step - loss: 0.1312 - accuracy: 0.9464\n",
      "Epoch 313/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9605\n",
      "Epoch 314/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9676\n",
      "Epoch 315/1500\n",
      "23/23 [==============================] - 0s 976us/step - loss: 0.1214 - accuracy: 0.9605\n",
      "Epoch 316/1500\n",
      "23/23 [==============================] - 0s 979us/step - loss: 0.1252 - accuracy: 0.9605\n",
      "Epoch 317/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9520\n",
      "Epoch 318/1500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.1312 - accuracy: 0.9535\n",
      "Epoch 319/1500\n",
      "23/23 [==============================] - 0s 7ms/step - loss: 0.1102 - accuracy: 0.9676\n",
      "Epoch 320/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9619\n",
      "Epoch 321/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9506\n",
      "Epoch 322/1500\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.1126 - accuracy: 0.9605\n",
      "Epoch 323/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9549\n",
      "Epoch 324/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9591\n",
      "Epoch 325/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9619\n",
      "Epoch 326/1500\n",
      "23/23 [==============================] - 0s 940us/step - loss: 0.1416 - accuracy: 0.9464\n",
      "Epoch 327/1500\n",
      "23/23 [==============================] - 0s 973us/step - loss: 0.1440 - accuracy: 0.9478\n",
      "Epoch 328/1500\n",
      "23/23 [==============================] - 0s 980us/step - loss: 0.1205 - accuracy: 0.9605\n",
      "Epoch 329/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9535\n",
      "Epoch 330/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9520\n",
      "Epoch 331/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9577\n",
      "Epoch 332/1500\n",
      "23/23 [==============================] - 0s 964us/step - loss: 0.1077 - accuracy: 0.9647\n",
      "Epoch 333/1500\n",
      "23/23 [==============================] - 0s 959us/step - loss: 0.1203 - accuracy: 0.9506\n",
      "Epoch 334/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9478\n",
      "Epoch 335/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9633\n",
      "Epoch 336/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9591\n",
      "Epoch 337/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9577\n",
      "Epoch 338/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9619\n",
      "Epoch 339/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9661\n",
      "Epoch 340/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9676\n",
      "Epoch 341/1500\n",
      "23/23 [==============================] - 0s 989us/step - loss: 0.1184 - accuracy: 0.9577\n",
      "Epoch 342/1500\n",
      "23/23 [==============================] - 0s 962us/step - loss: 0.1137 - accuracy: 0.9661\n",
      "Epoch 343/1500\n",
      "23/23 [==============================] - 0s 966us/step - loss: 0.1237 - accuracy: 0.9492\n",
      "Epoch 344/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9506\n",
      "Epoch 345/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9647\n",
      "Epoch 346/1500\n",
      "23/23 [==============================] - 0s 982us/step - loss: 0.1334 - accuracy: 0.9591\n",
      "Epoch 347/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9619\n",
      "Epoch 348/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9647\n",
      "Epoch 349/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9676\n",
      "Epoch 350/1500\n",
      "23/23 [==============================] - 0s 993us/step - loss: 0.1144 - accuracy: 0.9619\n",
      "Epoch 351/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9591\n",
      "Epoch 352/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9633\n",
      "Epoch 353/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9619\n",
      "Epoch 354/1500\n",
      "23/23 [==============================] - 0s 998us/step - loss: 0.1171 - accuracy: 0.9605\n",
      "Epoch 355/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9591\n",
      "Epoch 356/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9549\n",
      "Epoch 357/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9661\n",
      "Epoch 358/1500\n",
      "23/23 [==============================] - 0s 974us/step - loss: 0.1098 - accuracy: 0.9563\n",
      "Epoch 359/1500\n",
      "23/23 [==============================] - 0s 984us/step - loss: 0.1226 - accuracy: 0.9619\n",
      "Epoch 360/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9661\n",
      "Epoch 361/1500\n",
      "23/23 [==============================] - 0s 929us/step - loss: 0.1137 - accuracy: 0.9633\n",
      "Epoch 362/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9436\n",
      "Epoch 363/1500\n",
      "23/23 [==============================] - 0s 978us/step - loss: 0.1038 - accuracy: 0.9633\n",
      "Epoch 364/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.9450\n",
      "Epoch 365/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9732\n",
      "Epoch 366/1500\n",
      "23/23 [==============================] - 0s 995us/step - loss: 0.1150 - accuracy: 0.9619\n",
      "Epoch 367/1500\n",
      "23/23 [==============================] - 0s 988us/step - loss: 0.1222 - accuracy: 0.9633\n",
      "Epoch 368/1500\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9605\n",
      "Epoch 369/1500\n",
      " 1/23 [>.............................] - ETA: 0s - loss: 0.0798 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 339.\n",
      "23/23 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9647\n",
      "Epoch 369: early stopping\n",
      "8/8 [==============================] - 0s 919us/step - loss: 0.6370 - accuracy: 0.7763\n",
      "8/8 [==============================] - 0s 589us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.82 (23/28)\n",
      "Before appending - Cat IDs: 621, Predictions: 621, Actuals: 621, Gender: 621\n",
      "After appending - Cat IDs: 849, Predictions: 849, Actuals: 849, Gender: 849\n",
      "Final Test Results - Loss: 0.6370483040809631, Accuracy: 0.7763158082962036, Precision: 0.7435231435231434, Recall: 0.7682378614293509, F1 Score: 0.7543519057788132\n",
      "Confusion Matrix:\n",
      " [[126   4  26]\n",
      " [  3  22   0]\n",
      " [ 18   0  29]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6648288481766311\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8413750678300858\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7329080402851105\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.7155811998155249\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6747049186600224\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[4]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ###################\n",
    "    # PREPARING MODEL #\n",
    "    ###################\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90d80b-198d-4293-a1a0-73a65f6588d0",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "e9dd5399-64d4-435b-9e73-cb31f16d042d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 849, Predictions: 849, Actuals: 849, Gender: 849\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "9d9869f0-e23f-4453-a329-395fa44f1208",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "a7f81185-7b51-497f-98cb-80c4b8f35388",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.76 (84/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "adbd2ca0-3dea-4b42-bfad-93138cb1ae30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b1ea4fe7-6ee1-4185-a009-b864d9aa6b85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, kitten, kitten, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, adult, adult, senior,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, adult, kitten, adult, kitten, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, kitten, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[senior, adult, senior, adult, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, senior, adult, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, kitten, kitten, senior, adult, adult, ...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, s...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [senior, senior, senior, adult, adult, senior,...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, adult, adult, senior, senior, senior,...        senior           senior                   True\n",
       "61    055A  [adult, adult, senior, adult, senior, senior, ...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "51    045A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "50    044A             [kitten, adult, adult, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [senior, senior, adult]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "100   105A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "99    104A                    [senior, senior, adult, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                    [senior, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [senior, adult, kitten, adult, adult, adult, a...         adult            adult                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "29    025B                                    [adult, senior]         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "16    014B  [kitten, adult, kitten, adult, kitten, kitten,...        kitten           kitten                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "12    011A                                   [senior, senior]        senior           senior                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, senior, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "109   117A  [senior, senior, adult, adult, senior, adult, ...        senior           senior                   True\n",
       "40    034A              [adult, senior, adult, senior, adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "41    035A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "52    047A  [adult, adult, adult, adult, adult, kitten, ad...         adult           kitten                  False\n",
       "54    049A                                            [adult]         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "101   106A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "102   108A      [senior, adult, adult, senior, adult, senior]         adult           senior                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "56    051A  [adult, adult, adult, adult, senior, senior, a...         adult           senior                  False\n",
       "42    036A  [senior, senior, adult, senior, adult, adult, ...        senior            adult                  False\n",
       "48    042A  [kitten, adult, adult, adult, adult, adult, ad...         adult           kitten                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "92    097A  [senior, adult, senior, adult, adult, senior, ...         adult           senior                  False\n",
       "57    051B  [adult, adult, adult, adult, senior, adult, ad...         adult           senior                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "60    054A                                    [adult, senior]         adult           senior                  False\n",
       "90    095A  [senior, senior, adult, senior, senior, senior...        senior            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "62    056A                              [adult, adult, adult]         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, adult, senior, senior, a...         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "64    058A                             [senior, adult, adult]         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "47    041A                                    [adult, kitten]         adult           kitten                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e03366c5-a807-4159-b0c1-8dc88c04d91e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     66\n",
      "kitten     9\n",
      "senior     9\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "0ae8f86b-0e19-49df-a07c-f64383bce76b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             66  90.410959\n",
      "1           kitten           15              9  60.000000\n",
      "2           senior           22              9  40.909091\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "6350c1b2-050d-4bf3-98f1-5a80b3078d3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ea2c7-0827-4ffa-9777-ad2891e5f49b",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "94fbe612-c62e-4abe-8ec3-d75940a993cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          562            482  85.765125\n",
      "1           kitten          109             63  57.798165\n",
      "2           senior          178             75  42.134831\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "50e84f5a-909e-47ca-93ca-2e92abf7ded1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ac46e-4cc6-4237-925c-524d47e83b46",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "1743599c-0d3f-4b10-a095-02c36218c679",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    226      181     80.09\n",
      "1          M    337      263     78.04\n",
      "2          X    286      176     61.54\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b8546267-aa7a-4e4b-b60e-810f836ebf8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABN1UlEQVR4nO3deXxM9/7H8feIkGQSxJISsUftxNpQKnZVa2u7Vzeq6LVfV9uLoi0/vZZoo5aWq1XSkqpdq5baitBaY98aQtRWQhYkMr8/PHKuaUJjMjET83o+Hh6Pme/5njOfE077nm++53tMFovFIgAAAMBF5HJ0AQAAAMDjRAAGAACASyEAAwAAwKUQgAEAAOBSCMAAAABwKQRgAAAAuBQCMAAAAFwKARgAAAAuhQAMAAAAl5Lb0QUAeLIlJSWpdevWSkhIkCRVqFBB4eHhDq4KsbGxat++vfH+119/dWA10sWLF7Vq1Spt2bJFv//+u+Li4pQ3b14VLVpUNWrUUMeOHVW5cmWH1vgwderUMV6vWLFC/v7+DqwGwF8hAAPIVuvWrTPCryQdO3ZMhw4dUpUqVRxYFZzJihUrNGXKFKt/J5KUkpKiU6dO6dSpU1q6dKm6d++uf/7znzKZTA6qFMCTggAMIFstX748XdvSpUsJwJAkLViwQB9//LHxPn/+/HrmmWdUuHBhXblyRdu3b1d8fLwsFou++eYb+fr6qlevXo4rGMATgQAMINtER0dr//79kqR8+fLpxo0bkqS1a9dq6NChMpvNjiwPDhYVFaVp06YZ759//nm9++67Vv8u4uPj9fbbb2vXrl2SpLlz56pr167y9vZ+7PUCeHIQgAFkm/tHf7t06aLIyEgdOnRIiYmJWrNmjV566aUH7nv06FHNnz9fe/bs0fXr11WwYEGVK1dO3bt3V4MGDdL1j4+PV3h4uDZu3Khz587J3d1d/v7+atmypbp06SIvLy+j79ixY7Vq1SpJ0ptvvqm+ffsa23799Vf169dPklSsWDGtXLnS2JY2z7NQoUKaPXu2xo4dqyNHjihfvnx6++231axZM925c0fh4eFat26dYmJidPv2bZnNZpUpU0YvvfSSXnjhBZtr79Wrlw4cOCBJGjJkiF5++WWr43zzzTeaMmWKJKlhw4ZWI6t/5c6dO/riiy+0cuVK/fHHHwoICFD79u3VvXt35c59738VI0eO1I8//ihJ6tq1q95++22rY2zatEn/+te/JEnlypXTokWLHvqZs2bN0t27dyVJVapU0dixY+Xm5mbVx9vbW++//75GjhypUqVKqVy5ckpJSbHqk5qaqmXLlmnZsmU6ffq03NzcVLp0ab3wwgt68cUXjfrT3P/3+OOPP2rZsmWKiIjQmTNn5OPjoyZNmqhv374qUKCA1X53797VwoULtXz5cp07d04FCxZUu3bt1LNnz4ee55UrVzR37lxt3bpVV65cUb58+VS9enW99tprqlq1qlXfzz77TLNnz5Ykvfvuu7px44a+/vprJSUlqXLlysY2AFlDAAaQLVJSUrR69Wrjfbt27VS0aFEdOnRI0r1pEA8KwKtWrdKHH35ohCPp3k1SFy9e1Pbt2zVgwAC9/vrrxrbff/9db731lmJiYoy2W7du6dixYzp27Jg2bNigWbNmWYXgrLh165YGDBig2NhYSdLVq1f19NNPKzU1VSNHjtTGjRut+t+8eVMHDhzQgQMHdO7cOavA/Si1t2/f3gjAa9euTReA161bZ7xu27btI53TkCFDjFFWSTp9+rQ+/vhj7d+/XxMnTpTJZFKHDh2MALxhwwb961//Uq5c/1tM6FE+Py4uTr/88ovxvkePHunCb5oiRYro888/z3BbSkqK3nnnHW3evNmq/dChQzp06JA2b96sqVOnKk+ePBnu/9FHH2nx4sXG+9u3b+vbb7/VwYMH9cUXXxjh2WKx6N1337X6u/399981e/Zs4+8kIydPnlT//v119epVo+3q1avauHGjNm/erBEjRqhjx44Z7rtkyRIdP37ceF+0aNEHfg6AR8MyaACyxdatW/XHH39IkmrWrKmAgAC1bNlSnp6eku6N8B45ciTdfqdPn9b48eON8Fu+fHl16dJFwcHBRp9PP/1Ux44dM96PHDnSCJDe3t5q27atOnToYPwq/fDhw5o5c6bdzi0hIUGxsbFq1KiROnXqpGeeeUYlSpTQzz//bAQks9msDh06qHv37nr66aeNfb/++mtZLBabam/ZsqUR4g8fPqxz584Zx/n9998VFRUl6d50k+eee+6RzmnXrl2qVKmSunTpoooVKxrtGzduNEby69atq+LFi0u6F+J2795t9Lt9+7a2bt0qSXJzc9Pzzz//0M87duyYUlNTjfdBQUGPVG+aL7/80gi/uXPnVsuWLdWpUyfly5dPkrRz584HjppevXpVixcv1tNPP53u7+nIkSNWK2MsX77cKvxWqFDB+Fnt3Lkzw+OnhfO08FusWDF17txZzz77rKR7I9cfffSRTp48meH+x48fV+HChdW1a1fVqlVLrVq1yuyPBcBfYAQYQLa4f/pDu3btJN0Lhc2bNzemFSxZskQjR4602u+bb75RcnKyJCkkJEQfffSRMQo3btw4LVu2TGazWbt27VKFChW0f/9+Y56x2WzWggULFBAQYHxu79695ebmpkOHDik1NdVqxDIrmjRpokmTJlm15cmTRx07dtSJEyfUr18/1a9fX9K9Ed0WLVooKSlJCQkJun79unx9fR+5di8vLzVv3lwrVqyQdG8UOO2GsPXr1xvBumXLlg8c8XyQFi1aaPz48cqVK5dSU1P13nvvGaO9S5YsUceOHWUymdSuXTvNmjXL+Py6detKkrZt26bExERJMm5ie5i0L0dpChYsaPV+2bJlGjduXIb7pk1bSU5OtlpSb+rUqcbP/LXXXtPf//53JSYmKiIiQm+88YY8PDzSHathw4YKDQ1Vrly5dOvWLXXq1EmXL1+WdO/LWNoXryVLlhj7NGnSRB999JHc3NzS/azut2nTJp05c0aSVLJkSS1YsMD4AvPVV18pLCxMKSkpWrhwoUaNGpXhuU6bNk3ly5fPcBsA2zECDMDuLl26pB07dkiSPD091bx5c2Nbhw4djNdr1641QlOa+0fdunbtajV/s3///lq2bJk2bdqkV155JV3/5557zgiQ0r1RxQULFmjLli2aO3eu3cKvpAxH44KDgzVq1CjNmzdP9evX1+3bt7Vv3z7Nnz/fatT39u3bNtf+559fmvXr1xuvH3X6gyT17NnT+IxcuXLp1VdfNbYdO3bM+FLStm1bo99PP/1kzMe9f/pD2heeh8mbN6/V+z/P682Mo0eP6ubNm5Kk4sWLG+FXkgICAlSrVi1J90bsDx48mOExunfvbpyPh4eH1eokaf82k5OTrX7jkPbFREr/s7rf/VNK2rRpYzUF5/41mB80gly2bFnCL5BNGAEGYHcrV640pjC4ubkZN0alMZlMslgsSkhI0I8//qhOnToZ2y5dumS8LlasmNV+vr6+8vX1tWp7WH9JVr/Oz4z7g+rDZPRZ0r2pCEuWLFFkZKSOHTtmNY85Tdqv/m2pvUaNGipdurSio6N18uRJ/fbbb/L09DQCXunSpdPdWJUZJUuWtHpfunRp4/Xdu3cVFxenwoULq2jRogoODtb27dsVFxennTt3qnbt2vr5558lST4+PpmafuHn52f1/uLFiypVqpTxvnz58nrttdeM92vWrNHFixet9vn999+N1+fPn7d6GMWfRUdHZ7j9z/Nq7w+paX93cXFxVn+P99cpWf+sHlTfrFmzjJHzP7tw4YJu3bqVboT6Qf/GAGQdARiAXVksFuNX9NK9FQ7uHwn7s6VLl1oF4PtlFB4f5lH7S+kDb9pI51/JaAm3/fv3a+DAgUpMTJTJZFJQUJBq1aql6tWra9y4ccav1jPyKLV36NBBn3zyiaR7o8D3hzZbRn+le+d9fwD7cz3336DWvn17bd++3fj8pKQkJSUlSbo3leLPo7sZKVeunLy8vIxR1l9//dUqWFapUsVqNDYqKipdAL6/xty5cyt//vwP/LwHjTD/eapIZn5L8OdjPejY989xNpvNGU7BSJOYmJhuO8sEAtmHAAzArnbv3q3z589nuv/hw4d17NgxVahQQdK9kcG0m8Kio6OtRtfOnj2r7777TmXLllWFChVUsWJFq5HEtPmW95s5c6Z8fHxUrlw51axZUx4eHlYh59atW1b9r1+/nqm63d3d07WFhoYage7DDz9U69atjW0ZhSRbapekF154QdOnT1dKSorWrl1rBKVcuXKpTZs2mar/z06cOGFMGZDu/azT5M2b17ipTJIaN26sAgUK6Pr169q0aZOxvrOUuekP0r3pBo0bN9YPP/wg6d7c73bt2j1w7nJGI/P3//z8/f2t5ulK9wLyg1aWeBQFChRQnjx5dOfOHUn3fjb3P5b5t99+y3C/IkWKGK9ff/11q+XSMjMfPaN/YwDsgznAAOxq2bJlxuvu3bvr119/zfBPvXr1jH73B5fatWsbryMiIqxGZCMiIhQeHq4PP/xQ//3vf9P137Fjh06dOmW8P3r0qP773//q448/1pAhQ4wAc3+YO336tFX9GzZsyNR5ZvQ43hMnThiv719DdseOHbp27ZrxPm1k0JbapXs3jDVq1EjSveB8+PBhSVK9evXSTS3IrLlz5xoh3WKxaN68eca2qlWrWgVJd3d3I2gnJCQYqz+ULFlS1apVy/Rn9uzZ0xgtjo6O1rvvvmvM6U0THx+v0NBQ7du3L93+lStXNka/z549a0zDkO6tvdu0aVO9+OKLGj58+ENH3/9K7ty5rc7r/jndKSkpmjNnTob73f/3u2LFCsXHxxvvIyIi1LhxY7322msPnBrBI5+B7MMIMAC7uXnzptVSUfff/PZnrVq1MqZGrFmzRkOGDJGnp6e6d++uVatWKSUlRbt27dLf/vY31a1bV+fPnzd+7S5J3bp1k3TvZrHq1avrwIEDun37tnr27KnGjRvLw8PD6sasNm3aGMH3/huLtm/frgkTJqhChQravHmztm3bZvP5Fy5c2FgbeMSIEWrZsqWuXr2qLVu2WPVLuwnOltrTdOjQId16w7ZOf5CkyMhIvfzyy6pTp44OHjxoddNY165d0/Xv0KGDvv766yx9ftmyZTV48GBNnDhRkrRlyxa1b99e9evXV+HChXXx4kVFRkYqISHBar+0EW8PDw+9+OKLWrBggSRp2LBheu655+Tn56fNmzcrISFBCQkJ8vHxsRqNtUX37t2NZd/WrVunCxcuqEqVKtq7d6/VWr33a968uWbOnKmLFy8qJiZGXbp0UaNGjZSYmKj169crJSVFhw4dyvSoOQD7YQQYgN388MMPRrgrUqSIatSo8cC+TZs2NX7Fm3YznCQFBgbq3//+tzHiGB0drW+//dYq/Pbs2dPqhqZx48YZ69MmJibqhx9+0NKlS40Rt7Jly2rIkCFWn53WX5K+++47/d///Z+2bdumLl262Hz+aStTSNKNGze0ePFibdy4UXfv3rV6dO/9D7141NrT1K9f3yrUmc1mhYSE2FT3008/rVq1aunkyZNauHChVfht3769mjVrlm6fcuXKWd1sZ+v0i65du2rChAnGSO7Nmze1du1aff3119qwYYNV+C1cuLDefvtt9ejRw2jr16+fMdJ69+5dbdy4UYsWLTJuQHvqqac0fvz4R67rz5o0aWL14JaDBw9q0aJFOn78uGrVqmW1hnAaDw8P/ec//zEC++XLl7VkyRKtWbPGGG1//vnn9eKLL2a5PgCPhhFgAHZz/9q/TZs2feivcH18fNSgQQPjIQZLly41nojVoUMHlS9f3upRyGaz2XhQw5+Dnr+/v+bPn68FCxZo48aNxihsQECAmjVrpldeecV4AId0b2m2OXPmKCwsTDt27NCtW7cUGBio7t27q0mTJvr2229tOv8uXbrI19dXX331laKjo2WxWFSuXDl169ZNt2/fNta13bBhg3EOj1p7Gjc3N1WpUkWbNm2SdG+08WE3WT1Mnjx59Omnn+qLL77Q6tWrdeXKFQUEBKhr164PfVx1tWrVjLBcp04dm59U1qJFC9WqVUvLly/Xjh07dPr0acXHx8vLy0tFihRRtWrVVL9+fYWEhKR7rLGHh4emT59uBMvTp08rOTlZxYoVU6NGjfTyyy+rUKFCNtX1Z++++64qVqyoRYsW6ezZsypUqJBeeOEF9erVS3369Mlwn6pVq2rRokWaN2+eduzYocuXL8vT01OlSpXSiy++qOeff96uy/MByByTJbNr/gAAnMbZs2fVvXt3Y27wZ599ZjXnNLtdv35dXbp0MeY2jx07NktTMADgcWIEGAByiAsXLigiIkJ3797VmjVrjPBbrly5xxJ+k5KSNHPmTLm5uemnn34ywq+vr+9D53sDgLNx2gB88eJFdevWTZMnT7aa6xcTE6PQ0FDt3btXbm5uat68uQYOHGg1vy4xMVHTpk3TTz/9pMTERNWsWVP//Oc/H7hYOQDkBCaTSfPnz7dqc3d31/Dhwx/L5+fNm1cRERFWS7qZTCb985//tHn6BQA4glMG4N9//10DBw60WjJGundzRL9+/VSoUCGNHTtW165dU1hYmGJjYzVt2jSj38iRI3Xw4EENGjRIZrNZs2fPVr9+/RQREZHuTmoAyCmKFCmiEiVK6NKlS/Lw8FCFChXUq1evhz4BzZ5y5cqlatWq6ciRI3J3d1eZMmX08ssvq2nTpo/l8wHAXpwqAKempmr16tX6+OOPM9y+ePFixcXFKTw83Fhj08/PT4MHD9a+ffsUFBSkAwcOaOvWrfrkk0/07LPPSpJq1qyp9u3b69tvv9Ubb7zxmM4GAOzLzc1NS5cudWgNs2fPdujnA4A9ONWtpydOnNCECRP0wgsv6P3330+3fceOHapZs6bVAvPBwcEym83G2p07duyQp6engoODjT6+vr6qVatWltb3BAAAwJPBqQJw0aJFtXTp0gfOJ4uOjlbJkiWt2tzc3OTv7288RjQ6OlrFixdP9/jLEiVKZPioUQAAALgWp5oCkT9/fuXPn/+B2+Pj440Fxe/n5eVlLJaemT6P6tixY8a+PJsdAADAOSUnJ8tkMqlmzZoP7edUAfivpKamPnBb2kLimelji7TlktOWHQIAAEDOlKMCsLe3txITE9O1JyQkyM/Pz+jzxx9/ZNjn/qXSHkWFChUUFRUli8WiwMBAm44BAACA7HXy5MmHPoU0TY4KwKVKlVJMTIxV2927dxUbG6smTZoYfSIjI5Wammo14hsTE5PldYBNJpPxvHoAAAA4l8yEX8nJboL7K8HBwdqzZ4/x9CFJioyMVGJiorHqQ3BwsBISErRjxw6jz7Vr17R3716rlSEAAADgmnJUAO7cubPy5s2r/v37a+PGjVq2bJnee+89NWjQQDVq1JAk1apVS7Vr19Z7772nZcuWaePGjfrHP/4hHx8fde7c2cFnAAAAAEfLUVMgfH19NWvWLIWGhmrUqFEym81q1qyZhgwZYtVv0qRJmjp1qj755BOlpqaqRo0amjBhAk+BAwAAgEyWtOUN8FBRUVGSpGrVqjm4EgAAAGQks3ktR02BAAAAALKKAAwAAACXQgAGAACASyEAAwAAwKUQgAEAAOBSCMAAAABwKQRgAAAAuBQCMAAAAFwKARgAAAAuhQAMAAAAl0IABgAAgEshAAMAAMClEIABAADgUgjAAAAAcCkEYAAAALgUAjAAAABcCgEYAAAALoUADAAAAJdCAAYAAIBLIQADAADApRCAAQAA4FIIwAAAAHApBGAAAAC4FAIwAAAAXAoBGAAAAC6FAAwAAACXQgAGAACASyEAAwAAwKUQgAEAAOBSCMAAAABwKQRgAAAAuBQCMAAAAFwKARgAAAAuhQAMAAAAl0IABgAAgEshAMOpLF26VF27dlXDhg3VuXNnRUREyGKxGNtjYmI0dOhQhYSEqFmzZpowYYLi4+MzffyEhAS1b99eK1euzI7yAQBADpDb0QUAaZYtW6bx48erW7duaty4sfbu3atJkybpzp07evnll3Xz5k3169dPhQoV0tixY3Xt2jWFhYUpNjZW06ZN+8vj37hxQ8OGDVNsbOxjOBsAAOCsCMBwGitWrFBQUJCGDx8uSapXr57OnDmjiIgIvfzyy1q8eLHi4uIUHh6uAgUKSJL8/Pw0ePBg7du3T0FBQQ889ubNmzV58mQlJiY+hjMBAADOjCkQcBq3b9+W2Wy2asufP7/i4uIkSTt27FDNmjWN8CtJwcHBMpvN2rZt2wOPe/PmTQ0fPly1atXK1EgxAAB4shGA4TT+9re/KTIyUt9//73i4+O1Y8cOrV69Wm3atJEkRUdHq2TJklb7uLm5yd/fX2fOnHngcT08PBQREaH333/fKjwDAADXxBQIOI1WrVpp9+7dGj16tNFWv359DRs2TJIUHx+fboRYkry8vJSQkPDA47q7u6t06dJ2rxcAAORMBGA4jWHDhmnfvn0aNGiQqlSpopMnT+rzzz/XO++8o8mTJys1NfWB++bKxS8z4Jp+/fVX9evX74Hb+/Tpoz59+mjv3r2aPn26Tpw4IW9vbzVp0kRvvfVWhl8q73f48GF9/PHHOnLkiMxms9q1a6c+ffrI3d3d3qcCAI8NARhOYf/+/dq+fbtGjRqljh07SpJq166t4sWLa8iQIfr555/l7e2d4U1sCQkJ8vPze8wVA86hYsWK+uKLL9K1z5w5U4cOHVKrVq106tQp9e/fX0FBQZowYYIuXbqkadOm6fz585o6deoDj33u3Dn94x//UPXq1TVhwgRFR0drxowZiouL04gRI7LztAAgWxGA4RQuXLggSapRo4ZVe61atSRJp06dUqlSpRQTE2O1/e7du4qNjVWTJk0eT6GAk/H29la1atWs2jZv3qxdu3bpo48+UqlSpTR9+nSZTCZNnjxZXl5eku5dOxMmTNCFCxdUrFixDI89b948mc1mTZkyRe7u7mrYsKE8PDw0ceJE9erVS0WLFs328wOA7MDvjeEU0ubo7t2716p9//79kqSAgAAFBwdrz549unbtmrE9MjJSiYmJCg4Ofmy1As7s1q1bmjRpkho2bKjmzZtLurfCSu7cueXh4WH0y58/vyQZq6xkJDIyUs8++6zVdIdmzZopNTVVO3bsyKYzAIDsxwgwnELFihXVtGlTTZ06VTdu3FDVqlV1+vRpff7556pUqZJCQkJUu3ZtLVq0SP3799ebb76puLg4hYWFqUGDBlYjx1FRUfL19VVAQIADzwhwjIULF+ry5cuaOXOm0da+fXstX75cU6dO1RtvvKGrV69q9uzZCgwMVPny5TM8zq1bt3ThwoV0K6/4+vrKbDY/dOUVAHB2jADDaYwfP149evTQkiVLNHDgQH3zzTdq166dPvvsM+XOnVu+vr6aNWuWChQooFGjRmnGjBnG45Dv17NnT82ZM8dBZwE4TnJysr755hu1bNlSJUqUMNoDAwM1cOBALVq0SM2bN1e3bt2UmJiojz/+WG5ubhkeK+0R497e3um2mc3mh668AgDOjhFgOA13d3f169fvoXe0BwYGasaMGQ89zq+//vrAbf7+/g/dDuRkGzZs0NWrV/XKK69YtX/55Zf69NNP1aVLFzVt2lTXr1/XnDlz9I9//EOzZ89WoUKF0h3LYrE89LNMJpNdaweAx4kADABPiA0bNqhs2bJ6+umnjbaUlBTNmTNHzz//vN555x2jvXbt2urYsaPmz5+vIUOGpDtW2vJoGY30JiQkZDgyDAA5RY4MwEuXLtU333yj2NhYFS1aVF27dlWXLl2MEYmYmBiFhoZq7969cnNzU/PmzTVw4ED+gw3giZWSkqIdO3botddes2q/fv26bt26lW6FlYIFC6pUqVI6ffp0hsfz8vKSn5+fzp07Z9X+xx9/KCEhQWXKlLHvCQDAY5Tj5gAvW7ZM48ePV926dRUaGqoWLVpo0qRJCg8PlyTdvHlT/fr109WrVzV27FgNGDBAa9eu1b///W8HVw4A2efkyZMZBl1fX1/lz58/3Qor169f19mzZ1W8ePEHHvOZZ57R1q1bdefOHaPtp59+kpubm+rWrWvfEwCAxyjHjQCvWLFCQUFBGj58uCSpXr16OnPmjCIiIvTyyy9r8eLFiouLU3h4uAoUKCBJ8vPz0+DBg7Vv3z4FBQU5rngAyCYnT56UJJUtW9aq3c3NTX369NGkSZNkNpvVvHlzXb9+XV9++aVy5cqlHj16GH3/vILKa6+9prVr12rQoEHq0aOHzpw5oxkzZqhTp06sAQwgR8txI8C3b99O9+jO/PnzG2tZ7tixQzVr1jTCryQFBwfLbDZr27Ztj7NUAHhsrl69Kkny8fFJt61bt2764IMPdPDgQQ0ePFhTp05VqVKltGDBAqvlAv+8gkrp0qX16aef6tatW3rnnXf09ddf6+9//7v+9a9/Zf8JAUA2ynEjwH/729/04Ycf6vvvv9dzzz2nqKgorV69Wi+88IIkKTo6Wi1atLDax83NTf7+/qxbCeCJ9dprr6Wb/3u/Nm3aqE2bNg89RkYrpNSsWVNffvllVssDAKeS4wJwq1attHv3bo0ePdpoq1+/voYNGybp3tqVfx4hlu7d0JHVdSstFosSExOzdAxnwRJGzu+vlqECAADWLBZLpjJOjgvAw4YN0759+zRo0CBVqVJFJ0+e1Oeff6533nlHkydPVmpq6gP3zZUrazM+kpOTdeTIkSwdwxm4u7urcpUqyv2ABfDheCl37+rwoUNKTk52dCkAAOQoefLk+cs+OSoA79+/X9u3b9eoUaPUsWNHSffWsixevLiGDBmin3/+Wd7e3hmO0iYkJMjPzy9Ln+/u7q7AwMAsHcMZmEwm5XZz08LI47p048kY0X6S+OXzUvfgp1W+fHlGgQEAeARpNwT/lRwVgC9cuCBJ6Zb5qVWrliTp1KlTKlWqlGJiYqy23717V7GxsWrSpEmWPt9kMsnLyytLx3Aml24kKvYajzN1Vp6eno4uAQCAHCWzUzxz1CoQpUuXlqR061nu379fkhQQEKDg4GDt2bNH165dM7ZHRkYqMTFRwcHBj61WAAAAOKccNQJcsWJFNW3aVFOnTtWNGzdUtWpVnT59Wp9//rkqVaqkkJAQ1a5dW4sWLVL//v315ptvKi4uTmFhYWrQoEG6kWMAAAC4HpMlh00yTE5O1n//+199//33unz5sooWLaqQkBC9+eabxvSEkydPKjQ0VPv375fZbFbjxo01ZMiQDFeHyKyoqChJUrVq1exyHs4gbO0+pkA4IX9fswa1DHJ0GQAA5DiZzWs5agRYuncjWr9+/dSvX78H9gkMDNSMGTMeY1UAXEWqxaJcLCPolPi7AZBZOS4AA4Aj5TKZWEHFCaWtngIAmUEABoBHxAoqAJCz5ahVIAAAAICsIgADAADApRCAAQAA4FIIwAAAAHApBGAAAAC4FAIwAAAAXAoBGAAAAC6FAAwAAACXQgAGAACASyEAAwAAwKUQgAEAAOBSCMAAAABwKQRgAAAAuBQCMAAAAFwKARgAAAAuhQAMAAAAl0IABgAAgEshAAMAAMClEIABAADgUgjAAAAAcCkEYAAAALgUAjAAAABcCgEYAAAALiV3VnY+d+6cLl68qGvXril37twqUKCAypYtq3z58tmrPgAAAMCuHjkAHzx4UEuXLlVkZKQuX76cYZ+SJUuqUaNGateuncqWLZvlIgEAAAB7yXQA3rdvn8LCwnTw4EFJksVieWDfM2fO6OzZswoPD1dQUJCGDBmiypUrZ71aAAAAIIsyFYDHjx+vFStWKDU1VZJUunRpVatWTeXLl1eRIkVkNpslSTdu3NDly5d14sQJHT16VKdPn9bevXvVs2dPtWnTRmPGjMm+MwEAAAAyIVMBeNmyZfLz89OLL76o5s2bq1SpUpk6+NWrV7V+/XotWbJEq1evJgADAADA4TIVgCdOnKjGjRsrV65HWzSiUKFC6tatm7p166bIyEibCgQAAMguUVFR+vTTT3Xo0CF5eXmpfv36Gjx4sAoWLGjVLyUlRb1791b9+vXVt2/fvzxumzZtdOnSpXTt69evV4ECBexVPmyUqQDcpEmTLH9QcHBwlo8BAABgL0eOHFG/fv1Ur149TZ48WZcvX9ann36qmJgYzZ071+h3+/ZtjRkzRgcPHlT9+vX/8rjXr1/XpUuXNHjwYAUFBVlt8/b2tvdpwAZZWgZNkuLj4zVz5kz9/PPPunr1qvz8/NS6dWv17NlT7u7u9qgRAADA7sLCwlShQgVNmTLF+C232WzWlClTdP78eRUvXlx79+7VxIkTMxzNfZBjx45JujeAGBAQkC21I2uy/CCMDz74QBEREYqNjdXt27cVExOjOXPmaMaMGfaoDwAAwO6uX7+u3bt3q3PnzlZTPJs2barVq1erePHikqR//vOfKlq0qBYsWJDpYx8/flxms9k4BpxPlkaAk5OTtXnzZjVt2lSvvPKKChQooPj4eC1fvlw//vijBg8ebK86AQAA7ObkyZNKTU2Vr6+vRo0apS1btshisahJkyYaPny4fHx8JEmzZ89WYGDgIx37+PHjypcvn95++23t2rVLqampatiwoYYNG6bChQtnx+ngEWVqBHj8+PG6cuVKuvbbt28rNTVVZcuWVZUqVRQQEKCKFSuqSpUqun37tt2LBQAAsIdr165Juveb7Lx582ry5MkaPHiwtm7dqiFDhhjPO3jU8CvdmwJx6dIlVapUSR9//LGGDh2qPXv2qE+fPkpKSrLrecA2mV4G7YcfflDXrl31+uuvG4869vb2Vvny5fXf//5X4eHh8vHxUWJiohISEtS4ceNsLRwAAMBWycnJkqSKFSvqvffekyTVq1dPPj4+GjlypHbu3GnzDfyjRo2Sm5ubqlSpIkmqWbOmypYtq969e2v16tXq3LmzfU4CNsvUCPD777+vQoUKaf78+erQoYO++OIL3bp1y9hWunRpJSUl6dKlS4qPj1f16tU1fPjwbC0cAADAVl5eXpKkRo0aWbU3aNBAknT06FGbj129enUj/KYJCgqSt7e3jh8/bvNxYT+ZGgFu06aNWrZsqSVLlmju3LmaMWOGFi1apN69e6tTp05atGiRLly4oD/++EN+fn7y8/PL7roBAABsVrJkSUnSnTt3rNpTUlIkSR4eHjYdNz4+Xhs2bFCVKlWspk+kpqYqOTlZvr6+NlYMe8r0KhC5c+dW165dtWzZMr311lu6c+eOJk6cqM6dO+vHH3+Uv7+/qlatSvgFAABOr0yZMvL399fatWuN+b6StHnzZklKt35vZrm7u2vixIn68ssvrdq3bNmi27dvq06dOraWDDt65GXQPDw81KtXLy1fvlyvvPKKLl++rNGjR+vvf/+7tm3blh01AgAA2JXJZNKgQYMUFRWlESNGaOfOnVq4cKFCQ0PVtGlTVaxYMdPHioqK0rlz5yRJefPm1euvv641a9YoNDRUO3fuVHh4uMaMGaPGjRurbt262XVKeASZXgbt6tWrioyMNKY5PPvssxo4cKD+9re/afbs2VqxYoWGDh2qoKAgDRgwQNWrV8/OugEAALKkefPmyps3r2bPnq2hQ4cqX758eumll/TWW2890nF69uyptm3bauzYsZKkN954Q76+voqIiNB3332n/Pnz66WXXlKfPn2y4SxgC5Pl/nH/B/j11181bNgwq6U7fH199dlnn6l06dKSpHPnzmnmzJlat26dJKlhw4YKDQ3NnqodICoqSpJUrVo1B1diP2Fr9yn2WoKjy8Cf+PuaNahlkKPLwENw7TgfrhsAUubzWqamQISFhSl37tx69tln1apVKzVu3Fi5c+e2etpbQECAxo8frwULFqh+/fr6+eefs1A+AAAAkD0yNQUiOjpaYWFhVhPCb968qd69e6fr+/TTT+uTTz7Rvn377FUjAAAAYDeZCsBFixbVhx9+qAYNGsjb21tJSUnat2+fihUr9sB9bL17EgAAAMhOmQrAvXr10pgxY7Rw4UKZTCZZLBa5u7tbTYEAAAAAcoJMBeDWrVurTJky2rx5s7EKRMuWLRUQEJDd9QEAAAB2lell0CpUqKAKFSpkZy0AAABAtsvUKhDDhg3Trl27bP6Qw4cPa9SoUTbv/2dRUVHq27evGjZsqJYtW2rMmDH6448/jO0xMTEaOnSoQkJC1KxZM02YMEHx8fF2+3wAAADkXJkaAd66dau2bt2qgIAANWvWTCEhIapUqZJy5co4P6ekpGj//v3atWuXtm7dqpMnT0qSxo0bl+WCjxw5on79+qlevXqaPHmyLl++rE8//VQxMTGaO3eubt68qX79+qlQoUIaO3asrl27prCwMMXGxmratGlZ/nwAAPDoUi0W5TKZHF0GMuCKfzeZCsCzZ8/Wf/7zH504cULz5s3TvHnz5O7urjJlyqhIkSIym80ymUxKTEzU77//rrNnz+r27duSJIvFoooVK2rYsGF2KTgsLEwVKlTQlClTjABuNps1ZcoUnT9/XmvXrlVcXJzCw8NVoEABSZKfn58GDx6sffv2sToFAAAOkMtk0sLI47p0I9HRpeA+fvm81D34aUeX8dhlKgDXqFFDCxYs0IYNGzR//nwdOXJEd+7c0bFjx3T8+HGrvmkPljOZTKpXr55eeuklhYSEyGSHbxbXr1/X7t27NXbsWKvR56ZNm6pp06aSpB07dqhmzZpG+JWk4OBgmc1mbdu2jQAMAICDXLqRyFMU4RQyfRNcrly51KJFC7Vo0UKxsbHavn279u/fr8uXLxvzbwsWLKiAgAAFBQWpbt26euqpp+xa7MmTJ5WamipfX1+NGjVKW7ZskcViUZMmTTR8+HD5+PgoOjpaLVq0sNrPzc1N/v7+OnPmTJY+32KxKDEx539zNZlM8vT0dHQZ+AtJSUnKxJPK8Rhx7Tg/rhvnxLXj/J6Ua8disWRq0DXTAfh+/v7+6ty5szp37mzL7ja7du2aJOmDDz5QgwYNNHnyZJ09e1bTp0/X+fPnNWfOHMXHx8tsNqfb18vLSwkJWfvWmZycrCNHjmTpGM7A09NTlStXdnQZ+Au//fabkpKSHF0G7sO14/y4bpwT147ze5KunTx58vxlH5sCsKMkJydLkipWrKj33ntPklSvXj35+Pho5MiR2rlzp1JTUx+4/4Nu2sssd3d3BQYGZukYzsAe01GQ/cqUKfNEfBt/knDtOD+uG+fEteP8npRrJ23hhb+SowKwl5eXJKlRo0ZW7Q0aNJAkHT16VN7e3hlOU0hISJCfn1+WPt9kMhk1ANmNXxcCj47rBrDNk3LtZPbLVtaGRB+zkiVLSpLu3Llj1Z6SkiJJ8vDwUKlSpRQTE2O1/e7du4qNjVXp0qUfS50AAABwXjkqAJcpU0b+/v5au3at1TD95s2bJUlBQUEKDg7Wnj17jPnCkhQZGanExEQFBwc/9poBAADgXHJUADaZTBo0aJCioqI0YsQI7dy5UwsXLlRoaKiaNm2qihUrqnPnzsqbN6/69++vjRs3atmyZXrvvffUoEED1ahRw9GnAAAAAAezaQ7wwYMHVbVqVXvXkinNmzdX3rx5NXv2bA0dOlT58uXTSy+9pLfeekuS5Ovrq1mzZik0NFSjRo2S2WxWs2bNNGTIEIfUCwAAAOdiUwDu2bOnypQpoxdeeEFt2rRRkSJF7F3XQzVq1CjdjXD3CwwM1IwZMx5jRQAAAMgpbJ4CER0drenTp6tt27YaMGCAfvzxR+PxxwAAAICzsmkE+LXXXtOGDRt07tw5WSwW7dq1S7t27ZKXl5datGihF154gUcOAwAAwCnZFIAHDBigAQMG6NixY1q/fr02bNigmJgYJSQkaPny5Vq+fLn8/f3Vtm1btW3bVkWLFrV33QAAAIBNsrQKRIUKFdS/f38tWbJE4eHh6tChgywWiywWi2JjY/X555+rY8eOmjRp0kOf0AYAAAA8Lll+EtzNmze1YcMGrVu3Trt375bJZDJCsHTvIRTffvut8uXLp759+2a5YAAAACArbArAiYmJ2rRpk9auXatdu3YZT2KzWCzKlSuXnnnmGbVv314mk0nTpk1TbGys1qxZQwAGAACAw9kUgFu0aKHk5GRJMkZ6/f391a5du3Rzfv38/PTGG2/o0qVLdigXAAAAyBqbAvCdO3ckSXny5FHTpk3VoUMH1alTJ8O+/v7+kiQfHx8bSwQAAADsx6YAXKlSJbVv316tW7eWt7f3Q/t6enpq+vTpKl68uE0FAgAAAPZkUwD+6quvJN2bC5ycnCx3d3dJ0pkzZ1S4cGGZzWajr9lsVr169exQKgAAAJB1Ni+Dtnz5crVt21ZRUVFG24IFC/T8889rxYoVdikOAAAAsDebAvC2bds0btw4xcfH6+TJk0Z7dHS0kpKSNG7cOO3atctuRQIAAAD2YlMADg8PlyQVK1ZM5cqVM9p79OihEiVKyGKxaP78+fapEAAAALAjm+YAnzp1SiaTSaNHj1bt2rWN9pCQEOXPn199+vTRiRMn7FYkAAAAYC82jQDHx8dLknx9fdNtS1vu7ObNm1koCwAAAMgeNgXgp556SpK0ZMkSq3aLxaKFCxda9QEAAACciU1TIEJCQjR//nxFREQoMjJS5cuXV0pKio4fP64LFy7IZDKpcePG9q4VAAAAyDKbAnCvXr20adMmxcTE6OzZszp79qyxzWKxqESJEnrjjTfsViQAAABgLzZNgfD29tYXX3yhjh07ytvbWxaLRRaLRWazWR07dtTcuXP/8glxAAAAgCPYNAIsSfnz59fIkSM1YsQIXb9+XRaLRb6+vjKZTPasDwAAALArm58El8ZkMsnX11cFCxY0wm9qaqq2b9+e5eIAAAAAe7NpBNhisWju3LnasmWLbty4odTUVGNbSkqKrl+/rpSUFO3cudNuhQIAAAD2YFMAXrRokWbNmiWTySSLxWK1La2NqRAAAABwRjZNgVi9erUkydPTUyVKlJDJZFKVKlVUpkwZI/y+8847di0UAAAAsAebAvC5c+dkMpn0n//8RxMmTJDFYlHfvn0VERGhv//977JYLIqOjrZzqQAAAEDW2RSAb9++LUkqWbKknn76aXl5eengwYOSpE6dOkmStm3bZqcSAQAAAPuxKQAXLFhQknTs2DGZTCaVL1/eCLznzp2TJF26dMlOJQIAAAD2Y1MArlGjhiwWi9577z3FxMSoZs2aOnz4sLp27aoRI0ZI+l9IBgAAAJyJTQG4d+/eypcvn5KTk1WkSBG1atVKJpNJ0dHRSkpKkslkUvPmze1dKwAAAJBlNgXgMmXKaP78+XrzzTfl4eGhwMBAjRkzRk899ZTy5cunDh06qG/fvvauFQAAAMgym9YB3rZtm6pXr67evXsbbW3atFGbNm3sVhgAAACQHWwaAR49erRat26tLVu22LseAAAAIFvZFIBv3bql5ORklS5d2s7lAAAAANnLpgDcrFkzSdLGjRvtWgwAAACQ3WyaA/z000/r559/1vTp07VkyRKVLVtW3t7eyp37f4czmUwaPXq03QoFAAAA7MGmAPzJJ5/IZDJJki5cuKALFy5k2I8ADAAAAGdjUwCWJIvF8tDtaQEZAAAAcCY2BeAVK1bYuw4AAADgsbApABcrVszedQAAAACPhU0BeM+ePZnqV6tWLVsODwAAAGQbmwJw3759/3KOr8lk0s6dO20qCgAAAMgu2XYTHAAAAOCMbArAb775ptV7i8WiO3fu6Pfff9fGjRtVsWJF9erVyy4FAgAAAPZkUwDu06fPA7etX79eI0aM0M2bN20uCgAAAMguNj0K+WGaNm0qSfrmm2/sfWgAAAAgy+wegH/55RdZLBadOnXK3ocGAAAAssymKRD9+vVL15aamqr4+HidPn1aklSwYMGsVQYAAABkA5sC8O7dux+4DFra6hBt27a1vSoAAAAgm9h1GTR3d3cVKVJErVq1Uu/evbNUWGYNHz5cR48e1cqVK422mJgYhYaGau/evXJzc1Pz5s01cOBAeXt7P5aaAAAA4LxsCsC//PKLveuwyffff6+NGzdaPZr55s2b6tevnwoVKqSxY8fq2rVrCgsLU2xsrKZNm+bAagEAAOAMbB4BzkhycrLc3d3tecgHunz5siZPnqynnnrKqn3x4sWKi4tTeHi4ChQoIEny8/PT4MGDtW/fPgUFBT2W+gAAAOCcbF4F4tixY/rHP/6ho0ePGm1hYWHq3bu3Tpw4YZfiHubDDz/UM888o7p161q179ixQzVr1jTCryQFBwfLbDZr27Zt2V4XAAAAnJtNAfj06dPq27evfv31V6uwGx0drf3796tPnz6Kjo62V43pLFu2TEePHtU777yTblt0dLRKlixp1ebm5iZ/f3+dOXMm22oCAABAzmDTFIi5c+cqISFBefLksVoNolKlStqzZ48SEhL05ZdfauzYsfaq03DhwgVNnTpVo0ePthrlTRMfHy+z2Zyu3cvLSwkJCVn6bIvFosTExCwdwxmYTCZ5eno6ugz8haSkpAxvNoXjcO04P64b58S14/yelGvHYrE8cKWy+9kUgPft2yeTyaRRo0bp+eefN9r/8Y9/KDAwUCNHjtTevXttOfRDWSwWffDBB2rQoIGaNWuWYZ/U1NQH7p8rV9ae+5GcnKwjR45k6RjOwNPTU5UrV3Z0GfgLv/32m5KSkhxdBu7DteP8uG6cE9eO83uSrp08efL8ZR+bAvAff/whSapatWq6bRUqVJAkXblyxZZDP1RERIROnDihhQsXKiUlRdL/lmNLSUlRrly55O3tneEobUJCgvz8/LL0+e7u7goMDMzSMZxBZr4ZwfHKlCnzRHwbf5Jw7Tg/rhvnxLXj/J6Ua+fkyZOZ6mdTAM6fP7+uXr2qX375RSVKlLDatn37dkmSj4+PLYd+qA0bNuj69etq3bp1um3BwcF68803VapUKcXExFhtu3v3rmJjY9WkSZMsfb7JZJKXl1eWjgFkFr8uBB4d1w1gmyfl2snsly2bAnCdOnW0Zs0aTZkyRUeOHFGFChWUkpKiw4cPa926dTKZTOlWZ7CHESNGpBvdnT17to4cOaLQ0FAVKVJEuXLl0ldffaVr167J19dXkhQZGanExEQFBwfbvSYAAADkLDYF4N69e2vLli1KSkrS8uXLrbZZLBZ5enrqjTfesEuB9ytdunS6tvz588vd3d2YW9S5c2ctWrRI/fv315tvvqm4uDiFhYWpQYMGqlGjht1rAgAAQM5i011hpUqV0rRp01SyZElZLBarPyVLltS0adMyDKuPg6+vr2bNmqUCBQpo1KhRmjFjhpo1a6YJEyY4pB4AAAA4F5ufBFe9enUtXrxYx44dU0xMjCwWi0qUKKEKFSo81snuGS21FhgYqBkzZjy2GgAAAJBzZOlRyImJiSpbtqyx8sOZM2eUmJiY4Tq8AAAAgDOweWHc5cuXq23btoqKijLaFixYoOeff14rVqywS3EAAACAvdkUgLdt26Zx48YpPj7ear216OhoJSUlady4cdq1a5fdigQAAADsxaYAHB4eLkkqVqyYypUrZ7T36NFDJUqUkMVi0fz58+1TIQAAAGBHNs0BPnXqlEwmk0aPHq3atWsb7SEhIcqfP7/69OmjEydO2K1IAAAAwF5sGgGOj4+XJONBE/dLewLczZs3s1AWAAAAkD1sCsBPPfWUJGnJkiVW7RaLRQsXLrTqAwAAADgTm6ZAhISEaP78+YqIiFBkZKTKly+vlJQUHT9+XBcuXJDJZFLjxo3tXSsAAACQZTYF4F69emnTpk2KiYnR2bNndfbsWWNb2gMxsuNRyAAAAEBW2TQFwtvbW1988YU6duwob29v4zHIZrNZHTt21Ny5c+Xt7W3vWgEAAIAss/lJcPnz59fIkSM1YsQIXb9+XRaLRb6+vo/1McgAAADAo7L5SXBpTCaTfH19VbBgQZlMJiUlJWnp0qV69dVX7VEfAAAAYFc2jwD/2ZEjR7RkyRKtXbtWSUlJ9josAAAAYFdZCsCJiYn64YcftGzZMh07dsxot1gsTIUAAACAU7IpAB86dEhLly7VunXrjNFei8UiSXJzc1Pjxo310ksv2a9KAAAAwE4yHYATEhL0ww8/aOnSpcZjjtNCbxqTyaRVq1apcOHC9q0SAAAAsJNMBeAPPvhA69ev161bt6xCr5eXl5o2baqiRYtqzpw5kkT4BQAAgFPLVABeuXKlTCaTLBaLcufOreDgYD3//PNq3Lix8ubNqx07dmR3nQAAAIBdPNIyaCaTSX5+fqpataoqV66svHnzZlddAAAAQLbI1AhwUFCQ9u3bJ0m6cOGCPvvsM3322WeqXLmyWrduzVPfAAAAkGNkKgDPnj1bZ8+e1bJly/T999/r6tWrkqTDhw/r8OHDVn3v3r0rNzc3+1cKAAAA2EGmp0CULFlSgwYN0urVqzVp0iQ1bNjQmBd8/7q/rVu31scff6xTp05lW9EAAACArR55HWA3NzeFhIQoJCREV65c0YoVK7Ry5UqdO3dOkhQXF6evv/5a33zzjXbu3Gn3ggEAAICseKSb4P6scOHC6tWrl5YuXaqZM2eqdevWcnd3N0aFAQAAAGeTpUch369OnTqqU6eO3nnnHX3//fdasWKFvQ4NAAAA2I3dAnAab29vde3aVV27drX3oQEAAIAsy9IUCAAAACCnIQADAADApRCAAQAA4FIIwAAAAHApBGAAAAC4FAIwAAAAXAoBGAAAAC6FAAwAAACXQgAGAACASyEAAwAAwKUQgAEAAOBSCMAAAABwKQRgAAAAuBQCMAAAAFwKARgAAAAuhQAMAAAAl0IABgAAgEshAAMAAMClEIABAADgUgjAAAAAcCkEYAAAALgUAjAAAABcCgEYAAAALiW3owt4VKmpqVqyZIkWL16s8+fPq2DBgnruuefUt29feXt7S5JiYmIUGhqqvXv3ys3NTc2bN9fAgQON7QAAAHBdOS4Af/XVV5o5c6ZeeeUV1a1bV2fPntWsWbN06tQpTZ8+XfHx8erXr58KFSqksWPH6tq1awoLC1NsbKymTZvm6PIBAADgYDkqAKempmrevHl68cUXNWDAAEnSM888o/z582vEiBE6cuSIdu7cqbi4OIWHh6tAgQKSJD8/Pw0ePFj79u1TUFCQ404AAAAADpej5gAnJCSoTZs2atWqlVV76dKlJUnnzp3Tjh07VLNmTSP8SlJwcLDMZrO2bdv2GKsFAACAM8pRI8A+Pj4aPnx4uvZNmzZJksqWLavo6Gi1aNHCarubm5v8/f115syZx1EmAAAAnFiOCsAZOXjwoObNm6dGjRopMDBQ8fHxMpvN6fp5eXkpISEhS59lsViUmJiYpWM4A5PJJE9PT0eXgb+QlJQki8Xi6DJwH64d58d145y4dpzfk3LtWCwWmUymv+yXowPwvn37NHToUPn7+2vMmDGS7s0TfpBcubI24yM5OVlHjhzJ0jGcgaenpypXruzoMvAXfvvtNyUlJTm6DNyHa8f5cd04J64d5/ckXTt58uT5yz45NgCvXbtW77//vkqWLKlp06YZc369vb0zHKVNSEiQn59flj7T3d1dgYGBWTqGM8jMNyM4XpkyZZ6Ib+NPEq4d58d145y4dpzfk3LtnDx5MlP9cmQAnj9/vsLCwlS7dm1NnjzZan3fUqVKKSYmxqr/3bt3FRsbqyZNmmTpc00mk7y8vLJ0DCCz+HUh8Oi4bgDbPCnXTma/bOWoVSAk6bvvvtMnn3yi5s2ba9q0aekebhEcHKw9e/bo2rVrRltkZKQSExMVHBz8uMsFAACAk8lRI8BXrlxRaGio/P391a1bNx09etRqe0BAgDp37qxFixapf//+evPNNxUXF6ewsDA1aNBANWrUcFDlAAAAcBY5KgBv27ZNt2/fVmxsrHr37p1u+5gxY9SuXTvNmjVLoaGhGjVqlMxms5o1a6YhQ4Y8/oIBAADgdHJUAO7QoYM6dOjwl/0CAwM1Y8aMx1ARAAAAcpocNwcYAAAAyAoCMAAAAFwKARgAAAAuhQAMAAAAl0IABgAAgEshAAMAAMClEIABAADgUgjAAAAAcCkEYAAAALgUAjAAAABcCgEYAAAALoUADAAAAJdCAAYAAIBLIQADAADApRCAAQAA4FIIwAAAAHApBGAAAAC4FAIwAAAAXAoBGAAAAC6FAAwAAACXQgAGAACASyEAAwAAwKUQgAEAAOBSCMAAAABwKQRgAAAAuBQCMAAAAFwKARgAAAAuhQAMAAAAl0IABgAAgEshAAMAAMClEIABAADgUgjAAAAAcCkEYAAAALgUAjAAAABcCgEYAAAALoUADAAAAJdCAAYAAIBLIQADAADApRCAAQAA4FIIwAAAAHApBGAAAAC4FAIwAAAAXAoBGAAAAC6FAAwAAACXQgAGAACASyEAAwAAwKUQgAEAAOBSCMAAAABwKU90AI6MjNSrr76qZ599Vu3bt9f8+fNlsVgcXRYAAAAc6IkNwFFRURoyZIhKlSqlSZMmqXXr1goLC9O8efMcXRoAAAAcKLejC8gun332mSpUqKAPP/xQktSgQQOlpKToiy++UPfu3eXh4eHgCgEAAOAIT+QI8J07d7R79241adLEqr1Zs2ZKSEjQvn37HFMYAAAAHO6JDMDnz59XcnKySpYsadVeokQJSdKZM2ccURYAAACcwBM5BSI+Pl6SZDabrdq9vLwkSQkJCY90vGPHjunOnTuSpAMHDtihQsczmUyqVzBVdwswFcTZuOVKVVRUFDdsOimuHefEdeP8uHac05N27SQnJ8tkMv1lvycyAKempj50e65cjz7wnfbDzMwPNacw53V3dAl4iCfp39qThmvHeXHdODeuHef1pFw7JpPJdQOwt7e3JCkxMdGqPW3kN217ZlWoUME+hQEAAMDhnsg5wAEBAXJzc1NMTIxVe9r70qVLO6AqAAAAOIMnMgDnzZtXNWvW1MaNG63mtPz000/y9vZW1apVHVgdAAAAHOmJDMCS9MYbb+jgwYN69913tW3bNs2cOVPz589Xz549WQMYAADAhZksT8ptfxnYuHGjPvvsM505c0Z+fn7q0qWLXn75ZUeXBQAAAAd6ogMwAAAA8GdP7BQIAAAAICMEYAAAALgUAjAAAABcCgEYAAAALoUADAAAAJdCAAYAAIBLIQADAADApRCAkSONHTtWderUeeCf9evXO7pEwKn06dNHderUUa9evR7Y59///rfq1KmjsWPHPr7CACd35coVNWvWTN27d9edO3fSbV+4cKHq1q2rn3/+2QHVwVa5HV0AYKtChQpp8uTJGW4rWbLkY64GcH65cuVSVFSULl68qKeeespqW1JSkrZu3eqgygDnVbhwYY0cOVJvv/22ZsyYoSFDhhjbDh8+rE8++UQ9evRQw4YNHVckHhkBGDlWnjx5VK1aNUeXAeQYFStW1KlTp7R+/Xr16NHDatuWLVvk6empfPnyOag6wHk1bdpU7dq1U3h4uBo2bKg6dero5s2b+ve//63y5ctrwIABji4Rj4gpEADgIjw8PNSwYUNt2LAh3bZ169apWbNmcnNzc0BlgPMbPny4/P39NWbMGMXHx2v8+PGKi4vThAkTlDs344k5DQEYOVpKSkq6PxaLxdFlAU6rRYsWxjSINPHx8dq+fbtatWrlwMoA5+bl5aUPP/xQV65cUd++fbV+/XqNGjVKxYsXd3RpsAEBGDnWhQsXFBwcnO7PvHnzHF0a4LQaNmwoT09PqxtFN23aJF9fXwUFBTmuMCAHqF69urp3765jx44pJCREzZs3d3RJsBFj9sixChcurNDQ0HTtfn5+DqgGyBk8PDzUqFEjbdiwwZgHvHbtWrVs2VImk8nB1QHO7datW9q2bZtMJpN++eUXnTt3TgEBAY4uCzZgBBg5lru7uypXrpzuT+HChR1dGuDU7p8Gcf36de3cuVMtW7Z0dFmA0/vPf/6jc+fOadKkSbp7965Gjx6tu3fvOros2IAADAAupkGDBvLy8tKGDRu0ceNGFS9eXJUqVXJ0WYBTW7NmjVauXKm33npLISEhGjJkiA4cOKA5c+Y4ujTYgCkQAOBi8uTJo5CQEG3YsEF58+bl5jfgL5w7d04TJkxQ3bp19corr0iSOnfurK1bt2ru3LmqX7++qlev7uAq8SgYAQYAF9SiRQsdOHBAu3fvJgADD5GcnKwRI0Yod+7cev/995Ur1/+i03vvvScfHx+99957SkhIcGCVeFQEYABwQcHBwfLx8VG5cuVUunRpR5cDOK1p06bp8OHDGjFiRLqbrNOeEnf+/HlNnDjRQRXCFiYLi6YCAADAhTACDAAAAJdCAAYAAIBLIQADAADApRCAAQAA4FIIwAAAAHApBGAAAAC4FAIwAAAAXAqPQgYAJ/Dzzz9r1apVOnTokP744w9J0lNPPaWgoCB169ZNFSpUcGh9Fy9e1AsvvCBJatu2rcaOHevQegAgKwjAAOBAiYmJGjdunNauXZtu29mzZ3X27FmtWrVKb7/9tjp37uyACgHgyUMABgAH+uCDD7R+/XpJUvXq1fXqq6+qXLlyunHjhlatWqVvv/1WqampmjhxoipWrKiqVas6uGIAyPkIwADgIBs3bjTCb4MGDRQaGqrcuf/3n+UqVarI09NTX331lVJTU/X111/r//7v/xxVLgA8MQjAAOAgS5YsMV4PGzbMKvymefXVV+Xj46NKlSqpcuXKRvulS5f02Wefadu2bYqLi1ORIkXUpEkT9e7dWz4+Pka/sWPHatWqVcqfP7+WL1+uGTNmaMOGDbp586YCAwPVr18/NWjQwOozDx48qJkzZ+rAgQPKnTu3QkJC1L179weex8GDBzV79mzt379fycnJKlWqlNq3b6+uXbsqV67/3Wtdp04dSVKPHj0kSUuXLpXJZNKgQYP00ksvPeJPDwBsZ7JYLBZHFwEArqhhw4a6deuW/P39tWLFikzvd/78efXq1UtXr15Nt61MmTL64osv5O3tLel/AdhsNqt48eI6fvy4VX83NzdFRESoVKlSkqQ9e/aof//+Sk5OtupXpEgRXb58WZL1TXCbN2/WO++8o5SUlHS1tG7dWuPGjTPepwVgHx8f3bx502hfuHChAgMDM33+AJBVLIMGAA5w/fp13bp1S5JUuHBhq213797VxYsXM/wjSRMnTtTVq1eVN29ejR07VkuWLNG4cePk4eGh3377TbNmzUr3eQkJCbp586bCwsK0ePFiPfPMM8Znff/990a/yZMnG+H31VdfVUREhCZOnJhhwL1165bGjRunlJQUBQQE6NNPP9XixYvVu3dvSdKaNWu0cePGdPvdvHlTXbt21XfffaePPvqI8AvgsWMKBAA4wP1TA+7evWu1LTY2Vp06dcpwv59++kk7duyQJD333HOqW7euJKlmzZpq2rSpvv/+e33//fcaNmyYTCaT1b5Dhgwxpjv0799fO3fulCRjJPny5cvGCHFQUJAGDRokSSpbtqzi4uI0fvx4q+NFRkbq2rVrkqRu3bqpTJkykqROnTrpxx9/VExMjFatWqUmTZpY7Zc3b14NGjRIHh4exsgzADxOBGAAcIB8+fLJ09NTSUlJunDhQqb3i4mJUWpqqiRp3bp1WrduXbo+N27c0Pnz5xUQEGDVXrZsWeO1r6+v8TptdPf333832v682kS1atXSfc7Zs2eN11OmTNGUKVPS9Tl69Gi6tuLFi8vDwyNdOwA8LkyBAAAHqVevniTpjz/+0KFDh4z2EiVK6NdffzX+FCtWzNjm5uaWqWOnjczeL2/evMbr+0eg09w/YpwWsh/WPzO1ZFRH2vxkAHAURoABwEE6dOigzZs3S5JCQ0M1Y8YMq5AqScnJybpz547x/v5R3U6dOmnkyJHG+1OnTslsNqto0aI21VO8eHHj9f2BXJL279+frn+JEiWM1+PGjVPr1q2N9wcPHlSJEiWUP3/+dPtltNoFADxOjAADgIM899xzatmypaR7AfONN97QTz/9pHPnzun48eNauHChunbtarXag7e3txo1aiRJWrVqlb777judPXtWW7duVa9evdS2bVu98sorsmWBH19fX9WqVcuoZ+rUqTp58qTWr1+v6dOnp+tfr149FSpUSJI0Y8YMbd26VefOndOCBQv0+uuvq1mzZpo6deoj1wEA2Y2v4QDgQKNHj1bevHm1cuVKHT16VG+//XaG/by9vdW3b19J0qBBg3TgwAHFxcVpwoQJVv3y5s2rgQMHprsBLrOGDx+u3r17KyEhQeHh4QoPD5cklSxZUnfu3FFiYqLR18PDQ0OHDtXo0aMVGxuroUOHWh3L399fL7/8sk11AEB2IgADgAN5eHhozJgx6tChg1auXKn9+/fr8uXLSklJUaFChVSpUiXVr19frVq1kqenp6R7a/1+9dVXmjNnjnbt2qWrV6+qQIECql69unr16qWKFSvaXE/58uU1d+5cTZs2Tbt371aePHn03HPPacCAAeratWu6/q1bt1aRIkU0f/58RUVFKTExUX5+fmrYsKF69uyZbok3AHAGPAgDAAAALoU5wAAAAHApBGAAAAC4FAIwAAAAXAoBGAAAAC6FAAwAAACXQgAGAACASyEAAwAAwKUQgAEAAOBSCMAAAABwKQRgAAAAuBQCMAAAAFwKARgAAAAuhQAMAAAAl/L/xDh98u0W+D0AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613636d-22ae-4629-ac86-daf482925194",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
