{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17da2043-9a5b-40fe-b3cb-f755b9a98deb",
   "metadata": {},
   "source": [
    "# VGGIsh Multi Seed Validation\n",
    "#### 5 Random (but reproducible) seeds are selected for 5 different runs of train/test\n",
    "#### Models have been optimised and verified on validation sets thoroughly, running a final train/test evaluation here\n",
    "#### The setup:\n",
    "\n",
    "- 4 fold StratifiedGroupKFold for stratification and ensuring each cat_id group only appears in one set at a time\n",
    "- Final scores averaged over the 4 folds\n",
    "- For each seed run we will explore the cat_id predictions through majority voting\n",
    "- For each run we will explore the potential impact of gender\n",
    "\n",
    "The dataset is highly unbalanced, resources for an unbiased estimate have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac1e09ae-387c-4347-b6f5-8d09dd1bf3f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, SGD, RMSprop, AdamW\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import shap\n",
    "from keras.regularizers import l1, l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d005cfd2-3eda-44c4-bbb5-af343e979bc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [7270  860 5390 5191 5734]\n"
     ]
    }
   ],
   "source": [
    "# Set an initial seed for reproducibility\n",
    "np.random.seed(42)  \n",
    "\n",
    "# Generate a list of 5 random seeds\n",
    "random_seeds = np.random.randint(0, 10000, size=5)\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae122a-9f9c-47a3-8835-2d46d2f8e2f9",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5097e68-5153-440c-9294-dfa9e6061652",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_initial_group_split(groups_train, groups_test):\n",
    "    \"\"\"\n",
    "    Check if any group is present in both the train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - groups_train: Array of group identifiers for the train set\n",
    "    - groups_test: Array of group identifiers for the test set\n",
    "\n",
    "    Returns:\n",
    "    - Prints out any groups found in both sets and the count of such groups\n",
    "    \"\"\"\n",
    "    train_groups = set(groups_train)\n",
    "    test_groups = set(groups_test)\n",
    "    common_groups = train_groups.intersection(test_groups)\n",
    "\n",
    "    if common_groups:\n",
    "        print(f\"Warning: Found {len(common_groups)} common groups in both train/validation and test sets: {common_groups}\")\n",
    "    else:\n",
    "        print(\"No common groups found between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88dfe2de-bb1e-4d49-9260-e056c39627bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to perform the swaps based on cat_id, ensuring swaps within the same age_group\n",
    "def swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids):\n",
    "    for cat_id in specific_cat_ids:\n",
    "        # Check if the specific cat_id is not in the training set\n",
    "        if cat_id not in dataframe.iloc[train_val_idx]['cat_id'].values:\n",
    "            # Get the age_group of this cat_id\n",
    "            age_group = dataframe[dataframe['cat_id'] == cat_id]['age_group'].iloc[0]\n",
    "                \n",
    "            # Find a different cat_id within the same age_group in the train set that is not in the test set\n",
    "            other_cat_ids_in_age_group = dataframe[(dataframe['age_group'] == age_group) & \n",
    "                                                   (dataframe['cat_id'] != cat_id) &\n",
    "                                                   (~dataframe['cat_id'].isin(dataframe.iloc[test_idx]['cat_id']))]['cat_id'].unique()\n",
    "            \n",
    "            # Choose one other cat_id for swapping\n",
    "            if len(other_cat_ids_in_age_group) > 0:\n",
    "                other_cat_id = np.random.choice(other_cat_ids_in_age_group)\n",
    "\n",
    "                # Find all instances of the other_cat_id in the train set\n",
    "                other_cat_id_train_val_indices = train_val_idx[dataframe.iloc[train_val_idx]['cat_id'] == other_cat_id]\n",
    "                \n",
    "                # Find all instances of the specific cat_id in the test set\n",
    "                cat_id_test_indices = test_idx[dataframe.iloc[test_idx]['cat_id'] == cat_id]\n",
    "                \n",
    "                # Swap the indices\n",
    "                train_val_idx = np.setdiff1d(train_val_idx, other_cat_id_train_val_indices, assume_unique=True)\n",
    "                test_idx = np.setdiff1d(test_idx, cat_id_test_indices, assume_unique=True)\n",
    "\n",
    "                train_val_idx = np.concatenate((train_val_idx, cat_id_test_indices))\n",
    "                test_idx = np.concatenate((test_idx, other_cat_id_train_val_indices))\n",
    "            else:\n",
    "                print(f\"No alternative cat_id found in the same age_group as {cat_id} for swapping.\")\n",
    "                \n",
    "    return train_val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e6c2d87-cfab-493e-bca8-b3b8976a2bda",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify differences in groups\n",
    "def find_group_differences(original, new):\n",
    "    # Convert numpy arrays to sets for easy difference computation\n",
    "    original_set = set(original)\n",
    "    new_set = set(new)\n",
    "    # Find differences\n",
    "    moved_to_new = new_set - original_set\n",
    "    moved_to_original = original_set - new_set\n",
    "    return moved_to_new, moved_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6164b1c3-75dd-4549-aafd-cb6106297941",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create custom logger function for local logs & stored in a .txt\n",
    "def logger(message, file=None):\n",
    "    print(message)\n",
    "    if file is not None:\n",
    "        with open(file, \"a\") as log_file:\n",
    "            log_file.write(message + \"\\n\")\n",
    "\n",
    "log_file_path = \"multi-seed-val-D13.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "899fb535-0e25-4c7b-b6a5-13231e26c502",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "# Define a custom color palette\n",
    "colors = [\"#6aabd1\", \"#b6e2d3\", \"#dac292\"] \n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "# Function to create bar plots with enhanced style\n",
    "def styled_barplot(data, x, y, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bar_plot = sns.barplot(x=x, y=y, data=data, errorbar=None, width=0.5)  \n",
    "    plt.title(title, fontsize=16, fontweight='bold', color=\"#333333\")\n",
    "    plt.xlabel(xlabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.ylabel(ylabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.xticks(fontsize=12, color=\"#333333\")\n",
    "    plt.yticks(fontsize=12, color=\"#333333\")\n",
    "    plt.ylim(0, 100) \n",
    "\n",
    "    # Adding value labels on top of each bar\n",
    "    for p in bar_plot.patches:\n",
    "        height = p.get_height()\n",
    "        # Annotate the height value on the bar\n",
    "        bar_plot.annotate(f'{height:.1f}', \n",
    "                          (p.get_x() + p.get_width() / 2., height), \n",
    "                          ha='center', va='center', \n",
    "                          xytext=(0, 9), \n",
    "                          textcoords='offset points', fontsize=12, color=\"#333333\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936a24a-13bd-4bc3-be13-0b210a09b5da",
   "metadata": {},
   "source": [
    "# RANDOM SEED 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70136a37-0507-4c2e-8307-c2dd905432e8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14bb802b-f4bd-4464-a5fd-1977438428b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[0])) \n",
    "np.random.seed(int(random_seeds[0]))\n",
    "tf.random.set_seed(int(random_seeds[0]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce97c28-4210-4150-a60b-acdbf0e7716c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c26f17c3-4d21-4537-a090-9ca00f2be51a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91a9c3-3474-4e83-8a5b-c5eb4a9a9223",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4605c0b-f99d-48c3-8c2a-d87dd22c8946",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "        ..\n",
      "066A     1\n",
      "096A     1\n",
      "026C     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 87, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "055A    20\n",
      "001A    14\n",
      "025A    11\n",
      "016A    10\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "117A     7\n",
      "008A     6\n",
      "053A     6\n",
      "104A     4\n",
      "009A     4\n",
      "060A     3\n",
      "056A     3\n",
      "058A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "073A     1\n",
      "076A     1\n",
      "092A     1\n",
      "091A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    325\n",
      "M    253\n",
      "F    197\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    84\n",
      "F    55\n",
      "X    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [097A, 057A, 106A, 059A, 113A, 116A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 001A, 103A, 091A, 009A, 025A, 069A, 032...\n",
      "kitten                                         [045A, 110A]\n",
      "senior    [093A, 104A, 055A, 117A, 056A, 058A, 016A, 094...\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 14, 'senior': 13}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 2, 'senior': 9}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "Length of X_train_val:\n",
      "775\n",
      "Length of y_train_val:\n",
      "775\n",
      "Length of groups_train_val:\n",
      "775\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 988, 1: 805, 2: 600})\n",
      "Epoch 1/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.9058 - accuracy: 0.6243\n",
      "Epoch 2/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.7247 - accuracy: 0.6979\n",
      "Epoch 3/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6363 - accuracy: 0.7417\n",
      "Epoch 4/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5947 - accuracy: 0.7610\n",
      "Epoch 5/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5589 - accuracy: 0.7748\n",
      "Epoch 6/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5210 - accuracy: 0.7902\n",
      "Epoch 7/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5099 - accuracy: 0.7814\n",
      "Epoch 8/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4976 - accuracy: 0.7969\n",
      "Epoch 9/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4751 - accuracy: 0.8036\n",
      "Epoch 10/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4747 - accuracy: 0.8036\n",
      "Epoch 11/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4091 - accuracy: 0.8299\n",
      "Epoch 12/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4292 - accuracy: 0.8253\n",
      "Epoch 13/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4257 - accuracy: 0.8253\n",
      "Epoch 14/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4074 - accuracy: 0.8224\n",
      "Epoch 15/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3963 - accuracy: 0.8471\n",
      "Epoch 16/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3934 - accuracy: 0.8387\n",
      "Epoch 17/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3793 - accuracy: 0.8462\n",
      "Epoch 18/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3798 - accuracy: 0.8466\n",
      "Epoch 19/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8462\n",
      "Epoch 20/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3390 - accuracy: 0.8634\n",
      "Epoch 21/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3404 - accuracy: 0.8529\n",
      "Epoch 22/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3416 - accuracy: 0.8684\n",
      "Epoch 23/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3419 - accuracy: 0.8583\n",
      "Epoch 24/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3342 - accuracy: 0.8671\n",
      "Epoch 25/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3203 - accuracy: 0.8713\n",
      "Epoch 26/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2973 - accuracy: 0.8842\n",
      "Epoch 27/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2907 - accuracy: 0.8855\n",
      "Epoch 28/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3094 - accuracy: 0.8759\n",
      "Epoch 29/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2967 - accuracy: 0.8763\n",
      "Epoch 30/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.8859\n",
      "Epoch 31/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2738 - accuracy: 0.8905\n",
      "Epoch 32/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2952 - accuracy: 0.8826\n",
      "Epoch 33/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3011 - accuracy: 0.8734\n",
      "Epoch 34/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2880 - accuracy: 0.8838\n",
      "Epoch 35/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2788 - accuracy: 0.8897\n",
      "Epoch 36/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2653 - accuracy: 0.8997\n",
      "Epoch 37/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2717 - accuracy: 0.8943\n",
      "Epoch 38/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2742 - accuracy: 0.8943\n",
      "Epoch 39/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2794 - accuracy: 0.8893\n",
      "Epoch 40/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2706 - accuracy: 0.8884\n",
      "Epoch 41/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2478 - accuracy: 0.8939\n",
      "Epoch 42/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2701 - accuracy: 0.8876\n",
      "Epoch 43/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2386 - accuracy: 0.9168\n",
      "Epoch 44/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2522 - accuracy: 0.8989\n",
      "Epoch 45/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2704 - accuracy: 0.8947\n",
      "Epoch 46/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2333 - accuracy: 0.9072\n",
      "Epoch 47/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2326 - accuracy: 0.9076\n",
      "Epoch 48/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2231 - accuracy: 0.9156\n",
      "Epoch 49/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2259 - accuracy: 0.9093\n",
      "Epoch 50/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2276 - accuracy: 0.9139\n",
      "Epoch 51/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2373 - accuracy: 0.9081\n",
      "Epoch 52/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2158 - accuracy: 0.9156\n",
      "Epoch 53/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2337 - accuracy: 0.9114\n",
      "Epoch 54/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2330 - accuracy: 0.9085\n",
      "Epoch 55/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.9118\n",
      "Epoch 56/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.9168\n",
      "Epoch 57/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2069 - accuracy: 0.9189\n",
      "Epoch 58/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2354 - accuracy: 0.9139\n",
      "Epoch 59/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2076 - accuracy: 0.9223\n",
      "Epoch 60/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1935 - accuracy: 0.9256\n",
      "Epoch 61/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2144 - accuracy: 0.9152\n",
      "Epoch 62/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1994 - accuracy: 0.9231\n",
      "Epoch 63/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1882 - accuracy: 0.9273\n",
      "Epoch 64/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2089 - accuracy: 0.9148\n",
      "Epoch 65/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2109 - accuracy: 0.9294\n",
      "Epoch 66/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2110 - accuracy: 0.9148\n",
      "Epoch 67/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.9319\n",
      "Epoch 68/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9244\n",
      "Epoch 69/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2124 - accuracy: 0.9168\n",
      "Epoch 70/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9223\n",
      "Epoch 71/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.9315\n",
      "Epoch 72/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.9323\n",
      "Epoch 73/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1869 - accuracy: 0.9294\n",
      "Epoch 74/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2065 - accuracy: 0.9185\n",
      "Epoch 75/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9344\n",
      "Epoch 76/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.9306\n",
      "Epoch 77/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1706 - accuracy: 0.9336\n",
      "Epoch 78/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9306\n",
      "Epoch 79/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1747 - accuracy: 0.9319\n",
      "Epoch 80/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1747 - accuracy: 0.9369\n",
      "Epoch 81/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9298\n",
      "Epoch 82/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9269\n",
      "Epoch 83/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9310\n",
      "Epoch 84/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9277\n",
      "Epoch 85/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9373\n",
      "Epoch 86/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9356\n",
      "Epoch 87/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9302\n",
      "Epoch 88/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1742 - accuracy: 0.9361\n",
      "Epoch 89/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9473\n",
      "Epoch 90/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9402\n",
      "Epoch 91/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.9407\n",
      "Epoch 92/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1738 - accuracy: 0.9294\n",
      "Epoch 93/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9398\n",
      "Epoch 94/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1558 - accuracy: 0.9411\n",
      "Epoch 95/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9486\n",
      "Epoch 96/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1663 - accuracy: 0.9415\n",
      "Epoch 97/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9457\n",
      "Epoch 98/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1587 - accuracy: 0.9465\n",
      "Epoch 99/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9402\n",
      "Epoch 100/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9461\n",
      "Epoch 101/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1590 - accuracy: 0.9390\n",
      "Epoch 102/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9453\n",
      "Epoch 103/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9524\n",
      "Epoch 104/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9407\n",
      "Epoch 105/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9365\n",
      "Epoch 106/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9386\n",
      "Epoch 107/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1409 - accuracy: 0.9503\n",
      "Epoch 108/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.9528\n",
      "Epoch 109/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9465\n",
      "Epoch 110/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9503\n",
      "Epoch 111/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9386\n",
      "Epoch 112/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1699 - accuracy: 0.9352\n",
      "Epoch 113/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1409 - accuracy: 0.9478\n",
      "Epoch 114/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9419\n",
      "Epoch 115/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9532\n",
      "Epoch 116/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9503\n",
      "Epoch 117/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9486\n",
      "Epoch 118/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9444\n",
      "Epoch 119/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9448\n",
      "Epoch 120/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9507\n",
      "Epoch 121/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9549\n",
      "Epoch 122/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.9448\n",
      "Epoch 123/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.9440\n",
      "Epoch 124/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9478\n",
      "Epoch 125/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9528\n",
      "Epoch 126/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9482\n",
      "Epoch 127/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9524\n",
      "Epoch 128/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9448\n",
      "Epoch 129/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1351 - accuracy: 0.9528\n",
      "Epoch 130/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9549\n",
      "Epoch 131/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9478\n",
      "Epoch 132/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9486\n",
      "Epoch 133/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9557\n",
      "Epoch 134/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9570\n",
      "Epoch 135/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9557\n",
      "Epoch 136/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9503\n",
      "Epoch 137/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9528\n",
      "Epoch 138/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9519\n",
      "Epoch 139/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9536\n",
      "Epoch 140/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9536\n",
      "Epoch 141/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9586\n",
      "Epoch 142/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9515\n",
      "Epoch 143/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9565\n",
      "Epoch 144/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9519\n",
      "Epoch 145/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9565\n",
      "Epoch 146/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9590\n",
      "Epoch 147/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9595\n",
      "Epoch 148/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9578\n",
      "Epoch 149/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9632\n",
      "Epoch 150/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9561\n",
      "Epoch 151/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9499\n",
      "Epoch 152/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9515\n",
      "Epoch 153/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9611\n",
      "Epoch 154/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9570\n",
      "Epoch 155/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9540\n",
      "Epoch 156/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9578\n",
      "Epoch 157/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9432\n",
      "Epoch 158/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9636\n",
      "Epoch 159/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9553\n",
      "Epoch 160/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9599\n",
      "Epoch 161/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9557\n",
      "Epoch 162/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9611\n",
      "Epoch 163/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9549\n",
      "Epoch 164/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9620\n",
      "Epoch 165/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9519\n",
      "Epoch 166/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9616\n",
      "Epoch 167/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9586\n",
      "Epoch 168/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9582\n",
      "Epoch 169/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9628\n",
      "Epoch 170/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9590\n",
      "Epoch 171/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9641\n",
      "Epoch 172/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9519\n",
      "Epoch 173/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9565\n",
      "Epoch 174/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9611\n",
      "Epoch 175/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9582\n",
      "Epoch 176/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9616\n",
      "Epoch 177/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9549\n",
      "Epoch 178/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.9616\n",
      "Epoch 179/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9645\n",
      "Epoch 180/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9678\n",
      "Epoch 181/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9649\n",
      "Epoch 182/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9662\n",
      "Epoch 183/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9628\n",
      "Epoch 184/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9595\n",
      "Epoch 185/1500\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.0947 - accuracy: 0.9657\n",
      "Epoch 186/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9620\n",
      "Epoch 187/1500\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1117 - accuracy: 0.9578\n",
      "Epoch 188/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9611\n",
      "Epoch 189/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9649\n",
      "Epoch 190/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9678\n",
      "Epoch 191/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9682\n",
      "Epoch 192/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9624\n",
      "Epoch 193/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9657\n",
      "Epoch 194/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9645\n",
      "Epoch 195/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9603\n",
      "Epoch 196/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.9641\n",
      "Epoch 197/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9691\n",
      "Epoch 198/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.9632\n",
      "Epoch 199/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9616\n",
      "Epoch 200/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9620\n",
      "Epoch 201/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9607\n",
      "Epoch 202/1500\n",
      "75/75 [==============================] - 0s 987us/step - loss: 0.1028 - accuracy: 0.9653\n",
      "Epoch 203/1500\n",
      "75/75 [==============================] - 0s 977us/step - loss: 0.0917 - accuracy: 0.9670\n",
      "Epoch 204/1500\n",
      "75/75 [==============================] - 0s 976us/step - loss: 0.0905 - accuracy: 0.9682\n",
      "Epoch 205/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9645\n",
      "Epoch 206/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9657\n",
      "Epoch 207/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9607\n",
      "Epoch 208/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9687\n",
      "Epoch 209/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9624\n",
      "Epoch 210/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9662\n",
      "Epoch 211/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9699\n",
      "Epoch 212/1500\n",
      "40/75 [===============>..............] - ETA: 0s - loss: 0.0721 - accuracy: 0.9750Restoring model weights from the end of the best epoch: 182.\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9674\n",
      "Epoch 212: early stopping\n",
      "6/6 [==============================] - 0s 971us/step - loss: 1.1780 - accuracy: 0.6790\n",
      "6/6 [==============================] - 0s 712us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.68 (17/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "Final Test Results - Loss: 1.1779710054397583, Accuracy: 0.6790123581886292, Precision: 0.6868131868131869, Recall: 0.7263878698948397, F1 Score: 0.7036616161616162\n",
      "Confusion Matrix:\n",
      " [[70  3 21]\n",
      " [ 1  9  0]\n",
      " [27  0 31]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "        ..\n",
      "041A     1\n",
      "092A     1\n",
      "076A     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, Length: 83, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "057A    27\n",
      "000B    19\n",
      "029A    17\n",
      "106A    14\n",
      "028A    13\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "071A    10\n",
      "005A    10\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "037A     6\n",
      "044A     5\n",
      "070A     5\n",
      "105A     4\n",
      "014A     3\n",
      "054A     2\n",
      "025B     2\n",
      "096A     1\n",
      "049A     1\n",
      "048A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    264\n",
      "X    198\n",
      "F    193\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    150\n",
      "M     73\n",
      "F     59\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 019...\n",
      "kitten    [014B, 111A, 040A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 116A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 028A, 022A, 029A, 095A, 005A, 039A, 013...\n",
      "kitten                             [044A, 046A, 049A, 048A]\n",
      "senior                             [057A, 106A, 051B, 054A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '047A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '046A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'047A'}\n",
      "Moved to Test Set:\n",
      "{'047A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '046A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '047A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "Length of X_train_val:\n",
      "690\n",
      "Length of y_train_val:\n",
      "690\n",
      "Length of groups_train_val:\n",
      "690\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    126\n",
      "kitten    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     70\n",
      "senior     52\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "kitten    136\n",
      "senior    126\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "senior     52\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 856, 1: 680, 2: 630})\n",
      "Epoch 1/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.8831 - accuracy: 0.6274\n",
      "Epoch 2/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6814 - accuracy: 0.7105\n",
      "Epoch 3/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6089 - accuracy: 0.7632\n",
      "Epoch 4/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5930 - accuracy: 0.7585\n",
      "Epoch 5/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5233 - accuracy: 0.7973\n",
      "Epoch 6/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5013 - accuracy: 0.7982\n",
      "Epoch 7/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4832 - accuracy: 0.8121\n",
      "Epoch 8/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4749 - accuracy: 0.8149\n",
      "Epoch 9/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4335 - accuracy: 0.8232\n",
      "Epoch 10/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4381 - accuracy: 0.8315\n",
      "Epoch 11/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4143 - accuracy: 0.8375\n",
      "Epoch 12/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3900 - accuracy: 0.8440\n",
      "Epoch 13/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3818 - accuracy: 0.8467\n",
      "Epoch 14/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3709 - accuracy: 0.8523\n",
      "Epoch 15/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3639 - accuracy: 0.8620\n",
      "Epoch 16/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3528 - accuracy: 0.8541\n",
      "Epoch 17/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3463 - accuracy: 0.8670\n",
      "Epoch 18/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3524 - accuracy: 0.8490\n",
      "Epoch 19/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3135 - accuracy: 0.8735\n",
      "Epoch 20/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.8606\n",
      "Epoch 21/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3237 - accuracy: 0.8730\n",
      "Epoch 22/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3097 - accuracy: 0.8781\n",
      "Epoch 23/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2972 - accuracy: 0.8777\n",
      "Epoch 24/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8772\n",
      "Epoch 25/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2956 - accuracy: 0.8767\n",
      "Epoch 26/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2666 - accuracy: 0.8920\n",
      "Epoch 27/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3098 - accuracy: 0.8758\n",
      "Epoch 28/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2748 - accuracy: 0.8970\n",
      "Epoch 29/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2753 - accuracy: 0.8947\n",
      "Epoch 30/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2729 - accuracy: 0.9003\n",
      "Epoch 31/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2813 - accuracy: 0.8957\n",
      "Epoch 32/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2808 - accuracy: 0.8975\n",
      "Epoch 33/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2529 - accuracy: 0.9012\n",
      "Epoch 34/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2746 - accuracy: 0.8934\n",
      "Epoch 35/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2502 - accuracy: 0.9063\n",
      "Epoch 36/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.9030\n",
      "Epoch 37/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2268 - accuracy: 0.9118\n",
      "Epoch 38/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2406 - accuracy: 0.9026\n",
      "Epoch 39/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2184 - accuracy: 0.9201\n",
      "Epoch 40/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2281 - accuracy: 0.9146\n",
      "Epoch 41/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9132\n",
      "Epoch 42/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2199 - accuracy: 0.9183\n",
      "Epoch 43/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2235 - accuracy: 0.9127\n",
      "Epoch 44/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2105 - accuracy: 0.9174\n",
      "Epoch 45/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2136 - accuracy: 0.9169\n",
      "Epoch 46/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2266 - accuracy: 0.9164\n",
      "Epoch 47/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2155 - accuracy: 0.9215\n",
      "Epoch 48/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2076 - accuracy: 0.9234\n",
      "Epoch 49/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2168 - accuracy: 0.9192\n",
      "Epoch 50/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9224\n",
      "Epoch 51/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9201\n",
      "Epoch 52/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9238\n",
      "Epoch 53/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9326\n",
      "Epoch 54/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9215\n",
      "Epoch 55/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1934 - accuracy: 0.9261\n",
      "Epoch 56/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1779 - accuracy: 0.9386\n",
      "Epoch 57/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9349\n",
      "Epoch 58/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2037 - accuracy: 0.9243\n",
      "Epoch 59/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9317\n",
      "Epoch 60/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9391\n",
      "Epoch 61/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1909 - accuracy: 0.9271\n",
      "Epoch 62/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1871 - accuracy: 0.9317\n",
      "Epoch 63/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.9317\n",
      "Epoch 64/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.9340\n",
      "Epoch 65/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.9349\n",
      "Epoch 66/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9377\n",
      "Epoch 67/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1689 - accuracy: 0.9349\n",
      "Epoch 68/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1623 - accuracy: 0.9335\n",
      "Epoch 69/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9340\n",
      "Epoch 70/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.9367\n",
      "Epoch 71/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9437\n",
      "Epoch 72/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.9395\n",
      "Epoch 73/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9492\n",
      "Epoch 74/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9367\n",
      "Epoch 75/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9358\n",
      "Epoch 76/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9409\n",
      "Epoch 77/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9506\n",
      "Epoch 78/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9395\n",
      "Epoch 79/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9483\n",
      "Epoch 80/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9488\n",
      "Epoch 81/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9488\n",
      "Epoch 82/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9418\n",
      "Epoch 83/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.9492\n",
      "Epoch 84/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1496 - accuracy: 0.9455\n",
      "Epoch 85/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9478\n",
      "Epoch 86/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1497 - accuracy: 0.9432\n",
      "Epoch 87/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9432\n",
      "Epoch 88/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9423\n",
      "Epoch 89/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9506\n",
      "Epoch 90/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9451\n",
      "Epoch 91/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9538\n",
      "Epoch 92/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9543\n",
      "Epoch 93/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9520\n",
      "Epoch 94/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9506\n",
      "Epoch 95/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9506\n",
      "Epoch 96/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9515\n",
      "Epoch 97/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9492\n",
      "Epoch 98/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1330 - accuracy: 0.9566\n",
      "Epoch 99/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9524\n",
      "Epoch 100/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9474\n",
      "Epoch 101/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.9460\n",
      "Epoch 102/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9566\n",
      "Epoch 103/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9515\n",
      "Epoch 104/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9515\n",
      "Epoch 105/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9617\n",
      "Epoch 106/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9548\n",
      "Epoch 107/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9515\n",
      "Epoch 108/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9626\n",
      "Epoch 109/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9598\n",
      "Epoch 110/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9640\n",
      "Epoch 111/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9594\n",
      "Epoch 112/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9492\n",
      "Epoch 113/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9548\n",
      "Epoch 114/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9580\n",
      "Epoch 115/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9580\n",
      "Epoch 116/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9584\n",
      "Epoch 117/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.9645\n",
      "Epoch 118/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9543\n",
      "Epoch 119/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9603\n",
      "Epoch 120/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9571\n",
      "Epoch 121/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9626\n",
      "Epoch 122/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9552\n",
      "Epoch 123/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9723\n",
      "Epoch 124/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9668\n",
      "Epoch 125/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9584\n",
      "Epoch 126/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9617\n",
      "Epoch 127/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.9584\n",
      "Epoch 128/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.9672\n",
      "Epoch 129/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9672\n",
      "Epoch 130/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9681\n",
      "Epoch 131/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9515\n",
      "Epoch 132/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9598\n",
      "Epoch 133/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9626\n",
      "Epoch 134/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0974 - accuracy: 0.9686\n",
      "Epoch 135/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9755\n",
      "Epoch 136/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9677\n",
      "Epoch 137/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9603\n",
      "Epoch 138/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9598\n",
      "Epoch 139/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.9695\n",
      "Epoch 140/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9654\n",
      "Epoch 141/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9663\n",
      "Epoch 142/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9691\n",
      "Epoch 143/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9668\n",
      "Epoch 144/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9580\n",
      "Epoch 145/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9654\n",
      "Epoch 146/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9677\n",
      "Epoch 147/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9617\n",
      "Epoch 148/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0977 - accuracy: 0.9668\n",
      "Epoch 149/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9723\n",
      "Epoch 150/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9778\n",
      "Epoch 151/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9668\n",
      "Epoch 152/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9705\n",
      "Epoch 153/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9751\n",
      "Epoch 154/1500\n",
      "68/68 [==============================] - 0s 3ms/step - loss: 0.0973 - accuracy: 0.9691\n",
      "Epoch 155/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9691\n",
      "Epoch 156/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9663\n",
      "Epoch 157/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9695\n",
      "Epoch 158/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0882 - accuracy: 0.9700\n",
      "Epoch 159/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9755\n",
      "Epoch 160/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9695\n",
      "Epoch 161/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.9663\n",
      "Epoch 162/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9672\n",
      "Epoch 163/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9723\n",
      "Epoch 164/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9635\n",
      "Epoch 165/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9672\n",
      "Epoch 166/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9677\n",
      "Epoch 167/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9741\n",
      "Epoch 168/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9751\n",
      "Epoch 169/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9769\n",
      "Epoch 170/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9732\n",
      "Epoch 171/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9658\n",
      "Epoch 172/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9668\n",
      "Epoch 173/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9732\n",
      "Epoch 174/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.9751\n",
      "Epoch 175/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.9746\n",
      "Epoch 176/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9681\n",
      "Epoch 177/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.9760\n",
      "Epoch 178/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9778\n",
      "Epoch 179/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.9792\n",
      "Epoch 180/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9741\n",
      "Epoch 181/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9732\n",
      "Epoch 182/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9700\n",
      "Epoch 183/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9705\n",
      "Epoch 184/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9709\n",
      "Epoch 185/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9672\n",
      "Epoch 186/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9617\n",
      "Epoch 187/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.9755\n",
      "Epoch 188/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.9778\n",
      "Epoch 189/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.9751\n",
      "Epoch 190/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.9765\n",
      "Epoch 191/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9778\n",
      "Epoch 192/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.9751\n",
      "Epoch 193/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.9732\n",
      "Epoch 194/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.9765\n",
      "Epoch 195/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9705\n",
      "Epoch 196/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9635\n",
      "Epoch 197/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9741\n",
      "Epoch 198/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9774\n",
      "Epoch 199/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9815\n",
      "Epoch 200/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.9760\n",
      "Epoch 201/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0563 - accuracy: 0.9801\n",
      "Epoch 202/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9774\n",
      "Epoch 203/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.9732\n",
      "Epoch 204/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9718\n",
      "Epoch 205/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9705\n",
      "Epoch 206/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0679 - accuracy: 0.9746\n",
      "Epoch 207/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9737\n",
      "Epoch 208/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0561 - accuracy: 0.9852\n",
      "Epoch 209/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.9778\n",
      "Epoch 210/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9718\n",
      "Epoch 211/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9741\n",
      "Epoch 212/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9806\n",
      "Epoch 213/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9815\n",
      "Epoch 214/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9746\n",
      "Epoch 215/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0639 - accuracy: 0.9760\n",
      "Epoch 216/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9732\n",
      "Epoch 217/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0611 - accuracy: 0.9815\n",
      "Epoch 218/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.9815\n",
      "Epoch 219/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0556 - accuracy: 0.9820\n",
      "Epoch 220/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0651 - accuracy: 0.9788\n",
      "Epoch 221/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9755\n",
      "Epoch 222/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9755\n",
      "Epoch 223/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9677\n",
      "Epoch 224/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.9718\n",
      "Epoch 225/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.9774\n",
      "Epoch 226/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.9760\n",
      "Epoch 227/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.9760\n",
      "Epoch 228/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0772 - accuracy: 0.9760\n",
      "Epoch 229/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.9811\n",
      "Epoch 230/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.9792\n",
      "Epoch 231/1500\n",
      "44/68 [==================>...........] - ETA: 0s - loss: 0.0607 - accuracy: 0.9801Restoring model weights from the end of the best epoch: 201.\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.9769\n",
      "Epoch 231: early stopping\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.2793 - accuracy: 0.6397\n",
      "8/8 [==============================] - 0s 805us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.66 (19/29)\n",
      "Before appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "After appending - Cat IDs: 409, Predictions: 409, Actuals: 409, Gender: 409\n",
      "Final Test Results - Loss: 1.2793182134628296, Accuracy: 0.6396760940551758, Precision: 0.6881784881784881, Recall: 0.5674679487179488, F1 Score: 0.5855560999510043\n",
      "Confusion Matrix:\n",
      " [[113   1  46]\n",
      " [ 20  14   1]\n",
      " [ 21   0  31]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "055A    20\n",
      "        ..\n",
      "096A     1\n",
      "043A     1\n",
      "073A     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 82, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "059A    14\n",
      "097B    14\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "072A     9\n",
      "033A     9\n",
      "027A     7\n",
      "099A     7\n",
      "050A     7\n",
      "023A     6\n",
      "034A     5\n",
      "025C     5\n",
      "075A     5\n",
      "052A     4\n",
      "003A     4\n",
      "012A     3\n",
      "006A     3\n",
      "018A     2\n",
      "026C     1\n",
      "019B     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    250\n",
      "F    202\n",
      "M    180\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    157\n",
      "X     98\n",
      "F     50\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 103A, 071A, 028A, 062A, 101...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 043...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 097B, 019A, 074A, 067A, 020A, 002...\n",
      "kitten                                   [047A, 050A, 115A]\n",
      "senior                       [097A, 059A, 116A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 52, 'kitten': 13, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 22, 'kitten': 3, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "632\n",
      "Length of y_train_val:\n",
      "632\n",
      "Length of groups_train_val:\n",
      "632\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 748, 1: 675, 2: 615})\n",
      "Epoch 1/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.9240 - accuracy: 0.5991\n",
      "Epoch 2/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.7228 - accuracy: 0.6943\n",
      "Epoch 3/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.6389 - accuracy: 0.7277\n",
      "Epoch 4/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.6212 - accuracy: 0.7296\n",
      "Epoch 5/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5799 - accuracy: 0.7566\n",
      "Epoch 6/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5761 - accuracy: 0.7522\n",
      "Epoch 7/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5490 - accuracy: 0.7650\n",
      "Epoch 8/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5059 - accuracy: 0.7875\n",
      "Epoch 9/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5028 - accuracy: 0.7802\n",
      "Epoch 10/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4961 - accuracy: 0.7836\n",
      "Epoch 11/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4869 - accuracy: 0.7900\n",
      "Epoch 12/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4677 - accuracy: 0.7988\n",
      "Epoch 13/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4424 - accuracy: 0.8140\n",
      "Epoch 14/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4426 - accuracy: 0.8194\n",
      "Epoch 15/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4500 - accuracy: 0.8234\n",
      "Epoch 16/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4466 - accuracy: 0.8175\n",
      "Epoch 17/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4418 - accuracy: 0.8165\n",
      "Epoch 18/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4241 - accuracy: 0.8288\n",
      "Epoch 19/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4077 - accuracy: 0.8312\n",
      "Epoch 20/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4034 - accuracy: 0.8268\n",
      "Epoch 21/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4102 - accuracy: 0.8292\n",
      "Epoch 22/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3872 - accuracy: 0.8405\n",
      "Epoch 23/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3819 - accuracy: 0.8454\n",
      "Epoch 24/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3589 - accuracy: 0.8518\n",
      "Epoch 25/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3676 - accuracy: 0.8528\n",
      "Epoch 26/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3625 - accuracy: 0.8459\n",
      "Epoch 27/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3750 - accuracy: 0.8337\n",
      "Epoch 28/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3478 - accuracy: 0.8548\n",
      "Epoch 29/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.8656\n",
      "Epoch 30/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3544 - accuracy: 0.8499\n",
      "Epoch 31/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3248 - accuracy: 0.8744\n",
      "Epoch 32/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3231 - accuracy: 0.8641\n",
      "Epoch 33/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3504 - accuracy: 0.8528\n",
      "Epoch 34/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3647 - accuracy: 0.8474\n",
      "Epoch 35/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3284 - accuracy: 0.8690\n",
      "Epoch 36/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.3270 - accuracy: 0.8641\n",
      "Epoch 37/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3261 - accuracy: 0.8714\n",
      "Epoch 38/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2933 - accuracy: 0.8857\n",
      "Epoch 39/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3071 - accuracy: 0.8700\n",
      "Epoch 40/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2975 - accuracy: 0.8793\n",
      "Epoch 41/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3056 - accuracy: 0.8773\n",
      "Epoch 42/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3031 - accuracy: 0.8891\n",
      "Epoch 43/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2942 - accuracy: 0.8832\n",
      "Epoch 44/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2838 - accuracy: 0.8813\n",
      "Epoch 45/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2795 - accuracy: 0.8896\n",
      "Epoch 46/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2780 - accuracy: 0.8906\n",
      "Epoch 47/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2769 - accuracy: 0.8974\n",
      "Epoch 48/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2932 - accuracy: 0.8842\n",
      "Epoch 49/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2734 - accuracy: 0.8847\n",
      "Epoch 50/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2832 - accuracy: 0.8813\n",
      "Epoch 51/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2607 - accuracy: 0.8994\n",
      "Epoch 52/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2655 - accuracy: 0.8970\n",
      "Epoch 53/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2520 - accuracy: 0.9014\n",
      "Epoch 54/1500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.2670 - accuracy: 0.8945\n",
      "Epoch 55/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.9053\n",
      "Epoch 56/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2457 - accuracy: 0.8989\n",
      "Epoch 57/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2478 - accuracy: 0.9107\n",
      "Epoch 58/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2455 - accuracy: 0.9078\n",
      "Epoch 59/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.8974\n",
      "Epoch 60/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2419 - accuracy: 0.9136\n",
      "Epoch 61/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.9028\n",
      "Epoch 62/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.9024\n",
      "Epoch 63/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2446 - accuracy: 0.9073\n",
      "Epoch 64/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2555 - accuracy: 0.9082\n",
      "Epoch 65/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2434 - accuracy: 0.9009\n",
      "Epoch 66/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2585 - accuracy: 0.8916\n",
      "Epoch 67/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2275 - accuracy: 0.9082\n",
      "Epoch 68/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2276 - accuracy: 0.9176\n",
      "Epoch 69/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2187 - accuracy: 0.9151\n",
      "Epoch 70/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2268 - accuracy: 0.9136\n",
      "Epoch 71/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9097\n",
      "Epoch 72/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2205 - accuracy: 0.9156\n",
      "Epoch 73/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2136 - accuracy: 0.9210\n",
      "Epoch 74/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2268 - accuracy: 0.9117\n",
      "Epoch 75/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2081 - accuracy: 0.9249\n",
      "Epoch 76/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2028 - accuracy: 0.9249\n",
      "Epoch 77/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2114 - accuracy: 0.9200\n",
      "Epoch 78/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1936 - accuracy: 0.9289\n",
      "Epoch 79/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9200\n",
      "Epoch 80/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9239\n",
      "Epoch 81/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2033 - accuracy: 0.9269\n",
      "Epoch 82/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9225\n",
      "Epoch 83/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2140 - accuracy: 0.9156\n",
      "Epoch 84/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9239\n",
      "Epoch 85/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2111 - accuracy: 0.9220\n",
      "Epoch 86/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2156 - accuracy: 0.9161\n",
      "Epoch 87/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.2135 - accuracy: 0.9166\n",
      "Epoch 88/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1845 - accuracy: 0.9323\n",
      "Epoch 89/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.9269\n",
      "Epoch 90/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.9342\n",
      "Epoch 91/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9274\n",
      "Epoch 92/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2124 - accuracy: 0.9127\n",
      "Epoch 93/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.9352\n",
      "Epoch 94/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.9333\n",
      "Epoch 95/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.9323\n",
      "Epoch 96/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1971 - accuracy: 0.9220\n",
      "Epoch 97/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1738 - accuracy: 0.9347\n",
      "Epoch 98/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9342\n",
      "Epoch 99/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9264\n",
      "Epoch 100/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9401\n",
      "Epoch 101/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9352\n",
      "Epoch 102/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9347\n",
      "Epoch 103/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.9446\n",
      "Epoch 104/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1881 - accuracy: 0.9308\n",
      "Epoch 105/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.9323\n",
      "Epoch 106/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1722 - accuracy: 0.9387\n",
      "Epoch 107/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9279\n",
      "Epoch 108/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1746 - accuracy: 0.9416\n",
      "Epoch 109/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1659 - accuracy: 0.9431\n",
      "Epoch 110/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1698 - accuracy: 0.9387\n",
      "Epoch 111/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9264\n",
      "Epoch 112/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.9264\n",
      "Epoch 113/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1661 - accuracy: 0.9401\n",
      "Epoch 114/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.9342\n",
      "Epoch 115/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9274\n",
      "Epoch 116/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9318\n",
      "Epoch 117/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9416\n",
      "Epoch 118/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9446\n",
      "Epoch 119/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1907 - accuracy: 0.9264\n",
      "Epoch 120/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9436\n",
      "Epoch 121/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.9372\n",
      "Epoch 122/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9352\n",
      "Epoch 123/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9328\n",
      "Epoch 124/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9470\n",
      "Epoch 125/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9313\n",
      "Epoch 126/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9396\n",
      "Epoch 127/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9514\n",
      "Epoch 128/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9500\n",
      "Epoch 129/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9396\n",
      "Epoch 130/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.9436\n",
      "Epoch 131/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9352\n",
      "Epoch 132/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9455\n",
      "Epoch 133/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1590 - accuracy: 0.9465\n",
      "Epoch 134/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9553\n",
      "Epoch 135/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9495\n",
      "Epoch 136/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9465\n",
      "Epoch 137/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9436\n",
      "Epoch 138/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1477 - accuracy: 0.9509\n",
      "Epoch 139/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9411\n",
      "Epoch 140/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9426\n",
      "Epoch 141/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9480\n",
      "Epoch 142/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9490\n",
      "Epoch 143/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1521 - accuracy: 0.9455\n",
      "Epoch 144/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9460\n",
      "Epoch 145/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9460\n",
      "Epoch 146/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9426\n",
      "Epoch 147/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9490\n",
      "Epoch 148/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9431\n",
      "Epoch 149/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1569 - accuracy: 0.9450\n",
      "Epoch 150/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9416\n",
      "Epoch 151/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9534\n",
      "Epoch 152/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9593\n",
      "Epoch 153/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9495\n",
      "Epoch 154/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9480\n",
      "Epoch 155/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9455\n",
      "Epoch 156/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9524\n",
      "Epoch 157/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9573\n",
      "Epoch 158/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9509\n",
      "Epoch 159/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9514\n",
      "Epoch 160/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9392\n",
      "Epoch 161/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9544\n",
      "Epoch 162/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9539\n",
      "Epoch 163/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9524\n",
      "Epoch 164/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9622\n",
      "Epoch 165/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1558 - accuracy: 0.9411\n",
      "Epoch 166/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9524\n",
      "Epoch 167/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9490\n",
      "Epoch 168/1500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.1326 - accuracy: 0.9490\n",
      "Epoch 169/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9514\n",
      "Epoch 170/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9558\n",
      "Epoch 171/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9504\n",
      "Epoch 172/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9529\n",
      "Epoch 173/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9524\n",
      "Epoch 174/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9524\n",
      "Epoch 175/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9495\n",
      "Epoch 176/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9475\n",
      "Epoch 177/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9598\n",
      "Epoch 178/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9558\n",
      "Epoch 179/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9475\n",
      "Epoch 180/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9622\n",
      "Epoch 181/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.9460\n",
      "Epoch 182/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9603\n",
      "Epoch 183/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9426\n",
      "Epoch 184/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9568\n",
      "Epoch 185/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9504\n",
      "Epoch 186/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9539\n",
      "Epoch 187/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9411\n",
      "Epoch 188/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9544\n",
      "Epoch 189/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9539\n",
      "Epoch 190/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9539\n",
      "Epoch 191/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9627\n",
      "Epoch 192/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9534\n",
      "Epoch 193/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9421\n",
      "Epoch 194/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9534\n",
      "Epoch 195/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9573\n",
      "Epoch 196/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9652\n",
      "Epoch 197/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9568\n",
      "Epoch 198/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9558\n",
      "Epoch 199/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9632\n",
      "Epoch 200/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9514\n",
      "Epoch 201/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9578\n",
      "Epoch 202/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9671\n",
      "Epoch 203/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9514\n",
      "Epoch 204/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9578\n",
      "Epoch 205/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9622\n",
      "Epoch 206/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9490\n",
      "Epoch 207/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.9681\n",
      "Epoch 208/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9642\n",
      "Epoch 209/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9612\n",
      "Epoch 210/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9603\n",
      "Epoch 211/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9676\n",
      "Epoch 212/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9612\n",
      "Epoch 213/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9544\n",
      "Epoch 214/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9598\n",
      "Epoch 215/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9661\n",
      "Epoch 216/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9671\n",
      "Epoch 217/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9652\n",
      "Epoch 218/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9603\n",
      "Epoch 219/1500\n",
      "64/64 [==============================] - 0s 972us/step - loss: 0.1118 - accuracy: 0.9637\n",
      "Epoch 220/1500\n",
      "64/64 [==============================] - 0s 985us/step - loss: 0.1018 - accuracy: 0.9642\n",
      "Epoch 221/1500\n",
      "64/64 [==============================] - 0s 969us/step - loss: 0.1214 - accuracy: 0.9549\n",
      "Epoch 222/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9598\n",
      "Epoch 223/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9666\n",
      "Epoch 224/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9617\n",
      "Epoch 225/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9735\n",
      "Epoch 226/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9627\n",
      "Epoch 227/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9539\n",
      "Epoch 228/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9627\n",
      "Epoch 229/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9568\n",
      "Epoch 230/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9573\n",
      "Epoch 231/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9627\n",
      "Epoch 232/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9632\n",
      "Epoch 233/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9681\n",
      "Epoch 234/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9686\n",
      "Epoch 235/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9588\n",
      "Epoch 236/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9622\n",
      "Epoch 237/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9696\n",
      "Epoch 238/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.9701\n",
      "Epoch 239/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9676\n",
      "Epoch 240/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9598\n",
      "Epoch 241/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9652\n",
      "Epoch 242/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9637\n",
      "Epoch 243/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9637\n",
      "Epoch 244/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9686\n",
      "Epoch 245/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9612\n",
      "Epoch 246/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9661\n",
      "Epoch 247/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9711\n",
      "Epoch 248/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9637\n",
      "Epoch 249/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9573\n",
      "Epoch 250/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9622\n",
      "Epoch 251/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9642\n",
      "Epoch 252/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9607\n",
      "Epoch 253/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9661\n",
      "Epoch 254/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9642\n",
      "Epoch 255/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9593\n",
      "Epoch 256/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.9711\n",
      "Epoch 257/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9730\n",
      "Epoch 258/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9666\n",
      "Epoch 259/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1025 - accuracy: 0.9603\n",
      "Epoch 260/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9696\n",
      "Epoch 261/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9652\n",
      "Epoch 262/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.9666\n",
      "Epoch 263/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9563\n",
      "Epoch 264/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9661\n",
      "Epoch 265/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9676\n",
      "Epoch 266/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9642\n",
      "Epoch 267/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.9730\n",
      "Epoch 268/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.9671\n",
      "Epoch 269/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9676\n",
      "Epoch 270/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9617\n",
      "Epoch 271/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9725\n",
      "Epoch 272/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9558\n",
      "Epoch 273/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.9720\n",
      "Epoch 274/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9735\n",
      "Epoch 275/1500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0916 - accuracy: 0.9661\n",
      "Epoch 276/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9769\n",
      "Epoch 277/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9686\n",
      "Epoch 278/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0931 - accuracy: 0.9661\n",
      "Epoch 279/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9720\n",
      "Epoch 280/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9647\n",
      "Epoch 281/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9715\n",
      "Epoch 282/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9730\n",
      "Epoch 283/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9691\n",
      "Epoch 284/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9652\n",
      "Epoch 285/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9627\n",
      "Epoch 286/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9657\n",
      "Epoch 287/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.9769\n",
      "Epoch 288/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9617\n",
      "Epoch 289/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9691\n",
      "Epoch 290/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.9715\n",
      "Epoch 291/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9686\n",
      "Epoch 292/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9696\n",
      "Epoch 293/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9720\n",
      "Epoch 294/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9622\n",
      "Epoch 295/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9691\n",
      "Epoch 296/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9755\n",
      "Epoch 297/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9715\n",
      "Epoch 298/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9755\n",
      "Epoch 299/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9627\n",
      "Epoch 300/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9725\n",
      "Epoch 301/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.9607\n",
      "Epoch 302/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9686\n",
      "Epoch 303/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.9657\n",
      "Epoch 304/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9637\n",
      "Epoch 305/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9671\n",
      "Epoch 306/1500\n",
      "47/64 [=====================>........] - ETA: 0s - loss: 0.0760 - accuracy: 0.9727Restoring model weights from the end of the best epoch: 276.\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.9711\n",
      "Epoch 306: early stopping\n",
      "10/10 [==============================] - 0s 769us/step - loss: 0.7829 - accuracy: 0.7770\n",
      "10/10 [==============================] - 0s 643us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.77 (23/30)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before appending - Cat IDs: 409, Predictions: 409, Actuals: 409, Gender: 409\n",
      "After appending - Cat IDs: 714, Predictions: 714, Actuals: 714, Gender: 714\n",
      "Final Test Results - Loss: 0.7828835248947144, Accuracy: 0.77704918384552, Precision: 0.7370167300463327, Recall: 0.7553274174769502, F1 Score: 0.7355060660882421\n",
      "Confusion Matrix:\n",
      " [[169   6  39]\n",
      " [ 11  25   0]\n",
      " [ 12   0  43]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "        ..\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 84, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "065A     9\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "021A     5\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "113A     3\n",
      "064A     3\n",
      "087A     2\n",
      "038A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "043A     1\n",
      "041A     1\n",
      "066A     1\n",
      "004A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    314\n",
      "X    271\n",
      "F    164\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    88\n",
      "X    77\n",
      "M    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 097B, 028...\n",
      "kitten    [044A, 046A, 047A, 050A, 049A, 045A, 048A, 115...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 062A, 101A, 065A, 063A, 038A, 007A, 087...\n",
      "kitten           [014B, 111A, 040A, 042A, 109A, 043A, 041A]\n",
      "senior                             [113A, 108A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 9, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 7, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'037A'}\n",
      "Moved to Test Set:\n",
      "{'037A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A'\n",
      " '029A' '032A' '033A' '034A' '036A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A' '037A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "782\n",
      "Length of y_train_val:\n",
      "782\n",
      "Length of groups_train_val:\n",
      "782\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "kitten     55\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     501\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     87\n",
      "kitten    55\n",
      "senior    13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1002, 2: 825, 1: 580})\n",
      "Epoch 1/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.9403 - accuracy: 0.6053\n",
      "Epoch 2/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.7457 - accuracy: 0.6826\n",
      "Epoch 3/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.7020 - accuracy: 0.7050\n",
      "Epoch 4/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.6579 - accuracy: 0.7308\n",
      "Epoch 5/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.6013 - accuracy: 0.7532\n",
      "Epoch 6/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5898 - accuracy: 0.7586\n",
      "Epoch 7/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5890 - accuracy: 0.7553\n",
      "Epoch 8/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5407 - accuracy: 0.7823\n",
      "Epoch 9/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5266 - accuracy: 0.7744\n",
      "Epoch 10/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5362 - accuracy: 0.7848\n",
      "Epoch 11/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4961 - accuracy: 0.7873\n",
      "Epoch 12/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4796 - accuracy: 0.7906\n",
      "Epoch 13/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4732 - accuracy: 0.7931\n",
      "Epoch 14/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4506 - accuracy: 0.8238\n",
      "Epoch 15/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4460 - accuracy: 0.8151\n",
      "Epoch 16/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4213 - accuracy: 0.8214\n",
      "Epoch 17/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4312 - accuracy: 0.8288\n",
      "Epoch 18/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4270 - accuracy: 0.8243\n",
      "Epoch 19/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4161 - accuracy: 0.8301\n",
      "Epoch 20/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4063 - accuracy: 0.8342\n",
      "Epoch 21/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4011 - accuracy: 0.8384\n",
      "Epoch 22/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4105 - accuracy: 0.8355\n",
      "Epoch 23/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3927 - accuracy: 0.8388\n",
      "Epoch 24/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3710 - accuracy: 0.8442\n",
      "Epoch 25/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3855 - accuracy: 0.8450\n",
      "Epoch 26/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3765 - accuracy: 0.8392\n",
      "Epoch 27/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3781 - accuracy: 0.8529\n",
      "Epoch 28/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3728 - accuracy: 0.8446\n",
      "Epoch 29/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3694 - accuracy: 0.8500\n",
      "Epoch 30/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3595 - accuracy: 0.8496\n",
      "Epoch 31/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3526 - accuracy: 0.8563\n",
      "Epoch 32/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3638 - accuracy: 0.8542\n",
      "Epoch 33/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3468 - accuracy: 0.8625\n",
      "Epoch 34/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3238 - accuracy: 0.8683\n",
      "Epoch 35/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3323 - accuracy: 0.8604\n",
      "Epoch 36/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3131 - accuracy: 0.8766\n",
      "Epoch 37/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8646\n",
      "Epoch 38/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3007 - accuracy: 0.8803\n",
      "Epoch 39/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2986 - accuracy: 0.8853\n",
      "Epoch 40/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3069 - accuracy: 0.8824\n",
      "Epoch 41/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3207 - accuracy: 0.8687\n",
      "Epoch 42/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3117 - accuracy: 0.8795\n",
      "Epoch 43/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2988 - accuracy: 0.8791\n",
      "Epoch 44/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3067 - accuracy: 0.8725\n",
      "Epoch 45/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2915 - accuracy: 0.8870\n",
      "Epoch 46/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2944 - accuracy: 0.8795\n",
      "Epoch 47/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2710 - accuracy: 0.8974\n",
      "Epoch 48/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2727 - accuracy: 0.8928\n",
      "Epoch 49/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2655 - accuracy: 0.9024\n",
      "Epoch 50/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2859 - accuracy: 0.8870\n",
      "Epoch 51/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2937 - accuracy: 0.8874\n",
      "Epoch 52/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2629 - accuracy: 0.8912\n",
      "Epoch 53/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2722 - accuracy: 0.8928\n",
      "Epoch 54/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2608 - accuracy: 0.8982\n",
      "Epoch 55/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2750 - accuracy: 0.8916\n",
      "Epoch 56/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2606 - accuracy: 0.8974\n",
      "Epoch 57/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2516 - accuracy: 0.8999\n",
      "Epoch 58/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2349 - accuracy: 0.9086\n",
      "Epoch 59/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2477 - accuracy: 0.9024\n",
      "Epoch 60/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2605 - accuracy: 0.9011\n",
      "Epoch 61/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.9044\n",
      "Epoch 62/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.9061\n",
      "Epoch 63/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2263 - accuracy: 0.9123\n",
      "Epoch 64/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.9061\n",
      "Epoch 65/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2336 - accuracy: 0.9186\n",
      "Epoch 66/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.9024\n",
      "Epoch 67/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2339 - accuracy: 0.9036\n",
      "Epoch 68/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2254 - accuracy: 0.9186\n",
      "Epoch 69/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2238 - accuracy: 0.9190\n",
      "Epoch 70/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2256 - accuracy: 0.9094\n",
      "Epoch 71/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2314 - accuracy: 0.9132\n",
      "Epoch 72/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2212 - accuracy: 0.9148\n",
      "Epoch 73/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2180 - accuracy: 0.9157\n",
      "Epoch 74/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2167 - accuracy: 0.9148\n",
      "Epoch 75/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9211\n",
      "Epoch 76/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2175 - accuracy: 0.9215\n",
      "Epoch 77/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2108 - accuracy: 0.9202\n",
      "Epoch 78/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9252\n",
      "Epoch 79/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9148\n",
      "Epoch 80/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2216 - accuracy: 0.9152\n",
      "Epoch 81/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2126 - accuracy: 0.9165\n",
      "Epoch 82/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2166 - accuracy: 0.9107\n",
      "Epoch 83/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9256\n",
      "Epoch 84/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.9190\n",
      "Epoch 85/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2172 - accuracy: 0.9157\n",
      "Epoch 86/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1968 - accuracy: 0.9231\n",
      "Epoch 87/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2046 - accuracy: 0.9198\n",
      "Epoch 88/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1879 - accuracy: 0.9327\n",
      "Epoch 89/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.9202\n",
      "Epoch 90/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9290\n",
      "Epoch 91/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.9256\n",
      "Epoch 92/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9202\n",
      "Epoch 93/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9219\n",
      "Epoch 94/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2011 - accuracy: 0.9281\n",
      "Epoch 95/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1981 - accuracy: 0.9219\n",
      "Epoch 96/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9306\n",
      "Epoch 97/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2005 - accuracy: 0.9252\n",
      "Epoch 98/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2000 - accuracy: 0.9148\n",
      "Epoch 99/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9335\n",
      "Epoch 100/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9182\n",
      "Epoch 101/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1726 - accuracy: 0.9410\n",
      "Epoch 102/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1954 - accuracy: 0.9294\n",
      "Epoch 103/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2081 - accuracy: 0.9231\n",
      "Epoch 104/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9256\n",
      "Epoch 105/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1981 - accuracy: 0.9269\n",
      "Epoch 106/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1913 - accuracy: 0.9273\n",
      "Epoch 107/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.9248\n",
      "Epoch 108/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9335\n",
      "Epoch 109/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1925 - accuracy: 0.9348\n",
      "Epoch 110/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9339\n",
      "Epoch 111/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9281\n",
      "Epoch 112/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1721 - accuracy: 0.9335\n",
      "Epoch 113/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9356\n",
      "Epoch 114/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1838 - accuracy: 0.9281\n",
      "Epoch 115/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9431\n",
      "Epoch 116/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.9335\n",
      "Epoch 117/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9319\n",
      "Epoch 118/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1699 - accuracy: 0.9356\n",
      "Epoch 119/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1628 - accuracy: 0.9418\n",
      "Epoch 120/1500\n",
      "76/76 [==============================] - 0s 2ms/step - loss: 0.1574 - accuracy: 0.9385\n",
      "Epoch 121/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9348\n",
      "Epoch 122/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1561 - accuracy: 0.9398\n",
      "Epoch 123/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9443\n",
      "Epoch 124/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1649 - accuracy: 0.9393\n",
      "Epoch 125/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9377\n",
      "Epoch 126/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.9327\n",
      "Epoch 127/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9377\n",
      "Epoch 128/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9431\n",
      "Epoch 129/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1783 - accuracy: 0.9290\n",
      "Epoch 130/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.9331\n",
      "Epoch 131/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9439\n",
      "Epoch 132/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.9435\n",
      "Epoch 133/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9456\n",
      "Epoch 134/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9431\n",
      "Epoch 135/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9547\n",
      "Epoch 136/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1779 - accuracy: 0.9356\n",
      "Epoch 137/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9427\n",
      "Epoch 138/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1634 - accuracy: 0.9360\n",
      "Epoch 139/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9418\n",
      "Epoch 140/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9369\n",
      "Epoch 141/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9531\n",
      "Epoch 142/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9452\n",
      "Epoch 143/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9410\n",
      "Epoch 144/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9452\n",
      "Epoch 145/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1592 - accuracy: 0.9439\n",
      "Epoch 146/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9352\n",
      "Epoch 147/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1580 - accuracy: 0.9414\n",
      "Epoch 148/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9443\n",
      "Epoch 149/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9468\n",
      "Epoch 150/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9468\n",
      "Epoch 151/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9497\n",
      "Epoch 152/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9481\n",
      "Epoch 153/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9493\n",
      "Epoch 154/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9551\n",
      "Epoch 155/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.9427\n",
      "Epoch 156/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9518\n",
      "Epoch 157/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1599 - accuracy: 0.9364\n",
      "Epoch 158/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9360\n",
      "Epoch 159/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9356\n",
      "Epoch 160/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.9531\n",
      "Epoch 161/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9464\n",
      "Epoch 162/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9580\n",
      "Epoch 163/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9543\n",
      "Epoch 164/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9526\n",
      "Epoch 165/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9535\n",
      "Epoch 166/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9489\n",
      "Epoch 167/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9493\n",
      "Epoch 168/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9580\n",
      "Epoch 169/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.9477\n",
      "Epoch 170/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9510\n",
      "Epoch 171/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9551\n",
      "Epoch 172/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9618\n",
      "Epoch 173/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9514\n",
      "Epoch 174/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9547\n",
      "Epoch 175/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9585\n",
      "Epoch 176/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9593\n",
      "Epoch 177/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9576\n",
      "Epoch 178/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9435\n",
      "Epoch 179/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9564\n",
      "Epoch 180/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9518\n",
      "Epoch 181/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9506\n",
      "Epoch 182/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9560\n",
      "Epoch 183/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9572\n",
      "Epoch 184/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9506\n",
      "Epoch 185/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9526\n",
      "Epoch 186/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9568\n",
      "Epoch 187/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9614\n",
      "Epoch 188/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9522\n",
      "Epoch 189/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9531\n",
      "Epoch 190/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9489\n",
      "Epoch 191/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9605\n",
      "Epoch 192/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9547\n",
      "Epoch 193/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9481\n",
      "Epoch 194/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9564\n",
      "Epoch 195/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9485\n",
      "Epoch 196/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9535\n",
      "Epoch 197/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9526\n",
      "Epoch 198/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9580\n",
      "Epoch 199/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9547\n",
      "Epoch 200/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9580\n",
      "Epoch 201/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9539\n",
      "Epoch 202/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9564\n",
      "Epoch 203/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9589\n",
      "Epoch 204/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9585\n",
      "Epoch 205/1500\n",
      "45/76 [================>.............] - ETA: 0s - loss: 0.1194 - accuracy: 0.9528Restoring model weights from the end of the best epoch: 175.\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9564\n",
      "Epoch 205: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6199 - accuracy: 0.7613\n",
      "5/5 [==============================] - 0s 697us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (22/28)\n",
      "Before appending - Cat IDs: 714, Predictions: 714, Actuals: 714, Gender: 714\n",
      "After appending - Cat IDs: 869, Predictions: 869, Actuals: 869, Gender: 869\n",
      "Final Test Results - Loss: 0.6198680400848389, Accuracy: 0.7612903118133545, Precision: 0.6788389513108615, Recall: 0.6961712616885031, F1 Score: 0.6745791245791245\n",
      "Confusion Matrix:\n",
      " [[70  3 14]\n",
      " [13 41  1]\n",
      " [ 6  0  7]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6748257266949967\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.9650101959705353\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7142569869756699\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6977118390872173\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6863386244445604\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[0]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79e793-c694-4bc7-8cc6-05e124ddf71f",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c3c239d-6215-4589-a583-b37a18ca22cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 869, Predictions: 869, Actuals: 869, Gender: 869\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99978976-e4a9-4c04-a6b2-6b14a3111277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "189b812b-d37f-462f-a99f-6de8e4a7899f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.72 (79/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "907d957f-7340-4a76-b3fa-7cf166f43049",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7016fa20-094f-409a-9a2f-b6d6793aed03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, senior, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, adult, kitten, adult, kitten, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, kitten, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, senior, kitten, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "65    059A  [adult, senior, senior, senior, senior, senior...        senior           senior                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, senior, ad...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A              [adult, adult, senior, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "63    057A  [senior, adult, adult, senior, senior, senior,...        senior           senior                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "59    053A        [adult, adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "102   108A     [senior, adult, adult, senior, senior, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                    [senior, adult, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "50    044A           [kitten, senior, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "25    023A        [adult, kitten, adult, adult, adult, adult]         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A       [adult, senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, senior, senior, s...         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "40    034A              [adult, senior, adult, senior, adult]         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "43    037A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, adult, s...         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "2     002A  [adult, adult, senior, adult, senior, adult, a...         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                            [senior, senior, adult]        senior            adult                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "89    094A  [senior, senior, adult, adult, adult, senior, ...         adult           senior                  False\n",
       "101   106A  [adult, senior, adult, senior, adult, adult, a...         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "4     003A                    [senior, senior, adult, senior]        senior            adult                  False\n",
       "90    095A  [senior, adult, adult, senior, senior, senior,...        senior            adult                  False\n",
       "48    042A  [adult, adult, kitten, adult, kitten, adult, k...         adult           kitten                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "52    047A  [adult, adult, adult, adult, adult, kitten, ki...         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "42    036A  [senior, senior, senior, senior, adult, senior...        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "34    027A  [adult, senior, senior, senior, senior, adult,...        senior            adult                  False\n",
       "69    063A  [adult, senior, senior, kitten, senior, senior...        senior            adult                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "74    068A  [senior, senior, senior, senior, adult, adult,...        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "21    019B                                           [senior]        senior            adult                  False\n",
       "19    018A                                   [senior, senior]        senior            adult                  False\n",
       "109   117A  [senior, senior, adult, adult, adult, adult, a...         adult           senior                  False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "994531fd-e6fb-4491-9eab-86e41ac1ed3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     57\n",
      "kitten    10\n",
      "senior    12\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "229af3e2-6a64-4f3d-a594-e04108fce87f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             57  78.082192\n",
      "1           kitten           15             10  66.666667\n",
      "2           senior           22             12  54.545455\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3960450a-db52-4464-a965-00f1e600bda5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbf768-64c2-48ec-80e3-3a961b0b12a6",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe16003e-4015-4d28-ae37-ca0c94952758",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          555            422  76.036036\n",
      "1           kitten          136             89  65.441176\n",
      "2           senior          178            112  62.921348\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b5b8766-b4bd-4dd2-92e2-6513741b03ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efa447-1e32-42df-8664-a3f1f0e0f812",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5eecd6a6-4094-4747-8029-f795a87f8ff0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      177     83.10\n",
      "1          M    337      232     68.84\n",
      "2          X    319      214     67.08\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "115592e9-47eb-42fe-8cb5-26beeb7328ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae066af0-659b-43f0-924c-2c50be0e6c40",
   "metadata": {},
   "source": [
    "# RANDOM SEED 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc76296a-4029-447e-b12c-41520160251a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[1]))\n",
    "np.random.seed(int(random_seeds[1]))\n",
    "tf.random.set_seed(int(random_seeds[1]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "093977a2-1813-4a02-9573-f757b3d2a567",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c89dbfa-4c9b-4e46-a0d9-f659db30532f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a62fdf-86b5-45e7-9249-73a55985936a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee1b36c7-4a91-4071-a713-c904da913c3b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "106A    14\n",
      "097B    14\n",
      "001A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "005A    10\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "022A     9\n",
      "051B     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "033A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "027A     7\n",
      "117A     7\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "053A     6\n",
      "008A     6\n",
      "023A     6\n",
      "037A     6\n",
      "007A     6\n",
      "034A     5\n",
      "025C     5\n",
      "070A     5\n",
      "023B     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "062A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "052A     4\n",
      "009A     4\n",
      "060A     3\n",
      "012A     3\n",
      "006A     3\n",
      "064A     3\n",
      "058A     3\n",
      "054A     2\n",
      "061A     2\n",
      "087A     2\n",
      "069A     2\n",
      "032A     2\n",
      "011A     2\n",
      "018A     2\n",
      "025B     2\n",
      "093A     2\n",
      "088A     1\n",
      "091A     1\n",
      "100A     1\n",
      "090A     1\n",
      "019B     1\n",
      "115A     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "073A     1\n",
      "026C     1\n",
      "076A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "014B    10\n",
      "015A     9\n",
      "099A     7\n",
      "050A     7\n",
      "021A     5\n",
      "003A     4\n",
      "056A     3\n",
      "014A     3\n",
      "113A     3\n",
      "038A     2\n",
      "102A     2\n",
      "096A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    233\n",
      "M    226\n",
      "F    210\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    115\n",
      "M    111\n",
      "F     42\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 040A, 046A, 109A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 059A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 103A, 074A, 002B, 101A, 038A, 099A, 014...\n",
      "kitten                 [014B, 111A, 047A, 042A, 050A, 110A]\n",
      "senior                       [097A, 113A, 056A, 051A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "669\n",
      "Length of y_train_val:\n",
      "669\n",
      "Length of groups_train_val:\n",
      "669\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 856, 2: 715, 1: 490})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.9955 - accuracy: 0.5691\n",
      "Epoch 2/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.7540 - accuracy: 0.6710\n",
      "Epoch 3/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.7107 - accuracy: 0.7001\n",
      "Epoch 4/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.6485 - accuracy: 0.7312\n",
      "Epoch 5/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.6119 - accuracy: 0.7530\n",
      "Epoch 6/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5928 - accuracy: 0.7574\n",
      "Epoch 7/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5758 - accuracy: 0.7676\n",
      "Epoch 8/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5442 - accuracy: 0.7802\n",
      "Epoch 9/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5248 - accuracy: 0.7841\n",
      "Epoch 10/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5066 - accuracy: 0.7875\n",
      "Epoch 11/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4568 - accuracy: 0.8147\n",
      "Epoch 12/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4910 - accuracy: 0.8001\n",
      "Epoch 13/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4429 - accuracy: 0.8137\n",
      "Epoch 14/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4635 - accuracy: 0.8040\n",
      "Epoch 15/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4337 - accuracy: 0.8205\n",
      "Epoch 16/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4387 - accuracy: 0.8200\n",
      "Epoch 17/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4217 - accuracy: 0.8190\n",
      "Epoch 18/1500\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.4059 - accuracy: 0.8375\n",
      "Epoch 19/1500\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8409\n",
      "Epoch 20/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4041 - accuracy: 0.8321\n",
      "Epoch 21/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3941 - accuracy: 0.8355\n",
      "Epoch 22/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3737 - accuracy: 0.8443\n",
      "Epoch 23/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3704 - accuracy: 0.8428\n",
      "Epoch 24/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3719 - accuracy: 0.8535\n",
      "Epoch 25/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3646 - accuracy: 0.8462\n",
      "Epoch 26/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3458 - accuracy: 0.8593\n",
      "Epoch 27/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3706 - accuracy: 0.8535\n",
      "Epoch 28/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3479 - accuracy: 0.8554\n",
      "Epoch 29/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3389 - accuracy: 0.8627\n",
      "Epoch 30/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3401 - accuracy: 0.8646\n",
      "Epoch 31/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3360 - accuracy: 0.8646\n",
      "Epoch 32/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3177 - accuracy: 0.8748\n",
      "Epoch 33/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3252 - accuracy: 0.8661\n",
      "Epoch 34/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3046 - accuracy: 0.8753\n",
      "Epoch 35/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2998 - accuracy: 0.8816\n",
      "Epoch 36/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2906 - accuracy: 0.8836\n",
      "Epoch 37/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2856 - accuracy: 0.8899\n",
      "Epoch 38/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2845 - accuracy: 0.8884\n",
      "Epoch 39/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2854 - accuracy: 0.8874\n",
      "Epoch 40/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2953 - accuracy: 0.8855\n",
      "Epoch 41/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2995 - accuracy: 0.8772\n",
      "Epoch 42/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3019 - accuracy: 0.8743\n",
      "Epoch 43/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2805 - accuracy: 0.8884\n",
      "Epoch 44/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2790 - accuracy: 0.8884\n",
      "Epoch 45/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2910 - accuracy: 0.8777\n",
      "Epoch 46/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2533 - accuracy: 0.9020\n",
      "Epoch 47/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2689 - accuracy: 0.8952\n",
      "Epoch 48/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2675 - accuracy: 0.8850\n",
      "Epoch 49/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2751 - accuracy: 0.8971\n",
      "Epoch 50/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2733 - accuracy: 0.8923\n",
      "Epoch 51/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2546 - accuracy: 0.9030\n",
      "Epoch 52/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2663 - accuracy: 0.8918\n",
      "Epoch 53/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2531 - accuracy: 0.9010\n",
      "Epoch 54/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.9030\n",
      "Epoch 55/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2473 - accuracy: 0.9044\n",
      "Epoch 56/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2460 - accuracy: 0.9025\n",
      "Epoch 57/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2464 - accuracy: 0.9049\n",
      "Epoch 58/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2415 - accuracy: 0.9054\n",
      "Epoch 59/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2299 - accuracy: 0.9190\n",
      "Epoch 60/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2269 - accuracy: 0.9117\n",
      "Epoch 61/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2317 - accuracy: 0.9136\n",
      "Epoch 62/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.9117\n",
      "Epoch 63/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.9175\n",
      "Epoch 64/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2124 - accuracy: 0.9180\n",
      "Epoch 65/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2365 - accuracy: 0.9064\n",
      "Epoch 66/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2179 - accuracy: 0.9127\n",
      "Epoch 67/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2283 - accuracy: 0.9102\n",
      "Epoch 68/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2307 - accuracy: 0.9122\n",
      "Epoch 69/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2443 - accuracy: 0.8996\n",
      "Epoch 70/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2303 - accuracy: 0.9165\n",
      "Epoch 71/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9165\n",
      "Epoch 72/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2323 - accuracy: 0.9083\n",
      "Epoch 73/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2052 - accuracy: 0.9170\n",
      "Epoch 74/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2166 - accuracy: 0.9175\n",
      "Epoch 75/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2081 - accuracy: 0.9199\n",
      "Epoch 76/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2068 - accuracy: 0.9219\n",
      "Epoch 77/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1955 - accuracy: 0.9229\n",
      "Epoch 78/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9238\n",
      "Epoch 79/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2217 - accuracy: 0.9098\n",
      "Epoch 80/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2238 - accuracy: 0.9093\n",
      "Epoch 81/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2046 - accuracy: 0.9214\n",
      "Epoch 82/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9209\n",
      "Epoch 83/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9282\n",
      "Epoch 84/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9350\n",
      "Epoch 85/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9248\n",
      "Epoch 86/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9272\n",
      "Epoch 87/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1921 - accuracy: 0.9287\n",
      "Epoch 88/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9199\n",
      "Epoch 89/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9355\n",
      "Epoch 90/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2015 - accuracy: 0.9224\n",
      "Epoch 91/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2125 - accuracy: 0.9229\n",
      "Epoch 92/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2069 - accuracy: 0.9170\n",
      "Epoch 93/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9190\n",
      "Epoch 94/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9316\n",
      "Epoch 95/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2188 - accuracy: 0.9151\n",
      "Epoch 96/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9287\n",
      "Epoch 97/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1968 - accuracy: 0.9204\n",
      "Epoch 98/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.9316\n",
      "Epoch 99/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9364\n",
      "Epoch 100/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9379\n",
      "Epoch 101/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9360\n",
      "Epoch 102/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.9355\n",
      "Epoch 103/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9393\n",
      "Epoch 104/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9277\n",
      "Epoch 105/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9389\n",
      "Epoch 106/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9398\n",
      "Epoch 107/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9384\n",
      "Epoch 108/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9364\n",
      "Epoch 109/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1729 - accuracy: 0.9379\n",
      "Epoch 110/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1812 - accuracy: 0.9345\n",
      "Epoch 111/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9418\n",
      "Epoch 112/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.9403\n",
      "Epoch 113/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1900 - accuracy: 0.9248\n",
      "Epoch 114/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1602 - accuracy: 0.9413\n",
      "Epoch 115/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9277\n",
      "Epoch 116/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9374\n",
      "Epoch 117/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1631 - accuracy: 0.9423\n",
      "Epoch 118/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9379\n",
      "Epoch 119/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9369\n",
      "Epoch 120/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9393\n",
      "Epoch 121/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1590 - accuracy: 0.9393\n",
      "Epoch 122/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.9413\n",
      "Epoch 123/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9442\n",
      "Epoch 124/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9374\n",
      "Epoch 125/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9544\n",
      "Epoch 126/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9471\n",
      "Epoch 127/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1685 - accuracy: 0.9335\n",
      "Epoch 128/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9393\n",
      "Epoch 129/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1830 - accuracy: 0.9253\n",
      "Epoch 130/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.9355\n",
      "Epoch 131/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9529\n",
      "Epoch 132/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1810 - accuracy: 0.9335\n",
      "Epoch 133/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9457\n",
      "Epoch 134/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1566 - accuracy: 0.9476\n",
      "Epoch 135/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9447\n",
      "Epoch 136/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9452\n",
      "Epoch 137/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9393\n",
      "Epoch 138/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9466\n",
      "Epoch 139/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1494 - accuracy: 0.9432\n",
      "Epoch 140/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9345\n",
      "Epoch 141/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9471\n",
      "Epoch 142/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1632 - accuracy: 0.9408\n",
      "Epoch 143/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9563\n",
      "Epoch 144/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9447\n",
      "Epoch 145/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1493 - accuracy: 0.9476\n",
      "Epoch 146/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9491\n",
      "Epoch 147/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1429 - accuracy: 0.9486\n",
      "Epoch 148/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.9466\n",
      "Epoch 149/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9481\n",
      "Epoch 150/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9476\n",
      "Epoch 151/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9287\n",
      "Epoch 152/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9500\n",
      "Epoch 153/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9491\n",
      "Epoch 154/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9544\n",
      "Epoch 155/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1445 - accuracy: 0.9461\n",
      "Epoch 156/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1628 - accuracy: 0.9379\n",
      "Epoch 157/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.9520\n",
      "Epoch 158/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9447\n",
      "Epoch 159/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9452\n",
      "Epoch 160/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9500\n",
      "Epoch 161/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9476\n",
      "Epoch 162/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9486\n",
      "Epoch 163/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9563\n",
      "Epoch 164/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9432\n",
      "Epoch 165/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1446 - accuracy: 0.9481\n",
      "Epoch 166/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9617\n",
      "Epoch 167/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9466\n",
      "Epoch 168/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9583\n",
      "Epoch 169/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9534\n",
      "Epoch 170/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9486\n",
      "Epoch 171/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9573\n",
      "Epoch 172/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9607\n",
      "Epoch 173/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9554\n",
      "Epoch 174/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9466\n",
      "Epoch 175/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9525\n",
      "Epoch 176/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9568\n",
      "Epoch 177/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9505\n",
      "Epoch 178/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9592\n",
      "Epoch 179/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9578\n",
      "Epoch 180/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9554\n",
      "Epoch 181/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9558\n",
      "Epoch 182/1500\n",
      "65/65 [==============================] - 0s 3ms/step - loss: 0.1240 - accuracy: 0.9563\n",
      "Epoch 183/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9558\n",
      "Epoch 184/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9525\n",
      "Epoch 185/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9549\n",
      "Epoch 186/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9573\n",
      "Epoch 187/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1005 - accuracy: 0.9665\n",
      "Epoch 188/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9626\n",
      "Epoch 189/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9597\n",
      "Epoch 190/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9539\n",
      "Epoch 191/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9505\n",
      "Epoch 192/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9558\n",
      "Epoch 193/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9592\n",
      "Epoch 194/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9588\n",
      "Epoch 195/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9588\n",
      "Epoch 196/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9641\n",
      "Epoch 197/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9544\n",
      "Epoch 198/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9491\n",
      "Epoch 199/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9631\n",
      "Epoch 200/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9554\n",
      "Epoch 201/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9665\n",
      "Epoch 202/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9607\n",
      "Epoch 203/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9617\n",
      "Epoch 204/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9563\n",
      "Epoch 205/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9529\n",
      "Epoch 206/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.9646\n",
      "Epoch 207/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9549\n",
      "Epoch 208/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9622\n",
      "Epoch 209/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9515\n",
      "Epoch 210/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9631\n",
      "Epoch 211/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9626\n",
      "Epoch 212/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9554\n",
      "Epoch 213/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9656\n",
      "Epoch 214/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9699\n",
      "Epoch 215/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9680\n",
      "Epoch 216/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9626\n",
      "Epoch 217/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.9549\n",
      "Epoch 218/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9612\n",
      "Epoch 219/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9607\n",
      "Epoch 220/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9568\n",
      "Epoch 221/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9583\n",
      "Epoch 222/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9660\n",
      "Epoch 223/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9622\n",
      "Epoch 224/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9563\n",
      "Epoch 225/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9641\n",
      "Epoch 226/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9592\n",
      "Epoch 227/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9651\n",
      "Epoch 228/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9651\n",
      "Epoch 229/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9694\n",
      "Epoch 230/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9748\n",
      "Epoch 231/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9641\n",
      "Epoch 232/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9607\n",
      "Epoch 233/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1111 - accuracy: 0.9626\n",
      "Epoch 234/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9534\n",
      "Epoch 235/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9578\n",
      "Epoch 236/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9685\n",
      "Epoch 237/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9714\n",
      "Epoch 238/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9665\n",
      "Epoch 239/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9675\n",
      "Epoch 240/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9602\n",
      "Epoch 241/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9641\n",
      "Epoch 242/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9622\n",
      "Epoch 243/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9680\n",
      "Epoch 244/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.9670\n",
      "Epoch 245/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9631\n",
      "Epoch 246/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9607\n",
      "Epoch 247/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9592\n",
      "Epoch 248/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9597\n",
      "Epoch 249/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9675\n",
      "Epoch 250/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.9685\n",
      "Epoch 251/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9588\n",
      "Epoch 252/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9583\n",
      "Epoch 253/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9612\n",
      "Epoch 254/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9597\n",
      "Epoch 255/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9728\n",
      "Epoch 256/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9622\n",
      "Epoch 257/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9709\n",
      "Epoch 258/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9631\n",
      "Epoch 259/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9631\n",
      "Epoch 260/1500\n",
      "50/65 [======================>.......] - ETA: 0s - loss: 0.1065 - accuracy: 0.9619Restoring model weights from the end of the best epoch: 230.\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9636\n",
      "Epoch 260: early stopping\n",
      "9/9 [==============================] - 0s 801us/step - loss: 0.8831 - accuracy: 0.6716\n",
      "9/9 [==============================] - 0s 711us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (18/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Results - Loss: 0.883077085018158, Accuracy: 0.6716417670249939, Precision: 0.6854571142101905, Recall: 0.6628016960208741, F1 Score: 0.6332796996513811\n",
      "Confusion Matrix:\n",
      " [[116   2  42]\n",
      " [ 34  38   1]\n",
      " [  9   0  26]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "053A     6\n",
      "108A     6\n",
      "023A     6\n",
      "007A     6\n",
      "025C     5\n",
      "044A     5\n",
      "023B     5\n",
      "021A     5\n",
      "070A     5\n",
      "034A     5\n",
      "003A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "104A     4\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "113A     3\n",
      "056A     3\n",
      "018A     2\n",
      "038A     2\n",
      "025B     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "102A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "073A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "063A    11\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "095A     8\n",
      "037A     6\n",
      "008A     6\n",
      "109A     6\n",
      "075A     5\n",
      "105A     4\n",
      "064A     3\n",
      "060A     3\n",
      "093A     2\n",
      "087A     2\n",
      "011A     2\n",
      "061A     2\n",
      "076A     1\n",
      "043A     1\n",
      "091A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    238\n",
      "F    229\n",
      "X    221\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    127\n",
      "M     99\n",
      "F     23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 015A, 103A, 071A, 097B, 028A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 050A, 049...\n",
      "senior    [097A, 057A, 106A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 001A, 019A, 067A, 022A, 029A, 095A, 072...\n",
      "kitten                             [046A, 109A, 043A, 045A]\n",
      "senior                             [093A, 051B, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '044A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '043A'\n",
      " '045A' '046A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'040A'}\n",
      "Moved to Test Set:\n",
      "{'040A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '041A' '042A'\n",
      " '044A' '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '040A'\n",
      " '043A' '045A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "Length of X_train_val:\n",
      "741\n",
      "Length of y_train_val:\n",
      "741\n",
      "Length of groups_train_val:\n",
      "741\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     155\n",
      "kitten     79\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten    145\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     155\n",
      "kitten     26\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 866, 2: 815, 1: 725})\n",
      "Epoch 1/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.8862 - accuracy: 0.6189\n",
      "Epoch 2/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.6952 - accuracy: 0.7136\n",
      "Epoch 3/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.6374 - accuracy: 0.7398\n",
      "Epoch 4/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.6073 - accuracy: 0.7494\n",
      "Epoch 5/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5385 - accuracy: 0.7860\n",
      "Epoch 6/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5564 - accuracy: 0.7768\n",
      "Epoch 7/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5076 - accuracy: 0.8013\n",
      "Epoch 8/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4838 - accuracy: 0.8030\n",
      "Epoch 9/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4510 - accuracy: 0.8213\n",
      "Epoch 10/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4558 - accuracy: 0.8254\n",
      "Epoch 11/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4551 - accuracy: 0.8271\n",
      "Epoch 12/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4335 - accuracy: 0.8225\n",
      "Epoch 13/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4159 - accuracy: 0.8337\n",
      "Epoch 14/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4152 - accuracy: 0.8375\n",
      "Epoch 15/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3992 - accuracy: 0.8466\n",
      "Epoch 16/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4035 - accuracy: 0.8400\n",
      "Epoch 17/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3803 - accuracy: 0.8429\n",
      "Epoch 18/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3841 - accuracy: 0.8487\n",
      "Epoch 19/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3617 - accuracy: 0.8603\n",
      "Epoch 20/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3699 - accuracy: 0.8500\n",
      "Epoch 21/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3845 - accuracy: 0.8450\n",
      "Epoch 22/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3536 - accuracy: 0.8599\n",
      "Epoch 23/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3495 - accuracy: 0.8603\n",
      "Epoch 24/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3372 - accuracy: 0.8716\n",
      "Epoch 25/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3482 - accuracy: 0.8670\n",
      "Epoch 26/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3489 - accuracy: 0.8662\n",
      "Epoch 27/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3400 - accuracy: 0.8670\n",
      "Epoch 28/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3387 - accuracy: 0.8691\n",
      "Epoch 29/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8703\n",
      "Epoch 30/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3020 - accuracy: 0.8791\n",
      "Epoch 31/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3067 - accuracy: 0.8732\n",
      "Epoch 32/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2938 - accuracy: 0.8890\n",
      "Epoch 33/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3193 - accuracy: 0.8774\n",
      "Epoch 34/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2850 - accuracy: 0.8903\n",
      "Epoch 35/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3003 - accuracy: 0.8865\n",
      "Epoch 36/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2916 - accuracy: 0.8944\n",
      "Epoch 37/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3041 - accuracy: 0.8820\n",
      "Epoch 38/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2808 - accuracy: 0.8886\n",
      "Epoch 39/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2851 - accuracy: 0.8869\n",
      "Epoch 40/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2823 - accuracy: 0.8924\n",
      "Epoch 41/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.9002\n",
      "Epoch 42/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2596 - accuracy: 0.8998\n",
      "Epoch 43/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2706 - accuracy: 0.8878\n",
      "Epoch 44/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2628 - accuracy: 0.8982\n",
      "Epoch 45/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.9044\n",
      "Epoch 46/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.9144\n",
      "Epoch 47/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.9052\n",
      "Epoch 48/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2573 - accuracy: 0.8973\n",
      "Epoch 49/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2507 - accuracy: 0.9090\n",
      "Epoch 50/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2296 - accuracy: 0.9152\n",
      "Epoch 51/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2322 - accuracy: 0.9169\n",
      "Epoch 52/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2464 - accuracy: 0.9019\n",
      "Epoch 53/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2333 - accuracy: 0.9098\n",
      "Epoch 54/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2231 - accuracy: 0.9165\n",
      "Epoch 55/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2515 - accuracy: 0.9077\n",
      "Epoch 56/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.9102\n",
      "Epoch 57/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9048\n",
      "Epoch 58/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2284 - accuracy: 0.9115\n",
      "Epoch 59/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2248 - accuracy: 0.9090\n",
      "Epoch 60/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2112 - accuracy: 0.9194\n",
      "Epoch 61/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2228 - accuracy: 0.9198\n",
      "Epoch 62/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2347 - accuracy: 0.9152\n",
      "Epoch 63/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2222 - accuracy: 0.9181\n",
      "Epoch 64/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2249 - accuracy: 0.9214\n",
      "Epoch 65/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2157 - accuracy: 0.9169\n",
      "Epoch 66/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9181\n",
      "Epoch 67/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2098 - accuracy: 0.9185\n",
      "Epoch 68/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.9227\n",
      "Epoch 69/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2196 - accuracy: 0.9135\n",
      "Epoch 70/1500\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.1997 - accuracy: 0.9239\n",
      "Epoch 71/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9239\n",
      "Epoch 72/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1972 - accuracy: 0.9273\n",
      "Epoch 73/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9343\n",
      "Epoch 74/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1938 - accuracy: 0.9289\n",
      "Epoch 75/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.9327\n",
      "Epoch 76/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9260\n",
      "Epoch 77/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2032 - accuracy: 0.9235\n",
      "Epoch 78/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2114 - accuracy: 0.9239\n",
      "Epoch 79/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2178 - accuracy: 0.9181\n",
      "Epoch 80/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9219\n",
      "Epoch 81/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1929 - accuracy: 0.9227\n",
      "Epoch 82/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9335\n",
      "Epoch 83/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1824 - accuracy: 0.9281\n",
      "Epoch 84/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1954 - accuracy: 0.9331\n",
      "Epoch 85/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.9331\n",
      "Epoch 86/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1896 - accuracy: 0.9268\n",
      "Epoch 87/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9352\n",
      "Epoch 88/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.9352\n",
      "Epoch 89/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1795 - accuracy: 0.9364\n",
      "Epoch 90/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9343\n",
      "Epoch 91/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9393\n",
      "Epoch 92/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9347\n",
      "Epoch 93/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9339\n",
      "Epoch 94/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9318\n",
      "Epoch 95/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9426\n",
      "Epoch 96/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1872 - accuracy: 0.9293\n",
      "Epoch 97/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9460\n",
      "Epoch 98/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9418\n",
      "Epoch 99/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9431\n",
      "Epoch 100/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.9435\n",
      "Epoch 101/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1642 - accuracy: 0.9414\n",
      "Epoch 102/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9464\n",
      "Epoch 103/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.9281\n",
      "Epoch 104/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9489\n",
      "Epoch 105/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9401\n",
      "Epoch 106/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9472\n",
      "Epoch 107/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9406\n",
      "Epoch 108/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9418\n",
      "Epoch 109/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1678 - accuracy: 0.9406\n",
      "Epoch 110/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9431\n",
      "Epoch 111/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.9489\n",
      "Epoch 112/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9476\n",
      "Epoch 113/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1534 - accuracy: 0.9468\n",
      "Epoch 114/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9480\n",
      "Epoch 115/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1546 - accuracy: 0.9426\n",
      "Epoch 116/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9472\n",
      "Epoch 117/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9493\n",
      "Epoch 118/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9476\n",
      "Epoch 119/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9555\n",
      "Epoch 120/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9576\n",
      "Epoch 121/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.9472\n",
      "Epoch 122/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9468\n",
      "Epoch 123/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9584\n",
      "Epoch 124/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9489\n",
      "Epoch 125/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1404 - accuracy: 0.9476\n",
      "Epoch 126/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9518\n",
      "Epoch 127/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9431\n",
      "Epoch 128/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9451\n",
      "Epoch 129/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.9505\n",
      "Epoch 130/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9485\n",
      "Epoch 131/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9522\n",
      "Epoch 132/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9493\n",
      "Epoch 133/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9547\n",
      "Epoch 134/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9568\n",
      "Epoch 135/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9568\n",
      "Epoch 136/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9547\n",
      "Epoch 137/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9514\n",
      "Epoch 138/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9539\n",
      "Epoch 139/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9559\n",
      "Epoch 140/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9530\n",
      "Epoch 141/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9534\n",
      "Epoch 142/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9564\n",
      "Epoch 143/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9593\n",
      "Epoch 144/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9501\n",
      "Epoch 145/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9497\n",
      "Epoch 146/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9613\n",
      "Epoch 147/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9597\n",
      "Epoch 148/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9539\n",
      "Epoch 149/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9534\n",
      "Epoch 150/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9655\n",
      "Epoch 151/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9613\n",
      "Epoch 152/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9564\n",
      "Epoch 153/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9572\n",
      "Epoch 154/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9622\n",
      "Epoch 155/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9593\n",
      "Epoch 156/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9534\n",
      "Epoch 157/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9638\n",
      "Epoch 158/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9572\n",
      "Epoch 159/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9609\n",
      "Epoch 160/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9618\n",
      "Epoch 161/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9589\n",
      "Epoch 162/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9622\n",
      "Epoch 163/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9510\n",
      "Epoch 164/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9584\n",
      "Epoch 165/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9622\n",
      "Epoch 166/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9609\n",
      "Epoch 167/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9539\n",
      "Epoch 168/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9534\n",
      "Epoch 169/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9589\n",
      "Epoch 170/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.9514\n",
      "Epoch 171/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9564\n",
      "Epoch 172/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9676\n",
      "Epoch 173/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9663\n",
      "Epoch 174/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9634\n",
      "Epoch 175/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9622\n",
      "Epoch 176/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9605\n",
      "Epoch 177/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9534\n",
      "Epoch 178/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9539\n",
      "Epoch 179/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9605\n",
      "Epoch 180/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9630\n",
      "Epoch 181/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9630\n",
      "Epoch 182/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9638\n",
      "Epoch 183/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9643\n",
      "Epoch 184/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9601\n",
      "Epoch 185/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.9597\n",
      "Epoch 186/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9634\n",
      "Epoch 187/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9580\n",
      "Epoch 188/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9647\n",
      "Epoch 189/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9622\n",
      "Epoch 190/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9618\n",
      "Epoch 191/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9609\n",
      "Epoch 192/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9684\n",
      "Epoch 193/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9593\n",
      "Epoch 194/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9680\n",
      "Epoch 195/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9597\n",
      "Epoch 196/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9684\n",
      "Epoch 197/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9568\n",
      "Epoch 198/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9647\n",
      "Epoch 199/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9647\n",
      "Epoch 200/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9667\n",
      "Epoch 201/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9672\n",
      "Epoch 202/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9634\n",
      "Epoch 203/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9688\n",
      "Epoch 204/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.9667\n",
      "Epoch 205/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9643\n",
      "Epoch 206/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.9663\n",
      "Epoch 207/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9705\n",
      "Epoch 208/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9618\n",
      "Epoch 209/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9647\n",
      "Epoch 210/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9709\n",
      "Epoch 211/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9638\n",
      "Epoch 212/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9568\n",
      "Epoch 213/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9630\n",
      "Epoch 214/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9659\n",
      "Epoch 215/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9655\n",
      "Epoch 216/1500\n",
      "76/76 [==============================] - 0s 2ms/step - loss: 0.0865 - accuracy: 0.9713\n",
      "Epoch 217/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9609\n",
      "Epoch 218/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9605\n",
      "Epoch 219/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9643\n",
      "Epoch 220/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9647\n",
      "Epoch 221/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9647\n",
      "Epoch 222/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9663\n",
      "Epoch 223/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9647\n",
      "Epoch 224/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9630\n",
      "Epoch 225/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9688\n",
      "Epoch 226/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0994 - accuracy: 0.9655\n",
      "Epoch 227/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9717\n",
      "Epoch 228/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.9751\n",
      "Epoch 229/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9667\n",
      "Epoch 230/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9638\n",
      "Epoch 231/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9713\n",
      "Epoch 232/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.9746\n",
      "Epoch 233/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9634\n",
      "Epoch 234/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9730\n",
      "Epoch 235/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.9684\n",
      "Epoch 236/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.9705\n",
      "Epoch 237/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.9667\n",
      "Epoch 238/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0994 - accuracy: 0.9647\n",
      "Epoch 239/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9713\n",
      "Epoch 240/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9630\n",
      "Epoch 241/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9638\n",
      "Epoch 242/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9626\n",
      "Epoch 243/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9697\n",
      "Epoch 244/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.9659\n",
      "Epoch 245/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.9643\n",
      "Epoch 246/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9667\n",
      "Epoch 247/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9626\n",
      "Epoch 248/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.9655\n",
      "Epoch 249/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9734\n",
      "Epoch 250/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9643\n",
      "Epoch 251/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.9688\n",
      "Epoch 252/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9701\n",
      "Epoch 253/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0887 - accuracy: 0.9726\n",
      "Epoch 254/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9717\n",
      "Epoch 255/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9663\n",
      "Epoch 256/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9667\n",
      "Epoch 257/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9713\n",
      "Epoch 258/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9734\n",
      "Epoch 259/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9759\n",
      "Epoch 260/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.9738\n",
      "Epoch 261/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9647\n",
      "Epoch 262/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9697\n",
      "Epoch 263/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9651\n",
      "Epoch 264/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9593\n",
      "Epoch 265/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9722\n",
      "Epoch 266/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9759\n",
      "Epoch 267/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9759\n",
      "Epoch 268/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9759\n",
      "Epoch 269/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.9717\n",
      "Epoch 270/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9667\n",
      "Epoch 271/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9755\n",
      "Epoch 272/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.9726\n",
      "Epoch 273/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0660 - accuracy: 0.9771\n",
      "Epoch 274/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9697\n",
      "Epoch 275/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9705\n",
      "Epoch 276/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9734\n",
      "Epoch 277/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9705\n",
      "Epoch 278/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9742\n",
      "Epoch 279/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9755\n",
      "Epoch 280/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9701\n",
      "Epoch 281/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0801 - accuracy: 0.9755\n",
      "Epoch 282/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9618\n",
      "Epoch 283/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9697\n",
      "Epoch 284/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9684\n",
      "Epoch 285/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9742\n",
      "Epoch 286/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9697\n",
      "Epoch 287/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9701\n",
      "Epoch 288/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9717\n",
      "Epoch 289/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9742\n",
      "Epoch 290/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9742\n",
      "Epoch 291/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.9763\n",
      "Epoch 292/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.9755\n",
      "Epoch 293/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9742\n",
      "Epoch 294/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.9759\n",
      "Epoch 295/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9717\n",
      "Epoch 296/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9713\n",
      "Epoch 297/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9663\n",
      "Epoch 298/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9709\n",
      "Epoch 299/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9647\n",
      "Epoch 300/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0732 - accuracy: 0.9734\n",
      "Epoch 301/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9734\n",
      "Epoch 302/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9784\n",
      "Epoch 303/1500\n",
      "47/76 [=================>............] - ETA: 0s - loss: 0.0680 - accuracy: 0.9761Restoring model weights from the end of the best epoch: 273.\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.9751\n",
      "Epoch 303: early stopping\n",
      "7/7 [==============================] - 0s 832us/step - loss: 0.7299 - accuracy: 0.7551\n",
      "7/7 [==============================] - 0s 595us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (21/29)\n",
      "Before appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "After appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "Final Test Results - Loss: 0.7299075126647949, Accuracy: 0.7551020383834839, Precision: 0.5944346753981729, Recall: 0.6734491315136476, F1 Score: 0.617607862735299\n",
      "Confusion Matrix:\n",
      " [[120  10  25]\n",
      " [  4  22   0]\n",
      " [  9   0   6]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "042A    14\n",
      "097B    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "095A     8\n",
      "117A     7\n",
      "050A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "008A     6\n",
      "109A     6\n",
      "053A     6\n",
      "037A     6\n",
      "108A     6\n",
      "023A     6\n",
      "023B     5\n",
      "075A     5\n",
      "021A     5\n",
      "105A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "003A     4\n",
      "056A     3\n",
      "113A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "012A     3\n",
      "058A     3\n",
      "061A     2\n",
      "011A     2\n",
      "102A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "069A     2\n",
      "041A     1\n",
      "019B     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "096A     1\n",
      "076A     1\n",
      "088A     1\n",
      "092A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "073A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "000B    19\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "025A    11\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "010A     8\n",
      "013B     8\n",
      "007A     6\n",
      "070A     5\n",
      "044A     5\n",
      "034A     5\n",
      "025C     5\n",
      "104A     4\n",
      "009A     4\n",
      "006A     3\n",
      "018A     2\n",
      "054A     2\n",
      "025B     2\n",
      "032A     2\n",
      "049A     1\n",
      "026C     1\n",
      "066A     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    322\n",
      "M    241\n",
      "F    159\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "F    93\n",
      "X    26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 001A, 103A, 097B, 019A, 074...\n",
      "kitten    [014B, 111A, 040A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 057A, 055A, 113A, 051B, 117A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 071A, 028A, 020A, 034A, 005A, 002A, 009...\n",
      "kitten                                   [044A, 049A, 115A]\n",
      "senior           [106A, 104A, 059A, 116A, 054A, 016A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 13, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 3, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "722\n",
      "Length of y_train_val:\n",
      "722\n",
      "Length of groups_train_val:\n",
      "722\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 874, 1: 820, 2: 605})\n",
      "Epoch 1/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.8936 - accuracy: 0.6233\n",
      "Epoch 2/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.7009 - accuracy: 0.7190\n",
      "Epoch 3/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.6234 - accuracy: 0.7508\n",
      "Epoch 4/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5686 - accuracy: 0.7673\n",
      "Epoch 5/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5401 - accuracy: 0.7829\n",
      "Epoch 6/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4979 - accuracy: 0.7977\n",
      "Epoch 7/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4876 - accuracy: 0.7938\n",
      "Epoch 8/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4721 - accuracy: 0.8095\n",
      "Epoch 9/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4726 - accuracy: 0.8108\n",
      "Epoch 10/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4346 - accuracy: 0.8173\n",
      "Epoch 11/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4353 - accuracy: 0.8338\n",
      "Epoch 12/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4130 - accuracy: 0.8334\n",
      "Epoch 13/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3908 - accuracy: 0.8417\n",
      "Epoch 14/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3923 - accuracy: 0.8430\n",
      "Epoch 15/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3829 - accuracy: 0.8525\n",
      "Epoch 16/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3601 - accuracy: 0.8512\n",
      "Epoch 17/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3595 - accuracy: 0.8525\n",
      "Epoch 18/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3438 - accuracy: 0.8560\n",
      "Epoch 19/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3727 - accuracy: 0.8456\n",
      "Epoch 20/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3390 - accuracy: 0.8595\n",
      "Epoch 21/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.8669\n",
      "Epoch 22/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3206 - accuracy: 0.8721\n",
      "Epoch 23/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3356 - accuracy: 0.8669\n",
      "Epoch 24/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3032 - accuracy: 0.8826\n",
      "Epoch 25/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3063 - accuracy: 0.8808\n",
      "Epoch 26/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3097 - accuracy: 0.8773\n",
      "Epoch 27/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2801 - accuracy: 0.8895\n",
      "Epoch 28/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2996 - accuracy: 0.8760\n",
      "Epoch 29/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2894 - accuracy: 0.8826\n",
      "Epoch 30/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2764 - accuracy: 0.8926\n",
      "Epoch 31/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2941 - accuracy: 0.8873\n",
      "Epoch 32/1500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.2820 - accuracy: 0.8856\n",
      "Epoch 33/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2634 - accuracy: 0.8947\n",
      "Epoch 34/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2745 - accuracy: 0.8926\n",
      "Epoch 35/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2687 - accuracy: 0.8904\n",
      "Epoch 36/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2477 - accuracy: 0.9074\n",
      "Epoch 37/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2493 - accuracy: 0.9065\n",
      "Epoch 38/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2666 - accuracy: 0.8921\n",
      "Epoch 39/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2652 - accuracy: 0.8939\n",
      "Epoch 40/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2557 - accuracy: 0.9004\n",
      "Epoch 41/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2487 - accuracy: 0.9091\n",
      "Epoch 42/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2543 - accuracy: 0.9017\n",
      "Epoch 43/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9152\n",
      "Epoch 44/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2323 - accuracy: 0.9091\n",
      "Epoch 45/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2264 - accuracy: 0.9187\n",
      "Epoch 46/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2375 - accuracy: 0.9121\n",
      "Epoch 47/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2405 - accuracy: 0.9095\n",
      "Epoch 48/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2340 - accuracy: 0.9074\n",
      "Epoch 49/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2143 - accuracy: 0.9152\n",
      "Epoch 50/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2165 - accuracy: 0.9165\n",
      "Epoch 51/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9087\n",
      "Epoch 52/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2068 - accuracy: 0.9243\n",
      "Epoch 53/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2164 - accuracy: 0.9152\n",
      "Epoch 54/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2334 - accuracy: 0.9117\n",
      "Epoch 55/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2175 - accuracy: 0.9165\n",
      "Epoch 56/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9269\n",
      "Epoch 57/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2064 - accuracy: 0.9221\n",
      "Epoch 58/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1930 - accuracy: 0.9313\n",
      "Epoch 59/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9304\n",
      "Epoch 60/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9217\n",
      "Epoch 61/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9239\n",
      "Epoch 62/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9200\n",
      "Epoch 63/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1950 - accuracy: 0.9313\n",
      "Epoch 64/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1870 - accuracy: 0.9352\n",
      "Epoch 65/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2111 - accuracy: 0.9182\n",
      "Epoch 66/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9247\n",
      "Epoch 67/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.9295\n",
      "Epoch 68/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.9343\n",
      "Epoch 69/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.9348\n",
      "Epoch 70/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.9330\n",
      "Epoch 71/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9287\n",
      "Epoch 72/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9352\n",
      "Epoch 73/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.9361\n",
      "Epoch 74/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9404\n",
      "Epoch 75/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9343\n",
      "Epoch 76/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1778 - accuracy: 0.9365\n",
      "Epoch 77/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.9317\n",
      "Epoch 78/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9421\n",
      "Epoch 79/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.9321\n",
      "Epoch 80/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9361\n",
      "Epoch 81/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1618 - accuracy: 0.9421\n",
      "Epoch 82/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9400\n",
      "Epoch 83/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9443\n",
      "Epoch 84/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9330\n",
      "Epoch 85/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9343\n",
      "Epoch 86/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1602 - accuracy: 0.9435\n",
      "Epoch 87/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9469\n",
      "Epoch 88/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.9439\n",
      "Epoch 89/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9408\n",
      "Epoch 90/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9421\n",
      "Epoch 91/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9435\n",
      "Epoch 92/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9426\n",
      "Epoch 93/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9435\n",
      "Epoch 94/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9474\n",
      "Epoch 95/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9439\n",
      "Epoch 96/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9408\n",
      "Epoch 97/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9430\n",
      "Epoch 98/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9517\n",
      "Epoch 99/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.9439\n",
      "Epoch 100/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1685 - accuracy: 0.9391\n",
      "Epoch 101/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1537 - accuracy: 0.9408\n",
      "Epoch 102/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9517\n",
      "Epoch 103/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9504\n",
      "Epoch 104/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9478\n",
      "Epoch 105/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1518 - accuracy: 0.9452\n",
      "Epoch 106/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9469\n",
      "Epoch 107/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9478\n",
      "Epoch 108/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9448\n",
      "Epoch 109/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1414 - accuracy: 0.9487\n",
      "Epoch 110/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1403 - accuracy: 0.9513\n",
      "Epoch 111/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9517\n",
      "Epoch 112/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.9495\n",
      "Epoch 113/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9548\n",
      "Epoch 114/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9539\n",
      "Epoch 115/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9448\n",
      "Epoch 116/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9491\n",
      "Epoch 117/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.9504\n",
      "Epoch 118/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9561\n",
      "Epoch 119/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9556\n",
      "Epoch 120/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9504\n",
      "Epoch 121/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9626\n",
      "Epoch 122/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9548\n",
      "Epoch 123/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9535\n",
      "Epoch 124/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9491\n",
      "Epoch 125/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9552\n",
      "Epoch 126/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9508\n",
      "Epoch 127/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9487\n",
      "Epoch 128/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9561\n",
      "Epoch 129/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9461\n",
      "Epoch 130/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9569\n",
      "Epoch 131/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.9469\n",
      "Epoch 132/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9530\n",
      "Epoch 133/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9495\n",
      "Epoch 134/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9561\n",
      "Epoch 135/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9669\n",
      "Epoch 136/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9508\n",
      "Epoch 137/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9582\n",
      "Epoch 138/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9595\n",
      "Epoch 139/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.9600\n",
      "Epoch 140/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9543\n",
      "Epoch 141/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9591\n",
      "Epoch 142/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9578\n",
      "Epoch 143/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1330 - accuracy: 0.9522\n",
      "Epoch 144/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1007 - accuracy: 0.9622\n",
      "Epoch 145/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9578\n",
      "Epoch 146/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9600\n",
      "Epoch 147/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9635\n",
      "Epoch 148/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9648\n",
      "Epoch 149/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9569\n",
      "Epoch 150/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9561\n",
      "Epoch 151/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9587\n",
      "Epoch 152/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9613\n",
      "Epoch 153/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9669\n",
      "Epoch 154/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9600\n",
      "Epoch 155/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9504\n",
      "Epoch 156/1500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.1131 - accuracy: 0.9622\n",
      "Epoch 157/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9517\n",
      "Epoch 158/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9517\n",
      "Epoch 159/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9517\n",
      "Epoch 160/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9517\n",
      "Epoch 161/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9626\n",
      "Epoch 162/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9595\n",
      "Epoch 163/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9635\n",
      "Epoch 164/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9617\n",
      "Epoch 165/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.9678\n",
      "Epoch 166/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9604\n",
      "Epoch 167/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9626\n",
      "Epoch 168/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9691\n",
      "Epoch 169/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9595\n",
      "Epoch 170/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9691\n",
      "Epoch 171/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9648\n",
      "Epoch 172/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9661\n",
      "Epoch 173/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.9661\n",
      "Epoch 174/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9556\n",
      "Epoch 175/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9665\n",
      "Epoch 176/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9535\n",
      "Epoch 177/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9648\n",
      "Epoch 178/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9678\n",
      "Epoch 179/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9604\n",
      "Epoch 180/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9639\n",
      "Epoch 181/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9582\n",
      "Epoch 182/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9665\n",
      "Epoch 183/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9674\n",
      "Epoch 184/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9656\n",
      "Epoch 185/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9626\n",
      "Epoch 186/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9665\n",
      "Epoch 187/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9609\n",
      "Epoch 188/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1111 - accuracy: 0.9578\n",
      "Epoch 189/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9591\n",
      "Epoch 190/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9735\n",
      "Epoch 191/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.9687\n",
      "Epoch 192/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9756\n",
      "Epoch 193/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9704\n",
      "Epoch 194/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9691\n",
      "Epoch 195/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9687\n",
      "Epoch 196/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9661\n",
      "Epoch 197/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9687\n",
      "Epoch 198/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9639\n",
      "Epoch 199/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.9600\n",
      "Epoch 200/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9678\n",
      "Epoch 201/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9639\n",
      "Epoch 202/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9678\n",
      "Epoch 203/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9648\n",
      "Epoch 204/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9587\n",
      "Epoch 205/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.9752\n",
      "Epoch 206/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9669\n",
      "Epoch 207/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0884 - accuracy: 0.9700\n",
      "Epoch 208/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9678\n",
      "Epoch 209/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9669\n",
      "Epoch 210/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.9709\n",
      "Epoch 211/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9700\n",
      "Epoch 212/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9696\n",
      "Epoch 213/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9669\n",
      "Epoch 214/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9661\n",
      "Epoch 215/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9726\n",
      "Epoch 216/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9752\n",
      "Epoch 217/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9713\n",
      "Epoch 218/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9678\n",
      "Epoch 219/1500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0770 - accuracy: 0.9717\n",
      "Epoch 220/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9704\n",
      "Epoch 221/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9678\n",
      "Epoch 222/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9678\n",
      "Epoch 223/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9635\n",
      "Epoch 224/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9604\n",
      "Epoch 225/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.9756\n",
      "Epoch 226/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9791\n",
      "Epoch 227/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9713\n",
      "Epoch 228/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.9687\n",
      "Epoch 229/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9639\n",
      "Epoch 230/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9635\n",
      "Epoch 231/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9739\n",
      "Epoch 232/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9682\n",
      "Epoch 233/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.9696\n",
      "Epoch 234/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.9735\n",
      "Epoch 235/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.9748\n",
      "Epoch 236/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.9717\n",
      "Epoch 237/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9630\n",
      "Epoch 238/1500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0795 - accuracy: 0.9726\n",
      "Epoch 239/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9765\n",
      "Epoch 240/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9691\n",
      "Epoch 241/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9735\n",
      "Epoch 242/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9704\n",
      "Epoch 243/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9756\n",
      "Epoch 244/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.9761\n",
      "Epoch 245/1500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.9730\n",
      "Epoch 246/1500\n",
      "72/72 [==============================] - 0s 3ms/step - loss: 0.0946 - accuracy: 0.9696\n",
      "Epoch 247/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9739\n",
      "Epoch 248/1500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.1008 - accuracy: 0.9669\n",
      "Epoch 249/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9730\n",
      "Epoch 250/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0766 - accuracy: 0.9735\n",
      "Epoch 251/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9726\n",
      "Epoch 252/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.9726\n",
      "Epoch 253/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9722\n",
      "Epoch 254/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0884 - accuracy: 0.9687\n",
      "Epoch 255/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9791\n",
      "Epoch 256/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.9809\n",
      "Epoch 257/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9730\n",
      "Epoch 258/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9726\n",
      "Epoch 259/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9791\n",
      "Epoch 260/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.9752\n",
      "Epoch 261/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.9817\n",
      "Epoch 262/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9730\n",
      "Epoch 263/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0719 - accuracy: 0.9774\n",
      "Epoch 264/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9756\n",
      "Epoch 265/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9735\n",
      "Epoch 266/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9709\n",
      "Epoch 267/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9739\n",
      "Epoch 268/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9669\n",
      "Epoch 269/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.9774\n",
      "Epoch 270/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.9761\n",
      "Epoch 271/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9787\n",
      "Epoch 272/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.9769\n",
      "Epoch 273/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9774\n",
      "Epoch 274/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.9769\n",
      "Epoch 275/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.9761\n",
      "Epoch 276/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.9796\n",
      "Epoch 277/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9735\n",
      "Epoch 278/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9709\n",
      "Epoch 279/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9783\n",
      "Epoch 280/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0570 - accuracy: 0.9843\n",
      "Epoch 281/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9774\n",
      "Epoch 282/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9700\n",
      "Epoch 283/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9730\n",
      "Epoch 284/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.9778\n",
      "Epoch 285/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0603 - accuracy: 0.9791\n",
      "Epoch 286/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.9765\n",
      "Epoch 287/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9748\n",
      "Epoch 288/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0710 - accuracy: 0.9717\n",
      "Epoch 289/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.9774\n",
      "Epoch 290/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9752\n",
      "Epoch 291/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9756\n",
      "Epoch 292/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9739\n",
      "Epoch 293/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.9765\n",
      "Epoch 294/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9726\n",
      "Epoch 295/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9722\n",
      "Epoch 296/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0781 - accuracy: 0.9730\n",
      "Epoch 297/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.9752\n",
      "Epoch 298/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.9791\n",
      "Epoch 299/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0508 - accuracy: 0.9835\n",
      "Epoch 300/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9769\n",
      "Epoch 301/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.9791\n",
      "Epoch 302/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.9843\n",
      "Epoch 303/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.9765\n",
      "Epoch 304/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.9756\n",
      "Epoch 305/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9796\n",
      "Epoch 306/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9691\n",
      "Epoch 307/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0609 - accuracy: 0.9787\n",
      "Epoch 308/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0622 - accuracy: 0.9761\n",
      "Epoch 309/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9787\n",
      "Epoch 310/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.9787\n",
      "Epoch 311/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.9774\n",
      "Epoch 312/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.9804\n",
      "Epoch 313/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0595 - accuracy: 0.9774\n",
      "Epoch 314/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.9787\n",
      "Epoch 315/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0779 - accuracy: 0.9704\n",
      "Epoch 316/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.9704\n",
      "Epoch 317/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9726\n",
      "Epoch 318/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.9791\n",
      "Epoch 319/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0585 - accuracy: 0.9796\n",
      "Epoch 320/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.9735\n",
      "Epoch 321/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9730\n",
      "Epoch 322/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9722\n",
      "Epoch 323/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.9817\n",
      "Epoch 324/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9761\n",
      "Epoch 325/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9783\n",
      "Epoch 326/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9743\n",
      "Epoch 327/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9704\n",
      "Epoch 328/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9726\n",
      "Epoch 329/1500\n",
      "48/72 [===================>..........] - ETA: 0s - loss: 0.0614 - accuracy: 0.9792Restoring model weights from the end of the best epoch: 299.\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0552 - accuracy: 0.9822\n",
      "Epoch 329: early stopping\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 1.1300 - accuracy: 0.7767\n",
      "7/7 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (24/30)\n",
      "Before appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "After appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "Final Test Results - Loss: 1.1300290822982788, Accuracy: 0.7767441868782043, Precision: 0.7678571428571429, Recall: 0.788466751868634, F1 Score: 0.7743921666251762\n",
      "Confusion Matrix:\n",
      " [[132   1  18]\n",
      " [  0   7   0]\n",
      " [ 29   0  28]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "001A    14\n",
      "042A    14\n",
      "059A    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "025A    11\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "005A    10\n",
      "016A    10\n",
      "014B    10\n",
      "071A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "015A     9\n",
      "045A     9\n",
      "022A     9\n",
      "065A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "050A     7\n",
      "099A     7\n",
      "109A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "070A     5\n",
      "021A     5\n",
      "075A     5\n",
      "009A     4\n",
      "104A     4\n",
      "105A     4\n",
      "003A     4\n",
      "113A     3\n",
      "056A     3\n",
      "064A     3\n",
      "060A     3\n",
      "014A     3\n",
      "006A     3\n",
      "025B     2\n",
      "102A     2\n",
      "011A     2\n",
      "061A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "018A     2\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "043A     1\n",
      "115A     1\n",
      "076A     1\n",
      "066A     1\n",
      "026C     1\n",
      "096A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "057A    27\n",
      "055A    20\n",
      "097B    14\n",
      "039A    12\n",
      "040A    10\n",
      "094A     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "023A     6\n",
      "053A     6\n",
      "108A     6\n",
      "023B     5\n",
      "026A     4\n",
      "062A     4\n",
      "052A     4\n",
      "035A     4\n",
      "058A     3\n",
      "012A     3\n",
      "069A     2\n",
      "048A     1\n",
      "088A     1\n",
      "004A     1\n",
      "073A     1\n",
      "041A     1\n",
      "092A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    306\n",
      "X    268\n",
      "F    158\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    94\n",
      "X    80\n",
      "M    31\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 106A, 104A, 059A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 097B, 062A, 039A, 023A, 027A, 069A, 026...\n",
      "kitten                                   [040A, 041A, 048A]\n",
      "senior                 [057A, 055A, 117A, 058A, 094A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 13, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 3, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '020A'\n",
      " '021A' '022A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '012A' '019B' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'022A'}\n",
      "Moved to Test Set:\n",
      "{'022A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A'\n",
      " '009A' '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A'\n",
      " '020A' '021A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '012A' '019B' '022A' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "Length of X_train_val:\n",
      "762\n",
      "Length of y_train_val:\n",
      "762\n",
      "Length of groups_train_val:\n",
      "762\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     466\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     122\n",
      "senior     71\n",
      "kitten     12\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     496\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     92\n",
      "senior    71\n",
      "kitten    12\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 992, 1: 795, 2: 535})\n",
      "Epoch 1/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.9237 - accuracy: 0.6133\n",
      "Epoch 2/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.7047 - accuracy: 0.7222\n",
      "Epoch 3/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6337 - accuracy: 0.7575\n",
      "Epoch 4/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5910 - accuracy: 0.7670\n",
      "Epoch 5/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5130 - accuracy: 0.7860\n",
      "Epoch 6/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5273 - accuracy: 0.7812\n",
      "Epoch 7/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4925 - accuracy: 0.8006\n",
      "Epoch 8/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4838 - accuracy: 0.7972\n",
      "Epoch 9/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4681 - accuracy: 0.8187\n",
      "Epoch 10/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4416 - accuracy: 0.8239\n",
      "Epoch 11/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4170 - accuracy: 0.8368\n",
      "Epoch 12/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4176 - accuracy: 0.8320\n",
      "Epoch 13/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4124 - accuracy: 0.8355\n",
      "Epoch 14/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3703 - accuracy: 0.8419\n",
      "Epoch 15/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3916 - accuracy: 0.8463\n",
      "Epoch 16/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3571 - accuracy: 0.8527\n",
      "Epoch 17/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3601 - accuracy: 0.8454\n",
      "Epoch 18/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3539 - accuracy: 0.8587\n",
      "Epoch 19/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3441 - accuracy: 0.8497\n",
      "Epoch 20/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3402 - accuracy: 0.8592\n",
      "Epoch 21/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3191 - accuracy: 0.8699\n",
      "Epoch 22/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3408 - accuracy: 0.8587\n",
      "Epoch 23/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3315 - accuracy: 0.8665\n",
      "Epoch 24/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3249 - accuracy: 0.8691\n",
      "Epoch 25/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3124 - accuracy: 0.8742\n",
      "Epoch 26/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3077 - accuracy: 0.8738\n",
      "Epoch 27/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3250 - accuracy: 0.8682\n",
      "Epoch 28/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2954 - accuracy: 0.8790\n",
      "Epoch 29/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2943 - accuracy: 0.8773\n",
      "Epoch 30/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2938 - accuracy: 0.8820\n",
      "Epoch 31/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2812 - accuracy: 0.8850\n",
      "Epoch 32/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2849 - accuracy: 0.8798\n",
      "Epoch 33/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2793 - accuracy: 0.8936\n",
      "Epoch 34/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2842 - accuracy: 0.8898\n",
      "Epoch 35/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2783 - accuracy: 0.8850\n",
      "Epoch 36/1500\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.2757 - accuracy: 0.8893\n",
      "Epoch 37/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2757 - accuracy: 0.8984\n",
      "Epoch 38/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2866 - accuracy: 0.8850\n",
      "Epoch 39/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2550 - accuracy: 0.8997\n",
      "Epoch 40/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2403 - accuracy: 0.9009\n",
      "Epoch 41/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2334 - accuracy: 0.9078\n",
      "Epoch 42/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2611 - accuracy: 0.8932\n",
      "Epoch 43/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2446 - accuracy: 0.9014\n",
      "Epoch 44/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2644 - accuracy: 0.8958\n",
      "Epoch 45/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.9040\n",
      "Epoch 46/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2701 - accuracy: 0.8902\n",
      "Epoch 47/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2524 - accuracy: 0.9022\n",
      "Epoch 48/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.8992\n",
      "Epoch 49/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2335 - accuracy: 0.9091\n",
      "Epoch 50/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2651 - accuracy: 0.8936\n",
      "Epoch 51/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2275 - accuracy: 0.9152\n",
      "Epoch 52/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2214 - accuracy: 0.9152\n",
      "Epoch 53/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2495 - accuracy: 0.9027\n",
      "Epoch 54/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2272 - accuracy: 0.9100\n",
      "Epoch 55/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2230 - accuracy: 0.9031\n",
      "Epoch 56/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2188 - accuracy: 0.9109\n",
      "Epoch 57/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2178 - accuracy: 0.9160\n",
      "Epoch 58/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9225\n",
      "Epoch 59/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2181 - accuracy: 0.9143\n",
      "Epoch 60/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2119 - accuracy: 0.9147\n",
      "Epoch 61/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9251\n",
      "Epoch 62/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2118 - accuracy: 0.9199\n",
      "Epoch 63/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9229\n",
      "Epoch 64/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2068 - accuracy: 0.9208\n",
      "Epoch 65/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1896 - accuracy: 0.9208\n",
      "Epoch 66/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9186\n",
      "Epoch 67/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9298\n",
      "Epoch 68/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9289\n",
      "Epoch 69/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2028 - accuracy: 0.9199\n",
      "Epoch 70/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9251\n",
      "Epoch 71/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1879 - accuracy: 0.9294\n",
      "Epoch 72/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9229\n",
      "Epoch 73/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9242\n",
      "Epoch 74/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9238\n",
      "Epoch 75/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1944 - accuracy: 0.9238\n",
      "Epoch 76/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9298\n",
      "Epoch 77/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.9320\n",
      "Epoch 78/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1810 - accuracy: 0.9294\n",
      "Epoch 79/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9328\n",
      "Epoch 80/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.9264\n",
      "Epoch 81/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.9302\n",
      "Epoch 82/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9294\n",
      "Epoch 83/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9320\n",
      "Epoch 84/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9414\n",
      "Epoch 85/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1717 - accuracy: 0.9401\n",
      "Epoch 86/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.9397\n",
      "Epoch 87/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1875 - accuracy: 0.9298\n",
      "Epoch 88/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9414\n",
      "Epoch 89/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9345\n",
      "Epoch 90/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9363\n",
      "Epoch 91/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9341\n",
      "Epoch 92/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1852 - accuracy: 0.9315\n",
      "Epoch 93/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9384\n",
      "Epoch 94/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9388\n",
      "Epoch 95/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1623 - accuracy: 0.9337\n",
      "Epoch 96/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9328\n",
      "Epoch 97/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9453\n",
      "Epoch 98/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1700 - accuracy: 0.9341\n",
      "Epoch 99/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9440\n",
      "Epoch 100/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9479\n",
      "Epoch 101/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9500\n",
      "Epoch 102/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9384\n",
      "Epoch 103/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9406\n",
      "Epoch 104/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9444\n",
      "Epoch 105/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9358\n",
      "Epoch 106/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1698 - accuracy: 0.9380\n",
      "Epoch 107/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9466\n",
      "Epoch 108/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9513\n",
      "Epoch 109/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9462\n",
      "Epoch 110/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9479\n",
      "Epoch 111/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9462\n",
      "Epoch 112/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9414\n",
      "Epoch 113/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9483\n",
      "Epoch 114/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.9440\n",
      "Epoch 115/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9492\n",
      "Epoch 116/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9475\n",
      "Epoch 117/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9436\n",
      "Epoch 118/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9496\n",
      "Epoch 119/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9500\n",
      "Epoch 120/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9466\n",
      "Epoch 121/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9531\n",
      "Epoch 122/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9470\n",
      "Epoch 123/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1535 - accuracy: 0.9406\n",
      "Epoch 124/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9526\n",
      "Epoch 125/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9470\n",
      "Epoch 126/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9539\n",
      "Epoch 127/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9419\n",
      "Epoch 128/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9462\n",
      "Epoch 129/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9552\n",
      "Epoch 130/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9488\n",
      "Epoch 131/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9466\n",
      "Epoch 132/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9488\n",
      "Epoch 133/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9535\n",
      "Epoch 134/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9509\n",
      "Epoch 135/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9548\n",
      "Epoch 136/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.9531\n",
      "Epoch 137/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9634\n",
      "Epoch 138/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9543\n",
      "Epoch 139/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9617\n",
      "Epoch 140/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9556\n",
      "Epoch 141/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9531\n",
      "Epoch 142/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9595\n",
      "Epoch 143/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9565\n",
      "Epoch 144/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9518\n",
      "Epoch 145/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9535\n",
      "Epoch 146/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9582\n",
      "Epoch 147/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9604\n",
      "Epoch 148/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9625\n",
      "Epoch 149/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9569\n",
      "Epoch 150/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9518\n",
      "Epoch 151/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9625\n",
      "Epoch 152/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9608\n",
      "Epoch 153/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1294 - accuracy: 0.9535\n",
      "Epoch 154/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.9548\n",
      "Epoch 155/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9608\n",
      "Epoch 156/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9556\n",
      "Epoch 157/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9604\n",
      "Epoch 158/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9574\n",
      "Epoch 159/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9587\n",
      "Epoch 160/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9651\n",
      "Epoch 161/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9561\n",
      "Epoch 162/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9591\n",
      "Epoch 163/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.9531\n",
      "Epoch 164/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9574\n",
      "Epoch 165/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9548\n",
      "Epoch 166/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9647\n",
      "Epoch 167/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9660\n",
      "Epoch 168/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9599\n",
      "Epoch 169/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9651\n",
      "Epoch 170/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9630\n",
      "Epoch 171/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9604\n",
      "Epoch 172/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9599\n",
      "Epoch 173/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9569\n",
      "Epoch 174/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9561\n",
      "Epoch 175/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9513\n",
      "Epoch 176/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9574\n",
      "Epoch 177/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9582\n",
      "Epoch 178/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9604\n",
      "Epoch 179/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9578\n",
      "Epoch 180/1500\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1124 - accuracy: 0.9574\n",
      "Epoch 181/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9612\n",
      "Epoch 182/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9664\n",
      "Epoch 183/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9660\n",
      "Epoch 184/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9595\n",
      "Epoch 185/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9561\n",
      "Epoch 186/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9707\n",
      "Epoch 187/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9673\n",
      "Epoch 188/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9561\n",
      "Epoch 189/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9647\n",
      "Epoch 190/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9634\n",
      "Epoch 191/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9612\n",
      "Epoch 192/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9643\n",
      "Epoch 193/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9578\n",
      "Epoch 194/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9535\n",
      "Epoch 195/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9660\n",
      "Epoch 196/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9647\n",
      "Epoch 197/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9634\n",
      "Epoch 198/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9604\n",
      "Epoch 199/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9634\n",
      "Epoch 200/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.9677\n",
      "Epoch 201/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9625\n",
      "Epoch 202/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9630\n",
      "Epoch 203/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9608\n",
      "Epoch 204/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9737\n",
      "Epoch 205/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9543\n",
      "Epoch 206/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9548\n",
      "Epoch 207/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9655\n",
      "Epoch 208/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9638\n",
      "Epoch 209/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9707\n",
      "Epoch 210/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.9699\n",
      "Epoch 211/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9625\n",
      "Epoch 212/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9565\n",
      "Epoch 213/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9612\n",
      "Epoch 214/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9660\n",
      "Epoch 215/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9608\n",
      "Epoch 216/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9707\n",
      "Epoch 217/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9591\n",
      "Epoch 218/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9647\n",
      "Epoch 219/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9595\n",
      "Epoch 220/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9673\n",
      "Epoch 221/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9699\n",
      "Epoch 222/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9733\n",
      "Epoch 223/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9673\n",
      "Epoch 224/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.9733\n",
      "Epoch 225/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9703\n",
      "Epoch 226/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9625\n",
      "Epoch 227/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9785\n",
      "Epoch 228/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9729\n",
      "Epoch 229/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9716\n",
      "Epoch 230/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9703\n",
      "Epoch 231/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9664\n",
      "Epoch 232/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9716\n",
      "Epoch 233/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9707\n",
      "Epoch 234/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9612\n",
      "Epoch 235/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9703\n",
      "Epoch 236/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0786 - accuracy: 0.9742\n",
      "Epoch 237/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0994 - accuracy: 0.9625\n",
      "Epoch 238/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0977 - accuracy: 0.9673\n",
      "Epoch 239/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.9556\n",
      "Epoch 240/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9686\n",
      "Epoch 241/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9690\n",
      "Epoch 242/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0813 - accuracy: 0.9711\n",
      "Epoch 243/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9681\n",
      "Epoch 244/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9664\n",
      "Epoch 245/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9729\n",
      "Epoch 246/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9604\n",
      "Epoch 247/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9686\n",
      "Epoch 248/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9677\n",
      "Epoch 249/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9750\n",
      "Epoch 250/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9651\n",
      "Epoch 251/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9677\n",
      "Epoch 252/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0806 - accuracy: 0.9750\n",
      "Epoch 253/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9703\n",
      "Epoch 254/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9724\n",
      "Epoch 255/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9742\n",
      "Epoch 256/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9716\n",
      "Epoch 257/1500\n",
      "47/73 [==================>...........] - ETA: 0s - loss: 0.0773 - accuracy: 0.9714Restoring model weights from the end of the best epoch: 227.\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0816 - accuracy: 0.9711\n",
      "Epoch 257: early stopping\n",
      "6/6 [==============================] - 0s 928us/step - loss: 1.3307 - accuracy: 0.6571\n",
      "6/6 [==============================] - 0s 776us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.82 (23/28)\n",
      "Before appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "After appending - Cat IDs: 854, Predictions: 854, Actuals: 854, Gender: 854\n",
      "Final Test Results - Loss: 1.330725073814392, Accuracy: 0.6571428775787354, Precision: 0.7237373737373738, Recall: 0.7290263319044703, F1 Score: 0.6817580567580568\n",
      "Confusion Matrix:\n",
      " [[82  3  7]\n",
      " [ 0 12  0]\n",
      " [50  0 21]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6767594464424782\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 1.018434688448906\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7151577174663544\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.69287157655072\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7134359778269065\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[1]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2845ad-17c1-494a-8bd0-971aefca9f01",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3910db95-6772-4098-bef1-fb5857901e5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 854, Predictions: 854, Actuals: 854, Gender: 854\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d543c7c2-c11b-4511-ba5a-c52969a61a62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6b6f2173-89a8-40ba-afa6-410005c2dcb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.76 (84/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aa8d9ba5-9d96-4aee-b3e2-ba87553d1899",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b4ff1f7-d22d-4409-98d4-49e9a82bcb58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, senior, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, adult,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, senior, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, kit...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, kit...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, adult, adult, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[senior, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "68    062A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "65    059A  [senior, adult, senior, senior, senior, senior...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, adult, senior, adult, senior, adult, ...        senior           senior                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "53    048A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A           [kitten, kitten, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "77    071A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, senior, ...         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, senior, adult, adult, senior, adult, a...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, adult, ...        senior           senior                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "48    042A  [kitten, adult, kitten, kitten, kitten, adult,...        kitten           kitten                   True\n",
       "55    050A  [kitten, senior, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "28    025A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "25    023A        [adult, kitten, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, kit...         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, adult, adult, adult, se...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, senior, se...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "30    025C               [senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "52    047A  [adult, adult, adult, adult, adult, adult, kit...         adult           kitten                  False\n",
       "42    036A  [adult, senior, senior, senior, adult, senior,...        senior            adult                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "39    033A  [kitten, adult, kitten, kitten, adult, adult, ...        kitten            adult                  False\n",
       "102   108A         [adult, adult, adult, adult, adult, adult]         adult           senior                  False\n",
       "101   106A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "99    104A                     [adult, senior, adult, senior]         adult           senior                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, s...        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "61    055A  [adult, adult, senior, adult, senior, adult, s...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, senior, senior, senior, ...         adult           senior                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "66    060A                           [senior, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "109   117A  [adult, adult, adult, adult, adult, adult, adult]         adult           senior                  False"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ec47984-bc63-4f3f-a7a4-d3979dbd4c52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     63\n",
      "kitten    12\n",
      "senior     9\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "952d49d2-180d-4f36-85b0-6e2b96610559",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             63  86.301370\n",
      "1           kitten           15             12  80.000000\n",
      "2           senior           22              9  40.909091\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6a94de06-f9df-49f6-99e7-abc9024f3dc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885dd133-e156-4b01-8e84-4d94546e0eca",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dd0127df-e0fb-4862-9ac4-2113bd346e78",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          558            450  80.645161\n",
      "1           kitten          118             79  66.949153\n",
      "2           senior          178             81  45.505618\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a0df220-5e5c-4c46-8474-ac4ecf291071",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a7528-860f-45b0-8c00-d0ddf10a0d8f",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2924eee7-fd8b-4cee-9e43-bb0352e4c43b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    222      165     74.32\n",
      "1          M    337      247     73.29\n",
      "2          X    295      198     67.12\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b7b2139e-f100-44e6-aff4-807f96753499",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58f49-cac6-40b1-8422-e3d95576c453",
   "metadata": {},
   "source": [
    "# RANDOM SEED 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf9d1b64-575e-47ca-bf37-e8916438d506",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[2]))\n",
    "np.random.seed(int(random_seeds[2]))\n",
    "tf.random.set_seed(int(random_seeds[2]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "91d5db1d-6c9f-4047-97b9-5a0eef4fbaaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e435598c-cabc-4129-8504-68aac9cc06bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29b526-5098-4ce6-bc77-a93d1e6e1397",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "79a922e6-198c-4c43-a6f2-90dc580ae875",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "071A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "033A     9\n",
      "072A     9\n",
      "015A     9\n",
      "022A     9\n",
      "051B     9\n",
      "065A     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "108A     6\n",
      "109A     6\n",
      "037A     6\n",
      "007A     6\n",
      "008A     6\n",
      "025C     5\n",
      "070A     5\n",
      "021A     5\n",
      "034A     5\n",
      "075A     5\n",
      "023B     5\n",
      "035A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "058A     3\n",
      "014A     3\n",
      "011A     2\n",
      "061A     2\n",
      "032A     2\n",
      "093A     2\n",
      "025B     2\n",
      "087A     2\n",
      "069A     2\n",
      "073A     1\n",
      "115A     1\n",
      "088A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "019B     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "005A    10\n",
      "045A     9\n",
      "117A     7\n",
      "053A     6\n",
      "023A     6\n",
      "044A     5\n",
      "105A     4\n",
      "052A     4\n",
      "104A     4\n",
      "018A     2\n",
      "054A     2\n",
      "038A     2\n",
      "102A     2\n",
      "091A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    265\n",
      "M    256\n",
      "F    198\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    83\n",
      "M    81\n",
      "F    54\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 103A, 071A, 028A, 067...\n",
      "kitten    [014B, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [001A, 097B, 019A, 074A, 029A, 005A, 091A, 023...\n",
      "kitten                             [044A, 111A, 045A, 110A]\n",
      "senior                             [104A, 054A, 117A, 051A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "Length of X_train_val:\n",
      "719\n",
      "Length of y_train_val:\n",
      "719\n",
      "Length of groups_train_val:\n",
      "719\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 846, 2: 765, 1: 715})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.8986 - accuracy: 0.6242\n",
      "Epoch 2/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6946 - accuracy: 0.7313\n",
      "Epoch 3/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6124 - accuracy: 0.7562\n",
      "Epoch 4/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5692 - accuracy: 0.7691\n",
      "Epoch 5/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5584 - accuracy: 0.7820\n",
      "Epoch 6/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5239 - accuracy: 0.7915\n",
      "Epoch 7/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4909 - accuracy: 0.8027\n",
      "Epoch 8/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4577 - accuracy: 0.8306\n",
      "Epoch 9/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4606 - accuracy: 0.8199\n",
      "Epoch 10/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4553 - accuracy: 0.8143\n",
      "Epoch 11/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4334 - accuracy: 0.8267\n",
      "Epoch 12/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4118 - accuracy: 0.8362\n",
      "Epoch 13/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4109 - accuracy: 0.8371\n",
      "Epoch 14/1500\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.4055 - accuracy: 0.8328\n",
      "Epoch 15/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3827 - accuracy: 0.8422\n",
      "Epoch 16/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3604 - accuracy: 0.8697\n",
      "Epoch 17/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3652 - accuracy: 0.8534\n",
      "Epoch 18/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3650 - accuracy: 0.8538\n",
      "Epoch 19/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3572 - accuracy: 0.8663\n",
      "Epoch 20/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3416 - accuracy: 0.8667\n",
      "Epoch 21/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3411 - accuracy: 0.8710\n",
      "Epoch 22/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3428 - accuracy: 0.8611\n",
      "Epoch 23/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3273 - accuracy: 0.8684\n",
      "Epoch 24/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3219 - accuracy: 0.8702\n",
      "Epoch 25/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3148 - accuracy: 0.8848\n",
      "Epoch 26/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8796\n",
      "Epoch 27/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2904 - accuracy: 0.8861\n",
      "Epoch 28/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3091 - accuracy: 0.8783\n",
      "Epoch 29/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2995 - accuracy: 0.8874\n",
      "Epoch 30/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2879 - accuracy: 0.8942\n",
      "Epoch 31/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2799 - accuracy: 0.8925\n",
      "Epoch 32/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2865 - accuracy: 0.8955\n",
      "Epoch 33/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3036 - accuracy: 0.8775\n",
      "Epoch 34/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2666 - accuracy: 0.8947\n",
      "Epoch 35/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2918 - accuracy: 0.8895\n",
      "Epoch 36/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2793 - accuracy: 0.8934\n",
      "Epoch 37/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2597 - accuracy: 0.8960\n",
      "Epoch 38/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2718 - accuracy: 0.8981\n",
      "Epoch 39/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2403 - accuracy: 0.9089\n",
      "Epoch 40/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 0.9127\n",
      "Epoch 41/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.9067\n",
      "Epoch 42/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2423 - accuracy: 0.9101\n",
      "Epoch 43/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2375 - accuracy: 0.9127\n",
      "Epoch 44/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9132\n",
      "Epoch 45/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2333 - accuracy: 0.9067\n",
      "Epoch 46/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2446 - accuracy: 0.9080\n",
      "Epoch 47/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2369 - accuracy: 0.9050\n",
      "Epoch 48/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2385 - accuracy: 0.9071\n",
      "Epoch 49/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2198 - accuracy: 0.9166\n",
      "Epoch 50/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2296 - accuracy: 0.9101\n",
      "Epoch 51/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2363 - accuracy: 0.9076\n",
      "Epoch 52/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2290 - accuracy: 0.9119\n",
      "Epoch 53/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2359 - accuracy: 0.9071\n",
      "Epoch 54/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2296 - accuracy: 0.9153\n",
      "Epoch 55/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2110 - accuracy: 0.9239\n",
      "Epoch 56/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2121 - accuracy: 0.9187\n",
      "Epoch 57/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2181 - accuracy: 0.9187\n",
      "Epoch 58/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2014 - accuracy: 0.9230\n",
      "Epoch 59/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2222 - accuracy: 0.9162\n",
      "Epoch 60/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2119 - accuracy: 0.9230\n",
      "Epoch 61/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9321\n",
      "Epoch 62/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9252\n",
      "Epoch 63/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2101 - accuracy: 0.9166\n",
      "Epoch 64/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9308\n",
      "Epoch 65/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1936 - accuracy: 0.9278\n",
      "Epoch 66/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1834 - accuracy: 0.9312\n",
      "Epoch 67/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1896 - accuracy: 0.9282\n",
      "Epoch 68/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9312\n",
      "Epoch 69/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1972 - accuracy: 0.9291\n",
      "Epoch 70/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1891 - accuracy: 0.9282\n",
      "Epoch 71/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2114 - accuracy: 0.9226\n",
      "Epoch 72/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1942 - accuracy: 0.9304\n",
      "Epoch 73/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9252\n",
      "Epoch 74/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.9402\n",
      "Epoch 75/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.9325\n",
      "Epoch 76/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.9420\n",
      "Epoch 77/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9355\n",
      "Epoch 78/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9342\n",
      "Epoch 79/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9381\n",
      "Epoch 80/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1867 - accuracy: 0.9312\n",
      "Epoch 81/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1557 - accuracy: 0.9497\n",
      "Epoch 82/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9420\n",
      "Epoch 83/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1652 - accuracy: 0.9445\n",
      "Epoch 84/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9433\n",
      "Epoch 85/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9385\n",
      "Epoch 86/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1799 - accuracy: 0.9368\n",
      "Epoch 87/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1624 - accuracy: 0.9411\n",
      "Epoch 88/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1663 - accuracy: 0.9433\n",
      "Epoch 89/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9458\n",
      "Epoch 90/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9329\n",
      "Epoch 91/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.9510\n",
      "Epoch 92/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1497 - accuracy: 0.9458\n",
      "Epoch 93/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9347\n",
      "Epoch 94/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.9368\n",
      "Epoch 95/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9321\n",
      "Epoch 96/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1587 - accuracy: 0.9351\n",
      "Epoch 97/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9342\n",
      "Epoch 98/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9390\n",
      "Epoch 99/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9394\n",
      "Epoch 100/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9424\n",
      "Epoch 101/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9394\n",
      "Epoch 102/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9497\n",
      "Epoch 103/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9463\n",
      "Epoch 104/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9549\n",
      "Epoch 105/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9415\n",
      "Epoch 106/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9497\n",
      "Epoch 107/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.9484\n",
      "Epoch 108/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9424\n",
      "Epoch 109/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9484\n",
      "Epoch 110/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9463\n",
      "Epoch 111/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9583\n",
      "Epoch 112/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.9454\n",
      "Epoch 113/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1480 - accuracy: 0.9475\n",
      "Epoch 114/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9557\n",
      "Epoch 115/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9428\n",
      "Epoch 116/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9493\n",
      "Epoch 117/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9488\n",
      "Epoch 118/1500\n",
      "73/73 [==============================] - 0s 988us/step - loss: 0.1406 - accuracy: 0.9471\n",
      "Epoch 119/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9458\n",
      "Epoch 120/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.9579\n",
      "Epoch 121/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9450\n",
      "Epoch 122/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9600\n",
      "Epoch 123/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9420\n",
      "Epoch 124/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9463\n",
      "Epoch 125/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9471\n",
      "Epoch 126/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9536\n",
      "Epoch 127/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.9501\n",
      "Epoch 128/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9531\n",
      "Epoch 129/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9540\n",
      "Epoch 130/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9553\n",
      "Epoch 131/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9544\n",
      "Epoch 132/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9566\n",
      "Epoch 133/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9518\n",
      "Epoch 134/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9488\n",
      "Epoch 135/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9527\n",
      "Epoch 136/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9518\n",
      "Epoch 137/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9574\n",
      "Epoch 138/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9587\n",
      "Epoch 139/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9635\n",
      "Epoch 140/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9600\n",
      "Epoch 141/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1320 - accuracy: 0.9544\n",
      "Epoch 142/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9518\n",
      "Epoch 143/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9531\n",
      "Epoch 144/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9531\n",
      "Epoch 145/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9587\n",
      "Epoch 146/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9656\n",
      "Epoch 147/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9536\n",
      "Epoch 148/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9643\n",
      "Epoch 149/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9660\n",
      "Epoch 150/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9561\n",
      "Epoch 151/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9622\n",
      "Epoch 152/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9557\n",
      "Epoch 153/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9583\n",
      "Epoch 154/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.9695\n",
      "Epoch 155/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9501\n",
      "Epoch 156/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9643\n",
      "Epoch 157/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9609\n",
      "Epoch 158/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9566\n",
      "Epoch 159/1500\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1235 - accuracy: 0.9540\n",
      "Epoch 160/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9592\n",
      "Epoch 161/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9635\n",
      "Epoch 162/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9630\n",
      "Epoch 163/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9566\n",
      "Epoch 164/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9643\n",
      "Epoch 165/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9540\n",
      "Epoch 166/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9566\n",
      "Epoch 167/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9536\n",
      "Epoch 168/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9635\n",
      "Epoch 169/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9626\n",
      "Epoch 170/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9647\n",
      "Epoch 171/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9652\n",
      "Epoch 172/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9613\n",
      "Epoch 173/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9600\n",
      "Epoch 174/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9579\n",
      "Epoch 175/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9639\n",
      "Epoch 176/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9617\n",
      "Epoch 177/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9630\n",
      "Epoch 178/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9647\n",
      "Epoch 179/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9673\n",
      "Epoch 180/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9579\n",
      "Epoch 181/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9643\n",
      "Epoch 182/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9635\n",
      "Epoch 183/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9652\n",
      "Epoch 184/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9669\n",
      "Epoch 185/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9592\n",
      "Epoch 186/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9626\n",
      "Epoch 187/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9626\n",
      "Epoch 188/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9716\n",
      "Epoch 189/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9673\n",
      "Epoch 190/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9712\n",
      "Epoch 191/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9647\n",
      "Epoch 192/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9609\n",
      "Epoch 193/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9639\n",
      "Epoch 194/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9566\n",
      "Epoch 195/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9574\n",
      "Epoch 196/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9630\n",
      "Epoch 197/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9639\n",
      "Epoch 198/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9742\n",
      "Epoch 199/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9712\n",
      "Epoch 200/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9725\n",
      "Epoch 201/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9682\n",
      "Epoch 202/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9712\n",
      "Epoch 203/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9660\n",
      "Epoch 204/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9690\n",
      "Epoch 205/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9665\n",
      "Epoch 206/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0887 - accuracy: 0.9690\n",
      "Epoch 207/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.9703\n",
      "Epoch 208/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9639\n",
      "Epoch 209/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9536\n",
      "Epoch 210/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9660\n",
      "Epoch 211/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9639\n",
      "Epoch 212/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9592\n",
      "Epoch 213/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9716\n",
      "Epoch 214/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9712\n",
      "Epoch 215/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9755\n",
      "Epoch 216/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9712\n",
      "Epoch 217/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9708\n",
      "Epoch 218/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9725\n",
      "Epoch 219/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9678\n",
      "Epoch 220/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9630\n",
      "Epoch 221/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9682\n",
      "Epoch 222/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.9673\n",
      "Epoch 223/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9738\n",
      "Epoch 224/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9643\n",
      "Epoch 225/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9613\n",
      "Epoch 226/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9669\n",
      "Epoch 227/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9708\n",
      "Epoch 228/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9686\n",
      "Epoch 229/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9622\n",
      "Epoch 230/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9721\n",
      "Epoch 231/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.9755\n",
      "Epoch 232/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9682\n",
      "Epoch 233/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9708\n",
      "Epoch 234/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.9742\n",
      "Epoch 235/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9725\n",
      "Epoch 236/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0894 - accuracy: 0.9678\n",
      "Epoch 237/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9686\n",
      "Epoch 238/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9647\n",
      "Epoch 239/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.9682\n",
      "Epoch 240/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9721\n",
      "Epoch 241/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9690\n",
      "Epoch 242/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9690\n",
      "Epoch 243/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9682\n",
      "Epoch 244/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9613\n",
      "Epoch 245/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.9695\n",
      "Epoch 246/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9660\n",
      "Epoch 247/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9660\n",
      "Epoch 248/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9785\n",
      "Epoch 249/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.9686\n",
      "Epoch 250/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9630\n",
      "Epoch 251/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9716\n",
      "Epoch 252/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9690\n",
      "Epoch 253/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9678\n",
      "Epoch 254/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9695\n",
      "Epoch 255/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.9656\n",
      "Epoch 256/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9673\n",
      "Epoch 257/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.9686\n",
      "Epoch 258/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9721\n",
      "Epoch 259/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0704 - accuracy: 0.9776\n",
      "Epoch 260/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9789\n",
      "Epoch 261/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.9678\n",
      "Epoch 262/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9699\n",
      "Epoch 263/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9665\n",
      "Epoch 264/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9678\n",
      "Epoch 265/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9665\n",
      "Epoch 266/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9673\n",
      "Epoch 267/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0835 - accuracy: 0.9690\n",
      "Epoch 268/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9733\n",
      "Epoch 269/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9725\n",
      "Epoch 270/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9725\n",
      "Epoch 271/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.9789\n",
      "Epoch 272/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9742\n",
      "Epoch 273/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9764\n",
      "Epoch 274/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9695\n",
      "Epoch 275/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9733\n",
      "Epoch 276/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.9721\n",
      "Epoch 277/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.9729\n",
      "Epoch 278/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.9781\n",
      "Epoch 279/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.9686\n",
      "Epoch 280/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.9785\n",
      "Epoch 281/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9721\n",
      "Epoch 282/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0694 - accuracy: 0.9789\n",
      "Epoch 283/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9746\n",
      "Epoch 284/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9725\n",
      "Epoch 285/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9695\n",
      "Epoch 286/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9755\n",
      "Epoch 287/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.9768\n",
      "Epoch 288/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9746\n",
      "Epoch 289/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0682 - accuracy: 0.9785\n",
      "Epoch 290/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9699\n",
      "Epoch 291/1500\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0778 - accuracy: 0.9725\n",
      "Epoch 292/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9738\n",
      "Epoch 293/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.9742\n",
      "Epoch 294/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.9776\n",
      "Epoch 295/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.9798\n",
      "Epoch 296/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9742\n",
      "Epoch 297/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9785\n",
      "Epoch 298/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0621 - accuracy: 0.9798\n",
      "Epoch 299/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9733\n",
      "Epoch 300/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0714 - accuracy: 0.9712\n",
      "Epoch 301/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9733\n",
      "Epoch 302/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9729\n",
      "Epoch 303/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9802\n",
      "Epoch 304/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.9781\n",
      "Epoch 305/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.9768\n",
      "Epoch 306/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9656\n",
      "Epoch 307/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9785\n",
      "Epoch 308/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9721\n",
      "Epoch 309/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9703\n",
      "Epoch 310/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.9781\n",
      "Epoch 311/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0540 - accuracy: 0.9819\n",
      "Epoch 312/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9725\n",
      "Epoch 313/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.9751\n",
      "Epoch 314/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0623 - accuracy: 0.9798\n",
      "Epoch 315/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.9794\n",
      "Epoch 316/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0629 - accuracy: 0.9768\n",
      "Epoch 317/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9764\n",
      "Epoch 318/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9712\n",
      "Epoch 319/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9703\n",
      "Epoch 320/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9699\n",
      "Epoch 321/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9716\n",
      "Epoch 322/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0620 - accuracy: 0.9781\n",
      "Epoch 323/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.9755\n",
      "Epoch 324/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.9716\n",
      "Epoch 325/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.9746\n",
      "Epoch 326/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.9755\n",
      "Epoch 327/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.9798\n",
      "Epoch 328/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.9725\n",
      "Epoch 329/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9729\n",
      "Epoch 330/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.9738\n",
      "Epoch 331/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0657 - accuracy: 0.9811\n",
      "Epoch 332/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.9755\n",
      "Epoch 333/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9759\n",
      "Epoch 334/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.9712\n",
      "Epoch 335/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9721\n",
      "Epoch 336/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9716\n",
      "Epoch 337/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.9785\n",
      "Epoch 338/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9815\n",
      "Epoch 339/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0623 - accuracy: 0.9755\n",
      "Epoch 340/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0595 - accuracy: 0.9789\n",
      "Epoch 341/1500\n",
      "46/73 [=================>............] - ETA: 0s - loss: 0.0600 - accuracy: 0.9796Restoring model weights from the end of the best epoch: 311.\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.9759\n",
      "Epoch 341: early stopping\n",
      "7/7 [==============================] - 0s 811us/step - loss: 1.5217 - accuracy: 0.6147\n",
      "7/7 [==============================] - 0s 662us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.56 (14/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "Final Test Results - Loss: 1.5217124223709106, Accuracy: 0.6146789193153381, Precision: 0.6006126687435098, Recall: 0.6846608946608947, F1 Score: 0.5983088235294117\n",
      "Confusion Matrix:\n",
      " [[94  9 62]\n",
      " [ 1 27  0]\n",
      " [12  0 13]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "072A     9\n",
      "033A     9\n",
      "045A     9\n",
      "022A     9\n",
      "094A     8\n",
      "013B     8\n",
      "010A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "007A     6\n",
      "037A     6\n",
      "053A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "021A     5\n",
      "035A     4\n",
      "003A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "009A     4\n",
      "105A     4\n",
      "058A     3\n",
      "064A     3\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "014A     3\n",
      "093A     2\n",
      "025B     2\n",
      "038A     2\n",
      "087A     2\n",
      "102A     2\n",
      "032A     2\n",
      "054A     2\n",
      "018A     2\n",
      "115A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "110A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "088A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "042A    14\n",
      "106A    14\n",
      "116A    12\n",
      "039A    12\n",
      "063A    11\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "014B    10\n",
      "065A     9\n",
      "015A     9\n",
      "051B     9\n",
      "095A     8\n",
      "070A     5\n",
      "075A     5\n",
      "023B     5\n",
      "062A     4\n",
      "060A     3\n",
      "011A     2\n",
      "069A     2\n",
      "061A     2\n",
      "043A     1\n",
      "092A     1\n",
      "076A     1\n",
      "004A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    241\n",
      "F    181\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "X    75\n",
      "F    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 001A, 103A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 111A, 046A, 047A, 109A, 050A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 104A, 055A, 059A, 113A, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 071A, 062A, 002B, 095A, 065A, 039...\n",
      "kitten                             [014B, 040A, 042A, 043A]\n",
      "senior                 [106A, 116A, 051B, 016A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 12, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 4, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '018A' '019A' '020A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '004A' '011A' '014B' '015A' '016A' '019B' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'020A'}\n",
      "Moved to Test Set:\n",
      "{'020A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '018A' '019A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '011A' '014B' '015A' '016A' '019B' '020A' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "Length of X_train_val:\n",
      "711\n",
      "Length of y_train_val:\n",
      "711\n",
      "Length of groups_train_val:\n",
      "711\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     430\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     158\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     446\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     142\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 892, 1: 680, 2: 645})\n",
      "Epoch 1/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9690 - accuracy: 0.5855\n",
      "Epoch 2/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7388 - accuracy: 0.6874\n",
      "Epoch 3/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6675 - accuracy: 0.7289\n",
      "Epoch 4/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6068 - accuracy: 0.7452\n",
      "Epoch 5/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5823 - accuracy: 0.7542\n",
      "Epoch 6/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5412 - accuracy: 0.7718\n",
      "Epoch 7/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5062 - accuracy: 0.7966\n",
      "Epoch 8/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4813 - accuracy: 0.8146\n",
      "Epoch 9/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4713 - accuracy: 0.7988\n",
      "Epoch 10/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4601 - accuracy: 0.8110\n",
      "Epoch 11/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4415 - accuracy: 0.8223\n",
      "Epoch 12/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4271 - accuracy: 0.8290\n",
      "Epoch 13/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4177 - accuracy: 0.8286\n",
      "Epoch 14/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4119 - accuracy: 0.8318\n",
      "Epoch 15/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3984 - accuracy: 0.8358\n",
      "Epoch 16/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3880 - accuracy: 0.8354\n",
      "Epoch 17/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3937 - accuracy: 0.8426\n",
      "Epoch 18/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3663 - accuracy: 0.8516\n",
      "Epoch 19/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3612 - accuracy: 0.8480\n",
      "Epoch 20/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3465 - accuracy: 0.8597\n",
      "Epoch 21/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3425 - accuracy: 0.8593\n",
      "Epoch 22/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3274 - accuracy: 0.8742\n",
      "Epoch 23/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3371 - accuracy: 0.8633\n",
      "Epoch 24/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.8769\n",
      "Epoch 25/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3357 - accuracy: 0.8687\n",
      "Epoch 26/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3109 - accuracy: 0.8728\n",
      "Epoch 27/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2823 - accuracy: 0.8841\n",
      "Epoch 28/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3003 - accuracy: 0.8832\n",
      "Epoch 29/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2772 - accuracy: 0.8895\n",
      "Epoch 30/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2725 - accuracy: 0.8908\n",
      "Epoch 31/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2910 - accuracy: 0.8854\n",
      "Epoch 32/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2792 - accuracy: 0.8935\n",
      "Epoch 33/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2673 - accuracy: 0.8926\n",
      "Epoch 34/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2659 - accuracy: 0.8994\n",
      "Epoch 35/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2544 - accuracy: 0.9030\n",
      "Epoch 36/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2523 - accuracy: 0.8958\n",
      "Epoch 37/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2679 - accuracy: 0.8967\n",
      "Epoch 38/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2580 - accuracy: 0.8999\n",
      "Epoch 39/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2515 - accuracy: 0.9053\n",
      "Epoch 40/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2334 - accuracy: 0.9138\n",
      "Epoch 41/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2339 - accuracy: 0.9035\n",
      "Epoch 42/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9129\n",
      "Epoch 43/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2496 - accuracy: 0.9026\n",
      "Epoch 44/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2356 - accuracy: 0.9039\n",
      "Epoch 45/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2301 - accuracy: 0.9134\n",
      "Epoch 46/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2244 - accuracy: 0.9138\n",
      "Epoch 47/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2186 - accuracy: 0.9120\n",
      "Epoch 48/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2309 - accuracy: 0.9107\n",
      "Epoch 49/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 0.9184\n",
      "Epoch 50/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2273 - accuracy: 0.9138\n",
      "Epoch 51/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2230 - accuracy: 0.9197\n",
      "Epoch 52/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2101 - accuracy: 0.9220\n",
      "Epoch 53/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2053 - accuracy: 0.9202\n",
      "Epoch 54/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9206\n",
      "Epoch 55/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2102 - accuracy: 0.9193\n",
      "Epoch 56/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2141 - accuracy: 0.9157\n",
      "Epoch 57/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2150 - accuracy: 0.9211\n",
      "Epoch 58/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9220\n",
      "Epoch 59/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9283\n",
      "Epoch 60/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1935 - accuracy: 0.9229\n",
      "Epoch 61/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1958 - accuracy: 0.9301\n",
      "Epoch 62/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1865 - accuracy: 0.9378\n",
      "Epoch 63/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9364\n",
      "Epoch 64/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9287\n",
      "Epoch 65/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.9287\n",
      "Epoch 66/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9319\n",
      "Epoch 67/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1766 - accuracy: 0.9391\n",
      "Epoch 68/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1727 - accuracy: 0.9391\n",
      "Epoch 69/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9350\n",
      "Epoch 70/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9328\n",
      "Epoch 71/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.9378\n",
      "Epoch 72/1500\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9382\n",
      "Epoch 73/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9396\n",
      "Epoch 74/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1655 - accuracy: 0.9396\n",
      "Epoch 75/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9332\n",
      "Epoch 76/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9359\n",
      "Epoch 77/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1584 - accuracy: 0.9423\n",
      "Epoch 78/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1862 - accuracy: 0.9337\n",
      "Epoch 79/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9441\n",
      "Epoch 80/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9391\n",
      "Epoch 81/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.9441\n",
      "Epoch 82/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9373\n",
      "Epoch 83/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9445\n",
      "Epoch 84/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9490\n",
      "Epoch 85/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9459\n",
      "Epoch 86/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9558\n",
      "Epoch 87/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9445\n",
      "Epoch 88/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9387\n",
      "Epoch 89/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9486\n",
      "Epoch 90/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1498 - accuracy: 0.9459\n",
      "Epoch 91/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1525 - accuracy: 0.9463\n",
      "Epoch 92/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1642 - accuracy: 0.9405\n",
      "Epoch 93/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9477\n",
      "Epoch 94/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9576\n",
      "Epoch 95/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.9477\n",
      "Epoch 96/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9495\n",
      "Epoch 97/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9472\n",
      "Epoch 98/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.9508\n",
      "Epoch 99/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9603\n",
      "Epoch 100/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9459\n",
      "Epoch 101/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9486\n",
      "Epoch 102/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9508\n",
      "Epoch 103/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9495\n",
      "Epoch 104/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9581\n",
      "Epoch 105/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9653\n",
      "Epoch 106/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9558\n",
      "Epoch 107/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9608\n",
      "Epoch 108/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9441\n",
      "Epoch 109/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9535\n",
      "Epoch 110/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9558\n",
      "Epoch 111/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9517\n",
      "Epoch 112/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9531\n",
      "Epoch 113/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.9481\n",
      "Epoch 114/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9599\n",
      "Epoch 115/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9540\n",
      "Epoch 116/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1281 - accuracy: 0.9567\n",
      "Epoch 117/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9553\n",
      "Epoch 118/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9544\n",
      "Epoch 119/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9581\n",
      "Epoch 120/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9526\n",
      "Epoch 121/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9585\n",
      "Epoch 122/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9544\n",
      "Epoch 123/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9657\n",
      "Epoch 124/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9581\n",
      "Epoch 125/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9558\n",
      "Epoch 126/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9594\n",
      "Epoch 127/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9608\n",
      "Epoch 128/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.9567\n",
      "Epoch 129/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9558\n",
      "Epoch 130/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9707\n",
      "Epoch 131/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9571\n",
      "Epoch 132/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9617\n",
      "Epoch 133/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9608\n",
      "Epoch 134/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9608\n",
      "Epoch 135/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9585\n",
      "Epoch 136/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1139 - accuracy: 0.9581\n",
      "Epoch 137/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9635\n",
      "Epoch 138/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9635\n",
      "Epoch 139/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9553\n",
      "Epoch 140/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9626\n",
      "Epoch 141/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9553\n",
      "Epoch 142/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9635\n",
      "Epoch 143/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9639\n",
      "Epoch 144/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9648\n",
      "Epoch 145/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9599\n",
      "Epoch 146/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9693\n",
      "Epoch 147/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9635\n",
      "Epoch 148/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9608\n",
      "Epoch 149/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9639\n",
      "Epoch 150/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.9689\n",
      "Epoch 151/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9599\n",
      "Epoch 152/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9590\n",
      "Epoch 153/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9648\n",
      "Epoch 154/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9612\n",
      "Epoch 155/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9644\n",
      "Epoch 156/1500\n",
      "70/70 [==============================] - 0s 998us/step - loss: 0.1082 - accuracy: 0.9617\n",
      "Epoch 157/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9689\n",
      "Epoch 158/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9720\n",
      "Epoch 159/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9675\n",
      "Epoch 160/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9639\n",
      "Epoch 161/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9603\n",
      "Epoch 162/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9603\n",
      "Epoch 163/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9653\n",
      "Epoch 164/1500\n",
      "70/70 [==============================] - 0s 987us/step - loss: 0.0839 - accuracy: 0.9738\n",
      "Epoch 165/1500\n",
      "70/70 [==============================] - 0s 999us/step - loss: 0.0951 - accuracy: 0.9680\n",
      "Epoch 166/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9617\n",
      "Epoch 167/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9657\n",
      "Epoch 168/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9630\n",
      "Epoch 169/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0891 - accuracy: 0.9671\n",
      "Epoch 170/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9635\n",
      "Epoch 171/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9702\n",
      "Epoch 172/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.9648\n",
      "Epoch 173/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9608\n",
      "Epoch 174/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9702\n",
      "Epoch 175/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.9693\n",
      "Epoch 176/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9693\n",
      "Epoch 177/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9671\n",
      "Epoch 178/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.9689\n",
      "Epoch 179/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9689\n",
      "Epoch 180/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9720\n",
      "Epoch 181/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9716\n",
      "Epoch 182/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9747\n",
      "Epoch 183/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9689\n",
      "Epoch 184/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0849 - accuracy: 0.9693\n",
      "Epoch 185/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9720\n",
      "Epoch 186/1500\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0957 - accuracy: 0.9662\n",
      "Epoch 187/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.9734\n",
      "Epoch 188/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.9725\n",
      "Epoch 189/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9693\n",
      "Epoch 190/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9729\n",
      "Epoch 191/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9653\n",
      "Epoch 192/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9657\n",
      "Epoch 193/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9662\n",
      "Epoch 194/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9707\n",
      "Epoch 195/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9720\n",
      "Epoch 196/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9702\n",
      "Epoch 197/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9680\n",
      "Epoch 198/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0887 - accuracy: 0.9680\n",
      "Epoch 199/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.9725\n",
      "Epoch 200/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9765\n",
      "Epoch 201/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.9770\n",
      "Epoch 202/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9711\n",
      "Epoch 203/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0793 - accuracy: 0.9734\n",
      "Epoch 204/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9680\n",
      "Epoch 205/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9738\n",
      "Epoch 206/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9680\n",
      "Epoch 207/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9675\n",
      "Epoch 208/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9693\n",
      "Epoch 209/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9702\n",
      "Epoch 210/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.9734\n",
      "Epoch 211/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9779\n",
      "Epoch 212/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9707\n",
      "Epoch 213/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9720\n",
      "Epoch 214/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9734\n",
      "Epoch 215/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.9756\n",
      "Epoch 216/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0661 - accuracy: 0.9761\n",
      "Epoch 217/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9702\n",
      "Epoch 218/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0656 - accuracy: 0.9779\n",
      "Epoch 219/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.9752\n",
      "Epoch 220/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9756\n",
      "Epoch 221/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.9711\n",
      "Epoch 222/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9707\n",
      "Epoch 223/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9729\n",
      "Epoch 224/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9693\n",
      "Epoch 225/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9725\n",
      "Epoch 226/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.9747\n",
      "Epoch 227/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9725\n",
      "Epoch 228/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9662\n",
      "Epoch 229/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9716\n",
      "Epoch 230/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.9811\n",
      "Epoch 231/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9725\n",
      "Epoch 232/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.9811\n",
      "Epoch 233/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0672 - accuracy: 0.9774\n",
      "Epoch 234/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9711\n",
      "Epoch 235/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9729\n",
      "Epoch 236/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0723 - accuracy: 0.9743\n",
      "Epoch 237/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0597 - accuracy: 0.9820\n",
      "Epoch 238/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0654 - accuracy: 0.9797\n",
      "Epoch 239/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9720\n",
      "Epoch 240/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9783\n",
      "Epoch 241/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0635 - accuracy: 0.9793\n",
      "Epoch 242/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.9747\n",
      "Epoch 243/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.9793\n",
      "Epoch 244/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0606 - accuracy: 0.9815\n",
      "Epoch 245/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9761\n",
      "Epoch 246/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9788\n",
      "Epoch 247/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9698\n",
      "Epoch 248/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.9747\n",
      "Epoch 249/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0583 - accuracy: 0.9824\n",
      "Epoch 250/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.9756\n",
      "Epoch 251/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.9765\n",
      "Epoch 252/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9734\n",
      "Epoch 253/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0612 - accuracy: 0.9811\n",
      "Epoch 254/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9761\n",
      "Epoch 255/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9743\n",
      "Epoch 256/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.9765\n",
      "Epoch 257/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.9734\n",
      "Epoch 258/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0681 - accuracy: 0.9761\n",
      "Epoch 259/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0570 - accuracy: 0.9838\n",
      "Epoch 260/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.9743\n",
      "Epoch 261/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0633 - accuracy: 0.9793\n",
      "Epoch 262/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.9793\n",
      "Epoch 263/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.9747\n",
      "Epoch 264/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0673 - accuracy: 0.9761\n",
      "Epoch 265/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0618 - accuracy: 0.9842\n",
      "Epoch 266/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.9788\n",
      "Epoch 267/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9743\n",
      "Epoch 268/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9765\n",
      "Epoch 269/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0730 - accuracy: 0.9725\n",
      "Epoch 270/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9747\n",
      "Epoch 271/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0743 - accuracy: 0.9743\n",
      "Epoch 272/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9734\n",
      "Epoch 273/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9680\n",
      "Epoch 274/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9797\n",
      "Epoch 275/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.9811\n",
      "Epoch 276/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9756\n",
      "Epoch 277/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0592 - accuracy: 0.9842\n",
      "Epoch 278/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9680\n",
      "Epoch 279/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0670 - accuracy: 0.9783\n",
      "Epoch 280/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.9797\n",
      "Epoch 281/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.9743\n",
      "Epoch 282/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0617 - accuracy: 0.9806\n",
      "Epoch 283/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9743\n",
      "Epoch 284/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.9720\n",
      "Epoch 285/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.9747\n",
      "Epoch 286/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.9765\n",
      "Epoch 287/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0675 - accuracy: 0.9747\n",
      "Epoch 288/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0504 - accuracy: 0.9860\n",
      "Epoch 289/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0703 - accuracy: 0.9752\n",
      "Epoch 290/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0563 - accuracy: 0.9806\n",
      "Epoch 291/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0542 - accuracy: 0.9806\n",
      "Epoch 292/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0627 - accuracy: 0.9806\n",
      "Epoch 293/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.9774\n",
      "Epoch 294/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9793\n",
      "Epoch 295/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.9797\n",
      "Epoch 296/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0581 - accuracy: 0.9774\n",
      "Epoch 297/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9806\n",
      "Epoch 298/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0519 - accuracy: 0.9815\n",
      "Epoch 299/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0499 - accuracy: 0.9847\n",
      "Epoch 300/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9779\n",
      "Epoch 301/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.9788\n",
      "Epoch 302/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0634 - accuracy: 0.9756\n",
      "Epoch 303/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.9806\n",
      "Epoch 304/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0588 - accuracy: 0.9820\n",
      "Epoch 305/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.9824\n",
      "Epoch 306/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.9779\n",
      "Epoch 307/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0585 - accuracy: 0.9806\n",
      "Epoch 308/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0525 - accuracy: 0.9842\n",
      "Epoch 309/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0578 - accuracy: 0.9811\n",
      "Epoch 310/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0571 - accuracy: 0.9783\n",
      "Epoch 311/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.9738\n",
      "Epoch 312/1500\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.0551 - accuracy: 0.9824\n",
      "Epoch 313/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0607 - accuracy: 0.9783\n",
      "Epoch 314/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0690 - accuracy: 0.9743\n",
      "Epoch 315/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0600 - accuracy: 0.9793\n",
      "Epoch 316/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0528 - accuracy: 0.9829\n",
      "Epoch 317/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0511 - accuracy: 0.9829\n",
      "Epoch 318/1500\n",
      "47/70 [===================>..........] - ETA: 0s - loss: 0.0541 - accuracy: 0.9820Restoring model weights from the end of the best epoch: 288.\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.9788\n",
      "Epoch 318: early stopping\n",
      "8/8 [==============================] - 0s 829us/step - loss: 1.0383 - accuracy: 0.7699\n",
      "8/8 [==============================] - 0s 617us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.71 (20/28)\n",
      "Before appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "After appending - Cat IDs: 444, Predictions: 444, Actuals: 444, Gender: 444\n",
      "Final Test Results - Loss: 1.038326621055603, Accuracy: 0.769911527633667, Precision: 0.7347985347985349, Recall: 0.7378461243652391, F1 Score: 0.7335643640997244\n",
      "Confusion Matrix:\n",
      " [[119   8  15]\n",
      " [  3  31   1]\n",
      " [ 25   0  24]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "042A    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "040A    10\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "015A     9\n",
      "045A     9\n",
      "095A     8\n",
      "117A     7\n",
      "099A     7\n",
      "031A     7\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "012A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "060A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "038A     2\n",
      "093A     2\n",
      "018A     2\n",
      "054A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "091A     1\n",
      "019B     1\n",
      "092A     1\n",
      "004A     1\n",
      "049A     1\n",
      "076A     1\n",
      "043A     1\n",
      "026C     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "020A    23\n",
      "067A    19\n",
      "097A    16\n",
      "059A    14\n",
      "022A     9\n",
      "072A     9\n",
      "010A     8\n",
      "094A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "037A     6\n",
      "109A     6\n",
      "008A     6\n",
      "009A     4\n",
      "003A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "025B     2\n",
      "087A     2\n",
      "032A     2\n",
      "066A     1\n",
      "048A     1\n",
      "041A     1\n",
      "115A     1\n",
      "096A     1\n",
      "100A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    243\n",
      "X    229\n",
      "F    226\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    119\n",
      "M     94\n",
      "F     26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 043A, 049...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [067A, 020A, 022A, 072A, 009A, 027A, 013B, 014...\n",
      "kitten                 [046A, 109A, 050A, 041A, 048A, 115A]\n",
      "senior                       [097A, 059A, 058A, 094A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '044A' '045A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '046A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'044A'}\n",
      "Moved to Test Set:\n",
      "{'044A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '045A' '046A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '044A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "Length of X_train_val:\n",
      "756\n",
      "Length of y_train_val:\n",
      "756\n",
      "Length of groups_train_val:\n",
      "756\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     470\n",
      "senior    136\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     118\n",
      "kitten     79\n",
      "senior     42\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     470\n",
      "kitten    150\n",
      "senior    136\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     118\n",
      "senior     42\n",
      "kitten     21\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 940, 1: 750, 2: 680})\n",
      "Epoch 1/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.8729 - accuracy: 0.6266\n",
      "Epoch 2/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.7081 - accuracy: 0.7055\n",
      "Epoch 3/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6087 - accuracy: 0.7401\n",
      "Epoch 4/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6288 - accuracy: 0.7329\n",
      "Epoch 5/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5691 - accuracy: 0.7553\n",
      "Epoch 6/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5481 - accuracy: 0.7620\n",
      "Epoch 7/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5161 - accuracy: 0.7886\n",
      "Epoch 8/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5291 - accuracy: 0.7865\n",
      "Epoch 9/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5212 - accuracy: 0.7785\n",
      "Epoch 10/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4969 - accuracy: 0.7916\n",
      "Epoch 11/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4699 - accuracy: 0.7954\n",
      "Epoch 12/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4794 - accuracy: 0.8097\n",
      "Epoch 13/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4797 - accuracy: 0.8038\n",
      "Epoch 14/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4483 - accuracy: 0.8177\n",
      "Epoch 15/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4438 - accuracy: 0.8194\n",
      "Epoch 16/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4252 - accuracy: 0.8211\n",
      "Epoch 17/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4226 - accuracy: 0.8308\n",
      "Epoch 18/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4160 - accuracy: 0.8262\n",
      "Epoch 19/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4251 - accuracy: 0.8165\n",
      "Epoch 20/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4241 - accuracy: 0.8257\n",
      "Epoch 21/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3941 - accuracy: 0.8380\n",
      "Epoch 22/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3932 - accuracy: 0.8401\n",
      "Epoch 23/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3899 - accuracy: 0.8367\n",
      "Epoch 24/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3955 - accuracy: 0.8397\n",
      "Epoch 25/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3865 - accuracy: 0.8515\n",
      "Epoch 26/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3836 - accuracy: 0.8435\n",
      "Epoch 27/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3830 - accuracy: 0.8414\n",
      "Epoch 28/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3557 - accuracy: 0.8549\n",
      "Epoch 29/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3631 - accuracy: 0.8557\n",
      "Epoch 30/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3630 - accuracy: 0.8536\n",
      "Epoch 31/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3513 - accuracy: 0.8574\n",
      "Epoch 32/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3541 - accuracy: 0.8502\n",
      "Epoch 33/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3414 - accuracy: 0.8603\n",
      "Epoch 34/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3395 - accuracy: 0.8641\n",
      "Epoch 35/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3301 - accuracy: 0.8603\n",
      "Epoch 36/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3218 - accuracy: 0.8738\n",
      "Epoch 37/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3473 - accuracy: 0.8629\n",
      "Epoch 38/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3172 - accuracy: 0.8734\n",
      "Epoch 39/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3209 - accuracy: 0.8696\n",
      "Epoch 40/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3174 - accuracy: 0.8700\n",
      "Epoch 41/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3203 - accuracy: 0.8679\n",
      "Epoch 42/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3235 - accuracy: 0.8684\n",
      "Epoch 43/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3282 - accuracy: 0.8700\n",
      "Epoch 44/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3052 - accuracy: 0.8768\n",
      "Epoch 45/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3271 - accuracy: 0.8654\n",
      "Epoch 46/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3163 - accuracy: 0.8671\n",
      "Epoch 47/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3095 - accuracy: 0.8722\n",
      "Epoch 48/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3040 - accuracy: 0.8759\n",
      "Epoch 49/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8637\n",
      "Epoch 50/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3077 - accuracy: 0.8759\n",
      "Epoch 51/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3065 - accuracy: 0.8768\n",
      "Epoch 52/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8776\n",
      "Epoch 53/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2886 - accuracy: 0.8848\n",
      "Epoch 54/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2906 - accuracy: 0.8882\n",
      "Epoch 55/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2821 - accuracy: 0.8873\n",
      "Epoch 56/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2869 - accuracy: 0.8861\n",
      "Epoch 57/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2636 - accuracy: 0.8949\n",
      "Epoch 58/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2701 - accuracy: 0.8962\n",
      "Epoch 59/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2926 - accuracy: 0.8840\n",
      "Epoch 60/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2839 - accuracy: 0.8878\n",
      "Epoch 61/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2742 - accuracy: 0.8903\n",
      "Epoch 62/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3008 - accuracy: 0.8831\n",
      "Epoch 63/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2737 - accuracy: 0.8869\n",
      "Epoch 64/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2781 - accuracy: 0.8857\n",
      "Epoch 65/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2779 - accuracy: 0.8873\n",
      "Epoch 66/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2823 - accuracy: 0.8941\n",
      "Epoch 67/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2709 - accuracy: 0.8979\n",
      "Epoch 68/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2491 - accuracy: 0.9017\n",
      "Epoch 69/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2592 - accuracy: 0.8979\n",
      "Epoch 70/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2762 - accuracy: 0.8840\n",
      "Epoch 71/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2474 - accuracy: 0.9025\n",
      "Epoch 72/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2452 - accuracy: 0.9021\n",
      "Epoch 73/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2489 - accuracy: 0.9110\n",
      "Epoch 74/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2508 - accuracy: 0.9038\n",
      "Epoch 75/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2594 - accuracy: 0.9000\n",
      "Epoch 76/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.9038\n",
      "Epoch 77/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2334 - accuracy: 0.9072\n",
      "Epoch 78/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2587 - accuracy: 0.9017\n",
      "Epoch 79/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2591 - accuracy: 0.8966\n",
      "Epoch 80/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2513 - accuracy: 0.9021\n",
      "Epoch 81/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2544 - accuracy: 0.8983\n",
      "Epoch 82/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2428 - accuracy: 0.9055\n",
      "Epoch 83/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2468 - accuracy: 0.9017\n",
      "Epoch 84/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2454 - accuracy: 0.9042\n",
      "Epoch 85/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2438 - accuracy: 0.9068\n",
      "Epoch 86/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2341 - accuracy: 0.9165\n",
      "Epoch 87/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.9038\n",
      "Epoch 88/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2171 - accuracy: 0.9194\n",
      "Epoch 89/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.9017\n",
      "Epoch 90/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2272 - accuracy: 0.9135\n",
      "Epoch 91/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2373 - accuracy: 0.9072\n",
      "Epoch 92/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2188 - accuracy: 0.9143\n",
      "Epoch 93/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2064 - accuracy: 0.9215\n",
      "Epoch 94/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2090 - accuracy: 0.9270\n",
      "Epoch 95/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2305 - accuracy: 0.9093\n",
      "Epoch 96/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2137 - accuracy: 0.9156\n",
      "Epoch 97/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2251 - accuracy: 0.9173\n",
      "Epoch 98/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2206 - accuracy: 0.9169\n",
      "Epoch 99/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2223 - accuracy: 0.9034\n",
      "Epoch 100/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2102 - accuracy: 0.9190\n",
      "Epoch 101/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2144 - accuracy: 0.9127\n",
      "Epoch 102/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2255 - accuracy: 0.9089\n",
      "Epoch 103/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9190\n",
      "Epoch 104/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9143\n",
      "Epoch 105/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9215\n",
      "Epoch 106/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9232\n",
      "Epoch 107/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2106 - accuracy: 0.9203\n",
      "Epoch 108/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9228\n",
      "Epoch 109/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1959 - accuracy: 0.9219\n",
      "Epoch 110/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.9274\n",
      "Epoch 111/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2163 - accuracy: 0.9127\n",
      "Epoch 112/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9110\n",
      "Epoch 113/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2014 - accuracy: 0.9262\n",
      "Epoch 114/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9262\n",
      "Epoch 115/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9262\n",
      "Epoch 116/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2029 - accuracy: 0.9241\n",
      "Epoch 117/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9274\n",
      "Epoch 118/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9148\n",
      "Epoch 119/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2110 - accuracy: 0.9173\n",
      "Epoch 120/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9203\n",
      "Epoch 121/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1912 - accuracy: 0.9278\n",
      "Epoch 122/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9224\n",
      "Epoch 123/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9198\n",
      "Epoch 124/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9304\n",
      "Epoch 125/1500\n",
      "75/75 [==============================] - 0s 999us/step - loss: 0.2005 - accuracy: 0.9219\n",
      "Epoch 126/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9270\n",
      "Epoch 127/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9232\n",
      "Epoch 128/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1943 - accuracy: 0.9325\n",
      "Epoch 129/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9241\n",
      "Epoch 130/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.9241\n",
      "Epoch 131/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.9257\n",
      "Epoch 132/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1881 - accuracy: 0.9287\n",
      "Epoch 133/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.9262\n",
      "Epoch 134/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9304\n",
      "Epoch 135/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.9308\n",
      "Epoch 136/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9350\n",
      "Epoch 137/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1866 - accuracy: 0.9266\n",
      "Epoch 138/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.9363\n",
      "Epoch 139/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9354\n",
      "Epoch 140/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.9392\n",
      "Epoch 141/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.9329\n",
      "Epoch 142/1500\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1740 - accuracy: 0.9405\n",
      "Epoch 143/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1897 - accuracy: 0.9253\n",
      "Epoch 144/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9333\n",
      "Epoch 145/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9316\n",
      "Epoch 146/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.9325\n",
      "Epoch 147/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9371\n",
      "Epoch 148/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9414\n",
      "Epoch 149/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1677 - accuracy: 0.9384\n",
      "Epoch 150/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9371\n",
      "Epoch 151/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1742 - accuracy: 0.9329\n",
      "Epoch 152/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.9422\n",
      "Epoch 153/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1796 - accuracy: 0.9338\n",
      "Epoch 154/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.9363\n",
      "Epoch 155/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1779 - accuracy: 0.9346\n",
      "Epoch 156/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9295\n",
      "Epoch 157/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1657 - accuracy: 0.9392\n",
      "Epoch 158/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.9443\n",
      "Epoch 159/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1786 - accuracy: 0.9312\n",
      "Epoch 160/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1742 - accuracy: 0.9350\n",
      "Epoch 161/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1708 - accuracy: 0.9350\n",
      "Epoch 162/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1547 - accuracy: 0.9384\n",
      "Epoch 163/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9422\n",
      "Epoch 164/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1409 - accuracy: 0.9506\n",
      "Epoch 165/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9308\n",
      "Epoch 166/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9401\n",
      "Epoch 167/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9506\n",
      "Epoch 168/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1623 - accuracy: 0.9380\n",
      "Epoch 169/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9384\n",
      "Epoch 170/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.9485\n",
      "Epoch 171/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9422\n",
      "Epoch 172/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9447\n",
      "Epoch 173/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.9397\n",
      "Epoch 174/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9278\n",
      "Epoch 175/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1602 - accuracy: 0.9384\n",
      "Epoch 176/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9418\n",
      "Epoch 177/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9473\n",
      "Epoch 178/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.9435\n",
      "Epoch 179/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9532\n",
      "Epoch 180/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9456\n",
      "Epoch 181/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9447\n",
      "Epoch 182/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9473\n",
      "Epoch 183/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9532\n",
      "Epoch 184/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9477\n",
      "Epoch 185/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1518 - accuracy: 0.9468\n",
      "Epoch 186/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9489\n",
      "Epoch 187/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9451\n",
      "Epoch 188/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.9489\n",
      "Epoch 189/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9430\n",
      "Epoch 190/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9473\n",
      "Epoch 191/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1446 - accuracy: 0.9523\n",
      "Epoch 192/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.9485\n",
      "Epoch 193/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9473\n",
      "Epoch 194/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9443\n",
      "Epoch 195/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9439\n",
      "Epoch 196/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9477\n",
      "Epoch 197/1500\n",
      "75/75 [==============================] - 0s 999us/step - loss: 0.1231 - accuracy: 0.9599\n",
      "Epoch 198/1500\n",
      "75/75 [==============================] - 0s 967us/step - loss: 0.1497 - accuracy: 0.9447\n",
      "Epoch 199/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9435\n",
      "Epoch 200/1500\n",
      "75/75 [==============================] - 0s 994us/step - loss: 0.1644 - accuracy: 0.9376\n",
      "Epoch 201/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9536\n",
      "Epoch 202/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9502\n",
      "Epoch 203/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9519\n",
      "Epoch 204/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9418\n",
      "Epoch 205/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9460\n",
      "Epoch 206/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1376 - accuracy: 0.9477\n",
      "Epoch 207/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9549\n",
      "Epoch 208/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9536\n",
      "Epoch 209/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1521 - accuracy: 0.9473\n",
      "Epoch 210/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9553\n",
      "Epoch 211/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9481\n",
      "Epoch 212/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9498\n",
      "Epoch 213/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9549\n",
      "Epoch 214/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9532\n",
      "Epoch 215/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9498\n",
      "Epoch 216/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9557\n",
      "Epoch 217/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9557\n",
      "Epoch 218/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9464\n",
      "Epoch 219/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9532\n",
      "Epoch 220/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9468\n",
      "Epoch 221/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9477\n",
      "Epoch 222/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9418\n",
      "Epoch 223/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9578\n",
      "Epoch 224/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9565\n",
      "Epoch 225/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9612\n",
      "Epoch 226/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9506\n",
      "Epoch 227/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9557\n",
      "Epoch 228/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9481\n",
      "Epoch 229/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9464\n",
      "Epoch 230/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9502\n",
      "Epoch 231/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9414\n",
      "Epoch 232/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9498\n",
      "Epoch 233/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9511\n",
      "Epoch 234/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9586\n",
      "Epoch 235/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9477\n",
      "Epoch 236/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9565\n",
      "Epoch 237/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9451\n",
      "Epoch 238/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9502\n",
      "Epoch 239/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9477\n",
      "Epoch 240/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9532\n",
      "Epoch 241/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9481\n",
      "Epoch 242/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9527\n",
      "Epoch 243/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9456\n",
      "Epoch 244/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9515\n",
      "Epoch 245/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9527\n",
      "Epoch 246/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9595\n",
      "Epoch 247/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9608\n",
      "Epoch 248/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9388\n",
      "Epoch 249/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9536\n",
      "Epoch 250/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9565\n",
      "Epoch 251/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9557\n",
      "Epoch 252/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9637\n",
      "Epoch 253/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9629\n",
      "Epoch 254/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9633\n",
      "Epoch 255/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9532\n",
      "Epoch 256/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9637\n",
      "Epoch 257/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9620\n",
      "Epoch 258/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9544\n",
      "Epoch 259/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9489\n",
      "Epoch 260/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9582\n",
      "Epoch 261/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9599\n",
      "Epoch 262/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9485\n",
      "Epoch 263/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9544\n",
      "Epoch 264/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9570\n",
      "Epoch 265/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9473\n",
      "Epoch 266/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9536\n",
      "Epoch 267/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9561\n",
      "Epoch 268/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9561\n",
      "Epoch 269/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9561\n",
      "Epoch 270/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9574\n",
      "Epoch 271/1500\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1164 - accuracy: 0.9599\n",
      "Epoch 272/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9582\n",
      "Epoch 273/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9620\n",
      "Epoch 274/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9641\n",
      "Epoch 275/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9637\n",
      "Epoch 276/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9591\n",
      "Epoch 277/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9641\n",
      "Epoch 278/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9671\n",
      "Epoch 279/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9612\n",
      "Epoch 280/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9591\n",
      "Epoch 281/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9565\n",
      "Epoch 282/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9523\n",
      "Epoch 283/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9646\n",
      "Epoch 284/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9561\n",
      "Epoch 285/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9608\n",
      "Epoch 286/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.9620\n",
      "Epoch 287/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9650\n",
      "Epoch 288/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9603\n",
      "Epoch 289/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9540\n",
      "Epoch 290/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9574\n",
      "Epoch 291/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9553\n",
      "Epoch 292/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9553\n",
      "Epoch 293/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9574\n",
      "Epoch 294/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9633\n",
      "Epoch 295/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9667\n",
      "Epoch 296/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9650\n",
      "Epoch 297/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9599\n",
      "Epoch 298/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9654\n",
      "Epoch 299/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9603\n",
      "Epoch 300/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9565\n",
      "Epoch 301/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9667\n",
      "Epoch 302/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9616\n",
      "Epoch 303/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.9675\n",
      "Epoch 304/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9738\n",
      "Epoch 305/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9620\n",
      "Epoch 306/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9633\n",
      "Epoch 307/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9633\n",
      "Epoch 308/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9624\n",
      "Epoch 309/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9654\n",
      "Epoch 310/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9675\n",
      "Epoch 311/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9662\n",
      "Epoch 312/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.9612\n",
      "Epoch 313/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9650\n",
      "Epoch 314/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9603\n",
      "Epoch 315/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9654\n",
      "Epoch 316/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9646\n",
      "Epoch 317/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9629\n",
      "Epoch 318/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9654\n",
      "Epoch 319/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9637\n",
      "Epoch 320/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9641\n",
      "Epoch 321/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9667\n",
      "Epoch 322/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9646\n",
      "Epoch 323/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9624\n",
      "Epoch 324/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1111 - accuracy: 0.9641\n",
      "Epoch 325/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9629\n",
      "Epoch 326/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9671\n",
      "Epoch 327/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9620\n",
      "Epoch 328/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9662\n",
      "Epoch 329/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9650\n",
      "Epoch 330/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9603\n",
      "Epoch 331/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9650\n",
      "Epoch 332/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9620\n",
      "Epoch 333/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9658\n",
      "Epoch 334/1500\n",
      "50/75 [===================>..........] - ETA: 0s - loss: 0.1030 - accuracy: 0.9625Restoring model weights from the end of the best epoch: 304.\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9675\n",
      "Epoch 334: early stopping\n",
      "6/6 [==============================] - 0s 893us/step - loss: 0.4954 - accuracy: 0.7956\n",
      "6/6 [==============================] - 0s 695us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (24/30)\n",
      "Before appending - Cat IDs: 444, Predictions: 444, Actuals: 444, Gender: 444\n",
      "After appending - Cat IDs: 625, Predictions: 625, Actuals: 625, Gender: 625\n",
      "Final Test Results - Loss: 0.4954432249069214, Accuracy: 0.7955800890922546, Precision: 0.8116466437077124, Recall: 0.6962604250739844, F1 Score: 0.7385052745108007\n",
      "Confusion Matrix:\n",
      " [[106   1  11]\n",
      " [  9  12   0]\n",
      " [ 16   0  26]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "116A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "016A    10\n",
      "005A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "050A     7\n",
      "117A     7\n",
      "037A     6\n",
      "053A     6\n",
      "008A     6\n",
      "109A     6\n",
      "023A     6\n",
      "044A     5\n",
      "023B     5\n",
      "070A     5\n",
      "075A     5\n",
      "009A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "060A     3\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "025B     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "032A     2\n",
      "018A     2\n",
      "069A     2\n",
      "092A     1\n",
      "100A     1\n",
      "096A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "041A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "066A     1\n",
      "091A     1\n",
      "076A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "055A    20\n",
      "101A    15\n",
      "028A    13\n",
      "002A    13\n",
      "025A    11\n",
      "033A     9\n",
      "031A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "034A     5\n",
      "025C     5\n",
      "021A     5\n",
      "026A     4\n",
      "035A     4\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "093A     2\n",
      "049A     1\n",
      "026C     1\n",
      "073A     1\n",
      "088A     1\n",
      "090A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    277\n",
      "M    271\n",
      "F    151\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    101\n",
      "X     71\n",
      "M     66\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 071A, 097B, 019A, 074A, 067...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 050...\n",
      "senior    [097A, 106A, 104A, 059A, 116A, 051B, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 103A, 028A, 101A, 034A, 002A, 099...\n",
      "kitten                                         [047A, 049A]\n",
      "senior           [093A, 057A, 055A, 113A, 056A, 108A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 14, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 2, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "Length of X_train_val:\n",
      "699\n",
      "Length of y_train_val:\n",
      "699\n",
      "Length of groups_train_val:\n",
      "699\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 882, 1: 710, 2: 580})\n",
      "Epoch 1/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.9652 - accuracy: 0.5893\n",
      "Epoch 2/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7079 - accuracy: 0.7109\n",
      "Epoch 3/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6535 - accuracy: 0.7279\n",
      "Epoch 4/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6069 - accuracy: 0.7578\n",
      "Epoch 5/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5624 - accuracy: 0.7744\n",
      "Epoch 6/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5583 - accuracy: 0.7767\n",
      "Epoch 7/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5447 - accuracy: 0.7795\n",
      "Epoch 8/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4941 - accuracy: 0.7970\n",
      "Epoch 9/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4905 - accuracy: 0.8066\n",
      "Epoch 10/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4738 - accuracy: 0.8140\n",
      "Epoch 11/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4709 - accuracy: 0.8135\n",
      "Epoch 12/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4683 - accuracy: 0.8108\n",
      "Epoch 13/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4410 - accuracy: 0.8264\n",
      "Epoch 14/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4304 - accuracy: 0.8218\n",
      "Epoch 15/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4043 - accuracy: 0.8476\n",
      "Epoch 16/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4070 - accuracy: 0.8361\n",
      "Epoch 17/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3913 - accuracy: 0.8416\n",
      "Epoch 18/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3761 - accuracy: 0.8453\n",
      "Epoch 19/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3948 - accuracy: 0.8398\n",
      "Epoch 20/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3612 - accuracy: 0.8522\n",
      "Epoch 21/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3694 - accuracy: 0.8577\n",
      "Epoch 22/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3678 - accuracy: 0.8430\n",
      "Epoch 23/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3852 - accuracy: 0.8513\n",
      "Epoch 24/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3509 - accuracy: 0.8646\n",
      "Epoch 25/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3441 - accuracy: 0.8692\n",
      "Epoch 26/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3484 - accuracy: 0.8559\n",
      "Epoch 27/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3327 - accuracy: 0.8665\n",
      "Epoch 28/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3249 - accuracy: 0.8697\n",
      "Epoch 29/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3367 - accuracy: 0.8651\n",
      "Epoch 30/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3118 - accuracy: 0.8748\n",
      "Epoch 31/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3225 - accuracy: 0.8697\n",
      "Epoch 32/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3172 - accuracy: 0.8743\n",
      "Epoch 33/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3196 - accuracy: 0.8752\n",
      "Epoch 34/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2979 - accuracy: 0.8840\n",
      "Epoch 35/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2949 - accuracy: 0.8840\n",
      "Epoch 36/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3013 - accuracy: 0.8803\n",
      "Epoch 37/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2842 - accuracy: 0.8877\n",
      "Epoch 38/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2974 - accuracy: 0.8752\n",
      "Epoch 39/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2857 - accuracy: 0.8923\n",
      "Epoch 40/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2641 - accuracy: 0.8913\n",
      "Epoch 41/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2868 - accuracy: 0.8881\n",
      "Epoch 42/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2959 - accuracy: 0.8835\n",
      "Epoch 43/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2534 - accuracy: 0.8996\n",
      "Epoch 44/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2670 - accuracy: 0.8973\n",
      "Epoch 45/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2628 - accuracy: 0.8992\n",
      "Epoch 46/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2632 - accuracy: 0.9015\n",
      "Epoch 47/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2268 - accuracy: 0.9111\n",
      "Epoch 48/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2451 - accuracy: 0.9056\n",
      "Epoch 49/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2584 - accuracy: 0.9024\n",
      "Epoch 50/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9084\n",
      "Epoch 51/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2397 - accuracy: 0.9107\n",
      "Epoch 52/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2373 - accuracy: 0.9079\n",
      "Epoch 53/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.9079\n",
      "Epoch 54/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2466 - accuracy: 0.9024\n",
      "Epoch 55/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2388 - accuracy: 0.9070\n",
      "Epoch 56/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2263 - accuracy: 0.9125\n",
      "Epoch 57/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2479 - accuracy: 0.8983\n",
      "Epoch 58/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2239 - accuracy: 0.9162\n",
      "Epoch 59/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2111 - accuracy: 0.9176\n",
      "Epoch 60/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2238 - accuracy: 0.9079\n",
      "Epoch 61/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2207 - accuracy: 0.9148\n",
      "Epoch 62/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2503 - accuracy: 0.9042\n",
      "Epoch 63/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9107\n",
      "Epoch 64/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2263 - accuracy: 0.9162\n",
      "Epoch 65/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2203 - accuracy: 0.9185\n",
      "Epoch 66/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9217\n",
      "Epoch 67/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9199\n",
      "Epoch 68/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2182 - accuracy: 0.9167\n",
      "Epoch 69/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2084 - accuracy: 0.9263\n",
      "Epoch 70/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2120 - accuracy: 0.9203\n",
      "Epoch 71/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1963 - accuracy: 0.9282\n",
      "Epoch 72/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9273\n",
      "Epoch 73/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2171 - accuracy: 0.9203\n",
      "Epoch 74/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2051 - accuracy: 0.9190\n",
      "Epoch 75/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9291\n",
      "Epoch 76/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2067 - accuracy: 0.9203\n",
      "Epoch 77/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.9355\n",
      "Epoch 78/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2034 - accuracy: 0.9227\n",
      "Epoch 79/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9231\n",
      "Epoch 80/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1884 - accuracy: 0.9254\n",
      "Epoch 81/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.9180\n",
      "Epoch 82/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1941 - accuracy: 0.9236\n",
      "Epoch 83/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9360\n",
      "Epoch 84/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9360\n",
      "Epoch 85/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.9314\n",
      "Epoch 86/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1792 - accuracy: 0.9291\n",
      "Epoch 87/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.9305\n",
      "Epoch 88/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1806 - accuracy: 0.9355\n",
      "Epoch 89/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.1693 - accuracy: 0.9355\n",
      "Epoch 90/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9346\n",
      "Epoch 91/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1796 - accuracy: 0.9286\n",
      "Epoch 92/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9378\n",
      "Epoch 93/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1649 - accuracy: 0.9360\n",
      "Epoch 94/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9351\n",
      "Epoch 95/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9411\n",
      "Epoch 96/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1711 - accuracy: 0.9374\n",
      "Epoch 97/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1922 - accuracy: 0.9282\n",
      "Epoch 98/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1706 - accuracy: 0.9383\n",
      "Epoch 99/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9378\n",
      "Epoch 100/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1785 - accuracy: 0.9328\n",
      "Epoch 101/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9452\n",
      "Epoch 102/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9461\n",
      "Epoch 103/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9365\n",
      "Epoch 104/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9365\n",
      "Epoch 105/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9535\n",
      "Epoch 106/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9457\n",
      "Epoch 107/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9457\n",
      "Epoch 108/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9374\n",
      "Epoch 109/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9411\n",
      "Epoch 110/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9365\n",
      "Epoch 111/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9351\n",
      "Epoch 112/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9443\n",
      "Epoch 113/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1615 - accuracy: 0.9337\n",
      "Epoch 114/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9332\n",
      "Epoch 115/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9406\n",
      "Epoch 116/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9457\n",
      "Epoch 117/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9388\n",
      "Epoch 118/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9484\n",
      "Epoch 119/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.9429\n",
      "Epoch 120/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1595 - accuracy: 0.9383\n",
      "Epoch 121/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9563\n",
      "Epoch 122/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1388 - accuracy: 0.9503\n",
      "Epoch 123/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1397 - accuracy: 0.9466\n",
      "Epoch 124/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9397\n",
      "Epoch 125/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9475\n",
      "Epoch 126/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9466\n",
      "Epoch 127/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.9507\n",
      "Epoch 128/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9572\n",
      "Epoch 129/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1266 - accuracy: 0.9576\n",
      "Epoch 130/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.9457\n",
      "Epoch 131/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9503\n",
      "Epoch 132/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9517\n",
      "Epoch 133/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9457\n",
      "Epoch 134/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9461\n",
      "Epoch 135/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9563\n",
      "Epoch 136/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9484\n",
      "Epoch 137/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9480\n",
      "Epoch 138/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9443\n",
      "Epoch 139/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9576\n",
      "Epoch 140/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9581\n",
      "Epoch 141/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9572\n",
      "Epoch 142/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9540\n",
      "Epoch 143/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9535\n",
      "Epoch 144/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9507\n",
      "Epoch 145/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9622\n",
      "Epoch 146/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9480\n",
      "Epoch 147/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9512\n",
      "Epoch 148/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9507\n",
      "Epoch 149/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9599\n",
      "Epoch 150/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9535\n",
      "Epoch 151/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.9475\n",
      "Epoch 152/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9572\n",
      "Epoch 153/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9540\n",
      "Epoch 154/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.9599\n",
      "Epoch 155/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9526\n",
      "Epoch 156/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1365 - accuracy: 0.9576\n",
      "Epoch 157/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9558\n",
      "Epoch 158/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9572\n",
      "Epoch 159/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9604\n",
      "Epoch 160/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9586\n",
      "Epoch 161/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9512\n",
      "Epoch 162/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9512\n",
      "Epoch 163/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9572\n",
      "Epoch 164/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9659\n",
      "Epoch 165/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9507\n",
      "Epoch 166/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9636\n",
      "Epoch 167/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9613\n",
      "Epoch 168/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1266 - accuracy: 0.9521\n",
      "Epoch 169/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9604\n",
      "Epoch 170/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9581\n",
      "Epoch 171/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1409 - accuracy: 0.9443\n",
      "Epoch 172/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9563\n",
      "Epoch 173/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9521\n",
      "Epoch 174/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9604\n",
      "Epoch 175/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9576\n",
      "Epoch 176/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9678\n",
      "Epoch 177/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9581\n",
      "Epoch 178/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9692\n",
      "Epoch 179/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9586\n",
      "Epoch 180/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9664\n",
      "Epoch 181/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9636\n",
      "Epoch 182/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9457\n",
      "Epoch 183/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9471\n",
      "Epoch 184/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9659\n",
      "Epoch 185/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9576\n",
      "Epoch 186/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9636\n",
      "Epoch 187/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9632\n",
      "Epoch 188/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9613\n",
      "Epoch 189/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9650\n",
      "Epoch 190/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9664\n",
      "Epoch 191/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9632\n",
      "Epoch 192/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9632\n",
      "Epoch 193/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9613\n",
      "Epoch 194/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9641\n",
      "Epoch 195/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9549\n",
      "Epoch 196/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9641\n",
      "Epoch 197/1500\n",
      "50/68 [=====================>........] - ETA: 0s - loss: 0.1246 - accuracy: 0.9563Restoring model weights from the end of the best epoch: 167.\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9563\n",
      "Epoch 197: early stopping\n",
      "8/8 [==============================] - 0s 867us/step - loss: 1.0137 - accuracy: 0.6092\n",
      "8/8 [==============================] - 0s 667us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (21/29)\n",
      "Before appending - Cat IDs: 625, Predictions: 625, Actuals: 625, Gender: 625\n",
      "After appending - Cat IDs: 863, Predictions: 863, Actuals: 863, Gender: 863\n",
      "Final Test Results - Loss: 1.0137059688568115, Accuracy: 0.6092436909675598, Precision: 0.644027269027269, Recall: 0.5385008285850491, F1 Score: 0.5754705913720471\n",
      "Confusion Matrix:\n",
      " [[111   1  35]\n",
      " [ 12  17   0]\n",
      " [ 45   0  17]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.661462263377996\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 1.0172970592975616\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.6973535567522049\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6977712790692565\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6643170681712919\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[2]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be692ae4-6d3f-4353-b5bf-554d20da4df3",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8da9a092-ed2e-4397-a6c8-2c4888735265",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 863, Predictions: 863, Actuals: 863, Gender: 863\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51cf386a-c49e-4716-ba15-aa3b7930419a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8d43ac5-d50e-430d-98a1-ff4f45006bae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.71 (78/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccc9acb7-bb1b-42a6-bb25-cdf5a3356315",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95e69b27-cae1-4a3a-ba70-5244a11aadf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, senior, kitten, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, adult...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten, kitten, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, adult,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, kitten, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[senior, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, kitten, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, adult, a...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, senior, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, senior, adult, senior, senior, kitten...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, kitten, senior, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, senior, kitten, senior, adult, adult, ...         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, senior, senior, senior, senior, adult...        senior           senior                   True\n",
       "59    053A       [adult, adult, senior, senior, adult, adult]         adult            adult                   True\n",
       "56    051A  [adult, senior, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, adult, adult, kitten, kitten, kitten, ...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A  [kitten, adult, kitten, kitten, kitten, adult,...        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "48    042A  [adult, kitten, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A    [senior, senior, senior, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                     [senior, adult, adult, senior]         adult            adult                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, kitten, ...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, adult, adult, a...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "43    037A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "17    015A  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "32    026B                                            [adult]         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "34    027A  [adult, senior, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "8     007A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, senior, adult]         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "40    034A               [adult, senior, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "4     003A                     [adult, senior, adult, senior]         adult            adult                   True\n",
       "90    095A  [senior, senior, adult, senior, senior, kitten...        senior            adult                  False\n",
       "99    104A                     [adult, senior, adult, senior]         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "101   106A  [adult, senior, adult, senior, adult, adult, a...         adult           senior                  False\n",
       "96    101A  [adult, adult, senior, senior, adult, senior, ...        senior            adult                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "62    056A                             [senior, adult, adult]         adult           senior                  False\n",
       "42    036A  [senior, senior, senior, senior, adult, senior...        senior            adult                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "36    029A  [senior, senior, adult, senior, adult, senior,...        senior            adult                  False\n",
       "57    051B  [adult, adult, senior, adult, senior, adult, a...         adult           senior                  False\n",
       "58    052A                   [senior, senior, senior, senior]        senior            adult                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "60    054A                                    [adult, senior]         adult           senior                  False\n",
       "61    055A  [adult, adult, senior, adult, adult, adult, ad...         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "85    090A                                            [adult]         adult           senior                  False\n",
       "64    058A                             [senior, adult, adult]         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                    [senior, adult]         adult           senior                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "71    065A  [senior, adult, adult, adult, kitten, senior, ...        kitten            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, s...        senior            adult                  False\n",
       "18    016A  [senior, adult, adult, adult, senior, senior, ...         adult           senior                  False\n",
       "109   117A  [senior, senior, adult, adult, senior, adult, ...         adult           senior                  False"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d36b3c54-3377-4249-a774-6d31557e36da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     58\n",
      "kitten    13\n",
      "senior     7\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b36eb8a4-57f3-48c0-b92c-4a8e5a52c59e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             58  79.452055\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22              7  31.818182\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1750e2da-df8c-4f00-b860-539dd822864f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6b34e-2368-4956-8241-8e6ab6e8cf81",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fdefd9be-6e6d-4903-a0ec-7824da4313d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          572            430  75.174825\n",
      "1           kitten          113             87  76.991150\n",
      "2           senior          178             80  44.943820\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57576e25-349c-46e0-b57b-8bf3cee3b5de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b69785-d4c1-49ab-ba75-4a733326cdc1",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "15c5b6e5-d7b0-4089-ae78-3dafe1cead3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      143     67.14\n",
      "1          M    360      261     72.50\n",
      "2          X    290      193     66.55\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b6001f1b-3f01-42e9-8761-4253acbed5e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      143     67.14\n",
      "1          M    360      261     72.50\n",
      "2          X    290      193     66.55\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b5d44-f489-4de0-8785-4bfa52d09797",
   "metadata": {},
   "source": [
    "# RANDOM SEED 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c4439fc4-670c-4009-8ca9-23c419ee99ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[3])) \n",
    "np.random.seed(int(random_seeds[3]))\n",
    "tf.random.set_seed(int(random_seeds[3]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5fe04e3d-fd90-43c1-97af-5eac75e35f85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2ed5261d-2aeb-412e-a017-65b461a2cf5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70aa6-1716-400a-be48-cccb3511542e",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fb9d0248-481e-4807-85dd-407fa88963f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "072A     9\n",
      "015A     9\n",
      "051B     9\n",
      "022A     9\n",
      "033A     9\n",
      "045A     9\n",
      "095A     8\n",
      "013B     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "109A     6\n",
      "023A     6\n",
      "037A     6\n",
      "075A     5\n",
      "070A     5\n",
      "025C     5\n",
      "021A     5\n",
      "034A     5\n",
      "044A     5\n",
      "023B     5\n",
      "003A     4\n",
      "105A     4\n",
      "035A     4\n",
      "026A     4\n",
      "052A     4\n",
      "062A     4\n",
      "012A     3\n",
      "113A     3\n",
      "058A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "025B     2\n",
      "032A     2\n",
      "093A     2\n",
      "054A     2\n",
      "069A     2\n",
      "087A     2\n",
      "038A     2\n",
      "073A     1\n",
      "004A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "091A     1\n",
      "019B     1\n",
      "066A     1\n",
      "048A     1\n",
      "092A     1\n",
      "026C     1\n",
      "076A     1\n",
      "043A     1\n",
      "041A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "019A    17\n",
      "101A    15\n",
      "039A    12\n",
      "063A    11\n",
      "071A    10\n",
      "005A    10\n",
      "065A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "099A     7\n",
      "008A     6\n",
      "009A     4\n",
      "104A     4\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "096A     1\n",
      "049A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    296\n",
      "X    286\n",
      "F    208\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    62\n",
      "F    44\n",
      "M    41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 028...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 019A, 101A, 005A, 065A, 039A, 009A, 063...\n",
      "kitten                                         [050A, 049A]\n",
      "senior                       [104A, 056A, 094A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 14, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 2, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "Length of X_train_val:\n",
      "790\n",
      "Length of y_train_val:\n",
      "790\n",
      "Length of groups_train_val:\n",
      "790\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 936, 1: 815, 2: 795})\n",
      "Epoch 1/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.9373 - accuracy: 0.6017\n",
      "Epoch 2/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.7196 - accuracy: 0.7121\n",
      "Epoch 3/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.6262 - accuracy: 0.7361\n",
      "Epoch 4/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5948 - accuracy: 0.7561\n",
      "Epoch 5/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5638 - accuracy: 0.7581\n",
      "Epoch 6/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5236 - accuracy: 0.7859\n",
      "Epoch 7/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4986 - accuracy: 0.7926\n",
      "Epoch 8/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4910 - accuracy: 0.7997\n",
      "Epoch 9/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4730 - accuracy: 0.8036\n",
      "Epoch 10/1500\n",
      "80/80 [==============================] - 0s 953us/step - loss: 0.4750 - accuracy: 0.8091\n",
      "Epoch 11/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4441 - accuracy: 0.8185\n",
      "Epoch 12/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4408 - accuracy: 0.8162\n",
      "Epoch 13/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4114 - accuracy: 0.8335\n",
      "Epoch 14/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4185 - accuracy: 0.8240\n",
      "Epoch 15/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4185 - accuracy: 0.8346\n",
      "Epoch 16/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4031 - accuracy: 0.8366\n",
      "Epoch 17/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3801 - accuracy: 0.8445\n",
      "Epoch 18/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3845 - accuracy: 0.8437\n",
      "Epoch 19/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3645 - accuracy: 0.8452\n",
      "Epoch 20/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3610 - accuracy: 0.8488\n",
      "Epoch 21/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3617 - accuracy: 0.8535\n",
      "Epoch 22/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3603 - accuracy: 0.8476\n",
      "Epoch 23/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3240 - accuracy: 0.8782\n",
      "Epoch 24/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3351 - accuracy: 0.8633\n",
      "Epoch 25/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.8708\n",
      "Epoch 26/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3210 - accuracy: 0.8649\n",
      "Epoch 27/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3227 - accuracy: 0.8720\n",
      "Epoch 28/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3073 - accuracy: 0.8700\n",
      "Epoch 29/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3122 - accuracy: 0.8739\n",
      "Epoch 30/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3172 - accuracy: 0.8704\n",
      "Epoch 31/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3424 - accuracy: 0.8578\n",
      "Epoch 32/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8696\n",
      "Epoch 33/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2845 - accuracy: 0.8900\n",
      "Epoch 34/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.8833\n",
      "Epoch 35/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2884 - accuracy: 0.8865\n",
      "Epoch 36/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2869 - accuracy: 0.8857\n",
      "Epoch 37/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3029 - accuracy: 0.8790\n",
      "Epoch 38/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3067 - accuracy: 0.8810\n",
      "Epoch 39/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3176 - accuracy: 0.8790\n",
      "Epoch 40/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2850 - accuracy: 0.8873\n",
      "Epoch 41/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2863 - accuracy: 0.8908\n",
      "Epoch 42/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2717 - accuracy: 0.8947\n",
      "Epoch 43/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2891 - accuracy: 0.8810\n",
      "Epoch 44/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2710 - accuracy: 0.8928\n",
      "Epoch 45/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2682 - accuracy: 0.8959\n",
      "Epoch 46/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2546 - accuracy: 0.8975\n",
      "Epoch 47/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2577 - accuracy: 0.8995\n",
      "Epoch 48/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2551 - accuracy: 0.8967\n",
      "Epoch 49/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2659 - accuracy: 0.8963\n",
      "Epoch 50/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2621 - accuracy: 0.9010\n",
      "Epoch 51/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.9014\n",
      "Epoch 52/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2429 - accuracy: 0.9042\n",
      "Epoch 53/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2401 - accuracy: 0.9010\n",
      "Epoch 54/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2547 - accuracy: 0.8971\n",
      "Epoch 55/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2354 - accuracy: 0.9077\n",
      "Epoch 56/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.9042\n",
      "Epoch 57/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2270 - accuracy: 0.9136\n",
      "Epoch 58/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2416 - accuracy: 0.9085\n",
      "Epoch 59/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.9101\n",
      "Epoch 60/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2285 - accuracy: 0.9077\n",
      "Epoch 61/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2282 - accuracy: 0.9097\n",
      "Epoch 62/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2286 - accuracy: 0.9136\n",
      "Epoch 63/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2256 - accuracy: 0.9128\n",
      "Epoch 64/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2061 - accuracy: 0.9199\n",
      "Epoch 65/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9116\n",
      "Epoch 66/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2151 - accuracy: 0.9179\n",
      "Epoch 67/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2155 - accuracy: 0.9175\n",
      "Epoch 68/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1998 - accuracy: 0.9297\n",
      "Epoch 69/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2253 - accuracy: 0.9112\n",
      "Epoch 70/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9234\n",
      "Epoch 71/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2286 - accuracy: 0.9081\n",
      "Epoch 72/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2109 - accuracy: 0.9199\n",
      "Epoch 73/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1988 - accuracy: 0.9254\n",
      "Epoch 74/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2201 - accuracy: 0.9132\n",
      "Epoch 75/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9250\n",
      "Epoch 76/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9218\n",
      "Epoch 77/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9289\n",
      "Epoch 78/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.9250\n",
      "Epoch 79/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2061 - accuracy: 0.9226\n",
      "Epoch 80/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2072 - accuracy: 0.9262\n",
      "Epoch 81/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2135 - accuracy: 0.9171\n",
      "Epoch 82/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9179\n",
      "Epoch 83/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1906 - accuracy: 0.9281\n",
      "Epoch 84/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2083 - accuracy: 0.9152\n",
      "Epoch 85/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1900 - accuracy: 0.9266\n",
      "Epoch 86/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.9277\n",
      "Epoch 87/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9305\n",
      "Epoch 88/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9285\n",
      "Epoch 89/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1938 - accuracy: 0.9289\n",
      "Epoch 90/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1897 - accuracy: 0.9203\n",
      "Epoch 91/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1651 - accuracy: 0.9375\n",
      "Epoch 92/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9175\n",
      "Epoch 93/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.9293\n",
      "Epoch 94/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1771 - accuracy: 0.9375\n",
      "Epoch 95/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.9258\n",
      "Epoch 96/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9368\n",
      "Epoch 97/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1808 - accuracy: 0.9313\n",
      "Epoch 98/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9305\n",
      "Epoch 99/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.9336\n",
      "Epoch 100/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.9340\n",
      "Epoch 101/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.9179\n",
      "Epoch 102/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.9360\n",
      "Epoch 103/1500\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.1660 - accuracy: 0.9407\n",
      "Epoch 104/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1678 - accuracy: 0.9293\n",
      "Epoch 105/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9289\n",
      "Epoch 106/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1880 - accuracy: 0.9332\n",
      "Epoch 107/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9352\n",
      "Epoch 108/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9411\n",
      "Epoch 109/1500\n",
      "80/80 [==============================] - 0s 995us/step - loss: 0.1663 - accuracy: 0.9379\n",
      "Epoch 110/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9281\n",
      "Epoch 111/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9340\n",
      "Epoch 112/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9364\n",
      "Epoch 113/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1710 - accuracy: 0.9317\n",
      "Epoch 114/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1577 - accuracy: 0.9438\n",
      "Epoch 115/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9430\n",
      "Epoch 116/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9391\n",
      "Epoch 117/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9383\n",
      "Epoch 118/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9407\n",
      "Epoch 119/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9427\n",
      "Epoch 120/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1569 - accuracy: 0.9407\n",
      "Epoch 121/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9438\n",
      "Epoch 122/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9391\n",
      "Epoch 123/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9434\n",
      "Epoch 124/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1618 - accuracy: 0.9379\n",
      "Epoch 125/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9403\n",
      "Epoch 126/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.9478\n",
      "Epoch 127/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9399\n",
      "Epoch 128/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9450\n",
      "Epoch 129/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1643 - accuracy: 0.9419\n",
      "Epoch 130/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9419\n",
      "Epoch 131/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1584 - accuracy: 0.9434\n",
      "Epoch 132/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9466\n",
      "Epoch 133/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9478\n",
      "Epoch 134/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1439 - accuracy: 0.9489\n",
      "Epoch 135/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9344\n",
      "Epoch 136/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9427\n",
      "Epoch 137/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1508 - accuracy: 0.9482\n",
      "Epoch 138/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9462\n",
      "Epoch 139/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.9533\n",
      "Epoch 140/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9474\n",
      "Epoch 141/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9509\n",
      "Epoch 142/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9548\n",
      "Epoch 143/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9419\n",
      "Epoch 144/1500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.1405 - accuracy: 0.9529\n",
      "Epoch 145/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9450\n",
      "Epoch 146/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1404 - accuracy: 0.9485\n",
      "Epoch 147/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9466\n",
      "Epoch 148/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9485\n",
      "Epoch 149/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9544\n",
      "Epoch 150/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9584\n",
      "Epoch 151/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9399\n",
      "Epoch 152/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9485\n",
      "Epoch 153/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9458\n",
      "Epoch 154/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9517\n",
      "Epoch 155/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1351 - accuracy: 0.9482\n",
      "Epoch 156/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9517\n",
      "Epoch 157/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9497\n",
      "Epoch 158/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9517\n",
      "Epoch 159/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9450\n",
      "Epoch 160/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9533\n",
      "Epoch 161/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9513\n",
      "Epoch 162/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9568\n",
      "Epoch 163/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9446\n",
      "Epoch 164/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9611\n",
      "Epoch 165/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9470\n",
      "Epoch 166/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9462\n",
      "Epoch 167/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9501\n",
      "Epoch 168/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9533\n",
      "Epoch 169/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9521\n",
      "Epoch 170/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9540\n",
      "Epoch 171/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9505\n",
      "Epoch 172/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9564\n",
      "Epoch 173/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9576\n",
      "Epoch 174/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9552\n",
      "Epoch 175/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9572\n",
      "Epoch 176/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9529\n",
      "Epoch 177/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9458\n",
      "Epoch 178/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9466\n",
      "Epoch 179/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9568\n",
      "Epoch 180/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9462\n",
      "Epoch 181/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9497\n",
      "Epoch 182/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9540\n",
      "Epoch 183/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9485\n",
      "Epoch 184/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9599\n",
      "Epoch 185/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9525\n",
      "Epoch 186/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9580\n",
      "Epoch 187/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9470\n",
      "Epoch 188/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9552\n",
      "Epoch 189/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.9493\n",
      "Epoch 190/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9631\n",
      "Epoch 191/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9552\n",
      "Epoch 192/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9474\n",
      "Epoch 193/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9497\n",
      "Epoch 194/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9670\n",
      "Epoch 195/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9560\n",
      "Epoch 196/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9584\n",
      "Epoch 197/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9548\n",
      "Epoch 198/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9497\n",
      "Epoch 199/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9580\n",
      "Epoch 200/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9544\n",
      "Epoch 201/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9560\n",
      "Epoch 202/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.9623\n",
      "Epoch 203/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9576\n",
      "Epoch 204/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9592\n",
      "Epoch 205/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9584\n",
      "Epoch 206/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9599\n",
      "Epoch 207/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9607\n",
      "Epoch 208/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9548\n",
      "Epoch 209/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9650\n",
      "Epoch 210/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9521\n",
      "Epoch 211/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9619\n",
      "Epoch 212/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9560\n",
      "Epoch 213/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9607\n",
      "Epoch 214/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9611\n",
      "Epoch 215/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9588\n",
      "Epoch 216/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9595\n",
      "Epoch 217/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9631\n",
      "Epoch 218/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9560\n",
      "Epoch 219/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9607\n",
      "Epoch 220/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9595\n",
      "Epoch 221/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9619\n",
      "Epoch 222/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9611\n",
      "Epoch 223/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9603\n",
      "Epoch 224/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9650\n",
      "Epoch 225/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9615\n",
      "Epoch 226/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9595\n",
      "Epoch 227/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9576\n",
      "Epoch 228/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9635\n",
      "Epoch 229/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9572\n",
      "Epoch 230/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9654\n",
      "Epoch 231/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9623\n",
      "Epoch 232/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9564\n",
      "Epoch 233/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9603\n",
      "Epoch 234/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9654\n",
      "Epoch 235/1500\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.0905 - accuracy: 0.9698\n",
      "Epoch 236/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9584\n",
      "Epoch 237/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9607\n",
      "Epoch 238/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9595\n",
      "Epoch 239/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9662\n",
      "Epoch 240/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9615\n",
      "Epoch 241/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9639\n",
      "Epoch 242/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9615\n",
      "Epoch 243/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9701\n",
      "Epoch 244/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9592\n",
      "Epoch 245/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9595\n",
      "Epoch 246/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1031 - accuracy: 0.9650\n",
      "Epoch 247/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9643\n",
      "Epoch 248/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9643\n",
      "Epoch 249/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.9674\n",
      "Epoch 250/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9619\n",
      "Epoch 251/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9647\n",
      "Epoch 252/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9670\n",
      "Epoch 253/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9611\n",
      "Epoch 254/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9698\n",
      "Epoch 255/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9650\n",
      "Epoch 256/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.9572\n",
      "Epoch 257/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9674\n",
      "Epoch 258/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9643\n",
      "Epoch 259/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9552\n",
      "Epoch 260/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9595\n",
      "Epoch 261/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0851 - accuracy: 0.9705\n",
      "Epoch 262/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9599\n",
      "Epoch 263/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9584\n",
      "Epoch 264/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9741\n",
      "Epoch 265/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9737\n",
      "Epoch 266/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9674\n",
      "Epoch 267/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9564\n",
      "Epoch 268/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9654\n",
      "Epoch 269/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9588\n",
      "Epoch 270/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9607\n",
      "Epoch 271/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9686\n",
      "Epoch 272/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9647\n",
      "Epoch 273/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9666\n",
      "Epoch 274/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9662\n",
      "Epoch 275/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9607\n",
      "Epoch 276/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9607\n",
      "Epoch 277/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9717\n",
      "Epoch 278/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9686\n",
      "Epoch 279/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9540\n",
      "Epoch 280/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9760\n",
      "Epoch 281/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9650\n",
      "Epoch 282/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.9682\n",
      "Epoch 283/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.9662\n",
      "Epoch 284/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9635\n",
      "Epoch 285/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.9650\n",
      "Epoch 286/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9592\n",
      "Epoch 287/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9639\n",
      "Epoch 288/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9686\n",
      "Epoch 289/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9694\n",
      "Epoch 290/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9647\n",
      "Epoch 291/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9733\n",
      "Epoch 292/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9678\n",
      "Epoch 293/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.9674\n",
      "Epoch 294/1500\n",
      "49/80 [=================>............] - ETA: 0s - loss: 0.0806 - accuracy: 0.9796Restoring model weights from the end of the best epoch: 264.\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9753\n",
      "Epoch 294: early stopping\n",
      "5/5 [==============================] - 0s 916us/step - loss: 0.6850 - accuracy: 0.7687\n",
      "5/5 [==============================] - 0s 920us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.84 (21/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "Final Test Results - Loss: 0.6850073933601379, Accuracy: 0.7687074542045593, Precision: 0.6325745388245388, Recall: 0.8317251461988304, F1 Score: 0.6899551066217734\n",
      "Confusion Matrix:\n",
      " [[91  6 23]\n",
      " [ 0  8  0]\n",
      " [ 5  0 14]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "042A    14\n",
      "059A    14\n",
      "001A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "025A    11\n",
      "036A    11\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "065A     9\n",
      "051B     9\n",
      "072A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "099A     7\n",
      "008A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "026A     4\n",
      "009A     4\n",
      "105A     4\n",
      "035A     4\n",
      "003A     4\n",
      "104A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "014A     3\n",
      "012A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "018A     2\n",
      "032A     2\n",
      "093A     2\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "048A     1\n",
      "096A     1\n",
      "088A     1\n",
      "076A     1\n",
      "091A     1\n",
      "115A     1\n",
      "110A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "097B    14\n",
      "068A    11\n",
      "015A     9\n",
      "045A     9\n",
      "033A     9\n",
      "109A     6\n",
      "053A     6\n",
      "075A     5\n",
      "025C     5\n",
      "062A     4\n",
      "060A     3\n",
      "025B     2\n",
      "087A     2\n",
      "038A     2\n",
      "054A     2\n",
      "041A     1\n",
      "043A     1\n",
      "026C     1\n",
      "066A     1\n",
      "004A     1\n",
      "019B     1\n",
      "090A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    299\n",
      "F    216\n",
      "M    214\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    123\n",
      "X     49\n",
      "F     36\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 071A, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 050...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 097B, 067A, 020A, 062A, 002B, 029...\n",
      "kitten                             [109A, 043A, 041A, 045A]\n",
      "senior                             [055A, 054A, 090A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "Length of X_train_val:\n",
      "729\n",
      "Length of y_train_val:\n",
      "729\n",
      "Length of groups_train_val:\n",
      "729\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 842, 1: 770, 2: 770})\n",
      "Epoch 1/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.8810 - accuracy: 0.6146\n",
      "Epoch 2/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.7096 - accuracy: 0.7103\n",
      "Epoch 3/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6529 - accuracy: 0.7246\n",
      "Epoch 4/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5846 - accuracy: 0.7636\n",
      "Epoch 5/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5486 - accuracy: 0.7662\n",
      "Epoch 6/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5766 - accuracy: 0.7557\n",
      "Epoch 7/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5126 - accuracy: 0.7955\n",
      "Epoch 8/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5037 - accuracy: 0.7926\n",
      "Epoch 9/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5177 - accuracy: 0.7851\n",
      "Epoch 10/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4903 - accuracy: 0.8027\n",
      "Epoch 11/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4476 - accuracy: 0.8186\n",
      "Epoch 12/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4654 - accuracy: 0.8111\n",
      "Epoch 13/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.8119\n",
      "Epoch 14/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4575 - accuracy: 0.8102\n",
      "Epoch 15/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4072 - accuracy: 0.8300\n",
      "Epoch 16/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4283 - accuracy: 0.8254\n",
      "Epoch 17/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4072 - accuracy: 0.8308\n",
      "Epoch 18/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4047 - accuracy: 0.8296\n",
      "Epoch 19/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3900 - accuracy: 0.8350\n",
      "Epoch 20/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3795 - accuracy: 0.8413\n",
      "Epoch 21/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3729 - accuracy: 0.8484\n",
      "Epoch 22/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3627 - accuracy: 0.8484\n",
      "Epoch 23/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3748 - accuracy: 0.8430\n",
      "Epoch 24/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3634 - accuracy: 0.8421\n",
      "Epoch 25/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3612 - accuracy: 0.8497\n",
      "Epoch 26/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3380 - accuracy: 0.8514\n",
      "Epoch 27/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3480 - accuracy: 0.8560\n",
      "Epoch 28/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3412 - accuracy: 0.8606\n",
      "Epoch 29/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3488 - accuracy: 0.8644\n",
      "Epoch 30/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8652\n",
      "Epoch 31/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3272 - accuracy: 0.8657\n",
      "Epoch 32/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3056 - accuracy: 0.8795\n",
      "Epoch 33/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3068 - accuracy: 0.8774\n",
      "Epoch 34/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2997 - accuracy: 0.8808\n",
      "Epoch 35/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3153 - accuracy: 0.8703\n",
      "Epoch 36/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2953 - accuracy: 0.8715\n",
      "Epoch 37/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8816\n",
      "Epoch 38/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2941 - accuracy: 0.8846\n",
      "Epoch 39/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2905 - accuracy: 0.8825\n",
      "Epoch 40/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2764 - accuracy: 0.8896\n",
      "Epoch 41/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2854 - accuracy: 0.8866\n",
      "Epoch 42/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2991 - accuracy: 0.8833\n",
      "Epoch 43/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2659 - accuracy: 0.8913\n",
      "Epoch 44/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2815 - accuracy: 0.8866\n",
      "Epoch 45/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2737 - accuracy: 0.8934\n",
      "Epoch 46/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2628 - accuracy: 0.8917\n",
      "Epoch 47/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2540 - accuracy: 0.8963\n",
      "Epoch 48/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2521 - accuracy: 0.8955\n",
      "Epoch 49/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2812 - accuracy: 0.8871\n",
      "Epoch 50/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2677 - accuracy: 0.8904\n",
      "Epoch 51/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2632 - accuracy: 0.8887\n",
      "Epoch 52/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2503 - accuracy: 0.9060\n",
      "Epoch 53/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2472 - accuracy: 0.9051\n",
      "Epoch 54/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2464 - accuracy: 0.9039\n",
      "Epoch 55/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2700 - accuracy: 0.8892\n",
      "Epoch 56/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2477 - accuracy: 0.8997\n",
      "Epoch 57/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2361 - accuracy: 0.9051\n",
      "Epoch 58/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2428 - accuracy: 0.9043\n",
      "Epoch 59/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.9068\n",
      "Epoch 60/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2524 - accuracy: 0.8971\n",
      "Epoch 61/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2499 - accuracy: 0.8967\n",
      "Epoch 62/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2366 - accuracy: 0.9093\n",
      "Epoch 63/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2345 - accuracy: 0.9081\n",
      "Epoch 64/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2280 - accuracy: 0.9043\n",
      "Epoch 65/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2159 - accuracy: 0.9177\n",
      "Epoch 66/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2242 - accuracy: 0.9144\n",
      "Epoch 67/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2146 - accuracy: 0.9135\n",
      "Epoch 68/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2345 - accuracy: 0.9076\n",
      "Epoch 69/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2215 - accuracy: 0.9139\n",
      "Epoch 70/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9114\n",
      "Epoch 71/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2157 - accuracy: 0.9181\n",
      "Epoch 72/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2128 - accuracy: 0.9131\n",
      "Epoch 73/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2226 - accuracy: 0.9102\n",
      "Epoch 74/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2126 - accuracy: 0.9135\n",
      "Epoch 75/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2145 - accuracy: 0.9123\n",
      "Epoch 76/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9190\n",
      "Epoch 77/1500\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2057 - accuracy: 0.9190\n",
      "Epoch 78/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2162 - accuracy: 0.9160\n",
      "Epoch 79/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2028 - accuracy: 0.9198\n",
      "Epoch 80/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2158 - accuracy: 0.9160\n",
      "Epoch 81/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9223\n",
      "Epoch 82/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2174 - accuracy: 0.9198\n",
      "Epoch 83/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9232\n",
      "Epoch 84/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1890 - accuracy: 0.9249\n",
      "Epoch 85/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9240\n",
      "Epoch 86/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9274\n",
      "Epoch 87/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9165\n",
      "Epoch 88/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1911 - accuracy: 0.9295\n",
      "Epoch 89/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1879 - accuracy: 0.9257\n",
      "Epoch 90/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9345\n",
      "Epoch 91/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.9332\n",
      "Epoch 92/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1786 - accuracy: 0.9328\n",
      "Epoch 93/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9274\n",
      "Epoch 94/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2016 - accuracy: 0.9181\n",
      "Epoch 95/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.9291\n",
      "Epoch 96/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1901 - accuracy: 0.9257\n",
      "Epoch 97/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1721 - accuracy: 0.9374\n",
      "Epoch 98/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1930 - accuracy: 0.9265\n",
      "Epoch 99/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9244\n",
      "Epoch 100/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1901 - accuracy: 0.9215\n",
      "Epoch 101/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9307\n",
      "Epoch 102/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1709 - accuracy: 0.9353\n",
      "Epoch 103/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1865 - accuracy: 0.9286\n",
      "Epoch 104/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.9353\n",
      "Epoch 105/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1941 - accuracy: 0.9261\n",
      "Epoch 106/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9244\n",
      "Epoch 107/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1875 - accuracy: 0.9320\n",
      "Epoch 108/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1694 - accuracy: 0.9374\n",
      "Epoch 109/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.9337\n",
      "Epoch 110/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9349\n",
      "Epoch 111/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1657 - accuracy: 0.9358\n",
      "Epoch 112/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1786 - accuracy: 0.9341\n",
      "Epoch 113/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.9328\n",
      "Epoch 114/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9353\n",
      "Epoch 115/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.9299\n",
      "Epoch 116/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9332\n",
      "Epoch 117/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9316\n",
      "Epoch 118/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.9391\n",
      "Epoch 119/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9328\n",
      "Epoch 120/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.9463\n",
      "Epoch 121/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9395\n",
      "Epoch 122/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9421\n",
      "Epoch 123/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9307\n",
      "Epoch 124/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1699 - accuracy: 0.9337\n",
      "Epoch 125/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9429\n",
      "Epoch 126/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1640 - accuracy: 0.9370\n",
      "Epoch 127/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9467\n",
      "Epoch 128/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.9383\n",
      "Epoch 129/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1655 - accuracy: 0.9353\n",
      "Epoch 130/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9471\n",
      "Epoch 131/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9442\n",
      "Epoch 132/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9437\n",
      "Epoch 133/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9450\n",
      "Epoch 134/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1785 - accuracy: 0.9299\n",
      "Epoch 135/1500\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1628 - accuracy: 0.9358\n",
      "Epoch 136/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9425\n",
      "Epoch 137/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9446\n",
      "Epoch 138/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9463\n",
      "Epoch 139/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1412 - accuracy: 0.9513\n",
      "Epoch 140/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9492\n",
      "Epoch 141/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9484\n",
      "Epoch 142/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9446\n",
      "Epoch 143/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9534\n",
      "Epoch 144/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1494 - accuracy: 0.9408\n",
      "Epoch 145/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.9454\n",
      "Epoch 146/1500\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1358 - accuracy: 0.9492\n",
      "Epoch 147/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9450\n",
      "Epoch 148/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9526\n",
      "Epoch 149/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9467\n",
      "Epoch 150/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9484\n",
      "Epoch 151/1500\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1274 - accuracy: 0.9542\n",
      "Epoch 152/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9454\n",
      "Epoch 153/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9492\n",
      "Epoch 154/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9488\n",
      "Epoch 155/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1446 - accuracy: 0.9505\n",
      "Epoch 156/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9513\n",
      "Epoch 157/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1490 - accuracy: 0.9454\n",
      "Epoch 158/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.9458\n",
      "Epoch 159/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9509\n",
      "Epoch 160/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9454\n",
      "Epoch 161/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1404 - accuracy: 0.9500\n",
      "Epoch 162/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9530\n",
      "Epoch 163/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9484\n",
      "Epoch 164/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9526\n",
      "Epoch 165/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1244 - accuracy: 0.9555\n",
      "Epoch 166/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9479\n",
      "Epoch 167/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9538\n",
      "Epoch 168/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9542\n",
      "Epoch 169/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9526\n",
      "Epoch 170/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.9572\n",
      "Epoch 171/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9492\n",
      "Epoch 172/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.9416\n",
      "Epoch 173/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9538\n",
      "Epoch 174/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9521\n",
      "Epoch 175/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1494 - accuracy: 0.9374\n",
      "Epoch 176/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9584\n",
      "Epoch 177/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9521\n",
      "Epoch 178/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9534\n",
      "Epoch 179/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.9433\n",
      "Epoch 180/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9589\n",
      "Epoch 181/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.9618\n",
      "Epoch 182/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9542\n",
      "Epoch 183/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9563\n",
      "Epoch 184/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9555\n",
      "Epoch 185/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9538\n",
      "Epoch 186/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9559\n",
      "Epoch 187/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9517\n",
      "Epoch 188/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9488\n",
      "Epoch 189/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9643\n",
      "Epoch 190/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9458\n",
      "Epoch 191/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9471\n",
      "Epoch 192/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9593\n",
      "Epoch 193/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9467\n",
      "Epoch 194/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9509\n",
      "Epoch 195/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9593\n",
      "Epoch 196/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1321 - accuracy: 0.9500\n",
      "Epoch 197/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9521\n",
      "Epoch 198/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9614\n",
      "Epoch 199/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9517\n",
      "Epoch 200/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9538\n",
      "Epoch 201/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9538\n",
      "Epoch 202/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9559\n",
      "Epoch 203/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9551\n",
      "Epoch 204/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9555\n",
      "Epoch 205/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9605\n",
      "Epoch 206/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9496\n",
      "Epoch 207/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9538\n",
      "Epoch 208/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9484\n",
      "Epoch 209/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9593\n",
      "Epoch 210/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9685\n",
      "Epoch 211/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9601\n",
      "Epoch 212/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9593\n",
      "Epoch 213/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9664\n",
      "Epoch 214/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9631\n",
      "Epoch 215/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1135 - accuracy: 0.9597\n",
      "Epoch 216/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9526\n",
      "Epoch 217/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9576\n",
      "Epoch 218/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9526\n",
      "Epoch 219/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9534\n",
      "Epoch 220/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9673\n",
      "Epoch 221/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9635\n",
      "Epoch 222/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9568\n",
      "Epoch 223/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9635\n",
      "Epoch 224/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.9639\n",
      "Epoch 225/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9631\n",
      "Epoch 226/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9652\n",
      "Epoch 227/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9580\n",
      "Epoch 228/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9652\n",
      "Epoch 229/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9597\n",
      "Epoch 230/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.9572\n",
      "Epoch 231/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9639\n",
      "Epoch 232/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9610\n",
      "Epoch 233/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9597\n",
      "Epoch 234/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9744\n",
      "Epoch 235/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9618\n",
      "Epoch 236/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9626\n",
      "Epoch 237/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9601\n",
      "Epoch 238/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9589\n",
      "Epoch 239/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9622\n",
      "Epoch 240/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9694\n",
      "Epoch 241/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9639\n",
      "Epoch 242/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9626\n",
      "Epoch 243/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9614\n",
      "Epoch 244/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9622\n",
      "Epoch 245/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9589\n",
      "Epoch 246/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9677\n",
      "Epoch 247/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9601\n",
      "Epoch 248/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9664\n",
      "Epoch 249/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9660\n",
      "Epoch 250/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9631\n",
      "Epoch 251/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0892 - accuracy: 0.9719\n",
      "Epoch 252/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9631\n",
      "Epoch 253/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9635\n",
      "Epoch 254/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.9689\n",
      "Epoch 255/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9593\n",
      "Epoch 256/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9597\n",
      "Epoch 257/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9652\n",
      "Epoch 258/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0994 - accuracy: 0.9664\n",
      "Epoch 259/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9572\n",
      "Epoch 260/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9610\n",
      "Epoch 261/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9605\n",
      "Epoch 262/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9652\n",
      "Epoch 263/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.9673\n",
      "Epoch 264/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9664\n",
      "Epoch 265/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9689\n",
      "Epoch 266/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9681\n",
      "Epoch 267/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9660\n",
      "Epoch 268/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9639\n",
      "Epoch 269/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9689\n",
      "Epoch 270/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9685\n",
      "Epoch 271/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9689\n",
      "Epoch 272/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.9685\n",
      "Epoch 273/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9601\n",
      "Epoch 274/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9702\n",
      "Epoch 275/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9635\n",
      "Epoch 276/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9660\n",
      "Epoch 277/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0902 - accuracy: 0.9668\n",
      "Epoch 278/1500\n",
      "44/75 [================>.............] - ETA: 0s - loss: 0.1084 - accuracy: 0.9624Restoring model weights from the end of the best epoch: 248.\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9681\n",
      "Epoch 278: early stopping\n",
      "7/7 [==============================] - 0s 899us/step - loss: 0.5308 - accuracy: 0.8269\n",
      "7/7 [==============================] - 0s 639us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.82 (23/28)\n",
      "Before appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "After appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "Final Test Results - Loss: 0.53081214427948, Accuracy: 0.8269230723381042, Precision: 0.687482479428788, Recall: 0.8039509216860395, F1 Score: 0.7313962005806788\n",
      "Confusion Matrix:\n",
      " [[140   8  19]\n",
      " [  3  14   0]\n",
      " [  6   0  18]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "016A    10\n",
      "071A    10\n",
      "065A     9\n",
      "045A     9\n",
      "022A     9\n",
      "015A     9\n",
      "033A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "031A     7\n",
      "117A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "053A     6\n",
      "075A     5\n",
      "044A     5\n",
      "021A     5\n",
      "025C     5\n",
      "034A     5\n",
      "023B     5\n",
      "009A     4\n",
      "003A     4\n",
      "104A     4\n",
      "062A     4\n",
      "113A     3\n",
      "014A     3\n",
      "056A     3\n",
      "060A     3\n",
      "064A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "018A     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "011A     2\n",
      "100A     1\n",
      "090A     1\n",
      "115A     1\n",
      "088A     1\n",
      "024A     1\n",
      "019B     1\n",
      "096A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "043A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "116A    12\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "037A     6\n",
      "023A     6\n",
      "070A     5\n",
      "052A     4\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "012A     3\n",
      "006A     3\n",
      "058A     3\n",
      "032A     2\n",
      "076A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    235\n",
      "X    186\n",
      "F    169\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    162\n",
      "M    102\n",
      "F     83\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 071A, 097B, 028A, 019A, 074A, 067...\n",
      "kitten    [044A, 047A, 109A, 050A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 095A, 072A, 023A, 027...\n",
      "kitten                 [014B, 111A, 040A, 046A, 042A, 110A]\n",
      "senior                       [057A, 106A, 116A, 051B, 058A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A' '010A'\n",
      " '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '031A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '047A' '048A'\n",
      " '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A' '073A'\n",
      " '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A' '096A'\n",
      " '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '109A' '113A'\n",
      " '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A'\n",
      " '032A' '035A' '037A' '040A' '042A' '046A' '051B' '052A' '057A' '058A'\n",
      " '070A' '072A' '076A' '095A' '103A' '105A' '106A' '110A' '111A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A', '046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'109A', '031A'}\n",
      "Moved to Test Set:\n",
      "{'109A', '031A'}\n",
      "Removed from Test Set\n",
      "{'000A', '046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A'\n",
      " '010A' '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A'\n",
      " '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A'\n",
      " '073A' '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '096A' '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '113A'\n",
      " '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A' '031A'\n",
      " '032A' '035A' '037A' '040A' '042A' '051B' '052A' '057A' '058A' '070A'\n",
      " '072A' '076A' '095A' '103A' '105A' '106A' '109A' '110A' '111A' '116A']\n",
      "Length of X_train_val:\n",
      "679\n",
      "Length of y_train_val:\n",
      "679\n",
      "Length of groups_train_val:\n",
      "679\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     417\n",
      "senior    113\n",
      "kitten     60\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     171\n",
      "kitten    111\n",
      "senior     65\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     449\n",
      "kitten    117\n",
      "senior    113\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     139\n",
      "senior     65\n",
      "kitten     54\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 898, 1: 585, 2: 565})\n",
      "Epoch 1/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 1.0192 - accuracy: 0.5757\n",
      "Epoch 2/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.7173 - accuracy: 0.7109\n",
      "Epoch 3/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.6381 - accuracy: 0.7349\n",
      "Epoch 4/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.6118 - accuracy: 0.7456\n",
      "Epoch 5/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5865 - accuracy: 0.7627\n",
      "Epoch 6/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5245 - accuracy: 0.7900\n",
      "Epoch 7/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5097 - accuracy: 0.7983\n",
      "Epoch 8/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4977 - accuracy: 0.8096\n",
      "Epoch 9/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4721 - accuracy: 0.8066\n",
      "Epoch 10/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4308 - accuracy: 0.8330\n",
      "Epoch 11/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4576 - accuracy: 0.8149\n",
      "Epoch 12/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4460 - accuracy: 0.8223\n",
      "Epoch 13/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4147 - accuracy: 0.8340\n",
      "Epoch 14/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4020 - accuracy: 0.8447\n",
      "Epoch 15/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3876 - accuracy: 0.8472\n",
      "Epoch 16/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3932 - accuracy: 0.8423\n",
      "Epoch 17/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3631 - accuracy: 0.8638\n",
      "Epoch 18/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3616 - accuracy: 0.8740\n",
      "Epoch 19/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3567 - accuracy: 0.8613\n",
      "Epoch 20/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3336 - accuracy: 0.8687\n",
      "Epoch 21/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8643\n",
      "Epoch 22/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3462 - accuracy: 0.8647\n",
      "Epoch 23/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3145 - accuracy: 0.8833\n",
      "Epoch 24/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3289 - accuracy: 0.8691\n",
      "Epoch 25/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3212 - accuracy: 0.8740\n",
      "Epoch 26/1500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.3128 - accuracy: 0.8691\n",
      "Epoch 27/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3166 - accuracy: 0.8721\n",
      "Epoch 28/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2968 - accuracy: 0.8862\n",
      "Epoch 29/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3047 - accuracy: 0.8774\n",
      "Epoch 30/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3009 - accuracy: 0.8911\n",
      "Epoch 31/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2874 - accuracy: 0.8867\n",
      "Epoch 32/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2915 - accuracy: 0.8916\n",
      "Epoch 33/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2703 - accuracy: 0.8887\n",
      "Epoch 34/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2897 - accuracy: 0.8857\n",
      "Epoch 35/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2639 - accuracy: 0.9058\n",
      "Epoch 36/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2609 - accuracy: 0.8940\n",
      "Epoch 37/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2658 - accuracy: 0.8984\n",
      "Epoch 38/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2612 - accuracy: 0.9014\n",
      "Epoch 39/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2659 - accuracy: 0.9033\n",
      "Epoch 40/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2606 - accuracy: 0.8955\n",
      "Epoch 41/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.9082\n",
      "Epoch 42/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2518 - accuracy: 0.9009\n",
      "Epoch 43/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2445 - accuracy: 0.9097\n",
      "Epoch 44/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2245 - accuracy: 0.9185\n",
      "Epoch 45/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2439 - accuracy: 0.9043\n",
      "Epoch 46/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2417 - accuracy: 0.9092\n",
      "Epoch 47/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2283 - accuracy: 0.9131\n",
      "Epoch 48/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9185\n",
      "Epoch 49/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2320 - accuracy: 0.9062\n",
      "Epoch 50/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2198 - accuracy: 0.9165\n",
      "Epoch 51/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2380 - accuracy: 0.9116\n",
      "Epoch 52/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2212 - accuracy: 0.9155\n",
      "Epoch 53/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2131 - accuracy: 0.9180\n",
      "Epoch 54/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2089 - accuracy: 0.9170\n",
      "Epoch 55/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2081 - accuracy: 0.9204\n",
      "Epoch 56/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2193 - accuracy: 0.9150\n",
      "Epoch 57/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2121 - accuracy: 0.9214\n",
      "Epoch 58/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2218 - accuracy: 0.9170\n",
      "Epoch 59/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2199 - accuracy: 0.9175\n",
      "Epoch 60/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.9189\n",
      "Epoch 61/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2035 - accuracy: 0.9268\n",
      "Epoch 62/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1869 - accuracy: 0.9282\n",
      "Epoch 63/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9229\n",
      "Epoch 64/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.9229\n",
      "Epoch 65/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2063 - accuracy: 0.9277\n",
      "Epoch 66/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.9375\n",
      "Epoch 67/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.1856 - accuracy: 0.9297\n",
      "Epoch 68/1500\n",
      "64/64 [==============================] - 0s 965us/step - loss: 0.1929 - accuracy: 0.9248\n",
      "Epoch 69/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9360\n",
      "Epoch 70/1500\n",
      "64/64 [==============================] - 0s 925us/step - loss: 0.2022 - accuracy: 0.9194\n",
      "Epoch 71/1500\n",
      "64/64 [==============================] - 0s 907us/step - loss: 0.1815 - accuracy: 0.9307\n",
      "Epoch 72/1500\n",
      "64/64 [==============================] - 0s 904us/step - loss: 0.1755 - accuracy: 0.9355\n",
      "Epoch 73/1500\n",
      "64/64 [==============================] - 0s 888us/step - loss: 0.1740 - accuracy: 0.9346\n",
      "Epoch 74/1500\n",
      "64/64 [==============================] - 0s 920us/step - loss: 0.1775 - accuracy: 0.9316\n",
      "Epoch 75/1500\n",
      "64/64 [==============================] - 0s 936us/step - loss: 0.1757 - accuracy: 0.9385\n",
      "Epoch 76/1500\n",
      "64/64 [==============================] - 0s 936us/step - loss: 0.1731 - accuracy: 0.9395\n",
      "Epoch 77/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1711 - accuracy: 0.9395\n",
      "Epoch 78/1500\n",
      "64/64 [==============================] - 0s 961us/step - loss: 0.1683 - accuracy: 0.9375\n",
      "Epoch 79/1500\n",
      "64/64 [==============================] - 0s 981us/step - loss: 0.1643 - accuracy: 0.9409\n",
      "Epoch 80/1500\n",
      "64/64 [==============================] - 0s 962us/step - loss: 0.1637 - accuracy: 0.9351\n",
      "Epoch 81/1500\n",
      "64/64 [==============================] - 0s 996us/step - loss: 0.1681 - accuracy: 0.9341\n",
      "Epoch 82/1500\n",
      "64/64 [==============================] - 0s 971us/step - loss: 0.1723 - accuracy: 0.9355\n",
      "Epoch 83/1500\n",
      "64/64 [==============================] - 0s 890us/step - loss: 0.1691 - accuracy: 0.9341\n",
      "Epoch 84/1500\n",
      "64/64 [==============================] - 0s 947us/step - loss: 0.1669 - accuracy: 0.9395\n",
      "Epoch 85/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1672 - accuracy: 0.9375\n",
      "Epoch 86/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.1518 - accuracy: 0.9473\n",
      "Epoch 87/1500\n",
      "64/64 [==============================] - 0s 953us/step - loss: 0.1499 - accuracy: 0.9448\n",
      "Epoch 88/1500\n",
      "64/64 [==============================] - 0s 911us/step - loss: 0.1499 - accuracy: 0.9438\n",
      "Epoch 89/1500\n",
      "64/64 [==============================] - 0s 932us/step - loss: 0.1663 - accuracy: 0.9385\n",
      "Epoch 90/1500\n",
      "64/64 [==============================] - 0s 898us/step - loss: 0.1758 - accuracy: 0.9316\n",
      "Epoch 91/1500\n",
      "64/64 [==============================] - 0s 903us/step - loss: 0.1504 - accuracy: 0.9453\n",
      "Epoch 92/1500\n",
      "64/64 [==============================] - 0s 890us/step - loss: 0.1411 - accuracy: 0.9536\n",
      "Epoch 93/1500\n",
      "64/64 [==============================] - 0s 893us/step - loss: 0.1597 - accuracy: 0.9404\n",
      "Epoch 94/1500\n",
      "64/64 [==============================] - 0s 879us/step - loss: 0.1422 - accuracy: 0.9453\n",
      "Epoch 95/1500\n",
      "64/64 [==============================] - 0s 913us/step - loss: 0.1502 - accuracy: 0.9482\n",
      "Epoch 96/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9497\n",
      "Epoch 97/1500\n",
      "64/64 [==============================] - 0s 931us/step - loss: 0.1545 - accuracy: 0.9443\n",
      "Epoch 98/1500\n",
      "64/64 [==============================] - 0s 973us/step - loss: 0.1596 - accuracy: 0.9414\n",
      "Epoch 99/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9536\n",
      "Epoch 100/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9541\n",
      "Epoch 101/1500\n",
      "64/64 [==============================] - 0s 985us/step - loss: 0.1411 - accuracy: 0.9541\n",
      "Epoch 102/1500\n",
      "64/64 [==============================] - 0s 873us/step - loss: 0.1436 - accuracy: 0.9517\n",
      "Epoch 103/1500\n",
      "64/64 [==============================] - 0s 893us/step - loss: 0.1367 - accuracy: 0.9512\n",
      "Epoch 104/1500\n",
      "64/64 [==============================] - 0s 950us/step - loss: 0.1306 - accuracy: 0.9512\n",
      "Epoch 105/1500\n",
      "64/64 [==============================] - 0s 977us/step - loss: 0.1381 - accuracy: 0.9526\n",
      "Epoch 106/1500\n",
      "64/64 [==============================] - 0s 935us/step - loss: 0.1484 - accuracy: 0.9458\n",
      "Epoch 107/1500\n",
      "64/64 [==============================] - 0s 992us/step - loss: 0.1250 - accuracy: 0.9565\n",
      "Epoch 108/1500\n",
      "64/64 [==============================] - 0s 975us/step - loss: 0.1305 - accuracy: 0.9580\n",
      "Epoch 109/1500\n",
      "64/64 [==============================] - 0s 904us/step - loss: 0.1316 - accuracy: 0.9517\n",
      "Epoch 110/1500\n",
      "64/64 [==============================] - 0s 931us/step - loss: 0.1273 - accuracy: 0.9570\n",
      "Epoch 111/1500\n",
      "64/64 [==============================] - 0s 942us/step - loss: 0.1378 - accuracy: 0.9478\n",
      "Epoch 112/1500\n",
      "64/64 [==============================] - 0s 965us/step - loss: 0.1399 - accuracy: 0.9497\n",
      "Epoch 113/1500\n",
      "64/64 [==============================] - 0s 917us/step - loss: 0.1345 - accuracy: 0.9478\n",
      "Epoch 114/1500\n",
      "64/64 [==============================] - 0s 913us/step - loss: 0.1281 - accuracy: 0.9551\n",
      "Epoch 115/1500\n",
      "64/64 [==============================] - 0s 896us/step - loss: 0.1365 - accuracy: 0.9531\n",
      "Epoch 116/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9585\n",
      "Epoch 117/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9541\n",
      "Epoch 118/1500\n",
      "64/64 [==============================] - 0s 952us/step - loss: 0.1158 - accuracy: 0.9629\n",
      "Epoch 119/1500\n",
      "64/64 [==============================] - 0s 935us/step - loss: 0.1236 - accuracy: 0.9551\n",
      "Epoch 120/1500\n",
      "64/64 [==============================] - 0s 974us/step - loss: 0.1299 - accuracy: 0.9561\n",
      "Epoch 121/1500\n",
      "64/64 [==============================] - 0s 944us/step - loss: 0.1195 - accuracy: 0.9575\n",
      "Epoch 122/1500\n",
      "64/64 [==============================] - 0s 900us/step - loss: 0.1211 - accuracy: 0.9595\n",
      "Epoch 123/1500\n",
      "64/64 [==============================] - 0s 897us/step - loss: 0.1229 - accuracy: 0.9595\n",
      "Epoch 124/1500\n",
      "64/64 [==============================] - 0s 943us/step - loss: 0.1197 - accuracy: 0.9561\n",
      "Epoch 125/1500\n",
      "64/64 [==============================] - 0s 886us/step - loss: 0.1220 - accuracy: 0.9556\n",
      "Epoch 126/1500\n",
      "64/64 [==============================] - 0s 924us/step - loss: 0.1046 - accuracy: 0.9653\n",
      "Epoch 127/1500\n",
      "64/64 [==============================] - 0s 970us/step - loss: 0.1337 - accuracy: 0.9546\n",
      "Epoch 128/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1133 - accuracy: 0.9585\n",
      "Epoch 129/1500\n",
      "64/64 [==============================] - 0s 979us/step - loss: 0.1135 - accuracy: 0.9580\n",
      "Epoch 130/1500\n",
      "64/64 [==============================] - 0s 895us/step - loss: 0.1390 - accuracy: 0.9453\n",
      "Epoch 131/1500\n",
      "64/64 [==============================] - 0s 884us/step - loss: 0.1203 - accuracy: 0.9531\n",
      "Epoch 132/1500\n",
      "64/64 [==============================] - 0s 949us/step - loss: 0.1150 - accuracy: 0.9614\n",
      "Epoch 133/1500\n",
      "64/64 [==============================] - 0s 912us/step - loss: 0.1135 - accuracy: 0.9614\n",
      "Epoch 134/1500\n",
      "64/64 [==============================] - 0s 922us/step - loss: 0.1311 - accuracy: 0.9521\n",
      "Epoch 135/1500\n",
      "64/64 [==============================] - 0s 925us/step - loss: 0.1200 - accuracy: 0.9604\n",
      "Epoch 136/1500\n",
      "64/64 [==============================] - 0s 953us/step - loss: 0.1031 - accuracy: 0.9604\n",
      "Epoch 137/1500\n",
      "64/64 [==============================] - 0s 978us/step - loss: 0.1266 - accuracy: 0.9531\n",
      "Epoch 138/1500\n",
      "64/64 [==============================] - 0s 957us/step - loss: 0.1203 - accuracy: 0.9585\n",
      "Epoch 139/1500\n",
      "64/64 [==============================] - 0s 885us/step - loss: 0.1037 - accuracy: 0.9648\n",
      "Epoch 140/1500\n",
      "64/64 [==============================] - 0s 903us/step - loss: 0.1118 - accuracy: 0.9644\n",
      "Epoch 141/1500\n",
      "64/64 [==============================] - 0s 911us/step - loss: 0.1179 - accuracy: 0.9600\n",
      "Epoch 142/1500\n",
      "64/64 [==============================] - 0s 915us/step - loss: 0.1041 - accuracy: 0.9624\n",
      "Epoch 143/1500\n",
      "64/64 [==============================] - 0s 913us/step - loss: 0.1107 - accuracy: 0.9590\n",
      "Epoch 144/1500\n",
      "64/64 [==============================] - 0s 941us/step - loss: 0.1270 - accuracy: 0.9531\n",
      "Epoch 145/1500\n",
      "64/64 [==============================] - 0s 920us/step - loss: 0.1075 - accuracy: 0.9609\n",
      "Epoch 146/1500\n",
      "64/64 [==============================] - 0s 927us/step - loss: 0.0966 - accuracy: 0.9644\n",
      "Epoch 147/1500\n",
      "64/64 [==============================] - 0s 941us/step - loss: 0.1082 - accuracy: 0.9639\n",
      "Epoch 148/1500\n",
      "64/64 [==============================] - 0s 896us/step - loss: 0.0964 - accuracy: 0.9653\n",
      "Epoch 149/1500\n",
      "64/64 [==============================] - 0s 905us/step - loss: 0.0954 - accuracy: 0.9697\n",
      "Epoch 150/1500\n",
      "64/64 [==============================] - 0s 899us/step - loss: 0.1058 - accuracy: 0.9653\n",
      "Epoch 151/1500\n",
      "64/64 [==============================] - 0s 889us/step - loss: 0.0989 - accuracy: 0.9673\n",
      "Epoch 152/1500\n",
      "64/64 [==============================] - 0s 913us/step - loss: 0.1186 - accuracy: 0.9600\n",
      "Epoch 153/1500\n",
      "64/64 [==============================] - 0s 921us/step - loss: 0.1029 - accuracy: 0.9678\n",
      "Epoch 154/1500\n",
      "64/64 [==============================] - 0s 887us/step - loss: 0.1015 - accuracy: 0.9644\n",
      "Epoch 155/1500\n",
      "64/64 [==============================] - 0s 887us/step - loss: 0.1018 - accuracy: 0.9619\n",
      "Epoch 156/1500\n",
      "64/64 [==============================] - 0s 888us/step - loss: 0.1077 - accuracy: 0.9609\n",
      "Epoch 157/1500\n",
      "64/64 [==============================] - 0s 906us/step - loss: 0.0994 - accuracy: 0.9644\n",
      "Epoch 158/1500\n",
      "64/64 [==============================] - 0s 890us/step - loss: 0.1213 - accuracy: 0.9517\n",
      "Epoch 159/1500\n",
      "64/64 [==============================] - 0s 891us/step - loss: 0.1089 - accuracy: 0.9609\n",
      "Epoch 160/1500\n",
      "64/64 [==============================] - 0s 919us/step - loss: 0.0926 - accuracy: 0.9678\n",
      "Epoch 161/1500\n",
      "64/64 [==============================] - 0s 922us/step - loss: 0.0887 - accuracy: 0.9707\n",
      "Epoch 162/1500\n",
      "64/64 [==============================] - 0s 898us/step - loss: 0.0897 - accuracy: 0.9702\n",
      "Epoch 163/1500\n",
      "64/64 [==============================] - 0s 871us/step - loss: 0.0844 - accuracy: 0.9712\n",
      "Epoch 164/1500\n",
      "64/64 [==============================] - 0s 871us/step - loss: 0.0906 - accuracy: 0.9658\n",
      "Epoch 165/1500\n",
      "64/64 [==============================] - 0s 894us/step - loss: 0.1038 - accuracy: 0.9614\n",
      "Epoch 166/1500\n",
      "64/64 [==============================] - 0s 919us/step - loss: 0.1025 - accuracy: 0.9653\n",
      "Epoch 167/1500\n",
      "64/64 [==============================] - 0s 913us/step - loss: 0.1005 - accuracy: 0.9668\n",
      "Epoch 168/1500\n",
      "64/64 [==============================] - 0s 899us/step - loss: 0.1153 - accuracy: 0.9595\n",
      "Epoch 169/1500\n",
      "64/64 [==============================] - 0s 927us/step - loss: 0.0876 - accuracy: 0.9692\n",
      "Epoch 170/1500\n",
      "64/64 [==============================] - 0s 891us/step - loss: 0.0860 - accuracy: 0.9688\n",
      "Epoch 171/1500\n",
      "64/64 [==============================] - 0s 888us/step - loss: 0.1063 - accuracy: 0.9639\n",
      "Epoch 172/1500\n",
      "64/64 [==============================] - 0s 903us/step - loss: 0.1114 - accuracy: 0.9595\n",
      "Epoch 173/1500\n",
      "64/64 [==============================] - 0s 908us/step - loss: 0.0818 - accuracy: 0.9771\n",
      "Epoch 174/1500\n",
      "64/64 [==============================] - 0s 923us/step - loss: 0.0933 - accuracy: 0.9673\n",
      "Epoch 175/1500\n",
      "64/64 [==============================] - 0s 901us/step - loss: 0.0861 - accuracy: 0.9688\n",
      "Epoch 176/1500\n",
      "64/64 [==============================] - 0s 897us/step - loss: 0.0957 - accuracy: 0.9639\n",
      "Epoch 177/1500\n",
      "64/64 [==============================] - 0s 919us/step - loss: 0.0857 - accuracy: 0.9746\n",
      "Epoch 178/1500\n",
      "64/64 [==============================] - 0s 931us/step - loss: 0.0741 - accuracy: 0.9780\n",
      "Epoch 179/1500\n",
      "64/64 [==============================] - 0s 912us/step - loss: 0.0795 - accuracy: 0.9741\n",
      "Epoch 180/1500\n",
      "64/64 [==============================] - 0s 914us/step - loss: 0.0778 - accuracy: 0.9746\n",
      "Epoch 181/1500\n",
      "64/64 [==============================] - 0s 922us/step - loss: 0.0883 - accuracy: 0.9688\n",
      "Epoch 182/1500\n",
      "64/64 [==============================] - 0s 912us/step - loss: 0.0893 - accuracy: 0.9688\n",
      "Epoch 183/1500\n",
      "64/64 [==============================] - 0s 913us/step - loss: 0.0886 - accuracy: 0.9707\n",
      "Epoch 184/1500\n",
      "64/64 [==============================] - 0s 903us/step - loss: 0.1029 - accuracy: 0.9648\n",
      "Epoch 185/1500\n",
      "64/64 [==============================] - 0s 900us/step - loss: 0.0842 - accuracy: 0.9722\n",
      "Epoch 186/1500\n",
      "64/64 [==============================] - 0s 895us/step - loss: 0.0767 - accuracy: 0.9761\n",
      "Epoch 187/1500\n",
      "64/64 [==============================] - 0s 889us/step - loss: 0.0974 - accuracy: 0.9624\n",
      "Epoch 188/1500\n",
      "64/64 [==============================] - 0s 899us/step - loss: 0.1051 - accuracy: 0.9614\n",
      "Epoch 189/1500\n",
      "64/64 [==============================] - 0s 911us/step - loss: 0.0807 - accuracy: 0.9722\n",
      "Epoch 190/1500\n",
      "64/64 [==============================] - 0s 894us/step - loss: 0.0853 - accuracy: 0.9688\n",
      "Epoch 191/1500\n",
      "64/64 [==============================] - 0s 919us/step - loss: 0.0969 - accuracy: 0.9648\n",
      "Epoch 192/1500\n",
      "64/64 [==============================] - 0s 889us/step - loss: 0.0820 - accuracy: 0.9712\n",
      "Epoch 193/1500\n",
      "64/64 [==============================] - 0s 907us/step - loss: 0.0725 - accuracy: 0.9756\n",
      "Epoch 194/1500\n",
      "64/64 [==============================] - 0s 913us/step - loss: 0.0782 - accuracy: 0.9731\n",
      "Epoch 195/1500\n",
      "64/64 [==============================] - 0s 925us/step - loss: 0.0697 - accuracy: 0.9814\n",
      "Epoch 196/1500\n",
      "64/64 [==============================] - 0s 879us/step - loss: 0.0770 - accuracy: 0.9766\n",
      "Epoch 197/1500\n",
      "64/64 [==============================] - 0s 941us/step - loss: 0.1004 - accuracy: 0.9668\n",
      "Epoch 198/1500\n",
      "64/64 [==============================] - 0s 990us/step - loss: 0.0987 - accuracy: 0.9663\n",
      "Epoch 199/1500\n",
      "64/64 [==============================] - 0s 945us/step - loss: 0.0887 - accuracy: 0.9697\n",
      "Epoch 200/1500\n",
      "64/64 [==============================] - 0s 900us/step - loss: 0.0795 - accuracy: 0.9746\n",
      "Epoch 201/1500\n",
      "64/64 [==============================] - 0s 918us/step - loss: 0.0814 - accuracy: 0.9712\n",
      "Epoch 202/1500\n",
      "64/64 [==============================] - 0s 890us/step - loss: 0.0798 - accuracy: 0.9727\n",
      "Epoch 203/1500\n",
      "64/64 [==============================] - 0s 897us/step - loss: 0.0787 - accuracy: 0.9731\n",
      "Epoch 204/1500\n",
      "64/64 [==============================] - 0s 964us/step - loss: 0.0898 - accuracy: 0.9658\n",
      "Epoch 205/1500\n",
      "64/64 [==============================] - 0s 990us/step - loss: 0.0750 - accuracy: 0.9780\n",
      "Epoch 206/1500\n",
      "64/64 [==============================] - 0s 945us/step - loss: 0.0747 - accuracy: 0.9751\n",
      "Epoch 207/1500\n",
      "64/64 [==============================] - 0s 899us/step - loss: 0.0704 - accuracy: 0.9746\n",
      "Epoch 208/1500\n",
      "64/64 [==============================] - 0s 894us/step - loss: 0.0710 - accuracy: 0.9756\n",
      "Epoch 209/1500\n",
      "64/64 [==============================] - 0s 908us/step - loss: 0.0787 - accuracy: 0.9712\n",
      "Epoch 210/1500\n",
      "64/64 [==============================] - 0s 882us/step - loss: 0.0619 - accuracy: 0.9810\n",
      "Epoch 211/1500\n",
      "64/64 [==============================] - 0s 957us/step - loss: 0.0711 - accuracy: 0.9800\n",
      "Epoch 212/1500\n",
      "64/64 [==============================] - 0s 931us/step - loss: 0.0746 - accuracy: 0.9746\n",
      "Epoch 213/1500\n",
      "64/64 [==============================] - 0s 926us/step - loss: 0.0799 - accuracy: 0.9697\n",
      "Epoch 214/1500\n",
      "64/64 [==============================] - 0s 916us/step - loss: 0.0866 - accuracy: 0.9746\n",
      "Epoch 215/1500\n",
      "64/64 [==============================] - 0s 908us/step - loss: 0.0775 - accuracy: 0.9756\n",
      "Epoch 216/1500\n",
      "64/64 [==============================] - 0s 953us/step - loss: 0.0818 - accuracy: 0.9707\n",
      "Epoch 217/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9800\n",
      "Epoch 218/1500\n",
      "64/64 [==============================] - 0s 996us/step - loss: 0.0663 - accuracy: 0.9780\n",
      "Epoch 219/1500\n",
      "64/64 [==============================] - 0s 955us/step - loss: 0.0730 - accuracy: 0.9810\n",
      "Epoch 220/1500\n",
      "64/64 [==============================] - 0s 942us/step - loss: 0.0838 - accuracy: 0.9668\n",
      "Epoch 221/1500\n",
      "64/64 [==============================] - 0s 946us/step - loss: 0.0678 - accuracy: 0.9775\n",
      "Epoch 222/1500\n",
      "64/64 [==============================] - 0s 942us/step - loss: 0.0686 - accuracy: 0.9766\n",
      "Epoch 223/1500\n",
      "64/64 [==============================] - 0s 950us/step - loss: 0.0837 - accuracy: 0.9761\n",
      "Epoch 224/1500\n",
      "64/64 [==============================] - 0s 910us/step - loss: 0.0783 - accuracy: 0.9751\n",
      "Epoch 225/1500\n",
      "64/64 [==============================] - 0s 972us/step - loss: 0.0717 - accuracy: 0.9751\n",
      "Epoch 226/1500\n",
      "64/64 [==============================] - 0s 909us/step - loss: 0.0773 - accuracy: 0.9707\n",
      "Epoch 227/1500\n",
      "64/64 [==============================] - 0s 904us/step - loss: 0.0736 - accuracy: 0.9731\n",
      "Epoch 228/1500\n",
      "64/64 [==============================] - 0s 932us/step - loss: 0.0751 - accuracy: 0.9761\n",
      "Epoch 229/1500\n",
      "64/64 [==============================] - 0s 933us/step - loss: 0.0581 - accuracy: 0.9800\n",
      "Epoch 230/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0610 - accuracy: 0.9810\n",
      "Epoch 231/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0755 - accuracy: 0.9766\n",
      "Epoch 232/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0630 - accuracy: 0.9819\n",
      "Epoch 233/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9751\n",
      "Epoch 234/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.9775\n",
      "Epoch 235/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9697\n",
      "Epoch 236/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.9780\n",
      "Epoch 237/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.9785\n",
      "Epoch 238/1500\n",
      "64/64 [==============================] - 0s 3ms/step - loss: 0.0713 - accuracy: 0.9775\n",
      "Epoch 239/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0633 - accuracy: 0.9790\n",
      "Epoch 240/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9731\n",
      "Epoch 241/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0662 - accuracy: 0.9800\n",
      "Epoch 242/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9824\n",
      "Epoch 243/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0627 - accuracy: 0.9795\n",
      "Epoch 244/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.9790\n",
      "Epoch 245/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0669 - accuracy: 0.9805\n",
      "Epoch 246/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9780\n",
      "Epoch 247/1500\n",
      "64/64 [==============================] - 0s 911us/step - loss: 0.0772 - accuracy: 0.9727\n",
      "Epoch 248/1500\n",
      "64/64 [==============================] - 0s 902us/step - loss: 0.0635 - accuracy: 0.9780\n",
      "Epoch 249/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0725 - accuracy: 0.9766\n",
      "Epoch 250/1500\n",
      "64/64 [==============================] - 0s 959us/step - loss: 0.0720 - accuracy: 0.9727\n",
      "Epoch 251/1500\n",
      "64/64 [==============================] - 0s 985us/step - loss: 0.0680 - accuracy: 0.9756\n",
      "Epoch 252/1500\n",
      "64/64 [==============================] - 0s 991us/step - loss: 0.0729 - accuracy: 0.9780\n",
      "Epoch 253/1500\n",
      "64/64 [==============================] - 0s 955us/step - loss: 0.0614 - accuracy: 0.9805\n",
      "Epoch 254/1500\n",
      "64/64 [==============================] - 0s 917us/step - loss: 0.0623 - accuracy: 0.9756\n",
      "Epoch 255/1500\n",
      "64/64 [==============================] - 0s 955us/step - loss: 0.0545 - accuracy: 0.9824\n",
      "Epoch 256/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9751\n",
      "Epoch 257/1500\n",
      "64/64 [==============================] - 0s 975us/step - loss: 0.0527 - accuracy: 0.9854\n",
      "Epoch 258/1500\n",
      "64/64 [==============================] - 0s 937us/step - loss: 0.0591 - accuracy: 0.9810\n",
      "Epoch 259/1500\n",
      "64/64 [==============================] - 0s 997us/step - loss: 0.0608 - accuracy: 0.9790\n",
      "Epoch 260/1500\n",
      "64/64 [==============================] - 0s 948us/step - loss: 0.0718 - accuracy: 0.9771\n",
      "Epoch 261/1500\n",
      "64/64 [==============================] - 0s 988us/step - loss: 0.0599 - accuracy: 0.9805\n",
      "Epoch 262/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0594 - accuracy: 0.9829\n",
      "Epoch 263/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0616 - accuracy: 0.9790\n",
      "Epoch 264/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9736\n",
      "Epoch 265/1500\n",
      "64/64 [==============================] - 0s 975us/step - loss: 0.0698 - accuracy: 0.9741\n",
      "Epoch 266/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9805\n",
      "Epoch 267/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0628 - accuracy: 0.9824\n",
      "Epoch 268/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9824\n",
      "Epoch 269/1500\n",
      "64/64 [==============================] - 0s 987us/step - loss: 0.0748 - accuracy: 0.9741\n",
      "Epoch 270/1500\n",
      "64/64 [==============================] - 0s 923us/step - loss: 0.0528 - accuracy: 0.9844\n",
      "Epoch 271/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.9775\n",
      "Epoch 272/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0602 - accuracy: 0.9824\n",
      "Epoch 273/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0613 - accuracy: 0.9819\n",
      "Epoch 274/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0494 - accuracy: 0.9873\n",
      "Epoch 275/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0645 - accuracy: 0.9741\n",
      "Epoch 276/1500\n",
      "64/64 [==============================] - 0s 964us/step - loss: 0.0717 - accuracy: 0.9790\n",
      "Epoch 277/1500\n",
      "64/64 [==============================] - 0s 944us/step - loss: 0.0539 - accuracy: 0.9834\n",
      "Epoch 278/1500\n",
      "64/64 [==============================] - 0s 930us/step - loss: 0.0556 - accuracy: 0.9863\n",
      "Epoch 279/1500\n",
      "64/64 [==============================] - 0s 918us/step - loss: 0.0560 - accuracy: 0.9814\n",
      "Epoch 280/1500\n",
      "64/64 [==============================] - 0s 937us/step - loss: 0.0646 - accuracy: 0.9775\n",
      "Epoch 281/1500\n",
      "64/64 [==============================] - 0s 917us/step - loss: 0.0760 - accuracy: 0.9697\n",
      "Epoch 282/1500\n",
      "64/64 [==============================] - 0s 958us/step - loss: 0.0777 - accuracy: 0.9722\n",
      "Epoch 283/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.9790\n",
      "Epoch 284/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9810\n",
      "Epoch 285/1500\n",
      "64/64 [==============================] - 0s 928us/step - loss: 0.0540 - accuracy: 0.9849\n",
      "Epoch 286/1500\n",
      "64/64 [==============================] - 0s 945us/step - loss: 0.0561 - accuracy: 0.9800\n",
      "Epoch 287/1500\n",
      "64/64 [==============================] - 0s 911us/step - loss: 0.0604 - accuracy: 0.9829\n",
      "Epoch 288/1500\n",
      "64/64 [==============================] - 0s 950us/step - loss: 0.0651 - accuracy: 0.9800\n",
      "Epoch 289/1500\n",
      "64/64 [==============================] - 0s 911us/step - loss: 0.0695 - accuracy: 0.9746\n",
      "Epoch 290/1500\n",
      "64/64 [==============================] - 0s 918us/step - loss: 0.0690 - accuracy: 0.9736\n",
      "Epoch 291/1500\n",
      "64/64 [==============================] - 0s 927us/step - loss: 0.0659 - accuracy: 0.9771\n",
      "Epoch 292/1500\n",
      "64/64 [==============================] - 0s 936us/step - loss: 0.0575 - accuracy: 0.9800\n",
      "Epoch 293/1500\n",
      "64/64 [==============================] - 0s 923us/step - loss: 0.0493 - accuracy: 0.9858\n",
      "Epoch 294/1500\n",
      "64/64 [==============================] - 0s 944us/step - loss: 0.0732 - accuracy: 0.9756\n",
      "Epoch 295/1500\n",
      "64/64 [==============================] - 0s 909us/step - loss: 0.0527 - accuracy: 0.9854\n",
      "Epoch 296/1500\n",
      "64/64 [==============================] - 0s 917us/step - loss: 0.0528 - accuracy: 0.9858\n",
      "Epoch 297/1500\n",
      "64/64 [==============================] - 0s 937us/step - loss: 0.0655 - accuracy: 0.9771\n",
      "Epoch 298/1500\n",
      "64/64 [==============================] - 0s 927us/step - loss: 0.0553 - accuracy: 0.9829\n",
      "Epoch 299/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0544 - accuracy: 0.9819\n",
      "Epoch 300/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.9746\n",
      "Epoch 301/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.9805\n",
      "Epoch 302/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9746\n",
      "Epoch 303/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0671 - accuracy: 0.9751\n",
      "Epoch 304/1500\n",
      "64/64 [==============================] - 0s 928us/step - loss: 0.0464 - accuracy: 0.9863\n",
      "Epoch 305/1500\n",
      "64/64 [==============================] - 0s 942us/step - loss: 0.0569 - accuracy: 0.9795\n",
      "Epoch 306/1500\n",
      "64/64 [==============================] - 0s 927us/step - loss: 0.0580 - accuracy: 0.9834\n",
      "Epoch 307/1500\n",
      "64/64 [==============================] - 0s 941us/step - loss: 0.0448 - accuracy: 0.9863\n",
      "Epoch 308/1500\n",
      "64/64 [==============================] - 0s 930us/step - loss: 0.0648 - accuracy: 0.9780\n",
      "Epoch 309/1500\n",
      "64/64 [==============================] - 0s 900us/step - loss: 0.0777 - accuracy: 0.9761\n",
      "Epoch 310/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0565 - accuracy: 0.9819\n",
      "Epoch 311/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0574 - accuracy: 0.9829\n",
      "Epoch 312/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0508 - accuracy: 0.9790\n",
      "Epoch 313/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0561 - accuracy: 0.9839\n",
      "Epoch 314/1500\n",
      "64/64 [==============================] - 0s 934us/step - loss: 0.0478 - accuracy: 0.9839\n",
      "Epoch 315/1500\n",
      "64/64 [==============================] - 0s 924us/step - loss: 0.0409 - accuracy: 0.9883\n",
      "Epoch 316/1500\n",
      "64/64 [==============================] - 0s 942us/step - loss: 0.0486 - accuracy: 0.9844\n",
      "Epoch 317/1500\n",
      "64/64 [==============================] - 0s 879us/step - loss: 0.0465 - accuracy: 0.9844\n",
      "Epoch 318/1500\n",
      "64/64 [==============================] - 0s 963us/step - loss: 0.0592 - accuracy: 0.9800\n",
      "Epoch 319/1500\n",
      "64/64 [==============================] - 0s 955us/step - loss: 0.0516 - accuracy: 0.9834\n",
      "Epoch 320/1500\n",
      "64/64 [==============================] - 0s 965us/step - loss: 0.0394 - accuracy: 0.9878\n",
      "Epoch 321/1500\n",
      "64/64 [==============================] - 0s 978us/step - loss: 0.0527 - accuracy: 0.9844\n",
      "Epoch 322/1500\n",
      "64/64 [==============================] - 0s 997us/step - loss: 0.0540 - accuracy: 0.9824\n",
      "Epoch 323/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9790\n",
      "Epoch 324/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0482 - accuracy: 0.9868\n",
      "Epoch 325/1500\n",
      "64/64 [==============================] - 0s 913us/step - loss: 0.0498 - accuracy: 0.9849\n",
      "Epoch 326/1500\n",
      "64/64 [==============================] - 0s 935us/step - loss: 0.0673 - accuracy: 0.9790\n",
      "Epoch 327/1500\n",
      "64/64 [==============================] - 0s 929us/step - loss: 0.0444 - accuracy: 0.9854\n",
      "Epoch 328/1500\n",
      "64/64 [==============================] - 0s 916us/step - loss: 0.0484 - accuracy: 0.9834\n",
      "Epoch 329/1500\n",
      "64/64 [==============================] - 0s 958us/step - loss: 0.0470 - accuracy: 0.9849\n",
      "Epoch 330/1500\n",
      "64/64 [==============================] - 0s 965us/step - loss: 0.0568 - accuracy: 0.9775\n",
      "Epoch 331/1500\n",
      "64/64 [==============================] - 0s 909us/step - loss: 0.0706 - accuracy: 0.9761\n",
      "Epoch 332/1500\n",
      "64/64 [==============================] - 0s 906us/step - loss: 0.0821 - accuracy: 0.9707\n",
      "Epoch 333/1500\n",
      "64/64 [==============================] - 0s 903us/step - loss: 0.0533 - accuracy: 0.9819\n",
      "Epoch 334/1500\n",
      "64/64 [==============================] - 0s 908us/step - loss: 0.0443 - accuracy: 0.9868\n",
      "Epoch 335/1500\n",
      "64/64 [==============================] - 0s 937us/step - loss: 0.0451 - accuracy: 0.9868\n",
      "Epoch 336/1500\n",
      "64/64 [==============================] - 0s 897us/step - loss: 0.0507 - accuracy: 0.9854\n",
      "Epoch 337/1500\n",
      "64/64 [==============================] - 0s 922us/step - loss: 0.0537 - accuracy: 0.9834\n",
      "Epoch 338/1500\n",
      "64/64 [==============================] - 0s 964us/step - loss: 0.0447 - accuracy: 0.9868\n",
      "Epoch 339/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.9771\n",
      "Epoch 340/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0570 - accuracy: 0.9849\n",
      "Epoch 341/1500\n",
      "64/64 [==============================] - 0s 944us/step - loss: 0.0503 - accuracy: 0.9834\n",
      "Epoch 342/1500\n",
      "64/64 [==============================] - 0s 903us/step - loss: 0.0659 - accuracy: 0.9785\n",
      "Epoch 343/1500\n",
      "64/64 [==============================] - 0s 903us/step - loss: 0.0625 - accuracy: 0.9834\n",
      "Epoch 344/1500\n",
      "64/64 [==============================] - 0s 927us/step - loss: 0.0479 - accuracy: 0.9839\n",
      "Epoch 345/1500\n",
      "64/64 [==============================] - 0s 940us/step - loss: 0.0399 - accuracy: 0.9888\n",
      "Epoch 346/1500\n",
      "64/64 [==============================] - 0s 934us/step - loss: 0.0483 - accuracy: 0.9824\n",
      "Epoch 347/1500\n",
      "64/64 [==============================] - 0s 945us/step - loss: 0.0438 - accuracy: 0.9829\n",
      "Epoch 348/1500\n",
      "64/64 [==============================] - 0s 913us/step - loss: 0.0446 - accuracy: 0.9849\n",
      "Epoch 349/1500\n",
      "64/64 [==============================] - 0s 889us/step - loss: 0.0453 - accuracy: 0.9849\n",
      "Epoch 350/1500\n",
      "55/64 [========================>.....] - ETA: 0s - loss: 0.0560 - accuracy: 0.9858Restoring model weights from the end of the best epoch: 320.\n",
      "64/64 [==============================] - 0s 967us/step - loss: 0.0523 - accuracy: 0.9873\n",
      "Epoch 350: early stopping\n",
      "9/9 [==============================] - 0s 728us/step - loss: 1.1097 - accuracy: 0.7093\n",
      "9/9 [==============================] - 0s 605us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.77 (23/30)\n",
      "Before appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "After appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "Final Test Results - Loss: 1.1097211837768555, Accuracy: 0.7093023061752319, Precision: 0.7398738641066442, Recall: 0.6621041628235873, F1 Score: 0.6902215967995889\n",
      "Confusion Matrix:\n",
      " [[115   3  21]\n",
      " [ 18  36   0]\n",
      " [ 33   0  32]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "097B    14\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "068A    11\n",
      "025A    11\n",
      "063A    11\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "005A    10\n",
      "072A     9\n",
      "065A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "099A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "062A     4\n",
      "009A     4\n",
      "012A     3\n",
      "058A     3\n",
      "060A     3\n",
      "006A     3\n",
      "056A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "032A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "066A     1\n",
      "026C     1\n",
      "076A     1\n",
      "096A     1\n",
      "041A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "047A    28\n",
      "074A    25\n",
      "000B    19\n",
      "097A    16\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "036A    11\n",
      "016A    10\n",
      "022A     9\n",
      "117A     7\n",
      "031A     7\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "003A     4\n",
      "064A     3\n",
      "113A     3\n",
      "093A     2\n",
      "069A     2\n",
      "073A     1\n",
      "091A     1\n",
      "092A     1\n",
      "048A     1\n",
      "115A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    266\n",
      "F    163\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    89\n",
      "X    75\n",
      "M    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [014B, 111A, 040A, 046A, 042A, 109A, 050A, 043...\n",
      "senior    [057A, 106A, 104A, 055A, 116A, 051B, 054A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [028A, 074A, 022A, 034A, 091A, 002A, 007A, 069...\n",
      "kitten                             [044A, 047A, 048A, 115A]\n",
      "senior     [093A, 097A, 059A, 113A, 117A, 051A, 016A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "Length of X_train_val:\n",
      "702\n",
      "Length of y_train_val:\n",
      "702\n",
      "Length of groups_train_val:\n",
      "702\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 916, 1: 680, 2: 540})\n",
      "Epoch 1/1500\n",
      "67/67 [==============================] - 0s 978us/step - loss: 0.9412 - accuracy: 0.5955\n",
      "Epoch 2/1500\n",
      "67/67 [==============================] - 0s 954us/step - loss: 0.7325 - accuracy: 0.6863\n",
      "Epoch 3/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.6814 - accuracy: 0.7228\n",
      "Epoch 4/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.6156 - accuracy: 0.7406\n",
      "Epoch 5/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.5729 - accuracy: 0.7547\n",
      "Epoch 6/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.5386 - accuracy: 0.7757\n",
      "Epoch 7/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.5554 - accuracy: 0.7776\n",
      "Epoch 8/1500\n",
      "67/67 [==============================] - 0s 990us/step - loss: 0.5235 - accuracy: 0.7837\n",
      "Epoch 9/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.5014 - accuracy: 0.7926\n",
      "Epoch 10/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4973 - accuracy: 0.7921\n",
      "Epoch 11/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4706 - accuracy: 0.8001\n",
      "Epoch 12/1500\n",
      "67/67 [==============================] - 0s 946us/step - loss: 0.4601 - accuracy: 0.8099\n",
      "Epoch 13/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4538 - accuracy: 0.8137\n",
      "Epoch 14/1500\n",
      "67/67 [==============================] - 0s 997us/step - loss: 0.4208 - accuracy: 0.8258\n",
      "Epoch 15/1500\n",
      "67/67 [==============================] - 0s 936us/step - loss: 0.4489 - accuracy: 0.8240\n",
      "Epoch 16/1500\n",
      "67/67 [==============================] - 0s 988us/step - loss: 0.4155 - accuracy: 0.8277\n",
      "Epoch 17/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4044 - accuracy: 0.8408\n",
      "Epoch 18/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4101 - accuracy: 0.8287\n",
      "Epoch 19/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4041 - accuracy: 0.8399\n",
      "Epoch 20/1500\n",
      "67/67 [==============================] - 0s 995us/step - loss: 0.3789 - accuracy: 0.8427\n",
      "Epoch 21/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3963 - accuracy: 0.8511\n",
      "Epoch 22/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3875 - accuracy: 0.8380\n",
      "Epoch 23/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3822 - accuracy: 0.8478\n",
      "Epoch 24/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3572 - accuracy: 0.8525\n",
      "Epoch 25/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3507 - accuracy: 0.8605\n",
      "Epoch 26/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3340 - accuracy: 0.8600\n",
      "Epoch 27/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3359 - accuracy: 0.8666\n",
      "Epoch 28/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3429 - accuracy: 0.8666\n",
      "Epoch 29/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3391 - accuracy: 0.8577\n",
      "Epoch 30/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3391 - accuracy: 0.8619\n",
      "Epoch 31/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3240 - accuracy: 0.8741\n",
      "Epoch 32/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3241 - accuracy: 0.8722\n",
      "Epoch 33/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3210 - accuracy: 0.8713\n",
      "Epoch 34/1500\n",
      "67/67 [==============================] - 0s 958us/step - loss: 0.3115 - accuracy: 0.8713\n",
      "Epoch 35/1500\n",
      "67/67 [==============================] - 0s 4ms/step - loss: 0.3039 - accuracy: 0.8839\n",
      "Epoch 36/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.2968 - accuracy: 0.8825\n",
      "Epoch 37/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2950 - accuracy: 0.8834\n",
      "Epoch 38/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2939 - accuracy: 0.8900\n",
      "Epoch 39/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2847 - accuracy: 0.8933\n",
      "Epoch 40/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2806 - accuracy: 0.8919\n",
      "Epoch 41/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.8951\n",
      "Epoch 42/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2562 - accuracy: 0.8947\n",
      "Epoch 43/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2662 - accuracy: 0.8970\n",
      "Epoch 44/1500\n",
      "67/67 [==============================] - 0s 961us/step - loss: 0.2837 - accuracy: 0.8872\n",
      "Epoch 45/1500\n",
      "67/67 [==============================] - 0s 991us/step - loss: 0.2640 - accuracy: 0.8914\n",
      "Epoch 46/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2655 - accuracy: 0.8965\n",
      "Epoch 47/1500\n",
      "67/67 [==============================] - 0s 993us/step - loss: 0.2481 - accuracy: 0.9073\n",
      "Epoch 48/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2703 - accuracy: 0.9003\n",
      "Epoch 49/1500\n",
      "67/67 [==============================] - 0s 989us/step - loss: 0.2765 - accuracy: 0.8904\n",
      "Epoch 50/1500\n",
      "67/67 [==============================] - 0s 996us/step - loss: 0.2462 - accuracy: 0.9054\n",
      "Epoch 51/1500\n",
      "67/67 [==============================] - 0s 974us/step - loss: 0.2377 - accuracy: 0.9031\n",
      "Epoch 52/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2574 - accuracy: 0.8998\n",
      "Epoch 53/1500\n",
      "67/67 [==============================] - 0s 978us/step - loss: 0.2351 - accuracy: 0.9110\n",
      "Epoch 54/1500\n",
      "67/67 [==============================] - 0s 981us/step - loss: 0.2682 - accuracy: 0.8961\n",
      "Epoch 55/1500\n",
      "67/67 [==============================] - 0s 981us/step - loss: 0.2264 - accuracy: 0.9115\n",
      "Epoch 56/1500\n",
      "67/67 [==============================] - 0s 985us/step - loss: 0.2375 - accuracy: 0.9073\n",
      "Epoch 57/1500\n",
      "67/67 [==============================] - 0s 987us/step - loss: 0.2195 - accuracy: 0.9143\n",
      "Epoch 58/1500\n",
      "67/67 [==============================] - 0s 960us/step - loss: 0.2334 - accuracy: 0.9073\n",
      "Epoch 59/1500\n",
      "67/67 [==============================] - 0s 977us/step - loss: 0.2388 - accuracy: 0.9078\n",
      "Epoch 60/1500\n",
      "67/67 [==============================] - 0s 982us/step - loss: 0.2254 - accuracy: 0.9181\n",
      "Epoch 61/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.2451 - accuracy: 0.9040\n",
      "Epoch 62/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2138 - accuracy: 0.9143\n",
      "Epoch 63/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2499 - accuracy: 0.8979\n",
      "Epoch 64/1500\n",
      "67/67 [==============================] - 0s 973us/step - loss: 0.2226 - accuracy: 0.9157\n",
      "Epoch 65/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2101 - accuracy: 0.9153\n",
      "Epoch 66/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2185 - accuracy: 0.9167\n",
      "Epoch 67/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9087\n",
      "Epoch 68/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2139 - accuracy: 0.9148\n",
      "Epoch 69/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2259 - accuracy: 0.9153\n",
      "Epoch 70/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1950 - accuracy: 0.9279\n",
      "Epoch 71/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2272 - accuracy: 0.9176\n",
      "Epoch 72/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2121 - accuracy: 0.9181\n",
      "Epoch 73/1500\n",
      "67/67 [==============================] - 0s 980us/step - loss: 0.2080 - accuracy: 0.9232\n",
      "Epoch 74/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1980 - accuracy: 0.9209\n",
      "Epoch 75/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9213\n",
      "Epoch 76/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1888 - accuracy: 0.9279\n",
      "Epoch 77/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1976 - accuracy: 0.9288\n",
      "Epoch 78/1500\n",
      "67/67 [==============================] - 0s 992us/step - loss: 0.2008 - accuracy: 0.9246\n",
      "Epoch 79/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2017 - accuracy: 0.9204\n",
      "Epoch 80/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1885 - accuracy: 0.9293\n",
      "Epoch 81/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1971 - accuracy: 0.9237\n",
      "Epoch 82/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1923 - accuracy: 0.9279\n",
      "Epoch 83/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9256\n",
      "Epoch 84/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9382\n",
      "Epoch 85/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9363\n",
      "Epoch 86/1500\n",
      "67/67 [==============================] - 0s 986us/step - loss: 0.1759 - accuracy: 0.9373\n",
      "Epoch 87/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1792 - accuracy: 0.9340\n",
      "Epoch 88/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.9256\n",
      "Epoch 89/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1909 - accuracy: 0.9312\n",
      "Epoch 90/1500\n",
      "67/67 [==============================] - 0s 971us/step - loss: 0.1864 - accuracy: 0.9223\n",
      "Epoch 91/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1712 - accuracy: 0.9340\n",
      "Epoch 92/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9419\n",
      "Epoch 93/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9354\n",
      "Epoch 94/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9363\n",
      "Epoch 95/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.1515 - accuracy: 0.9452\n",
      "Epoch 96/1500\n",
      "67/67 [==============================] - 0s 974us/step - loss: 0.1775 - accuracy: 0.9382\n",
      "Epoch 97/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.1655 - accuracy: 0.9391\n",
      "Epoch 98/1500\n",
      "67/67 [==============================] - 0s 983us/step - loss: 0.1686 - accuracy: 0.9368\n",
      "Epoch 99/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1635 - accuracy: 0.9368\n",
      "Epoch 100/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.9387\n",
      "Epoch 101/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1826 - accuracy: 0.9237\n",
      "Epoch 102/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9321\n",
      "Epoch 103/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9508\n",
      "Epoch 104/1500\n",
      "67/67 [==============================] - 0s 965us/step - loss: 0.1678 - accuracy: 0.9354\n",
      "Epoch 105/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9387\n",
      "Epoch 106/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9480\n",
      "Epoch 107/1500\n",
      "67/67 [==============================] - 0s 984us/step - loss: 0.1465 - accuracy: 0.9513\n",
      "Epoch 108/1500\n",
      "67/67 [==============================] - 0s 988us/step - loss: 0.1469 - accuracy: 0.9462\n",
      "Epoch 109/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9452\n",
      "Epoch 110/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9424\n",
      "Epoch 111/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9518\n",
      "Epoch 112/1500\n",
      "67/67 [==============================] - 0s 958us/step - loss: 0.1485 - accuracy: 0.9466\n",
      "Epoch 113/1500\n",
      "67/67 [==============================] - 0s 976us/step - loss: 0.1489 - accuracy: 0.9508\n",
      "Epoch 114/1500\n",
      "67/67 [==============================] - 0s 981us/step - loss: 0.1450 - accuracy: 0.9485\n",
      "Epoch 115/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9429\n",
      "Epoch 116/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9490\n",
      "Epoch 117/1500\n",
      "67/67 [==============================] - 0s 973us/step - loss: 0.1342 - accuracy: 0.9532\n",
      "Epoch 118/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1683 - accuracy: 0.9345\n",
      "Epoch 119/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9462\n",
      "Epoch 120/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9443\n",
      "Epoch 121/1500\n",
      "67/67 [==============================] - 0s 985us/step - loss: 0.1308 - accuracy: 0.9546\n",
      "Epoch 122/1500\n",
      "67/67 [==============================] - 0s 982us/step - loss: 0.1559 - accuracy: 0.9448\n",
      "Epoch 123/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9499\n",
      "Epoch 124/1500\n",
      "67/67 [==============================] - 0s 975us/step - loss: 0.1557 - accuracy: 0.9415\n",
      "Epoch 125/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9546\n",
      "Epoch 126/1500\n",
      "67/67 [==============================] - 0s 985us/step - loss: 0.1441 - accuracy: 0.9513\n",
      "Epoch 127/1500\n",
      "67/67 [==============================] - 0s 953us/step - loss: 0.1354 - accuracy: 0.9504\n",
      "Epoch 128/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9443\n",
      "Epoch 129/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9541\n",
      "Epoch 130/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9508\n",
      "Epoch 131/1500\n",
      "67/67 [==============================] - 0s 970us/step - loss: 0.1435 - accuracy: 0.9466\n",
      "Epoch 132/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9532\n",
      "Epoch 133/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9518\n",
      "Epoch 134/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9537\n",
      "Epoch 135/1500\n",
      "67/67 [==============================] - 0s 973us/step - loss: 0.1325 - accuracy: 0.9560\n",
      "Epoch 136/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9537\n",
      "Epoch 137/1500\n",
      "67/67 [==============================] - 0s 983us/step - loss: 0.1315 - accuracy: 0.9532\n",
      "Epoch 138/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9583\n",
      "Epoch 139/1500\n",
      "67/67 [==============================] - 0s 991us/step - loss: 0.1103 - accuracy: 0.9649\n",
      "Epoch 140/1500\n",
      "67/67 [==============================] - 0s 977us/step - loss: 0.1191 - accuracy: 0.9616\n",
      "Epoch 141/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9532\n",
      "Epoch 142/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9527\n",
      "Epoch 143/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9499\n",
      "Epoch 144/1500\n",
      "67/67 [==============================] - 0s 968us/step - loss: 0.1229 - accuracy: 0.9569\n",
      "Epoch 145/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9448\n",
      "Epoch 146/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9504\n",
      "Epoch 147/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9616\n",
      "Epoch 148/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1104 - accuracy: 0.9579\n",
      "Epoch 149/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9621\n",
      "Epoch 150/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9602\n",
      "Epoch 151/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9546\n",
      "Epoch 152/1500\n",
      "67/67 [==============================] - 0s 990us/step - loss: 0.1230 - accuracy: 0.9579\n",
      "Epoch 153/1500\n",
      "67/67 [==============================] - 0s 988us/step - loss: 0.1144 - accuracy: 0.9583\n",
      "Epoch 154/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9579\n",
      "Epoch 155/1500\n",
      "67/67 [==============================] - 0s 987us/step - loss: 0.1160 - accuracy: 0.9551\n",
      "Epoch 156/1500\n",
      "67/67 [==============================] - 0s 953us/step - loss: 0.1255 - accuracy: 0.9555\n",
      "Epoch 157/1500\n",
      "67/67 [==============================] - 0s 932us/step - loss: 0.1106 - accuracy: 0.9625\n",
      "Epoch 158/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9443\n",
      "Epoch 159/1500\n",
      "67/67 [==============================] - 0s 957us/step - loss: 0.1283 - accuracy: 0.9532\n",
      "Epoch 160/1500\n",
      "67/67 [==============================] - 0s 964us/step - loss: 0.1207 - accuracy: 0.9597\n",
      "Epoch 161/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9546\n",
      "Epoch 162/1500\n",
      "67/67 [==============================] - 0s 992us/step - loss: 0.1185 - accuracy: 0.9569\n",
      "Epoch 163/1500\n",
      "67/67 [==============================] - 0s 946us/step - loss: 0.1280 - accuracy: 0.9588\n",
      "Epoch 164/1500\n",
      "67/67 [==============================] - 0s 986us/step - loss: 0.1197 - accuracy: 0.9560\n",
      "Epoch 165/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9565\n",
      "Epoch 166/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9588\n",
      "Epoch 167/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9621\n",
      "Epoch 168/1500\n",
      "67/67 [==============================] - 0s 990us/step - loss: 0.1121 - accuracy: 0.9625\n",
      "Epoch 169/1500\n",
      "49/67 [====================>.........] - ETA: 0s - loss: 0.1340 - accuracy: 0.9534Restoring model weights from the end of the best epoch: 139.\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9565\n",
      "Epoch 169: early stopping\n",
      "8/8 [==============================] - 0s 854us/step - loss: 0.7279 - accuracy: 0.7617\n",
      "8/8 [==============================] - 0s 600us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (22/29)\n",
      "Before appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "After appending - Cat IDs: 848, Predictions: 848, Actuals: 848, Gender: 848\n",
      "Final Test Results - Loss: 0.7279452085494995, Accuracy: 0.7617021203041077, Precision: 0.8024010263929618, Recall: 0.7285714285714286, F1 Score: 0.7506242094263874\n",
      "Confusion Matrix:\n",
      " [[117   3  10]\n",
      " [  7  28   0]\n",
      " [ 36   0  34]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.7155492783571071\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.7633714824914932\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7666587382555008\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.7155829771882333\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7565879148199715\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[3]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0522ad-89f9-479c-b073-c5e720fc6221",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ecac477e-680c-4a82-abcb-2b3a2dbfbea4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 848, Predictions: 848, Actuals: 848, Gender: 848\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ce33a298-d7a3-40ec-8f78-00c109118230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "23d083a1-d61a-4fce-9b56-667a9930bfe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.80 (88/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "431e9ecf-be90-466a-a1b7-dd98f7fb2a9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42e4a5c0-56dd-4771-820f-bcecaf391f0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, adult, kitten, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, senior, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, senior, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[kitten, adult, adult, kitten, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, kitten, adult, adult, adult, ad...         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, k...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, adult, kitten, adult, senior, ...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, adult, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [senior, adult, senior, senior, senior, senior...        senior           senior                   True\n",
       "59    053A       [adult, adult, senior, senior, adult, adult]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, adult, senior, adult, senior, adult, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "102   108A     [senior, senior, adult, adult, senior, senior]        senior           senior                   True\n",
       "100   105A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "99    104A                    [adult, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "92    097A  [adult, senior, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, senior, adult, adult, adult, a...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "50    044A            [kitten, adult, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "25    023A        [adult, kitten, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "29    025B                                    [senior, adult]         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, adult, s...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "45    039A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "39    033A  [kitten, adult, adult, kitten, adult, adult, a...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "42    036A  [senior, senior, senior, senior, adult, adult,...         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "106   113A                             [adult, senior, adult]         adult           senior                  False\n",
       "103   109A  [adult, adult, kitten, kitten, kitten, adult, ...         adult           kitten                  False\n",
       "101   106A  [adult, senior, adult, senior, adult, adult, a...         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "48    042A  [adult, adult, kitten, adult, adult, adult, ad...         adult           kitten                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "18    016A  [senior, adult, adult, senior, adult, senior, ...         adult           senior                  False\n",
       "82    076A                                           [kitten]        kitten            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "40    034A             [adult, senior, senior, senior, adult]        senior            adult                  False\n",
       "56    051A  [adult, senior, adult, adult, adult, senior, a...         adult           senior                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc2e5ff6-b761-4b71-b7cd-9eb24aae0ebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     66\n",
      "kitten    11\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ff925f9d-efd5-46a0-89a0-1fb3da66a46a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             66  90.410959\n",
      "1           kitten           15             11  73.333333\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d8f08817-c78c-4654-b195-911f2a9ccb2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6f284-fe35-4071-8054-5e5a086b4ed9",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4394c6ec-6da8-42de-aaaa-bc1b17bbf74b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          556            463  83.273381\n",
      "1           kitten          114             86  75.438596\n",
      "2           senior          178             98  55.056180\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8d541c77-590d-4b79-b526-85520ea5c4a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c98d75-f483-4819-b292-975bd229cfd2",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7325b11-ab06-45bd-8e77-d498737b7552",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    220      170     77.27\n",
      "1          M    337      269     79.82\n",
      "2          X    291      208     71.48\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "777ae0eb-dab5-47c0-86a7-0d3c6b01b9c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40270db-853d-46a1-acb4-8dab8a4f9c81",
   "metadata": {},
   "source": [
    "# RANDOM SEED 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "588e6cfb-6ee5-4b51-a9ee-79dcbfbb208c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[4]))\n",
    "np.random.seed(int(random_seeds[4]))\n",
    "tf.random.set_seed(int(random_seeds[4]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_16.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "432a9515-e7f1-482c-bdaf-73cb07a9132e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8231e441-4472-4d9b-8ddf-dc285c07923b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9ed6b-6d2f-491e-95d8-73d33edeef6a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6cd63258-8a3b-4bc0-9ac6-ae8aa4b515b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "106A    14\n",
      "097B    14\n",
      "028A    13\n",
      "111A    13\n",
      "051A    12\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "068A    11\n",
      "014B    10\n",
      "016A    10\n",
      "071A    10\n",
      "005A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "045A     9\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "095A     8\n",
      "010A     8\n",
      "050A     7\n",
      "027A     7\n",
      "031A     7\n",
      "099A     7\n",
      "117A     7\n",
      "053A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "075A     5\n",
      "021A     5\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "062A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "009A     4\n",
      "035A     4\n",
      "056A     3\n",
      "014A     3\n",
      "058A     3\n",
      "060A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "032A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "073A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "041A     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "000B    19\n",
      "001A    14\n",
      "002A    13\n",
      "040A    10\n",
      "022A     9\n",
      "065A     9\n",
      "109A     6\n",
      "108A     6\n",
      "044A     5\n",
      "026A     4\n",
      "113A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "011A     2\n",
      "054A     2\n",
      "018A     2\n",
      "092A     1\n",
      "049A     1\n",
      "004A     1\n",
      "019B     1\n",
      "115A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    280\n",
      "X    256\n",
      "F    186\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    92\n",
      "F    66\n",
      "M    57\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [014B, 111A, 047A, 042A, 050A, 043A, 041A, 045...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 001A, 103A, 022A, 065A, 002A, 000B, 026...\n",
      "kitten                 [044A, 040A, 046A, 109A, 049A, 115A]\n",
      "senior                             [113A, 054A, 108A, 011A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 59, 'kitten': 10, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 15, 'kitten': 6, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '041A' '042A' '043A' '045A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '044A' '046A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'041A'}\n",
      "Moved to Test Set:\n",
      "{'041A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '042A' '043A' '045A' '046A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '041A' '044A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "Length of X_train_val:\n",
      "784\n",
      "Length of y_train_val:\n",
      "784\n",
      "Length of groups_train_val:\n",
      "784\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten     85\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     116\n",
      "kitten     86\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten    147\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     116\n",
      "kitten     24\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 944, 2: 825, 1: 735})\n",
      "Epoch 1/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.9237 - accuracy: 0.6194\n",
      "Epoch 2/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.7335 - accuracy: 0.7045\n",
      "Epoch 3/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.6496 - accuracy: 0.7388\n",
      "Epoch 4/1500\n",
      "79/79 [==============================] - 0s 937us/step - loss: 0.6166 - accuracy: 0.7544\n",
      "Epoch 5/1500\n",
      "79/79 [==============================] - 0s 955us/step - loss: 0.6039 - accuracy: 0.7600\n",
      "Epoch 6/1500\n",
      "79/79 [==============================] - 0s 948us/step - loss: 0.5563 - accuracy: 0.7764\n",
      "Epoch 7/1500\n",
      "79/79 [==============================] - 0s 967us/step - loss: 0.5494 - accuracy: 0.7887\n",
      "Epoch 8/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5143 - accuracy: 0.7915\n",
      "Epoch 9/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4853 - accuracy: 0.7939\n",
      "Epoch 10/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4714 - accuracy: 0.8187\n",
      "Epoch 11/1500\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 0.4744 - accuracy: 0.8103\n",
      "Epoch 12/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4727 - accuracy: 0.8159\n",
      "Epoch 13/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4375 - accuracy: 0.8227\n",
      "Epoch 14/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4270 - accuracy: 0.8259\n",
      "Epoch 15/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4480 - accuracy: 0.8191\n",
      "Epoch 16/1500\n",
      "79/79 [==============================] - 0s 936us/step - loss: 0.4132 - accuracy: 0.8311\n",
      "Epoch 17/1500\n",
      "79/79 [==============================] - 0s 960us/step - loss: 0.3915 - accuracy: 0.8403\n",
      "Epoch 18/1500\n",
      "79/79 [==============================] - 0s 942us/step - loss: 0.4024 - accuracy: 0.8331\n",
      "Epoch 19/1500\n",
      "79/79 [==============================] - 0s 925us/step - loss: 0.3757 - accuracy: 0.8522\n",
      "Epoch 20/1500\n",
      "79/79 [==============================] - 0s 970us/step - loss: 0.3757 - accuracy: 0.8518\n",
      "Epoch 21/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3834 - accuracy: 0.8478\n",
      "Epoch 22/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3547 - accuracy: 0.8582\n",
      "Epoch 23/1500\n",
      "79/79 [==============================] - 0s 982us/step - loss: 0.3773 - accuracy: 0.8478\n",
      "Epoch 24/1500\n",
      "79/79 [==============================] - 0s 989us/step - loss: 0.3405 - accuracy: 0.8650\n",
      "Epoch 25/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3458 - accuracy: 0.8574\n",
      "Epoch 26/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3358 - accuracy: 0.8630\n",
      "Epoch 27/1500\n",
      "79/79 [==============================] - 0s 994us/step - loss: 0.3294 - accuracy: 0.8682\n",
      "Epoch 28/1500\n",
      "79/79 [==============================] - 0s 961us/step - loss: 0.3478 - accuracy: 0.8582\n",
      "Epoch 29/1500\n",
      "79/79 [==============================] - 0s 994us/step - loss: 0.3521 - accuracy: 0.8578\n",
      "Epoch 30/1500\n",
      "79/79 [==============================] - 0s 952us/step - loss: 0.3146 - accuracy: 0.8714\n",
      "Epoch 31/1500\n",
      "79/79 [==============================] - 0s 928us/step - loss: 0.3146 - accuracy: 0.8754\n",
      "Epoch 32/1500\n",
      "79/79 [==============================] - 0s 923us/step - loss: 0.3186 - accuracy: 0.8726\n",
      "Epoch 33/1500\n",
      "79/79 [==============================] - 0s 922us/step - loss: 0.3321 - accuracy: 0.8670\n",
      "Epoch 34/1500\n",
      "79/79 [==============================] - 0s 911us/step - loss: 0.3103 - accuracy: 0.8786\n",
      "Epoch 35/1500\n",
      "79/79 [==============================] - 0s 956us/step - loss: 0.3238 - accuracy: 0.8726\n",
      "Epoch 36/1500\n",
      "79/79 [==============================] - 0s 924us/step - loss: 0.3127 - accuracy: 0.8758\n",
      "Epoch 37/1500\n",
      "79/79 [==============================] - 0s 922us/step - loss: 0.3235 - accuracy: 0.8746\n",
      "Epoch 38/1500\n",
      "79/79 [==============================] - 0s 903us/step - loss: 0.3121 - accuracy: 0.8790\n",
      "Epoch 39/1500\n",
      "79/79 [==============================] - 0s 949us/step - loss: 0.2914 - accuracy: 0.8898\n",
      "Epoch 40/1500\n",
      "79/79 [==============================] - 0s 909us/step - loss: 0.2898 - accuracy: 0.8866\n",
      "Epoch 41/1500\n",
      "79/79 [==============================] - 0s 919us/step - loss: 0.2928 - accuracy: 0.8918\n",
      "Epoch 42/1500\n",
      "79/79 [==============================] - 0s 908us/step - loss: 0.2794 - accuracy: 0.8954\n",
      "Epoch 43/1500\n",
      "79/79 [==============================] - 0s 909us/step - loss: 0.2804 - accuracy: 0.8902\n",
      "Epoch 44/1500\n",
      "79/79 [==============================] - 0s 912us/step - loss: 0.2826 - accuracy: 0.8898\n",
      "Epoch 45/1500\n",
      "79/79 [==============================] - 0s 923us/step - loss: 0.2949 - accuracy: 0.8838\n",
      "Epoch 46/1500\n",
      "79/79 [==============================] - 0s 909us/step - loss: 0.2756 - accuracy: 0.8974\n",
      "Epoch 47/1500\n",
      "79/79 [==============================] - 0s 921us/step - loss: 0.2716 - accuracy: 0.8894\n",
      "Epoch 48/1500\n",
      "79/79 [==============================] - 0s 905us/step - loss: 0.2581 - accuracy: 0.8998\n",
      "Epoch 49/1500\n",
      "79/79 [==============================] - 0s 904us/step - loss: 0.2500 - accuracy: 0.9038\n",
      "Epoch 50/1500\n",
      "79/79 [==============================] - 0s 910us/step - loss: 0.2706 - accuracy: 0.8946\n",
      "Epoch 51/1500\n",
      "79/79 [==============================] - 0s 919us/step - loss: 0.2575 - accuracy: 0.9014\n",
      "Epoch 52/1500\n",
      "79/79 [==============================] - 0s 895us/step - loss: 0.2596 - accuracy: 0.9030\n",
      "Epoch 53/1500\n",
      "79/79 [==============================] - 0s 914us/step - loss: 0.2528 - accuracy: 0.9050\n",
      "Epoch 54/1500\n",
      "79/79 [==============================] - 0s 943us/step - loss: 0.2459 - accuracy: 0.9038\n",
      "Epoch 55/1500\n",
      "79/79 [==============================] - 0s 914us/step - loss: 0.2482 - accuracy: 0.9026\n",
      "Epoch 56/1500\n",
      "79/79 [==============================] - 0s 946us/step - loss: 0.2550 - accuracy: 0.8994\n",
      "Epoch 57/1500\n",
      "79/79 [==============================] - 0s 934us/step - loss: 0.2433 - accuracy: 0.9054\n",
      "Epoch 58/1500\n",
      "79/79 [==============================] - 0s 930us/step - loss: 0.2440 - accuracy: 0.9054\n",
      "Epoch 59/1500\n",
      "79/79 [==============================] - 0s 932us/step - loss: 0.2421 - accuracy: 0.9121\n",
      "Epoch 60/1500\n",
      "79/79 [==============================] - 0s 927us/step - loss: 0.2346 - accuracy: 0.9117\n",
      "Epoch 61/1500\n",
      "79/79 [==============================] - 0s 910us/step - loss: 0.2443 - accuracy: 0.9073\n",
      "Epoch 62/1500\n",
      "79/79 [==============================] - 0s 934us/step - loss: 0.2403 - accuracy: 0.9042\n",
      "Epoch 63/1500\n",
      "79/79 [==============================] - 0s 917us/step - loss: 0.2310 - accuracy: 0.9133\n",
      "Epoch 64/1500\n",
      "79/79 [==============================] - 0s 904us/step - loss: 0.2458 - accuracy: 0.9133\n",
      "Epoch 65/1500\n",
      "79/79 [==============================] - 0s 946us/step - loss: 0.2290 - accuracy: 0.9137\n",
      "Epoch 66/1500\n",
      "79/79 [==============================] - 0s 944us/step - loss: 0.2224 - accuracy: 0.9113\n",
      "Epoch 67/1500\n",
      "79/79 [==============================] - 0s 914us/step - loss: 0.2187 - accuracy: 0.9169\n",
      "Epoch 68/1500\n",
      "79/79 [==============================] - 0s 905us/step - loss: 0.2287 - accuracy: 0.9193\n",
      "Epoch 69/1500\n",
      "79/79 [==============================] - 0s 932us/step - loss: 0.2230 - accuracy: 0.9193\n",
      "Epoch 70/1500\n",
      "79/79 [==============================] - 0s 936us/step - loss: 0.2349 - accuracy: 0.9153\n",
      "Epoch 71/1500\n",
      "79/79 [==============================] - 0s 908us/step - loss: 0.2391 - accuracy: 0.9125\n",
      "Epoch 72/1500\n",
      "79/79 [==============================] - 0s 925us/step - loss: 0.2166 - accuracy: 0.9149\n",
      "Epoch 73/1500\n",
      "79/79 [==============================] - 0s 896us/step - loss: 0.2170 - accuracy: 0.9233\n",
      "Epoch 74/1500\n",
      "79/79 [==============================] - 0s 921us/step - loss: 0.2022 - accuracy: 0.9265\n",
      "Epoch 75/1500\n",
      "79/79 [==============================] - 0s 915us/step - loss: 0.2139 - accuracy: 0.9225\n",
      "Epoch 76/1500\n",
      "79/79 [==============================] - 0s 911us/step - loss: 0.2014 - accuracy: 0.9273\n",
      "Epoch 77/1500\n",
      "79/79 [==============================] - 0s 906us/step - loss: 0.2095 - accuracy: 0.9237\n",
      "Epoch 78/1500\n",
      "79/79 [==============================] - 0s 924us/step - loss: 0.2236 - accuracy: 0.9117\n",
      "Epoch 79/1500\n",
      "79/79 [==============================] - 0s 920us/step - loss: 0.2121 - accuracy: 0.9213\n",
      "Epoch 80/1500\n",
      "79/79 [==============================] - 0s 911us/step - loss: 0.2007 - accuracy: 0.9333\n",
      "Epoch 81/1500\n",
      "79/79 [==============================] - 0s 949us/step - loss: 0.2057 - accuracy: 0.9189\n",
      "Epoch 82/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9261\n",
      "Epoch 83/1500\n",
      "79/79 [==============================] - 0s 940us/step - loss: 0.2140 - accuracy: 0.9253\n",
      "Epoch 84/1500\n",
      "79/79 [==============================] - 0s 957us/step - loss: 0.2065 - accuracy: 0.9225\n",
      "Epoch 85/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9293\n",
      "Epoch 86/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9301\n",
      "Epoch 87/1500\n",
      "79/79 [==============================] - 0s 974us/step - loss: 0.1967 - accuracy: 0.9297\n",
      "Epoch 88/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2054 - accuracy: 0.9249\n",
      "Epoch 89/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.9273\n",
      "Epoch 90/1500\n",
      "79/79 [==============================] - 0s 966us/step - loss: 0.1956 - accuracy: 0.9293\n",
      "Epoch 91/1500\n",
      "79/79 [==============================] - 0s 996us/step - loss: 0.1885 - accuracy: 0.9321\n",
      "Epoch 92/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2004 - accuracy: 0.9309\n",
      "Epoch 93/1500\n",
      "79/79 [==============================] - 0s 967us/step - loss: 0.1890 - accuracy: 0.9273\n",
      "Epoch 94/1500\n",
      "79/79 [==============================] - 0s 941us/step - loss: 0.1805 - accuracy: 0.9389\n",
      "Epoch 95/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1941 - accuracy: 0.9245\n",
      "Epoch 96/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9349\n",
      "Epoch 97/1500\n",
      "79/79 [==============================] - 0s 942us/step - loss: 0.1716 - accuracy: 0.9369\n",
      "Epoch 98/1500\n",
      "79/79 [==============================] - 0s 923us/step - loss: 0.1685 - accuracy: 0.9357\n",
      "Epoch 99/1500\n",
      "79/79 [==============================] - 0s 970us/step - loss: 0.1764 - accuracy: 0.9329\n",
      "Epoch 100/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.9361\n",
      "Epoch 101/1500\n",
      "79/79 [==============================] - 0s 968us/step - loss: 0.1739 - accuracy: 0.9361\n",
      "Epoch 102/1500\n",
      "79/79 [==============================] - 0s 912us/step - loss: 0.1750 - accuracy: 0.9369\n",
      "Epoch 103/1500\n",
      "79/79 [==============================] - 0s 925us/step - loss: 0.1835 - accuracy: 0.9305\n",
      "Epoch 104/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1912 - accuracy: 0.9317\n",
      "Epoch 105/1500\n",
      "79/79 [==============================] - 0s 969us/step - loss: 0.1959 - accuracy: 0.9257\n",
      "Epoch 106/1500\n",
      "79/79 [==============================] - 0s 952us/step - loss: 0.1832 - accuracy: 0.9317\n",
      "Epoch 107/1500\n",
      "79/79 [==============================] - 0s 970us/step - loss: 0.1647 - accuracy: 0.9357\n",
      "Epoch 108/1500\n",
      "79/79 [==============================] - 0s 950us/step - loss: 0.1740 - accuracy: 0.9361\n",
      "Epoch 109/1500\n",
      "79/79 [==============================] - 0s 983us/step - loss: 0.1647 - accuracy: 0.9429\n",
      "Epoch 110/1500\n",
      "79/79 [==============================] - 0s 975us/step - loss: 0.1706 - accuracy: 0.9381\n",
      "Epoch 111/1500\n",
      "79/79 [==============================] - 0s 944us/step - loss: 0.1766 - accuracy: 0.9365\n",
      "Epoch 112/1500\n",
      "79/79 [==============================] - 0s 937us/step - loss: 0.1790 - accuracy: 0.9373\n",
      "Epoch 113/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9381\n",
      "Epoch 114/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9445\n",
      "Epoch 115/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.9361\n",
      "Epoch 116/1500\n",
      "79/79 [==============================] - 0s 997us/step - loss: 0.1713 - accuracy: 0.9361\n",
      "Epoch 117/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9445\n",
      "Epoch 118/1500\n",
      "79/79 [==============================] - 0s 971us/step - loss: 0.1783 - accuracy: 0.9317\n",
      "Epoch 119/1500\n",
      "79/79 [==============================] - 0s 904us/step - loss: 0.1717 - accuracy: 0.9377\n",
      "Epoch 120/1500\n",
      "79/79 [==============================] - 0s 946us/step - loss: 0.1810 - accuracy: 0.9293\n",
      "Epoch 121/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.9373\n",
      "Epoch 122/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1692 - accuracy: 0.9361\n",
      "Epoch 123/1500\n",
      "79/79 [==============================] - 0s 976us/step - loss: 0.1534 - accuracy: 0.9437\n",
      "Epoch 124/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9461\n",
      "Epoch 125/1500\n",
      "79/79 [==============================] - 0s 997us/step - loss: 0.1597 - accuracy: 0.9421\n",
      "Epoch 126/1500\n",
      "79/79 [==============================] - 0s 939us/step - loss: 0.1673 - accuracy: 0.9377\n",
      "Epoch 127/1500\n",
      "79/79 [==============================] - 0s 938us/step - loss: 0.1689 - accuracy: 0.9333\n",
      "Epoch 128/1500\n",
      "79/79 [==============================] - 0s 968us/step - loss: 0.1635 - accuracy: 0.9485\n",
      "Epoch 129/1500\n",
      "79/79 [==============================] - 0s 941us/step - loss: 0.1659 - accuracy: 0.9377\n",
      "Epoch 130/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.9517\n",
      "Epoch 131/1500\n",
      "79/79 [==============================] - 0s 955us/step - loss: 0.1677 - accuracy: 0.9385\n",
      "Epoch 132/1500\n",
      "79/79 [==============================] - 0s 905us/step - loss: 0.1690 - accuracy: 0.9393\n",
      "Epoch 133/1500\n",
      "79/79 [==============================] - 0s 978us/step - loss: 0.1528 - accuracy: 0.9425\n",
      "Epoch 134/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9485\n",
      "Epoch 135/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.9501\n",
      "Epoch 136/1500\n",
      "79/79 [==============================] - 0s 931us/step - loss: 0.1532 - accuracy: 0.9433\n",
      "Epoch 137/1500\n",
      "79/79 [==============================] - 0s 963us/step - loss: 0.1464 - accuracy: 0.9477\n",
      "Epoch 138/1500\n",
      "79/79 [==============================] - 0s 991us/step - loss: 0.1380 - accuracy: 0.9457\n",
      "Epoch 139/1500\n",
      "79/79 [==============================] - 0s 975us/step - loss: 0.1558 - accuracy: 0.9445\n",
      "Epoch 140/1500\n",
      "79/79 [==============================] - 0s 984us/step - loss: 0.1420 - accuracy: 0.9425\n",
      "Epoch 141/1500\n",
      "79/79 [==============================] - 0s 933us/step - loss: 0.1709 - accuracy: 0.9377\n",
      "Epoch 142/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9453\n",
      "Epoch 143/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9521\n",
      "Epoch 144/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9533\n",
      "Epoch 145/1500\n",
      "79/79 [==============================] - 0s 932us/step - loss: 0.1711 - accuracy: 0.9369\n",
      "Epoch 146/1500\n",
      "79/79 [==============================] - 0s 934us/step - loss: 0.1475 - accuracy: 0.9421\n",
      "Epoch 147/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9469\n",
      "Epoch 148/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9529\n",
      "Epoch 149/1500\n",
      "79/79 [==============================] - 0s 998us/step - loss: 0.1412 - accuracy: 0.9473\n",
      "Epoch 150/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1295 - accuracy: 0.9549\n",
      "Epoch 151/1500\n",
      "79/79 [==============================] - 0s 969us/step - loss: 0.1403 - accuracy: 0.9473\n",
      "Epoch 152/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1369 - accuracy: 0.9497\n",
      "Epoch 153/1500\n",
      "79/79 [==============================] - 0s 990us/step - loss: 0.1340 - accuracy: 0.9537\n",
      "Epoch 154/1500\n",
      "79/79 [==============================] - 0s 993us/step - loss: 0.1384 - accuracy: 0.9537\n",
      "Epoch 155/1500\n",
      "79/79 [==============================] - 0s 995us/step - loss: 0.1274 - accuracy: 0.9537\n",
      "Epoch 156/1500\n",
      "79/79 [==============================] - 0s 957us/step - loss: 0.1328 - accuracy: 0.9497\n",
      "Epoch 157/1500\n",
      "79/79 [==============================] - 0s 932us/step - loss: 0.1264 - accuracy: 0.9541\n",
      "Epoch 158/1500\n",
      "79/79 [==============================] - 0s 925us/step - loss: 0.1169 - accuracy: 0.9545\n",
      "Epoch 159/1500\n",
      "79/79 [==============================] - 0s 949us/step - loss: 0.1430 - accuracy: 0.9473\n",
      "Epoch 160/1500\n",
      "79/79 [==============================] - 0s 974us/step - loss: 0.1299 - accuracy: 0.9525\n",
      "Epoch 161/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9477\n",
      "Epoch 162/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9481\n",
      "Epoch 163/1500\n",
      "79/79 [==============================] - 0s 968us/step - loss: 0.1476 - accuracy: 0.9405\n",
      "Epoch 164/1500\n",
      "79/79 [==============================] - 0s 947us/step - loss: 0.1331 - accuracy: 0.9541\n",
      "Epoch 165/1500\n",
      "79/79 [==============================] - 0s 975us/step - loss: 0.1536 - accuracy: 0.9437\n",
      "Epoch 166/1500\n",
      "79/79 [==============================] - 0s 946us/step - loss: 0.1407 - accuracy: 0.9469\n",
      "Epoch 167/1500\n",
      "79/79 [==============================] - 0s 927us/step - loss: 0.1374 - accuracy: 0.9489\n",
      "Epoch 168/1500\n",
      "79/79 [==============================] - 0s 935us/step - loss: 0.1549 - accuracy: 0.9437\n",
      "Epoch 169/1500\n",
      "79/79 [==============================] - 0s 971us/step - loss: 0.1432 - accuracy: 0.9513\n",
      "Epoch 170/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9537\n",
      "Epoch 171/1500\n",
      "79/79 [==============================] - 0s 989us/step - loss: 0.1259 - accuracy: 0.9573\n",
      "Epoch 172/1500\n",
      "79/79 [==============================] - 0s 985us/step - loss: 0.1425 - accuracy: 0.9493\n",
      "Epoch 173/1500\n",
      "79/79 [==============================] - 0s 965us/step - loss: 0.1187 - accuracy: 0.9589\n",
      "Epoch 174/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9489\n",
      "Epoch 175/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9609\n",
      "Epoch 176/1500\n",
      "79/79 [==============================] - 0s 995us/step - loss: 0.1215 - accuracy: 0.9545\n",
      "Epoch 177/1500\n",
      "79/79 [==============================] - 0s 989us/step - loss: 0.1281 - accuracy: 0.9545\n",
      "Epoch 178/1500\n",
      "79/79 [==============================] - 0s 958us/step - loss: 0.1385 - accuracy: 0.9493\n",
      "Epoch 179/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9561\n",
      "Epoch 180/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9521\n",
      "Epoch 181/1500\n",
      "79/79 [==============================] - 0s 948us/step - loss: 0.1344 - accuracy: 0.9497\n",
      "Epoch 182/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9565\n",
      "Epoch 183/1500\n",
      "79/79 [==============================] - 0s 981us/step - loss: 0.1357 - accuracy: 0.9549\n",
      "Epoch 184/1500\n",
      "79/79 [==============================] - 0s 991us/step - loss: 0.1246 - accuracy: 0.9573\n",
      "Epoch 185/1500\n",
      "79/79 [==============================] - 0s 990us/step - loss: 0.1222 - accuracy: 0.9553\n",
      "Epoch 186/1500\n",
      "79/79 [==============================] - 0s 957us/step - loss: 0.1210 - accuracy: 0.9533\n",
      "Epoch 187/1500\n",
      "79/79 [==============================] - 0s 944us/step - loss: 0.1121 - accuracy: 0.9609\n",
      "Epoch 188/1500\n",
      "79/79 [==============================] - 0s 968us/step - loss: 0.1203 - accuracy: 0.9545\n",
      "Epoch 189/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9613\n",
      "Epoch 190/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9533\n",
      "Epoch 191/1500\n",
      "79/79 [==============================] - 0s 985us/step - loss: 0.1487 - accuracy: 0.9457\n",
      "Epoch 192/1500\n",
      "79/79 [==============================] - 0s 966us/step - loss: 0.1198 - accuracy: 0.9625\n",
      "Epoch 193/1500\n",
      "79/79 [==============================] - 0s 958us/step - loss: 0.1250 - accuracy: 0.9577\n",
      "Epoch 194/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9517\n",
      "Epoch 195/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9457\n",
      "Epoch 196/1500\n",
      "79/79 [==============================] - 0s 938us/step - loss: 0.1350 - accuracy: 0.9525\n",
      "Epoch 197/1500\n",
      "79/79 [==============================] - 0s 984us/step - loss: 0.1247 - accuracy: 0.9605\n",
      "Epoch 198/1500\n",
      "79/79 [==============================] - 0s 963us/step - loss: 0.1127 - accuracy: 0.9585\n",
      "Epoch 199/1500\n",
      "79/79 [==============================] - 0s 967us/step - loss: 0.1292 - accuracy: 0.9521\n",
      "Epoch 200/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9593\n",
      "Epoch 201/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9645\n",
      "Epoch 202/1500\n",
      "79/79 [==============================] - 0s 996us/step - loss: 0.1196 - accuracy: 0.9609\n",
      "Epoch 203/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9533\n",
      "Epoch 204/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9521\n",
      "Epoch 205/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9529\n",
      "Epoch 206/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1161 - accuracy: 0.9593\n",
      "Epoch 207/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9661\n",
      "Epoch 208/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9605\n",
      "Epoch 209/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9613\n",
      "Epoch 210/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9597\n",
      "Epoch 211/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9633\n",
      "Epoch 212/1500\n",
      "79/79 [==============================] - 0s 988us/step - loss: 0.1316 - accuracy: 0.9541\n",
      "Epoch 213/1500\n",
      "79/79 [==============================] - 0s 996us/step - loss: 0.1188 - accuracy: 0.9633\n",
      "Epoch 214/1500\n",
      "79/79 [==============================] - 0s 932us/step - loss: 0.1119 - accuracy: 0.9569\n",
      "Epoch 215/1500\n",
      "79/79 [==============================] - 0s 958us/step - loss: 0.1169 - accuracy: 0.9605\n",
      "Epoch 216/1500\n",
      "79/79 [==============================] - 0s 971us/step - loss: 0.1164 - accuracy: 0.9565\n",
      "Epoch 217/1500\n",
      "79/79 [==============================] - 0s 952us/step - loss: 0.1321 - accuracy: 0.9545\n",
      "Epoch 218/1500\n",
      "79/79 [==============================] - 0s 970us/step - loss: 0.0998 - accuracy: 0.9657\n",
      "Epoch 219/1500\n",
      "79/79 [==============================] - 0s 938us/step - loss: 0.1127 - accuracy: 0.9609\n",
      "Epoch 220/1500\n",
      "79/79 [==============================] - 0s 924us/step - loss: 0.1163 - accuracy: 0.9581\n",
      "Epoch 221/1500\n",
      "79/79 [==============================] - 0s 999us/step - loss: 0.0990 - accuracy: 0.9633\n",
      "Epoch 222/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9549\n",
      "Epoch 223/1500\n",
      "79/79 [==============================] - 0s 974us/step - loss: 0.1053 - accuracy: 0.9653\n",
      "Epoch 224/1500\n",
      "79/79 [==============================] - 0s 982us/step - loss: 0.1244 - accuracy: 0.9565\n",
      "Epoch 225/1500\n",
      "79/79 [==============================] - 0s 982us/step - loss: 0.1183 - accuracy: 0.9609\n",
      "Epoch 226/1500\n",
      "79/79 [==============================] - 0s 995us/step - loss: 0.1141 - accuracy: 0.9617\n",
      "Epoch 227/1500\n",
      "79/79 [==============================] - 0s 972us/step - loss: 0.1034 - accuracy: 0.9593\n",
      "Epoch 228/1500\n",
      "79/79 [==============================] - 0s 969us/step - loss: 0.0989 - accuracy: 0.9629\n",
      "Epoch 229/1500\n",
      "79/79 [==============================] - 0s 959us/step - loss: 0.1025 - accuracy: 0.9637\n",
      "Epoch 230/1500\n",
      "79/79 [==============================] - 0s 963us/step - loss: 0.0906 - accuracy: 0.9688\n",
      "Epoch 231/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9589\n",
      "Epoch 232/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9561\n",
      "Epoch 233/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9637\n",
      "Epoch 234/1500\n",
      "79/79 [==============================] - 0s 979us/step - loss: 0.1163 - accuracy: 0.9545\n",
      "Epoch 235/1500\n",
      "79/79 [==============================] - 0s 991us/step - loss: 0.1005 - accuracy: 0.9649\n",
      "Epoch 236/1500\n",
      "79/79 [==============================] - 0s 990us/step - loss: 0.1112 - accuracy: 0.9633\n",
      "Epoch 237/1500\n",
      "79/79 [==============================] - 0s 988us/step - loss: 0.1116 - accuracy: 0.9593\n",
      "Epoch 238/1500\n",
      "79/79 [==============================] - 0s 969us/step - loss: 0.1191 - accuracy: 0.9581\n",
      "Epoch 239/1500\n",
      "79/79 [==============================] - 0s 980us/step - loss: 0.1247 - accuracy: 0.9545\n",
      "Epoch 240/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9525\n",
      "Epoch 241/1500\n",
      "79/79 [==============================] - 0s 950us/step - loss: 0.1074 - accuracy: 0.9597\n",
      "Epoch 242/1500\n",
      "79/79 [==============================] - 0s 946us/step - loss: 0.1014 - accuracy: 0.9645\n",
      "Epoch 243/1500\n",
      "79/79 [==============================] - 0s 962us/step - loss: 0.1147 - accuracy: 0.9565\n",
      "Epoch 244/1500\n",
      "79/79 [==============================] - 0s 951us/step - loss: 0.1132 - accuracy: 0.9597\n",
      "Epoch 245/1500\n",
      "79/79 [==============================] - 0s 960us/step - loss: 0.0986 - accuracy: 0.9621\n",
      "Epoch 246/1500\n",
      "79/79 [==============================] - 0s 935us/step - loss: 0.1115 - accuracy: 0.9641\n",
      "Epoch 247/1500\n",
      "79/79 [==============================] - 0s 953us/step - loss: 0.1205 - accuracy: 0.9505\n",
      "Epoch 248/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.0893 - accuracy: 0.9669\n",
      "Epoch 249/1500\n",
      "79/79 [==============================] - 0s 945us/step - loss: 0.1158 - accuracy: 0.9565\n",
      "Epoch 250/1500\n",
      "79/79 [==============================] - 0s 953us/step - loss: 0.1085 - accuracy: 0.9593\n",
      "Epoch 251/1500\n",
      "79/79 [==============================] - 0s 984us/step - loss: 0.1161 - accuracy: 0.9581\n",
      "Epoch 252/1500\n",
      "79/79 [==============================] - 0s 982us/step - loss: 0.1009 - accuracy: 0.9673\n",
      "Epoch 253/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9696\n",
      "Epoch 254/1500\n",
      "79/79 [==============================] - 0s 933us/step - loss: 0.1072 - accuracy: 0.9621\n",
      "Epoch 255/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9696\n",
      "Epoch 256/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9625\n",
      "Epoch 257/1500\n",
      "79/79 [==============================] - 0s 989us/step - loss: 0.1087 - accuracy: 0.9621\n",
      "Epoch 258/1500\n",
      "79/79 [==============================] - 0s 983us/step - loss: 0.1062 - accuracy: 0.9621\n",
      "Epoch 259/1500\n",
      "79/79 [==============================] - 0s 991us/step - loss: 0.1021 - accuracy: 0.9673\n",
      "Epoch 260/1500\n",
      "79/79 [==============================] - 0s 994us/step - loss: 0.0943 - accuracy: 0.9696\n",
      "Epoch 261/1500\n",
      "79/79 [==============================] - 0s 962us/step - loss: 0.0979 - accuracy: 0.9653\n",
      "Epoch 262/1500\n",
      "79/79 [==============================] - 0s 984us/step - loss: 0.1010 - accuracy: 0.9629\n",
      "Epoch 263/1500\n",
      "79/79 [==============================] - 0s 994us/step - loss: 0.1072 - accuracy: 0.9637\n",
      "Epoch 264/1500\n",
      "79/79 [==============================] - 0s 961us/step - loss: 0.0975 - accuracy: 0.9681\n",
      "Epoch 265/1500\n",
      "79/79 [==============================] - 0s 980us/step - loss: 0.1014 - accuracy: 0.9641\n",
      "Epoch 266/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9673\n",
      "Epoch 267/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9657\n",
      "Epoch 268/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9609\n",
      "Epoch 269/1500\n",
      "79/79 [==============================] - 0s 947us/step - loss: 0.0906 - accuracy: 0.9657\n",
      "Epoch 270/1500\n",
      "79/79 [==============================] - 0s 979us/step - loss: 0.0990 - accuracy: 0.9665\n",
      "Epoch 271/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9625\n",
      "Epoch 272/1500\n",
      "79/79 [==============================] - 0s 982us/step - loss: 0.1030 - accuracy: 0.9653\n",
      "Epoch 273/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9696\n",
      "Epoch 274/1500\n",
      "79/79 [==============================] - 0s 947us/step - loss: 0.1103 - accuracy: 0.9601\n",
      "Epoch 275/1500\n",
      "79/79 [==============================] - 0s 976us/step - loss: 0.1116 - accuracy: 0.9629\n",
      "Epoch 276/1500\n",
      "79/79 [==============================] - 0s 980us/step - loss: 0.0987 - accuracy: 0.9669\n",
      "Epoch 277/1500\n",
      "79/79 [==============================] - 0s 939us/step - loss: 0.1091 - accuracy: 0.9605\n",
      "Epoch 278/1500\n",
      "79/79 [==============================] - 0s 947us/step - loss: 0.0944 - accuracy: 0.9685\n",
      "Epoch 279/1500\n",
      "79/79 [==============================] - 0s 943us/step - loss: 0.0929 - accuracy: 0.9716\n",
      "Epoch 280/1500\n",
      "79/79 [==============================] - 0s 942us/step - loss: 0.0906 - accuracy: 0.9692\n",
      "Epoch 281/1500\n",
      "79/79 [==============================] - 0s 941us/step - loss: 0.1062 - accuracy: 0.9569\n",
      "Epoch 282/1500\n",
      "79/79 [==============================] - 0s 953us/step - loss: 0.0940 - accuracy: 0.9641\n",
      "Epoch 283/1500\n",
      "79/79 [==============================] - 0s 943us/step - loss: 0.1124 - accuracy: 0.9581\n",
      "Epoch 284/1500\n",
      "79/79 [==============================] - 0s 926us/step - loss: 0.0954 - accuracy: 0.9657\n",
      "Epoch 285/1500\n",
      "79/79 [==============================] - 0s 923us/step - loss: 0.1117 - accuracy: 0.9625\n",
      "Epoch 286/1500\n",
      "79/79 [==============================] - 0s 948us/step - loss: 0.1004 - accuracy: 0.9657\n",
      "Epoch 287/1500\n",
      "79/79 [==============================] - 0s 938us/step - loss: 0.0904 - accuracy: 0.9688\n",
      "Epoch 288/1500\n",
      "79/79 [==============================] - 0s 946us/step - loss: 0.0827 - accuracy: 0.9696\n",
      "Epoch 289/1500\n",
      "79/79 [==============================] - 0s 938us/step - loss: 0.1011 - accuracy: 0.9637\n",
      "Epoch 290/1500\n",
      "79/79 [==============================] - 0s 932us/step - loss: 0.0900 - accuracy: 0.9685\n",
      "Epoch 291/1500\n",
      "79/79 [==============================] - 0s 918us/step - loss: 0.0920 - accuracy: 0.9688\n",
      "Epoch 292/1500\n",
      "79/79 [==============================] - 0s 943us/step - loss: 0.1167 - accuracy: 0.9585\n",
      "Epoch 293/1500\n",
      "79/79 [==============================] - 0s 929us/step - loss: 0.0850 - accuracy: 0.9716\n",
      "Epoch 294/1500\n",
      "79/79 [==============================] - 0s 921us/step - loss: 0.0911 - accuracy: 0.9673\n",
      "Epoch 295/1500\n",
      "79/79 [==============================] - 0s 955us/step - loss: 0.1083 - accuracy: 0.9633\n",
      "Epoch 296/1500\n",
      "79/79 [==============================] - 0s 934us/step - loss: 0.0846 - accuracy: 0.9716\n",
      "Epoch 297/1500\n",
      "79/79 [==============================] - 0s 940us/step - loss: 0.0890 - accuracy: 0.9692\n",
      "Epoch 298/1500\n",
      "79/79 [==============================] - 0s 949us/step - loss: 0.0952 - accuracy: 0.9661\n",
      "Epoch 299/1500\n",
      "79/79 [==============================] - 0s 927us/step - loss: 0.0839 - accuracy: 0.9692\n",
      "Epoch 300/1500\n",
      "79/79 [==============================] - 0s 946us/step - loss: 0.1061 - accuracy: 0.9625\n",
      "Epoch 301/1500\n",
      "79/79 [==============================] - 0s 950us/step - loss: 0.1057 - accuracy: 0.9609\n",
      "Epoch 302/1500\n",
      "79/79 [==============================] - 0s 944us/step - loss: 0.0707 - accuracy: 0.9776\n",
      "Epoch 303/1500\n",
      "79/79 [==============================] - 0s 932us/step - loss: 0.0818 - accuracy: 0.9716\n",
      "Epoch 304/1500\n",
      "79/79 [==============================] - 0s 949us/step - loss: 0.0828 - accuracy: 0.9716\n",
      "Epoch 305/1500\n",
      "79/79 [==============================] - 0s 928us/step - loss: 0.0824 - accuracy: 0.9681\n",
      "Epoch 306/1500\n",
      "79/79 [==============================] - 0s 952us/step - loss: 0.0856 - accuracy: 0.9700\n",
      "Epoch 307/1500\n",
      "79/79 [==============================] - 0s 948us/step - loss: 0.1013 - accuracy: 0.9641\n",
      "Epoch 308/1500\n",
      "79/79 [==============================] - 0s 937us/step - loss: 0.0848 - accuracy: 0.9712\n",
      "Epoch 309/1500\n",
      "79/79 [==============================] - 0s 946us/step - loss: 0.1074 - accuracy: 0.9629\n",
      "Epoch 310/1500\n",
      "79/79 [==============================] - 0s 950us/step - loss: 0.0934 - accuracy: 0.9665\n",
      "Epoch 311/1500\n",
      "79/79 [==============================] - 0s 928us/step - loss: 0.1050 - accuracy: 0.9629\n",
      "Epoch 312/1500\n",
      "79/79 [==============================] - 0s 937us/step - loss: 0.0823 - accuracy: 0.9688\n",
      "Epoch 313/1500\n",
      "79/79 [==============================] - 0s 934us/step - loss: 0.0874 - accuracy: 0.9692\n",
      "Epoch 314/1500\n",
      "79/79 [==============================] - 0s 927us/step - loss: 0.0948 - accuracy: 0.9665\n",
      "Epoch 315/1500\n",
      "79/79 [==============================] - 0s 940us/step - loss: 0.0709 - accuracy: 0.9788\n",
      "Epoch 316/1500\n",
      "79/79 [==============================] - 0s 956us/step - loss: 0.0805 - accuracy: 0.9728\n",
      "Epoch 317/1500\n",
      "79/79 [==============================] - 0s 946us/step - loss: 0.0872 - accuracy: 0.9653\n",
      "Epoch 318/1500\n",
      "79/79 [==============================] - 0s 975us/step - loss: 0.0813 - accuracy: 0.9704\n",
      "Epoch 319/1500\n",
      "79/79 [==============================] - 0s 944us/step - loss: 0.0904 - accuracy: 0.9677\n",
      "Epoch 320/1500\n",
      "79/79 [==============================] - 0s 900us/step - loss: 0.0963 - accuracy: 0.9677\n",
      "Epoch 321/1500\n",
      "79/79 [==============================] - 0s 971us/step - loss: 0.0885 - accuracy: 0.9688\n",
      "Epoch 322/1500\n",
      "79/79 [==============================] - 0s 944us/step - loss: 0.0878 - accuracy: 0.9657\n",
      "Epoch 323/1500\n",
      "79/79 [==============================] - 0s 955us/step - loss: 0.0920 - accuracy: 0.9665\n",
      "Epoch 324/1500\n",
      "79/79 [==============================] - 0s 941us/step - loss: 0.0880 - accuracy: 0.9681\n",
      "Epoch 325/1500\n",
      "79/79 [==============================] - 0s 928us/step - loss: 0.0855 - accuracy: 0.9720\n",
      "Epoch 326/1500\n",
      "79/79 [==============================] - 0s 920us/step - loss: 0.0854 - accuracy: 0.9692\n",
      "Epoch 327/1500\n",
      "79/79 [==============================] - 0s 932us/step - loss: 0.0895 - accuracy: 0.9700\n",
      "Epoch 328/1500\n",
      "79/79 [==============================] - 0s 946us/step - loss: 0.0789 - accuracy: 0.9732\n",
      "Epoch 329/1500\n",
      "79/79 [==============================] - 0s 955us/step - loss: 0.0969 - accuracy: 0.9669\n",
      "Epoch 330/1500\n",
      "79/79 [==============================] - 0s 946us/step - loss: 0.1022 - accuracy: 0.9645\n",
      "Epoch 331/1500\n",
      "79/79 [==============================] - 0s 938us/step - loss: 0.0845 - accuracy: 0.9720\n",
      "Epoch 332/1500\n",
      "56/79 [====================>.........] - ETA: 0s - loss: 0.0998 - accuracy: 0.9665Restoring model weights from the end of the best epoch: 302.\n",
      "79/79 [==============================] - 0s 954us/step - loss: 0.1014 - accuracy: 0.9637\n",
      "Epoch 332: early stopping\n",
      "5/5 [==============================] - 0s 875us/step - loss: 0.9318 - accuracy: 0.7124\n",
      "5/5 [==============================] - 0s 657us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (19/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "Final Test Results - Loss: 0.931765615940094, Accuracy: 0.7124183177947998, Precision: 0.6359958720330237, Recall: 0.6488358384910109, F1 Score: 0.612784894411152\n",
      "Confusion Matrix:\n",
      " [[86  3 27]\n",
      " [ 8 16  0]\n",
      " [ 6  0  7]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "005A    10\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "027A     7\n",
      "008A     6\n",
      "108A     6\n",
      "109A     6\n",
      "053A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "003A     4\n",
      "105A     4\n",
      "009A     4\n",
      "026A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "012A     3\n",
      "113A     3\n",
      "014A     3\n",
      "060A     3\n",
      "011A     2\n",
      "102A     2\n",
      "032A     2\n",
      "069A     2\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "038A     2\n",
      "019B     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "092A     1\n",
      "004A     1\n",
      "041A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "073A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "067A    19\n",
      "042A    14\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "063A    11\n",
      "016A    10\n",
      "033A     9\n",
      "051B     9\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "117A     7\n",
      "050A     7\n",
      "007A     6\n",
      "075A     5\n",
      "025C     5\n",
      "087A     2\n",
      "061A     2\n",
      "025B     2\n",
      "043A     1\n",
      "066A     1\n",
      "048A     1\n",
      "088A     1\n",
      "091A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    263\n",
      "M    226\n",
      "F    177\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    111\n",
      "X     85\n",
      "F     75\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 015A, 001A, 103A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 109A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 113...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 067A, 002B, 091A, 039A, 063A, 013...\n",
      "kitten                       [047A, 042A, 050A, 043A, 048A]\n",
      "senior                 [116A, 051B, 117A, 016A, 061A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 11, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 5, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A' '011A'\n",
      " '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A' '045A'\n",
      " '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '062A' '064A' '065A' '069A' '070A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A'\n",
      " '039A' '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A'\n",
      " '067A' '068A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'070A'}\n",
      "Moved to Test Set:\n",
      "{'070A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A'\n",
      " '011A' '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A'\n",
      " '031A' '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A'\n",
      " '045A' '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '059A' '060A' '062A' '064A' '065A' '069A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A' '039A'\n",
      " '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A' '067A'\n",
      " '068A' '070A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "Length of X_train_val:\n",
      "700\n",
      "Length of y_train_val:\n",
      "700\n",
      "Length of groups_train_val:\n",
      "700\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     409\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     179\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     443\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     145\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 886, 2: 685, 1: 600})\n",
      "Epoch 1/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.9428 - accuracy: 0.5993\n",
      "Epoch 2/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7243 - accuracy: 0.6859\n",
      "Epoch 3/1500\n",
      "68/68 [==============================] - 0s 938us/step - loss: 0.6573 - accuracy: 0.7163\n",
      "Epoch 4/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5936 - accuracy: 0.7462\n",
      "Epoch 5/1500\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.5825 - accuracy: 0.7605\n",
      "Epoch 6/1500\n",
      "68/68 [==============================] - 0s 977us/step - loss: 0.5705 - accuracy: 0.7623\n",
      "Epoch 7/1500\n",
      "68/68 [==============================] - 0s 945us/step - loss: 0.5464 - accuracy: 0.7734\n",
      "Epoch 8/1500\n",
      "68/68 [==============================] - 0s 930us/step - loss: 0.5202 - accuracy: 0.7890\n",
      "Epoch 9/1500\n",
      "68/68 [==============================] - 0s 966us/step - loss: 0.5039 - accuracy: 0.7932\n",
      "Epoch 10/1500\n",
      "68/68 [==============================] - 0s 978us/step - loss: 0.4962 - accuracy: 0.7858\n",
      "Epoch 11/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4580 - accuracy: 0.8153\n",
      "Epoch 12/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4529 - accuracy: 0.8079\n",
      "Epoch 13/1500\n",
      "68/68 [==============================] - 0s 968us/step - loss: 0.4352 - accuracy: 0.8208\n",
      "Epoch 14/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.4389 - accuracy: 0.8144\n",
      "Epoch 15/1500\n",
      "68/68 [==============================] - 0s 943us/step - loss: 0.4078 - accuracy: 0.8222\n",
      "Epoch 16/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4030 - accuracy: 0.8342\n",
      "Epoch 17/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4069 - accuracy: 0.8259\n",
      "Epoch 18/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3774 - accuracy: 0.8402\n",
      "Epoch 19/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3904 - accuracy: 0.8333\n",
      "Epoch 20/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3733 - accuracy: 0.8466\n",
      "Epoch 21/1500\n",
      "68/68 [==============================] - 0s 994us/step - loss: 0.3608 - accuracy: 0.8508\n",
      "Epoch 22/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3429 - accuracy: 0.8572\n",
      "Epoch 23/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.3474 - accuracy: 0.8586\n",
      "Epoch 24/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3621 - accuracy: 0.8535\n",
      "Epoch 25/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3450 - accuracy: 0.8535\n",
      "Epoch 26/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3404 - accuracy: 0.8577\n",
      "Epoch 27/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3454 - accuracy: 0.8627\n",
      "Epoch 28/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3207 - accuracy: 0.8719\n",
      "Epoch 29/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3299 - accuracy: 0.8641\n",
      "Epoch 30/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3444 - accuracy: 0.8540\n",
      "Epoch 31/1500\n",
      "68/68 [==============================] - 0s 989us/step - loss: 0.3303 - accuracy: 0.8600\n",
      "Epoch 32/1500\n",
      "68/68 [==============================] - 0s 919us/step - loss: 0.3382 - accuracy: 0.8586\n",
      "Epoch 33/1500\n",
      "68/68 [==============================] - 0s 960us/step - loss: 0.3108 - accuracy: 0.8655\n",
      "Epoch 34/1500\n",
      "68/68 [==============================] - 0s 944us/step - loss: 0.3091 - accuracy: 0.8779\n",
      "Epoch 35/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.3159 - accuracy: 0.8766\n",
      "Epoch 36/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2874 - accuracy: 0.8853\n",
      "Epoch 37/1500\n",
      "68/68 [==============================] - 0s 925us/step - loss: 0.3128 - accuracy: 0.8706\n",
      "Epoch 38/1500\n",
      "68/68 [==============================] - 0s 954us/step - loss: 0.2761 - accuracy: 0.8867\n",
      "Epoch 39/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2639 - accuracy: 0.8959\n",
      "Epoch 40/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2840 - accuracy: 0.8848\n",
      "Epoch 41/1500\n",
      "68/68 [==============================] - 0s 994us/step - loss: 0.2746 - accuracy: 0.8885\n",
      "Epoch 42/1500\n",
      "68/68 [==============================] - 0s 929us/step - loss: 0.2687 - accuracy: 0.8964\n",
      "Epoch 43/1500\n",
      "68/68 [==============================] - 0s 937us/step - loss: 0.2740 - accuracy: 0.8812\n",
      "Epoch 44/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2631 - accuracy: 0.8945\n",
      "Epoch 45/1500\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.2612 - accuracy: 0.8931\n",
      "Epoch 46/1500\n",
      "68/68 [==============================] - 0s 947us/step - loss: 0.2652 - accuracy: 0.8895\n",
      "Epoch 47/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.9047\n",
      "Epoch 48/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.2552 - accuracy: 0.8936\n",
      "Epoch 49/1500\n",
      "68/68 [==============================] - 0s 934us/step - loss: 0.2441 - accuracy: 0.9070\n",
      "Epoch 50/1500\n",
      "68/68 [==============================] - 0s 938us/step - loss: 0.2490 - accuracy: 0.9079\n",
      "Epoch 51/1500\n",
      "68/68 [==============================] - 0s 977us/step - loss: 0.2418 - accuracy: 0.9042\n",
      "Epoch 52/1500\n",
      "68/68 [==============================] - 0s 991us/step - loss: 0.2510 - accuracy: 0.8968\n",
      "Epoch 53/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.9051\n",
      "Epoch 54/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2336 - accuracy: 0.9042\n",
      "Epoch 55/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2346 - accuracy: 0.9051\n",
      "Epoch 56/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2410 - accuracy: 0.9005\n",
      "Epoch 57/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2414 - accuracy: 0.8996\n",
      "Epoch 58/1500\n",
      "68/68 [==============================] - 0s 966us/step - loss: 0.2214 - accuracy: 0.9175\n",
      "Epoch 59/1500\n",
      "68/68 [==============================] - 0s 933us/step - loss: 0.2328 - accuracy: 0.9070\n",
      "Epoch 60/1500\n",
      "68/68 [==============================] - 0s 944us/step - loss: 0.2172 - accuracy: 0.9129\n",
      "Epoch 61/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.2298 - accuracy: 0.9148\n",
      "Epoch 62/1500\n",
      "68/68 [==============================] - 0s 964us/step - loss: 0.2221 - accuracy: 0.9139\n",
      "Epoch 63/1500\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.2329 - accuracy: 0.9093\n",
      "Epoch 64/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2305 - accuracy: 0.9051\n",
      "Epoch 65/1500\n",
      "68/68 [==============================] - 0s 948us/step - loss: 0.2296 - accuracy: 0.9111\n",
      "Epoch 66/1500\n",
      "68/68 [==============================] - 0s 920us/step - loss: 0.2194 - accuracy: 0.9157\n",
      "Epoch 67/1500\n",
      "68/68 [==============================] - 0s 948us/step - loss: 0.1880 - accuracy: 0.9281\n",
      "Epoch 68/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2114 - accuracy: 0.9185\n",
      "Epoch 69/1500\n",
      "68/68 [==============================] - 0s 956us/step - loss: 0.1940 - accuracy: 0.9235\n",
      "Epoch 70/1500\n",
      "68/68 [==============================] - 0s 943us/step - loss: 0.2231 - accuracy: 0.9157\n",
      "Epoch 71/1500\n",
      "68/68 [==============================] - 0s 942us/step - loss: 0.2107 - accuracy: 0.9189\n",
      "Epoch 72/1500\n",
      "68/68 [==============================] - 0s 997us/step - loss: 0.1825 - accuracy: 0.9309\n",
      "Epoch 73/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.9309\n",
      "Epoch 74/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9240\n",
      "Epoch 75/1500\n",
      "68/68 [==============================] - 0s 935us/step - loss: 0.2084 - accuracy: 0.9235\n",
      "Epoch 76/1500\n",
      "68/68 [==============================] - 0s 966us/step - loss: 0.2038 - accuracy: 0.9226\n",
      "Epoch 77/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1957 - accuracy: 0.9268\n",
      "Epoch 78/1500\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.1939 - accuracy: 0.9222\n",
      "Epoch 79/1500\n",
      "68/68 [==============================] - 0s 954us/step - loss: 0.1889 - accuracy: 0.9300\n",
      "Epoch 80/1500\n",
      "68/68 [==============================] - 0s 986us/step - loss: 0.1928 - accuracy: 0.9281\n",
      "Epoch 81/1500\n",
      "68/68 [==============================] - 0s 944us/step - loss: 0.1813 - accuracy: 0.9295\n",
      "Epoch 82/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.1853 - accuracy: 0.9295\n",
      "Epoch 83/1500\n",
      "68/68 [==============================] - 0s 938us/step - loss: 0.1865 - accuracy: 0.9346\n",
      "Epoch 84/1500\n",
      "68/68 [==============================] - 0s 951us/step - loss: 0.2099 - accuracy: 0.9212\n",
      "Epoch 85/1500\n",
      "68/68 [==============================] - 0s 984us/step - loss: 0.1633 - accuracy: 0.9392\n",
      "Epoch 86/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1839 - accuracy: 0.9323\n",
      "Epoch 87/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9337\n",
      "Epoch 88/1500\n",
      "68/68 [==============================] - 0s 993us/step - loss: 0.1655 - accuracy: 0.9369\n",
      "Epoch 89/1500\n",
      "68/68 [==============================] - 0s 987us/step - loss: 0.1722 - accuracy: 0.9351\n",
      "Epoch 90/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.9337\n",
      "Epoch 91/1500\n",
      "68/68 [==============================] - 0s 958us/step - loss: 0.1657 - accuracy: 0.9374\n",
      "Epoch 92/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9304\n",
      "Epoch 93/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1748 - accuracy: 0.9300\n",
      "Epoch 94/1500\n",
      "68/68 [==============================] - 0s 969us/step - loss: 0.1765 - accuracy: 0.9327\n",
      "Epoch 95/1500\n",
      "68/68 [==============================] - 0s 983us/step - loss: 0.1622 - accuracy: 0.9392\n",
      "Epoch 96/1500\n",
      "68/68 [==============================] - 0s 973us/step - loss: 0.1850 - accuracy: 0.9291\n",
      "Epoch 97/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9360\n",
      "Epoch 98/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1468 - accuracy: 0.9521\n",
      "Epoch 99/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9383\n",
      "Epoch 100/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9378\n",
      "Epoch 101/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9452\n",
      "Epoch 102/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1550 - accuracy: 0.9397\n",
      "Epoch 103/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.9374\n",
      "Epoch 104/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1736 - accuracy: 0.9341\n",
      "Epoch 105/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9475\n",
      "Epoch 106/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1695 - accuracy: 0.9374\n",
      "Epoch 107/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9392\n",
      "Epoch 108/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9433\n",
      "Epoch 109/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1634 - accuracy: 0.9378\n",
      "Epoch 110/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9438\n",
      "Epoch 111/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9369\n",
      "Epoch 112/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9433\n",
      "Epoch 113/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1538 - accuracy: 0.9429\n",
      "Epoch 114/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.9516\n",
      "Epoch 115/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1557 - accuracy: 0.9429\n",
      "Epoch 116/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.1553 - accuracy: 0.9443\n",
      "Epoch 117/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.1627 - accuracy: 0.9383\n",
      "Epoch 118/1500\n",
      "68/68 [==============================] - 0s 944us/step - loss: 0.1625 - accuracy: 0.9323\n",
      "Epoch 119/1500\n",
      "68/68 [==============================] - 0s 951us/step - loss: 0.1614 - accuracy: 0.9397\n",
      "Epoch 120/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1473 - accuracy: 0.9443\n",
      "Epoch 121/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.9447\n",
      "Epoch 122/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9466\n",
      "Epoch 123/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9429\n",
      "Epoch 124/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1562 - accuracy: 0.9392\n",
      "Epoch 125/1500\n",
      "68/68 [==============================] - 0s 993us/step - loss: 0.1506 - accuracy: 0.9443\n",
      "Epoch 126/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9387\n",
      "Epoch 127/1500\n",
      "68/68 [==============================] - 0s 990us/step - loss: 0.1579 - accuracy: 0.9387\n",
      "Epoch 128/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9447\n",
      "Epoch 129/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.1558 - accuracy: 0.9401\n",
      "Epoch 130/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.1428 - accuracy: 0.9470\n",
      "Epoch 131/1500\n",
      "68/68 [==============================] - 0s 956us/step - loss: 0.1377 - accuracy: 0.9470\n",
      "Epoch 132/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9604\n",
      "Epoch 133/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.1206 - accuracy: 0.9572\n",
      "Epoch 134/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.1438 - accuracy: 0.9433\n",
      "Epoch 135/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9438\n",
      "Epoch 136/1500\n",
      "68/68 [==============================] - 0s 982us/step - loss: 0.1466 - accuracy: 0.9498\n",
      "Epoch 137/1500\n",
      "68/68 [==============================] - 0s 994us/step - loss: 0.1572 - accuracy: 0.9429\n",
      "Epoch 138/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9443\n",
      "Epoch 139/1500\n",
      "68/68 [==============================] - 0s 952us/step - loss: 0.1299 - accuracy: 0.9544\n",
      "Epoch 140/1500\n",
      "68/68 [==============================] - 0s 992us/step - loss: 0.1413 - accuracy: 0.9489\n",
      "Epoch 141/1500\n",
      "68/68 [==============================] - 0s 994us/step - loss: 0.1185 - accuracy: 0.9599\n",
      "Epoch 142/1500\n",
      "68/68 [==============================] - 0s 983us/step - loss: 0.1336 - accuracy: 0.9498\n",
      "Epoch 143/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9553\n",
      "Epoch 144/1500\n",
      "68/68 [==============================] - 0s 943us/step - loss: 0.1257 - accuracy: 0.9539\n",
      "Epoch 145/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9585\n",
      "Epoch 146/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9558\n",
      "Epoch 147/1500\n",
      "68/68 [==============================] - 0s 974us/step - loss: 0.1457 - accuracy: 0.9498\n",
      "Epoch 148/1500\n",
      "68/68 [==============================] - 0s 964us/step - loss: 0.1415 - accuracy: 0.9498\n",
      "Epoch 149/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9590\n",
      "Epoch 150/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9576\n",
      "Epoch 151/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9466\n",
      "Epoch 152/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9526\n",
      "Epoch 153/1500\n",
      "68/68 [==============================] - 0s 974us/step - loss: 0.1102 - accuracy: 0.9632\n",
      "Epoch 154/1500\n",
      "68/68 [==============================] - 0s 983us/step - loss: 0.1378 - accuracy: 0.9475\n",
      "Epoch 155/1500\n",
      "68/68 [==============================] - 0s 931us/step - loss: 0.1338 - accuracy: 0.9539\n",
      "Epoch 156/1500\n",
      "68/68 [==============================] - 0s 996us/step - loss: 0.1123 - accuracy: 0.9590\n",
      "Epoch 157/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.1240 - accuracy: 0.9562\n",
      "Epoch 158/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.1302 - accuracy: 0.9521\n",
      "Epoch 159/1500\n",
      "68/68 [==============================] - 0s 943us/step - loss: 0.1206 - accuracy: 0.9512\n",
      "Epoch 160/1500\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.1114 - accuracy: 0.9572\n",
      "Epoch 161/1500\n",
      "68/68 [==============================] - 0s 983us/step - loss: 0.1099 - accuracy: 0.9641\n",
      "Epoch 162/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9549\n",
      "Epoch 163/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9599\n",
      "Epoch 164/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9493\n",
      "Epoch 165/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.1211 - accuracy: 0.9567\n",
      "Epoch 166/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9618\n",
      "Epoch 167/1500\n",
      "68/68 [==============================] - 0s 992us/step - loss: 0.1117 - accuracy: 0.9576\n",
      "Epoch 168/1500\n",
      "68/68 [==============================] - 0s 948us/step - loss: 0.1231 - accuracy: 0.9549\n",
      "Epoch 169/1500\n",
      "68/68 [==============================] - 0s 935us/step - loss: 0.1154 - accuracy: 0.9553\n",
      "Epoch 170/1500\n",
      "68/68 [==============================] - 0s 913us/step - loss: 0.1143 - accuracy: 0.9581\n",
      "Epoch 171/1500\n",
      "68/68 [==============================] - 0s 940us/step - loss: 0.1014 - accuracy: 0.9655\n",
      "Epoch 172/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9581\n",
      "Epoch 173/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9585\n",
      "Epoch 174/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.9498\n",
      "Epoch 175/1500\n",
      "68/68 [==============================] - 0s 957us/step - loss: 0.1197 - accuracy: 0.9553\n",
      "Epoch 176/1500\n",
      "68/68 [==============================] - 0s 976us/step - loss: 0.1223 - accuracy: 0.9530\n",
      "Epoch 177/1500\n",
      "68/68 [==============================] - 0s 998us/step - loss: 0.1011 - accuracy: 0.9645\n",
      "Epoch 178/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9604\n",
      "Epoch 179/1500\n",
      "68/68 [==============================] - 0s 991us/step - loss: 0.0941 - accuracy: 0.9668\n",
      "Epoch 180/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.9641\n",
      "Epoch 181/1500\n",
      "68/68 [==============================] - 0s 1000us/step - loss: 0.1177 - accuracy: 0.9539\n",
      "Epoch 182/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.9668\n",
      "Epoch 183/1500\n",
      "68/68 [==============================] - 0s 961us/step - loss: 0.1126 - accuracy: 0.9576\n",
      "Epoch 184/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.1458 - accuracy: 0.9378\n",
      "Epoch 185/1500\n",
      "68/68 [==============================] - 0s 938us/step - loss: 0.1182 - accuracy: 0.9572\n",
      "Epoch 186/1500\n",
      "68/68 [==============================] - 0s 986us/step - loss: 0.1004 - accuracy: 0.9691\n",
      "Epoch 187/1500\n",
      "68/68 [==============================] - 0s 950us/step - loss: 0.0975 - accuracy: 0.9668\n",
      "Epoch 188/1500\n",
      "68/68 [==============================] - 0s 935us/step - loss: 0.1105 - accuracy: 0.9590\n",
      "Epoch 189/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.0985 - accuracy: 0.9632\n",
      "Epoch 190/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.0980 - accuracy: 0.9673\n",
      "Epoch 191/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.1098 - accuracy: 0.9590\n",
      "Epoch 192/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.1203 - accuracy: 0.9567\n",
      "Epoch 193/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9613\n",
      "Epoch 194/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9608\n",
      "Epoch 195/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9636\n",
      "Epoch 196/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9655\n",
      "Epoch 197/1500\n",
      "68/68 [==============================] - 0s 981us/step - loss: 0.0981 - accuracy: 0.9636\n",
      "Epoch 198/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9664\n",
      "Epoch 199/1500\n",
      "68/68 [==============================] - 0s 966us/step - loss: 0.1048 - accuracy: 0.9636\n",
      "Epoch 200/1500\n",
      "68/68 [==============================] - 0s 999us/step - loss: 0.1148 - accuracy: 0.9553\n",
      "Epoch 201/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9595\n",
      "Epoch 202/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.9701\n",
      "Epoch 203/1500\n",
      "68/68 [==============================] - 0s 951us/step - loss: 0.0982 - accuracy: 0.9641\n",
      "Epoch 204/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.0924 - accuracy: 0.9664\n",
      "Epoch 205/1500\n",
      "68/68 [==============================] - 0s 972us/step - loss: 0.0858 - accuracy: 0.9691\n",
      "Epoch 206/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9664\n",
      "Epoch 207/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9705\n",
      "Epoch 208/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.0875 - accuracy: 0.9724\n",
      "Epoch 209/1500\n",
      "68/68 [==============================] - 0s 994us/step - loss: 0.1029 - accuracy: 0.9678\n",
      "Epoch 210/1500\n",
      "68/68 [==============================] - 0s 922us/step - loss: 0.0976 - accuracy: 0.9622\n",
      "Epoch 211/1500\n",
      "68/68 [==============================] - 0s 940us/step - loss: 0.1129 - accuracy: 0.9590\n",
      "Epoch 212/1500\n",
      "68/68 [==============================] - 0s 923us/step - loss: 0.0957 - accuracy: 0.9682\n",
      "Epoch 213/1500\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.0914 - accuracy: 0.9696\n",
      "Epoch 214/1500\n",
      "68/68 [==============================] - 0s 961us/step - loss: 0.1167 - accuracy: 0.9608\n",
      "Epoch 215/1500\n",
      "68/68 [==============================] - 0s 958us/step - loss: 0.0973 - accuracy: 0.9673\n",
      "Epoch 216/1500\n",
      "68/68 [==============================] - 0s 936us/step - loss: 0.0884 - accuracy: 0.9733\n",
      "Epoch 217/1500\n",
      "68/68 [==============================] - 0s 981us/step - loss: 0.0905 - accuracy: 0.9668\n",
      "Epoch 218/1500\n",
      "68/68 [==============================] - 0s 980us/step - loss: 0.0765 - accuracy: 0.9788\n",
      "Epoch 219/1500\n",
      "68/68 [==============================] - 0s 962us/step - loss: 0.0897 - accuracy: 0.9696\n",
      "Epoch 220/1500\n",
      "68/68 [==============================] - 0s 921us/step - loss: 0.0898 - accuracy: 0.9659\n",
      "Epoch 221/1500\n",
      "68/68 [==============================] - 0s 937us/step - loss: 0.0884 - accuracy: 0.9678\n",
      "Epoch 222/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.1078 - accuracy: 0.9632\n",
      "Epoch 223/1500\n",
      "68/68 [==============================] - 0s 940us/step - loss: 0.1036 - accuracy: 0.9618\n",
      "Epoch 224/1500\n",
      "68/68 [==============================] - 0s 997us/step - loss: 0.1026 - accuracy: 0.9627\n",
      "Epoch 225/1500\n",
      "68/68 [==============================] - 0s 987us/step - loss: 0.0992 - accuracy: 0.9608\n",
      "Epoch 226/1500\n",
      "68/68 [==============================] - 0s 942us/step - loss: 0.1004 - accuracy: 0.9673\n",
      "Epoch 227/1500\n",
      "68/68 [==============================] - 0s 939us/step - loss: 0.1062 - accuracy: 0.9641\n",
      "Epoch 228/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.0936 - accuracy: 0.9645\n",
      "Epoch 229/1500\n",
      "68/68 [==============================] - 0s 972us/step - loss: 0.0924 - accuracy: 0.9691\n",
      "Epoch 230/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9613\n",
      "Epoch 231/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9655\n",
      "Epoch 232/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9581\n",
      "Epoch 233/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.9567\n",
      "Epoch 234/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9650\n",
      "Epoch 235/1500\n",
      "68/68 [==============================] - 0s 990us/step - loss: 0.0952 - accuracy: 0.9650\n",
      "Epoch 236/1500\n",
      "68/68 [==============================] - 0s 941us/step - loss: 0.1064 - accuracy: 0.9655\n",
      "Epoch 237/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.0897 - accuracy: 0.9687\n",
      "Epoch 238/1500\n",
      "68/68 [==============================] - 0s 944us/step - loss: 0.0888 - accuracy: 0.9701\n",
      "Epoch 239/1500\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.0925 - accuracy: 0.9659\n",
      "Epoch 240/1500\n",
      "68/68 [==============================] - 0s 959us/step - loss: 0.0696 - accuracy: 0.9774\n",
      "Epoch 241/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9705\n",
      "Epoch 242/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9664\n",
      "Epoch 243/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9645\n",
      "Epoch 244/1500\n",
      "68/68 [==============================] - 0s 987us/step - loss: 0.0851 - accuracy: 0.9710\n",
      "Epoch 245/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9678\n",
      "Epoch 246/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9636\n",
      "Epoch 247/1500\n",
      "68/68 [==============================] - 0s 940us/step - loss: 0.1022 - accuracy: 0.9673\n",
      "Epoch 248/1500\n",
      "68/68 [==============================] - 0s 958us/step - loss: 0.0939 - accuracy: 0.9673\n",
      "Epoch 249/1500\n",
      "68/68 [==============================] - 0s 952us/step - loss: 0.1049 - accuracy: 0.9604\n",
      "Epoch 250/1500\n",
      "68/68 [==============================] - 0s 961us/step - loss: 0.0901 - accuracy: 0.9678\n",
      "Epoch 251/1500\n",
      "68/68 [==============================] - 0s 952us/step - loss: 0.0911 - accuracy: 0.9701\n",
      "Epoch 252/1500\n",
      "68/68 [==============================] - 0s 947us/step - loss: 0.0787 - accuracy: 0.9696\n",
      "Epoch 253/1500\n",
      "68/68 [==============================] - 0s 936us/step - loss: 0.0794 - accuracy: 0.9747\n",
      "Epoch 254/1500\n",
      "68/68 [==============================] - 0s 961us/step - loss: 0.0832 - accuracy: 0.9705\n",
      "Epoch 255/1500\n",
      "68/68 [==============================] - 0s 945us/step - loss: 0.0773 - accuracy: 0.9747\n",
      "Epoch 256/1500\n",
      "68/68 [==============================] - 0s 956us/step - loss: 0.0986 - accuracy: 0.9636\n",
      "Epoch 257/1500\n",
      "68/68 [==============================] - 0s 946us/step - loss: 0.0749 - accuracy: 0.9760\n",
      "Epoch 258/1500\n",
      "68/68 [==============================] - 0s 960us/step - loss: 0.0843 - accuracy: 0.9687\n",
      "Epoch 259/1500\n",
      "68/68 [==============================] - 0s 924us/step - loss: 0.0831 - accuracy: 0.9728\n",
      "Epoch 260/1500\n",
      "68/68 [==============================] - 0s 968us/step - loss: 0.0781 - accuracy: 0.9742\n",
      "Epoch 261/1500\n",
      "68/68 [==============================] - 0s 959us/step - loss: 0.1342 - accuracy: 0.9549\n",
      "Epoch 262/1500\n",
      "68/68 [==============================] - 0s 952us/step - loss: 0.0897 - accuracy: 0.9668\n",
      "Epoch 263/1500\n",
      "68/68 [==============================] - 0s 946us/step - loss: 0.0911 - accuracy: 0.9691\n",
      "Epoch 264/1500\n",
      "68/68 [==============================] - 0s 928us/step - loss: 0.0785 - accuracy: 0.9751\n",
      "Epoch 265/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.0762 - accuracy: 0.9779\n",
      "Epoch 266/1500\n",
      "68/68 [==============================] - 0s 930us/step - loss: 0.0788 - accuracy: 0.9733\n",
      "Epoch 267/1500\n",
      "68/68 [==============================] - 0s 946us/step - loss: 0.0846 - accuracy: 0.9714\n",
      "Epoch 268/1500\n",
      "68/68 [==============================] - 0s 943us/step - loss: 0.0918 - accuracy: 0.9691\n",
      "Epoch 269/1500\n",
      "68/68 [==============================] - 0s 960us/step - loss: 0.0908 - accuracy: 0.9655\n",
      "Epoch 270/1500\n",
      "56/68 [=======================>......] - ETA: 0s - loss: 0.0777 - accuracy: 0.9727Restoring model weights from the end of the best epoch: 240.\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.0792 - accuracy: 0.9710\n",
      "Epoch 270: early stopping\n",
      "8/8 [==============================] - 0s 772us/step - loss: 0.8790 - accuracy: 0.6920\n",
      "8/8 [==============================] - 0s 629us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (21/29)\n",
      "Before appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "After appending - Cat IDs: 390, Predictions: 390, Actuals: 390, Gender: 390\n",
      "Final Test Results - Loss: 0.8790414333343506, Accuracy: 0.6919831037521362, Precision: 0.6930736749227958, Recall: 0.5900405129811948, F1 Score: 0.6132115958383558\n",
      "Confusion Matrix:\n",
      " [[122   4  19]\n",
      " [ 31  20   0]\n",
      " [ 19   0  22]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "001A    14\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "071A    10\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "007A     6\n",
      "109A     6\n",
      "108A     6\n",
      "037A     6\n",
      "008A     6\n",
      "044A     5\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "034A     5\n",
      "023B     5\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "060A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "054A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "018A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "091A     1\n",
      "110A     1\n",
      "115A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "057A    27\n",
      "074A    25\n",
      "029A    17\n",
      "097A    16\n",
      "106A    14\n",
      "028A    13\n",
      "025A    11\n",
      "005A    10\n",
      "015A     9\n",
      "045A     9\n",
      "072A     9\n",
      "094A     8\n",
      "023A     6\n",
      "053A     6\n",
      "021A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "003A     4\n",
      "058A     3\n",
      "056A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "038A     2\n",
      "073A     1\n",
      "076A     1\n",
      "026C     1\n",
      "100A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    266\n",
      "M    237\n",
      "F    211\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    100\n",
      "X     82\n",
      "F     41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 103A, 071A, 097B, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [055A, 059A, 113A, 116A, 051B, 054A, 117A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 028A, 074A, 062A, 029A, 005A, 072A, 009...\n",
      "kitten                                               [045A]\n",
      "senior     [093A, 097A, 057A, 106A, 104A, 056A, 058A, 094A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 15, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 1, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "Length of X_train_val:\n",
      "714\n",
      "Length of y_train_val:\n",
      "714\n",
      "Length of groups_train_val:\n",
      "714\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 902, 1: 810, 2: 505})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8792 - accuracy: 0.6166\n",
      "Epoch 2/1500\n",
      "70/70 [==============================] - 0s 991us/step - loss: 0.6458 - accuracy: 0.7456\n",
      "Epoch 3/1500\n",
      "70/70 [==============================] - 0s 915us/step - loss: 0.5677 - accuracy: 0.7713\n",
      "Epoch 4/1500\n",
      "70/70 [==============================] - 0s 960us/step - loss: 0.5321 - accuracy: 0.7903\n",
      "Epoch 5/1500\n",
      "70/70 [==============================] - 0s 930us/step - loss: 0.4902 - accuracy: 0.8051\n",
      "Epoch 6/1500\n",
      "70/70 [==============================] - 0s 920us/step - loss: 0.4906 - accuracy: 0.7988\n",
      "Epoch 7/1500\n",
      "70/70 [==============================] - 0s 974us/step - loss: 0.4399 - accuracy: 0.8345\n",
      "Epoch 8/1500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.4352 - accuracy: 0.8227\n",
      "Epoch 9/1500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.4200 - accuracy: 0.8209\n",
      "Epoch 10/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4154 - accuracy: 0.8313\n",
      "Epoch 11/1500\n",
      "70/70 [==============================] - 0s 911us/step - loss: 0.3549 - accuracy: 0.8566\n",
      "Epoch 12/1500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.3692 - accuracy: 0.8516\n",
      "Epoch 13/1500\n",
      "70/70 [==============================] - 0s 903us/step - loss: 0.3715 - accuracy: 0.8552\n",
      "Epoch 14/1500\n",
      "70/70 [==============================] - 0s 915us/step - loss: 0.3629 - accuracy: 0.8566\n",
      "Epoch 15/1500\n",
      "70/70 [==============================] - 0s 897us/step - loss: 0.3536 - accuracy: 0.8633\n",
      "Epoch 16/1500\n",
      "70/70 [==============================] - 0s 931us/step - loss: 0.3306 - accuracy: 0.8683\n",
      "Epoch 17/1500\n",
      "70/70 [==============================] - 0s 918us/step - loss: 0.3339 - accuracy: 0.8620\n",
      "Epoch 18/1500\n",
      "70/70 [==============================] - 0s 865us/step - loss: 0.3329 - accuracy: 0.8696\n",
      "Epoch 19/1500\n",
      "70/70 [==============================] - 0s 918us/step - loss: 0.3297 - accuracy: 0.8760\n",
      "Epoch 20/1500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.3327 - accuracy: 0.8674\n",
      "Epoch 21/1500\n",
      "70/70 [==============================] - 0s 909us/step - loss: 0.3226 - accuracy: 0.8678\n",
      "Epoch 22/1500\n",
      "70/70 [==============================] - 0s 908us/step - loss: 0.3162 - accuracy: 0.8751\n",
      "Epoch 23/1500\n",
      "70/70 [==============================] - 0s 904us/step - loss: 0.3146 - accuracy: 0.8809\n",
      "Epoch 24/1500\n",
      "70/70 [==============================] - 0s 920us/step - loss: 0.2916 - accuracy: 0.8791\n",
      "Epoch 25/1500\n",
      "70/70 [==============================] - 0s 909us/step - loss: 0.2789 - accuracy: 0.8890\n",
      "Epoch 26/1500\n",
      "70/70 [==============================] - 0s 908us/step - loss: 0.2922 - accuracy: 0.8859\n",
      "Epoch 27/1500\n",
      "70/70 [==============================] - 0s 944us/step - loss: 0.2924 - accuracy: 0.8863\n",
      "Epoch 28/1500\n",
      "70/70 [==============================] - 0s 946us/step - loss: 0.2834 - accuracy: 0.8881\n",
      "Epoch 29/1500\n",
      "70/70 [==============================] - 0s 935us/step - loss: 0.2643 - accuracy: 0.8976\n",
      "Epoch 30/1500\n",
      "70/70 [==============================] - 0s 944us/step - loss: 0.2927 - accuracy: 0.8850\n",
      "Epoch 31/1500\n",
      "70/70 [==============================] - 0s 899us/step - loss: 0.2768 - accuracy: 0.9003\n",
      "Epoch 32/1500\n",
      "70/70 [==============================] - 0s 950us/step - loss: 0.2546 - accuracy: 0.9030\n",
      "Epoch 33/1500\n",
      "70/70 [==============================] - 0s 921us/step - loss: 0.2533 - accuracy: 0.9044\n",
      "Epoch 34/1500\n",
      "70/70 [==============================] - 0s 921us/step - loss: 0.2475 - accuracy: 0.9021\n",
      "Epoch 35/1500\n",
      "70/70 [==============================] - 0s 907us/step - loss: 0.2470 - accuracy: 0.9057\n",
      "Epoch 36/1500\n",
      "70/70 [==============================] - 0s 936us/step - loss: 0.2643 - accuracy: 0.8922\n",
      "Epoch 37/1500\n",
      "70/70 [==============================] - 0s 942us/step - loss: 0.2628 - accuracy: 0.8981\n",
      "Epoch 38/1500\n",
      "70/70 [==============================] - 0s 910us/step - loss: 0.2557 - accuracy: 0.8990\n",
      "Epoch 39/1500\n",
      "70/70 [==============================] - 0s 967us/step - loss: 0.2460 - accuracy: 0.9071\n",
      "Epoch 40/1500\n",
      "70/70 [==============================] - 0s 917us/step - loss: 0.2364 - accuracy: 0.9075\n",
      "Epoch 41/1500\n",
      "70/70 [==============================] - 0s 921us/step - loss: 0.2300 - accuracy: 0.9107\n",
      "Epoch 42/1500\n",
      "70/70 [==============================] - 0s 920us/step - loss: 0.2143 - accuracy: 0.9188\n",
      "Epoch 43/1500\n",
      "70/70 [==============================] - 0s 913us/step - loss: 0.2137 - accuracy: 0.9120\n",
      "Epoch 44/1500\n",
      "70/70 [==============================] - 0s 919us/step - loss: 0.2170 - accuracy: 0.9134\n",
      "Epoch 45/1500\n",
      "70/70 [==============================] - 0s 924us/step - loss: 0.2346 - accuracy: 0.9120\n",
      "Epoch 46/1500\n",
      "70/70 [==============================] - 0s 930us/step - loss: 0.2197 - accuracy: 0.9242\n",
      "Epoch 47/1500\n",
      "70/70 [==============================] - 0s 925us/step - loss: 0.2248 - accuracy: 0.9147\n",
      "Epoch 48/1500\n",
      "70/70 [==============================] - 0s 913us/step - loss: 0.2442 - accuracy: 0.9111\n",
      "Epoch 49/1500\n",
      "70/70 [==============================] - 0s 919us/step - loss: 0.2181 - accuracy: 0.9161\n",
      "Epoch 50/1500\n",
      "70/70 [==============================] - 0s 924us/step - loss: 0.2144 - accuracy: 0.9197\n",
      "Epoch 51/1500\n",
      "70/70 [==============================] - 0s 925us/step - loss: 0.2070 - accuracy: 0.9229\n",
      "Epoch 52/1500\n",
      "70/70 [==============================] - 0s 915us/step - loss: 0.2194 - accuracy: 0.9143\n",
      "Epoch 53/1500\n",
      "70/70 [==============================] - 0s 906us/step - loss: 0.2134 - accuracy: 0.9224\n",
      "Epoch 54/1500\n",
      "70/70 [==============================] - 0s 978us/step - loss: 0.2127 - accuracy: 0.9238\n",
      "Epoch 55/1500\n",
      "70/70 [==============================] - 0s 944us/step - loss: 0.2094 - accuracy: 0.9211\n",
      "Epoch 56/1500\n",
      "70/70 [==============================] - 0s 924us/step - loss: 0.2081 - accuracy: 0.9256\n",
      "Epoch 57/1500\n",
      "70/70 [==============================] - 0s 932us/step - loss: 0.1897 - accuracy: 0.9296\n",
      "Epoch 58/1500\n",
      "70/70 [==============================] - 0s 916us/step - loss: 0.2036 - accuracy: 0.9238\n",
      "Epoch 59/1500\n",
      "70/70 [==============================] - 0s 939us/step - loss: 0.2072 - accuracy: 0.9251\n",
      "Epoch 60/1500\n",
      "70/70 [==============================] - 0s 938us/step - loss: 0.1855 - accuracy: 0.9369\n",
      "Epoch 61/1500\n",
      "70/70 [==============================] - 0s 935us/step - loss: 0.2018 - accuracy: 0.9193\n",
      "Epoch 62/1500\n",
      "70/70 [==============================] - 0s 925us/step - loss: 0.1968 - accuracy: 0.9269\n",
      "Epoch 63/1500\n",
      "70/70 [==============================] - 0s 945us/step - loss: 0.1833 - accuracy: 0.9364\n",
      "Epoch 64/1500\n",
      "70/70 [==============================] - 0s 912us/step - loss: 0.1951 - accuracy: 0.9238\n",
      "Epoch 65/1500\n",
      "70/70 [==============================] - 0s 909us/step - loss: 0.1824 - accuracy: 0.9355\n",
      "Epoch 66/1500\n",
      "70/70 [==============================] - 0s 932us/step - loss: 0.1855 - accuracy: 0.9283\n",
      "Epoch 67/1500\n",
      "70/70 [==============================] - 0s 916us/step - loss: 0.1843 - accuracy: 0.9301\n",
      "Epoch 68/1500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.1818 - accuracy: 0.9328\n",
      "Epoch 69/1500\n",
      "70/70 [==============================] - 0s 920us/step - loss: 0.1839 - accuracy: 0.9305\n",
      "Epoch 70/1500\n",
      "70/70 [==============================] - 0s 919us/step - loss: 0.1720 - accuracy: 0.9373\n",
      "Epoch 71/1500\n",
      "70/70 [==============================] - 0s 921us/step - loss: 0.1861 - accuracy: 0.9233\n",
      "Epoch 72/1500\n",
      "70/70 [==============================] - 0s 913us/step - loss: 0.1804 - accuracy: 0.9278\n",
      "Epoch 73/1500\n",
      "70/70 [==============================] - 0s 951us/step - loss: 0.1607 - accuracy: 0.9373\n",
      "Epoch 74/1500\n",
      "70/70 [==============================] - 0s 925us/step - loss: 0.1669 - accuracy: 0.9387\n",
      "Epoch 75/1500\n",
      "70/70 [==============================] - 0s 920us/step - loss: 0.1625 - accuracy: 0.9400\n",
      "Epoch 76/1500\n",
      "70/70 [==============================] - 0s 919us/step - loss: 0.1565 - accuracy: 0.9396\n",
      "Epoch 77/1500\n",
      "70/70 [==============================] - 0s 942us/step - loss: 0.1863 - accuracy: 0.9305\n",
      "Epoch 78/1500\n",
      "70/70 [==============================] - 0s 905us/step - loss: 0.1789 - accuracy: 0.9369\n",
      "Epoch 79/1500\n",
      "70/70 [==============================] - 0s 930us/step - loss: 0.1531 - accuracy: 0.9486\n",
      "Epoch 80/1500\n",
      "70/70 [==============================] - 0s 931us/step - loss: 0.1590 - accuracy: 0.9391\n",
      "Epoch 81/1500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.1632 - accuracy: 0.9369\n",
      "Epoch 82/1500\n",
      "70/70 [==============================] - 0s 935us/step - loss: 0.1559 - accuracy: 0.9450\n",
      "Epoch 83/1500\n",
      "70/70 [==============================] - 0s 919us/step - loss: 0.1647 - accuracy: 0.9382\n",
      "Epoch 84/1500\n",
      "70/70 [==============================] - 0s 915us/step - loss: 0.1633 - accuracy: 0.9373\n",
      "Epoch 85/1500\n",
      "70/70 [==============================] - 0s 908us/step - loss: 0.1813 - accuracy: 0.9378\n",
      "Epoch 86/1500\n",
      "70/70 [==============================] - 0s 918us/step - loss: 0.1603 - accuracy: 0.9472\n",
      "Epoch 87/1500\n",
      "70/70 [==============================] - 0s 946us/step - loss: 0.1514 - accuracy: 0.9450\n",
      "Epoch 88/1500\n",
      "70/70 [==============================] - 0s 942us/step - loss: 0.1482 - accuracy: 0.9481\n",
      "Epoch 89/1500\n",
      "70/70 [==============================] - 0s 928us/step - loss: 0.1478 - accuracy: 0.9463\n",
      "Epoch 90/1500\n",
      "70/70 [==============================] - 0s 982us/step - loss: 0.1533 - accuracy: 0.9490\n",
      "Epoch 91/1500\n",
      "70/70 [==============================] - 0s 3ms/step - loss: 0.1873 - accuracy: 0.9278\n",
      "Epoch 92/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1492 - accuracy: 0.9481\n",
      "Epoch 93/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1322 - accuracy: 0.9477\n",
      "Epoch 94/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9436\n",
      "Epoch 95/1500\n",
      "70/70 [==============================] - 0s 967us/step - loss: 0.1368 - accuracy: 0.9567\n",
      "Epoch 96/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9463\n",
      "Epoch 97/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9490\n",
      "Epoch 98/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9562\n",
      "Epoch 99/1500\n",
      "70/70 [==============================] - 0s 962us/step - loss: 0.1513 - accuracy: 0.9427\n",
      "Epoch 100/1500\n",
      "70/70 [==============================] - 0s 905us/step - loss: 0.1345 - accuracy: 0.9504\n",
      "Epoch 101/1500\n",
      "70/70 [==============================] - 0s 911us/step - loss: 0.1377 - accuracy: 0.9504\n",
      "Epoch 102/1500\n",
      "70/70 [==============================] - 0s 917us/step - loss: 0.1408 - accuracy: 0.9454\n",
      "Epoch 103/1500\n",
      "70/70 [==============================] - 0s 937us/step - loss: 0.1527 - accuracy: 0.9332\n",
      "Epoch 104/1500\n",
      "70/70 [==============================] - 0s 976us/step - loss: 0.1488 - accuracy: 0.9477\n",
      "Epoch 105/1500\n",
      "70/70 [==============================] - 0s 951us/step - loss: 0.1234 - accuracy: 0.9562\n",
      "Epoch 106/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9517\n",
      "Epoch 107/1500\n",
      "70/70 [==============================] - 0s 964us/step - loss: 0.1320 - accuracy: 0.9526\n",
      "Epoch 108/1500\n",
      "70/70 [==============================] - 0s 949us/step - loss: 0.1453 - accuracy: 0.9427\n",
      "Epoch 109/1500\n",
      "70/70 [==============================] - 0s 943us/step - loss: 0.1324 - accuracy: 0.9468\n",
      "Epoch 110/1500\n",
      "70/70 [==============================] - 0s 948us/step - loss: 0.1273 - accuracy: 0.9571\n",
      "Epoch 111/1500\n",
      "70/70 [==============================] - 0s 939us/step - loss: 0.1232 - accuracy: 0.9544\n",
      "Epoch 112/1500\n",
      "70/70 [==============================] - 0s 941us/step - loss: 0.1298 - accuracy: 0.9513\n",
      "Epoch 113/1500\n",
      "70/70 [==============================] - 0s 912us/step - loss: 0.1243 - accuracy: 0.9567\n",
      "Epoch 114/1500\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.1414 - accuracy: 0.9495\n",
      "Epoch 115/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9477\n",
      "Epoch 116/1500\n",
      "70/70 [==============================] - 0s 987us/step - loss: 0.1173 - accuracy: 0.9621\n",
      "Epoch 117/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9544\n",
      "Epoch 118/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9544\n",
      "Epoch 119/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9612\n",
      "Epoch 120/1500\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.1183 - accuracy: 0.9553\n",
      "Epoch 121/1500\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.1359 - accuracy: 0.9495\n",
      "Epoch 122/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9585\n",
      "Epoch 123/1500\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1196 - accuracy: 0.9567\n",
      "Epoch 124/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9626\n",
      "Epoch 125/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9639\n",
      "Epoch 126/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9603\n",
      "Epoch 127/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9630\n",
      "Epoch 128/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9621\n",
      "Epoch 129/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9576\n",
      "Epoch 130/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9590\n",
      "Epoch 131/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9531\n",
      "Epoch 132/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9603\n",
      "Epoch 133/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9680\n",
      "Epoch 134/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9639\n",
      "Epoch 135/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9590\n",
      "Epoch 136/1500\n",
      "70/70 [==============================] - 0s 963us/step - loss: 0.1126 - accuracy: 0.9630\n",
      "Epoch 137/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9639\n",
      "Epoch 138/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9653\n",
      "Epoch 139/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9612\n",
      "Epoch 140/1500\n",
      "70/70 [==============================] - 0s 891us/step - loss: 0.1218 - accuracy: 0.9571\n",
      "Epoch 141/1500\n",
      "70/70 [==============================] - 0s 896us/step - loss: 0.1104 - accuracy: 0.9639\n",
      "Epoch 142/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9576\n",
      "Epoch 143/1500\n",
      "70/70 [==============================] - 0s 995us/step - loss: 0.1061 - accuracy: 0.9675\n",
      "Epoch 144/1500\n",
      "70/70 [==============================] - 0s 950us/step - loss: 0.1123 - accuracy: 0.9617\n",
      "Epoch 145/1500\n",
      "70/70 [==============================] - 0s 946us/step - loss: 0.0971 - accuracy: 0.9693\n",
      "Epoch 146/1500\n",
      "70/70 [==============================] - 0s 934us/step - loss: 0.0964 - accuracy: 0.9689\n",
      "Epoch 147/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9630\n",
      "Epoch 148/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9576\n",
      "Epoch 149/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9675\n",
      "Epoch 150/1500\n",
      "70/70 [==============================] - 0s 993us/step - loss: 0.1140 - accuracy: 0.9567\n",
      "Epoch 151/1500\n",
      "70/70 [==============================] - 0s 968us/step - loss: 0.1086 - accuracy: 0.9581\n",
      "Epoch 152/1500\n",
      "70/70 [==============================] - 0s 963us/step - loss: 0.1186 - accuracy: 0.9590\n",
      "Epoch 153/1500\n",
      "70/70 [==============================] - 0s 973us/step - loss: 0.1234 - accuracy: 0.9562\n",
      "Epoch 154/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9666\n",
      "Epoch 155/1500\n",
      "70/70 [==============================] - 0s 916us/step - loss: 0.0919 - accuracy: 0.9707\n",
      "Epoch 156/1500\n",
      "70/70 [==============================] - 0s 928us/step - loss: 0.0810 - accuracy: 0.9756\n",
      "Epoch 157/1500\n",
      "70/70 [==============================] - 0s 900us/step - loss: 0.0950 - accuracy: 0.9662\n",
      "Epoch 158/1500\n",
      "70/70 [==============================] - 0s 911us/step - loss: 0.0955 - accuracy: 0.9671\n",
      "Epoch 159/1500\n",
      "70/70 [==============================] - 0s 898us/step - loss: 0.1127 - accuracy: 0.9576\n",
      "Epoch 160/1500\n",
      "70/70 [==============================] - 0s 941us/step - loss: 0.1005 - accuracy: 0.9653\n",
      "Epoch 161/1500\n",
      "70/70 [==============================] - 0s 927us/step - loss: 0.1019 - accuracy: 0.9639\n",
      "Epoch 162/1500\n",
      "70/70 [==============================] - 0s 954us/step - loss: 0.0960 - accuracy: 0.9630\n",
      "Epoch 163/1500\n",
      "70/70 [==============================] - 0s 951us/step - loss: 0.1165 - accuracy: 0.9626\n",
      "Epoch 164/1500\n",
      "70/70 [==============================] - 0s 959us/step - loss: 0.1047 - accuracy: 0.9599\n",
      "Epoch 165/1500\n",
      "70/70 [==============================] - 0s 991us/step - loss: 0.1146 - accuracy: 0.9617\n",
      "Epoch 166/1500\n",
      "70/70 [==============================] - 0s 933us/step - loss: 0.1058 - accuracy: 0.9626\n",
      "Epoch 167/1500\n",
      "70/70 [==============================] - 0s 923us/step - loss: 0.0919 - accuracy: 0.9707\n",
      "Epoch 168/1500\n",
      "70/70 [==============================] - 0s 938us/step - loss: 0.0968 - accuracy: 0.9666\n",
      "Epoch 169/1500\n",
      "70/70 [==============================] - 0s 923us/step - loss: 0.0921 - accuracy: 0.9662\n",
      "Epoch 170/1500\n",
      "70/70 [==============================] - 0s 945us/step - loss: 0.0867 - accuracy: 0.9707\n",
      "Epoch 171/1500\n",
      "70/70 [==============================] - 0s 955us/step - loss: 0.0812 - accuracy: 0.9729\n",
      "Epoch 172/1500\n",
      "70/70 [==============================] - 0s 916us/step - loss: 0.0927 - accuracy: 0.9693\n",
      "Epoch 173/1500\n",
      "70/70 [==============================] - 0s 926us/step - loss: 0.0987 - accuracy: 0.9648\n",
      "Epoch 174/1500\n",
      "70/70 [==============================] - 0s 942us/step - loss: 0.0948 - accuracy: 0.9644\n",
      "Epoch 175/1500\n",
      "70/70 [==============================] - 0s 930us/step - loss: 0.0921 - accuracy: 0.9675\n",
      "Epoch 176/1500\n",
      "70/70 [==============================] - 0s 955us/step - loss: 0.1005 - accuracy: 0.9662\n",
      "Epoch 177/1500\n",
      "70/70 [==============================] - 0s 914us/step - loss: 0.0941 - accuracy: 0.9639\n",
      "Epoch 178/1500\n",
      "70/70 [==============================] - 0s 910us/step - loss: 0.0866 - accuracy: 0.9680\n",
      "Epoch 179/1500\n",
      "70/70 [==============================] - 0s 928us/step - loss: 0.0965 - accuracy: 0.9693\n",
      "Epoch 180/1500\n",
      "70/70 [==============================] - 0s 895us/step - loss: 0.0870 - accuracy: 0.9680\n",
      "Epoch 181/1500\n",
      "70/70 [==============================] - 0s 916us/step - loss: 0.0953 - accuracy: 0.9648\n",
      "Epoch 182/1500\n",
      "70/70 [==============================] - 0s 957us/step - loss: 0.0855 - accuracy: 0.9662\n",
      "Epoch 183/1500\n",
      "70/70 [==============================] - 0s 950us/step - loss: 0.0881 - accuracy: 0.9680\n",
      "Epoch 184/1500\n",
      "70/70 [==============================] - 0s 934us/step - loss: 0.0902 - accuracy: 0.9707\n",
      "Epoch 185/1500\n",
      "70/70 [==============================] - 0s 918us/step - loss: 0.1062 - accuracy: 0.9612\n",
      "Epoch 186/1500\n",
      "59/70 [========================>.....] - ETA: 0s - loss: 0.0857 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 156.\n",
      "70/70 [==============================] - 0s 909us/step - loss: 0.0843 - accuracy: 0.9702\n",
      "Epoch 186: early stopping\n",
      "7/7 [==============================] - 0s 791us/step - loss: 0.8358 - accuracy: 0.7085\n",
      "7/7 [==============================] - 0s 626us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (25/30)\n",
      "Before appending - Cat IDs: 390, Predictions: 390, Actuals: 390, Gender: 390\n",
      "After appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "Final Test Results - Loss: 0.8357598185539246, Accuracy: 0.7085201740264893, Precision: 0.6879610115911485, Recall: 0.7717003191455746, F1 Score: 0.7186252419693514\n",
      "Confusion Matrix:\n",
      " [[109   4  24]\n",
      " [  0   9   0]\n",
      " [ 37   0  40]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "063A    11\n",
      "068A    11\n",
      "040A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "022A     9\n",
      "045A     9\n",
      "065A     9\n",
      "072A     9\n",
      "033A     9\n",
      "015A     9\n",
      "094A     8\n",
      "010A     8\n",
      "013B     8\n",
      "117A     7\n",
      "099A     7\n",
      "050A     7\n",
      "007A     6\n",
      "053A     6\n",
      "108A     6\n",
      "109A     6\n",
      "023A     6\n",
      "021A     5\n",
      "025C     5\n",
      "044A     5\n",
      "075A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "056A     3\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "113A     3\n",
      "069A     2\n",
      "025B     2\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "087A     2\n",
      "011A     2\n",
      "093A     2\n",
      "032A     2\n",
      "054A     2\n",
      "088A     1\n",
      "115A     1\n",
      "100A     1\n",
      "024A     1\n",
      "019B     1\n",
      "043A     1\n",
      "091A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "055A    20\n",
      "019A    17\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "111A    13\n",
      "051A    12\n",
      "036A    11\n",
      "014B    10\n",
      "071A    10\n",
      "095A     8\n",
      "027A     7\n",
      "031A     7\n",
      "037A     6\n",
      "008A     6\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "105A     4\n",
      "052A     4\n",
      "014A     3\n",
      "060A     3\n",
      "102A     2\n",
      "110A     1\n",
      "096A     1\n",
      "041A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    268\n",
      "X    259\n",
      "F    182\n",
      "Name: gender, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    89\n",
      "F    70\n",
      "M    69\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 028A, 074...\n",
      "kitten    [044A, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 097B, 019A, 020A, 101A, 095A, 034A, 027...\n",
      "kitten                             [014B, 111A, 041A, 110A]\n",
      "senior                             [055A, 059A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "Length of X_train_val:\n",
      "709\n",
      "Length of y_train_val:\n",
      "709\n",
      "Length of groups_train_val:\n",
      "709\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 864, 1: 730, 2: 655})\n",
      "Epoch 1/1500\n",
      "71/71 [==============================] - 0s 973us/step - loss: 0.9246 - accuracy: 0.5976\n",
      "Epoch 2/1500\n",
      "71/71 [==============================] - 0s 950us/step - loss: 0.7408 - accuracy: 0.6936\n",
      "Epoch 3/1500\n",
      "71/71 [==============================] - 0s 941us/step - loss: 0.7132 - accuracy: 0.6981\n",
      "Epoch 4/1500\n",
      "71/71 [==============================] - 0s 930us/step - loss: 0.6388 - accuracy: 0.7319\n",
      "Epoch 5/1500\n",
      "71/71 [==============================] - 0s 912us/step - loss: 0.6068 - accuracy: 0.7417\n",
      "Epoch 6/1500\n",
      "71/71 [==============================] - 0s 914us/step - loss: 0.5647 - accuracy: 0.7732\n",
      "Epoch 7/1500\n",
      "71/71 [==============================] - 0s 928us/step - loss: 0.5469 - accuracy: 0.7666\n",
      "Epoch 8/1500\n",
      "71/71 [==============================] - 0s 947us/step - loss: 0.5207 - accuracy: 0.7817\n",
      "Epoch 9/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5260 - accuracy: 0.7852\n",
      "Epoch 10/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4998 - accuracy: 0.8039\n",
      "Epoch 11/1500\n",
      "71/71 [==============================] - 0s 981us/step - loss: 0.5106 - accuracy: 0.7901\n",
      "Epoch 12/1500\n",
      "71/71 [==============================] - 0s 975us/step - loss: 0.4757 - accuracy: 0.8057\n",
      "Epoch 13/1500\n",
      "71/71 [==============================] - 0s 980us/step - loss: 0.4579 - accuracy: 0.8146\n",
      "Epoch 14/1500\n",
      "71/71 [==============================] - 0s 931us/step - loss: 0.4650 - accuracy: 0.8119\n",
      "Epoch 15/1500\n",
      "71/71 [==============================] - 0s 923us/step - loss: 0.4368 - accuracy: 0.8221\n",
      "Epoch 16/1500\n",
      "71/71 [==============================] - 0s 947us/step - loss: 0.4243 - accuracy: 0.8288\n",
      "Epoch 17/1500\n",
      "71/71 [==============================] - 0s 977us/step - loss: 0.4180 - accuracy: 0.8301\n",
      "Epoch 18/1500\n",
      "71/71 [==============================] - 0s 958us/step - loss: 0.4100 - accuracy: 0.8382\n",
      "Epoch 19/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4094 - accuracy: 0.8324\n",
      "Epoch 20/1500\n",
      "71/71 [==============================] - 0s 949us/step - loss: 0.3921 - accuracy: 0.8377\n",
      "Epoch 21/1500\n",
      "71/71 [==============================] - 0s 922us/step - loss: 0.3871 - accuracy: 0.8439\n",
      "Epoch 22/1500\n",
      "71/71 [==============================] - 0s 906us/step - loss: 0.3893 - accuracy: 0.8364\n",
      "Epoch 23/1500\n",
      "71/71 [==============================] - 0s 956us/step - loss: 0.3893 - accuracy: 0.8430\n",
      "Epoch 24/1500\n",
      "71/71 [==============================] - 0s 966us/step - loss: 0.3626 - accuracy: 0.8510\n",
      "Epoch 25/1500\n",
      "71/71 [==============================] - 0s 945us/step - loss: 0.3601 - accuracy: 0.8528\n",
      "Epoch 26/1500\n",
      "71/71 [==============================] - 0s 955us/step - loss: 0.3596 - accuracy: 0.8573\n",
      "Epoch 27/1500\n",
      "71/71 [==============================] - 0s 912us/step - loss: 0.3301 - accuracy: 0.8675\n",
      "Epoch 28/1500\n",
      "71/71 [==============================] - 0s 925us/step - loss: 0.3514 - accuracy: 0.8608\n",
      "Epoch 29/1500\n",
      "71/71 [==============================] - 0s 958us/step - loss: 0.3490 - accuracy: 0.8613\n",
      "Epoch 30/1500\n",
      "71/71 [==============================] - 0s 957us/step - loss: 0.3469 - accuracy: 0.8666\n",
      "Epoch 31/1500\n",
      "71/71 [==============================] - 0s 960us/step - loss: 0.3402 - accuracy: 0.8644\n",
      "Epoch 32/1500\n",
      "71/71 [==============================] - 0s 957us/step - loss: 0.3263 - accuracy: 0.8759\n",
      "Epoch 33/1500\n",
      "71/71 [==============================] - 0s 911us/step - loss: 0.3464 - accuracy: 0.8653\n",
      "Epoch 34/1500\n",
      "71/71 [==============================] - 0s 932us/step - loss: 0.3159 - accuracy: 0.8688\n",
      "Epoch 35/1500\n",
      "71/71 [==============================] - 0s 927us/step - loss: 0.3240 - accuracy: 0.8697\n",
      "Epoch 36/1500\n",
      "71/71 [==============================] - 0s 911us/step - loss: 0.3117 - accuracy: 0.8755\n",
      "Epoch 37/1500\n",
      "71/71 [==============================] - 0s 926us/step - loss: 0.3195 - accuracy: 0.8679\n",
      "Epoch 38/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3180 - accuracy: 0.8688\n",
      "Epoch 39/1500\n",
      "71/71 [==============================] - 0s 891us/step - loss: 0.3098 - accuracy: 0.8724\n",
      "Epoch 40/1500\n",
      "71/71 [==============================] - 0s 969us/step - loss: 0.3102 - accuracy: 0.8737\n",
      "Epoch 41/1500\n",
      "71/71 [==============================] - 0s 973us/step - loss: 0.3074 - accuracy: 0.8773\n",
      "Epoch 42/1500\n",
      "71/71 [==============================] - 0s 961us/step - loss: 0.2796 - accuracy: 0.8862\n",
      "Epoch 43/1500\n",
      "71/71 [==============================] - 0s 938us/step - loss: 0.2943 - accuracy: 0.8817\n",
      "Epoch 44/1500\n",
      "71/71 [==============================] - 0s 932us/step - loss: 0.2769 - accuracy: 0.8848\n",
      "Epoch 45/1500\n",
      "71/71 [==============================] - 0s 920us/step - loss: 0.2910 - accuracy: 0.8831\n",
      "Epoch 46/1500\n",
      "71/71 [==============================] - 0s 939us/step - loss: 0.3021 - accuracy: 0.8924\n",
      "Epoch 47/1500\n",
      "71/71 [==============================] - 0s 921us/step - loss: 0.2898 - accuracy: 0.8813\n",
      "Epoch 48/1500\n",
      "71/71 [==============================] - 0s 919us/step - loss: 0.2883 - accuracy: 0.8906\n",
      "Epoch 49/1500\n",
      "71/71 [==============================] - 0s 935us/step - loss: 0.2742 - accuracy: 0.8951\n",
      "Epoch 50/1500\n",
      "71/71 [==============================] - 0s 909us/step - loss: 0.2901 - accuracy: 0.8844\n",
      "Epoch 51/1500\n",
      "71/71 [==============================] - 0s 913us/step - loss: 0.2689 - accuracy: 0.8968\n",
      "Epoch 52/1500\n",
      "71/71 [==============================] - 0s 925us/step - loss: 0.2706 - accuracy: 0.8915\n",
      "Epoch 53/1500\n",
      "71/71 [==============================] - 0s 990us/step - loss: 0.2752 - accuracy: 0.8880\n",
      "Epoch 54/1500\n",
      "71/71 [==============================] - 0s 946us/step - loss: 0.2676 - accuracy: 0.8937\n",
      "Epoch 55/1500\n",
      "71/71 [==============================] - 0s 929us/step - loss: 0.2664 - accuracy: 0.8964\n",
      "Epoch 56/1500\n",
      "71/71 [==============================] - 0s 917us/step - loss: 0.2552 - accuracy: 0.8995\n",
      "Epoch 57/1500\n",
      "71/71 [==============================] - 0s 900us/step - loss: 0.2421 - accuracy: 0.9075\n",
      "Epoch 58/1500\n",
      "71/71 [==============================] - 0s 899us/step - loss: 0.2464 - accuracy: 0.9040\n",
      "Epoch 59/1500\n",
      "71/71 [==============================] - 0s 922us/step - loss: 0.2497 - accuracy: 0.9026\n",
      "Epoch 60/1500\n",
      "71/71 [==============================] - 0s 911us/step - loss: 0.2420 - accuracy: 0.9062\n",
      "Epoch 61/1500\n",
      "71/71 [==============================] - 0s 920us/step - loss: 0.2523 - accuracy: 0.9044\n",
      "Epoch 62/1500\n",
      "71/71 [==============================] - 0s 905us/step - loss: 0.2423 - accuracy: 0.9035\n",
      "Epoch 63/1500\n",
      "71/71 [==============================] - 0s 925us/step - loss: 0.2571 - accuracy: 0.8986\n",
      "Epoch 64/1500\n",
      "71/71 [==============================] - 0s 914us/step - loss: 0.2518 - accuracy: 0.9022\n",
      "Epoch 65/1500\n",
      "71/71 [==============================] - 0s 907us/step - loss: 0.2297 - accuracy: 0.9173\n",
      "Epoch 66/1500\n",
      "71/71 [==============================] - 0s 919us/step - loss: 0.2225 - accuracy: 0.9133\n",
      "Epoch 67/1500\n",
      "71/71 [==============================] - 0s 904us/step - loss: 0.2405 - accuracy: 0.9066\n",
      "Epoch 68/1500\n",
      "71/71 [==============================] - 0s 920us/step - loss: 0.2264 - accuracy: 0.9071\n",
      "Epoch 69/1500\n",
      "71/71 [==============================] - 0s 904us/step - loss: 0.2182 - accuracy: 0.9213\n",
      "Epoch 70/1500\n",
      "71/71 [==============================] - 0s 908us/step - loss: 0.2354 - accuracy: 0.9133\n",
      "Epoch 71/1500\n",
      "71/71 [==============================] - 0s 910us/step - loss: 0.2261 - accuracy: 0.9071\n",
      "Epoch 72/1500\n",
      "71/71 [==============================] - 0s 925us/step - loss: 0.2373 - accuracy: 0.9044\n",
      "Epoch 73/1500\n",
      "71/71 [==============================] - 0s 892us/step - loss: 0.2179 - accuracy: 0.9177\n",
      "Epoch 74/1500\n",
      "71/71 [==============================] - 0s 907us/step - loss: 0.2230 - accuracy: 0.9146\n",
      "Epoch 75/1500\n",
      "71/71 [==============================] - 0s 909us/step - loss: 0.2262 - accuracy: 0.9120\n",
      "Epoch 76/1500\n",
      "71/71 [==============================] - 0s 936us/step - loss: 0.2144 - accuracy: 0.9164\n",
      "Epoch 77/1500\n",
      "71/71 [==============================] - 0s 916us/step - loss: 0.2157 - accuracy: 0.9151\n",
      "Epoch 78/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2058 - accuracy: 0.9253\n",
      "Epoch 79/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2207 - accuracy: 0.9177\n",
      "Epoch 80/1500\n",
      "71/71 [==============================] - 0s 933us/step - loss: 0.2407 - accuracy: 0.9031\n",
      "Epoch 81/1500\n",
      "71/71 [==============================] - 0s 919us/step - loss: 0.2105 - accuracy: 0.9217\n",
      "Epoch 82/1500\n",
      "71/71 [==============================] - 0s 911us/step - loss: 0.2184 - accuracy: 0.9182\n",
      "Epoch 83/1500\n",
      "71/71 [==============================] - 0s 934us/step - loss: 0.1998 - accuracy: 0.9231\n",
      "Epoch 84/1500\n",
      "71/71 [==============================] - 0s 948us/step - loss: 0.2161 - accuracy: 0.9182\n",
      "Epoch 85/1500\n",
      "71/71 [==============================] - 0s 921us/step - loss: 0.2047 - accuracy: 0.9253\n",
      "Epoch 86/1500\n",
      "71/71 [==============================] - 0s 925us/step - loss: 0.2135 - accuracy: 0.9124\n",
      "Epoch 87/1500\n",
      "71/71 [==============================] - 0s 919us/step - loss: 0.2035 - accuracy: 0.9213\n",
      "Epoch 88/1500\n",
      "71/71 [==============================] - 0s 914us/step - loss: 0.1956 - accuracy: 0.9280\n",
      "Epoch 89/1500\n",
      "71/71 [==============================] - 0s 908us/step - loss: 0.2201 - accuracy: 0.9124\n",
      "Epoch 90/1500\n",
      "71/71 [==============================] - 0s 898us/step - loss: 0.2003 - accuracy: 0.9302\n",
      "Epoch 91/1500\n",
      "71/71 [==============================] - 0s 925us/step - loss: 0.1827 - accuracy: 0.9360\n",
      "Epoch 92/1500\n",
      "71/71 [==============================] - 0s 905us/step - loss: 0.1948 - accuracy: 0.9275\n",
      "Epoch 93/1500\n",
      "71/71 [==============================] - 0s 899us/step - loss: 0.2066 - accuracy: 0.9204\n",
      "Epoch 94/1500\n",
      "71/71 [==============================] - 0s 927us/step - loss: 0.2085 - accuracy: 0.9164\n",
      "Epoch 95/1500\n",
      "71/71 [==============================] - 0s 916us/step - loss: 0.1938 - accuracy: 0.9342\n",
      "Epoch 96/1500\n",
      "71/71 [==============================] - 0s 907us/step - loss: 0.1860 - accuracy: 0.9293\n",
      "Epoch 97/1500\n",
      "71/71 [==============================] - 0s 906us/step - loss: 0.1793 - accuracy: 0.9342\n",
      "Epoch 98/1500\n",
      "71/71 [==============================] - 0s 954us/step - loss: 0.2075 - accuracy: 0.9209\n",
      "Epoch 99/1500\n",
      "71/71 [==============================] - 0s 907us/step - loss: 0.2035 - accuracy: 0.9249\n",
      "Epoch 100/1500\n",
      "71/71 [==============================] - 0s 917us/step - loss: 0.1886 - accuracy: 0.9284\n",
      "Epoch 101/1500\n",
      "71/71 [==============================] - 0s 987us/step - loss: 0.1803 - accuracy: 0.9355\n",
      "Epoch 102/1500\n",
      "71/71 [==============================] - 0s 940us/step - loss: 0.1840 - accuracy: 0.9302\n",
      "Epoch 103/1500\n",
      "71/71 [==============================] - 0s 946us/step - loss: 0.1963 - accuracy: 0.9306\n",
      "Epoch 104/1500\n",
      "71/71 [==============================] - 0s 929us/step - loss: 0.1741 - accuracy: 0.9324\n",
      "Epoch 105/1500\n",
      "71/71 [==============================] - 0s 933us/step - loss: 0.1749 - accuracy: 0.9391\n",
      "Epoch 106/1500\n",
      "71/71 [==============================] - 0s 947us/step - loss: 0.1768 - accuracy: 0.9297\n",
      "Epoch 107/1500\n",
      "71/71 [==============================] - 0s 933us/step - loss: 0.1714 - accuracy: 0.9360\n",
      "Epoch 108/1500\n",
      "71/71 [==============================] - 0s 955us/step - loss: 0.1738 - accuracy: 0.9355\n",
      "Epoch 109/1500\n",
      "71/71 [==============================] - 0s 981us/step - loss: 0.1688 - accuracy: 0.9422\n",
      "Epoch 110/1500\n",
      "71/71 [==============================] - 0s 959us/step - loss: 0.1606 - accuracy: 0.9453\n",
      "Epoch 111/1500\n",
      "71/71 [==============================] - 0s 993us/step - loss: 0.1532 - accuracy: 0.9400\n",
      "Epoch 112/1500\n",
      "71/71 [==============================] - 0s 943us/step - loss: 0.1548 - accuracy: 0.9422\n",
      "Epoch 113/1500\n",
      "71/71 [==============================] - 0s 921us/step - loss: 0.1593 - accuracy: 0.9466\n",
      "Epoch 114/1500\n",
      "71/71 [==============================] - 0s 940us/step - loss: 0.1594 - accuracy: 0.9409\n",
      "Epoch 115/1500\n",
      "71/71 [==============================] - 0s 899us/step - loss: 0.1733 - accuracy: 0.9360\n",
      "Epoch 116/1500\n",
      "71/71 [==============================] - 0s 945us/step - loss: 0.1623 - accuracy: 0.9409\n",
      "Epoch 117/1500\n",
      "71/71 [==============================] - 0s 912us/step - loss: 0.1710 - accuracy: 0.9373\n",
      "Epoch 118/1500\n",
      "71/71 [==============================] - 0s 929us/step - loss: 0.1765 - accuracy: 0.9346\n",
      "Epoch 119/1500\n",
      "71/71 [==============================] - 0s 936us/step - loss: 0.1899 - accuracy: 0.9249\n",
      "Epoch 120/1500\n",
      "71/71 [==============================] - 0s 909us/step - loss: 0.1718 - accuracy: 0.9364\n",
      "Epoch 121/1500\n",
      "71/71 [==============================] - 0s 916us/step - loss: 0.1628 - accuracy: 0.9355\n",
      "Epoch 122/1500\n",
      "71/71 [==============================] - 0s 915us/step - loss: 0.1578 - accuracy: 0.9484\n",
      "Epoch 123/1500\n",
      "71/71 [==============================] - 0s 946us/step - loss: 0.1611 - accuracy: 0.9431\n",
      "Epoch 124/1500\n",
      "71/71 [==============================] - 0s 932us/step - loss: 0.1454 - accuracy: 0.9515\n",
      "Epoch 125/1500\n",
      "71/71 [==============================] - 0s 918us/step - loss: 0.1598 - accuracy: 0.9426\n",
      "Epoch 126/1500\n",
      "71/71 [==============================] - 0s 919us/step - loss: 0.1532 - accuracy: 0.9453\n",
      "Epoch 127/1500\n",
      "71/71 [==============================] - 0s 903us/step - loss: 0.1545 - accuracy: 0.9493\n",
      "Epoch 128/1500\n",
      "71/71 [==============================] - 0s 965us/step - loss: 0.1578 - accuracy: 0.9409\n",
      "Epoch 129/1500\n",
      "71/71 [==============================] - 0s 944us/step - loss: 0.1408 - accuracy: 0.9564\n",
      "Epoch 130/1500\n",
      "71/71 [==============================] - 0s 986us/step - loss: 0.1468 - accuracy: 0.9475\n",
      "Epoch 131/1500\n",
      "71/71 [==============================] - 0s 980us/step - loss: 0.1506 - accuracy: 0.9431\n",
      "Epoch 132/1500\n",
      "71/71 [==============================] - 0s 929us/step - loss: 0.1469 - accuracy: 0.9493\n",
      "Epoch 133/1500\n",
      "71/71 [==============================] - 0s 947us/step - loss: 0.1493 - accuracy: 0.9440\n",
      "Epoch 134/1500\n",
      "71/71 [==============================] - 0s 961us/step - loss: 0.1475 - accuracy: 0.9458\n",
      "Epoch 135/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9560\n",
      "Epoch 136/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1531 - accuracy: 0.9453\n",
      "Epoch 137/1500\n",
      "71/71 [==============================] - 0s 998us/step - loss: 0.1491 - accuracy: 0.9462\n",
      "Epoch 138/1500\n",
      "71/71 [==============================] - 0s 962us/step - loss: 0.1564 - accuracy: 0.9413\n",
      "Epoch 139/1500\n",
      "71/71 [==============================] - 0s 919us/step - loss: 0.1558 - accuracy: 0.9480\n",
      "Epoch 140/1500\n",
      "71/71 [==============================] - 0s 962us/step - loss: 0.1472 - accuracy: 0.9471\n",
      "Epoch 141/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9498\n",
      "Epoch 142/1500\n",
      "71/71 [==============================] - 0s 987us/step - loss: 0.1319 - accuracy: 0.9551\n",
      "Epoch 143/1500\n",
      "71/71 [==============================] - 0s 970us/step - loss: 0.1492 - accuracy: 0.9453\n",
      "Epoch 144/1500\n",
      "71/71 [==============================] - 0s 988us/step - loss: 0.1272 - accuracy: 0.9564\n",
      "Epoch 145/1500\n",
      "71/71 [==============================] - 0s 994us/step - loss: 0.1474 - accuracy: 0.9489\n",
      "Epoch 146/1500\n",
      "71/71 [==============================] - 0s 968us/step - loss: 0.1509 - accuracy: 0.9373\n",
      "Epoch 147/1500\n",
      "71/71 [==============================] - 0s 987us/step - loss: 0.1519 - accuracy: 0.9444\n",
      "Epoch 148/1500\n",
      "71/71 [==============================] - 0s 929us/step - loss: 0.1633 - accuracy: 0.9444\n",
      "Epoch 149/1500\n",
      "71/71 [==============================] - 0s 919us/step - loss: 0.1397 - accuracy: 0.9511\n",
      "Epoch 150/1500\n",
      "71/71 [==============================] - 0s 954us/step - loss: 0.1505 - accuracy: 0.9449\n",
      "Epoch 151/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9511\n",
      "Epoch 152/1500\n",
      "71/71 [==============================] - 0s 986us/step - loss: 0.1384 - accuracy: 0.9569\n",
      "Epoch 153/1500\n",
      "71/71 [==============================] - 0s 940us/step - loss: 0.1269 - accuracy: 0.9524\n",
      "Epoch 154/1500\n",
      "71/71 [==============================] - 0s 921us/step - loss: 0.1329 - accuracy: 0.9538\n",
      "Epoch 155/1500\n",
      "71/71 [==============================] - 0s 961us/step - loss: 0.1282 - accuracy: 0.9618\n",
      "Epoch 156/1500\n",
      "71/71 [==============================] - 0s 934us/step - loss: 0.1342 - accuracy: 0.9564\n",
      "Epoch 157/1500\n",
      "71/71 [==============================] - 0s 928us/step - loss: 0.1422 - accuracy: 0.9471\n",
      "Epoch 158/1500\n",
      "71/71 [==============================] - 0s 981us/step - loss: 0.1427 - accuracy: 0.9475\n",
      "Epoch 159/1500\n",
      "71/71 [==============================] - 0s 968us/step - loss: 0.1362 - accuracy: 0.9529\n",
      "Epoch 160/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9546\n",
      "Epoch 161/1500\n",
      "71/71 [==============================] - 0s 919us/step - loss: 0.1309 - accuracy: 0.9533\n",
      "Epoch 162/1500\n",
      "71/71 [==============================] - 0s 922us/step - loss: 0.1430 - accuracy: 0.9533\n",
      "Epoch 163/1500\n",
      "71/71 [==============================] - 0s 963us/step - loss: 0.1316 - accuracy: 0.9498\n",
      "Epoch 164/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9600\n",
      "Epoch 165/1500\n",
      "71/71 [==============================] - 0s 937us/step - loss: 0.1412 - accuracy: 0.9480\n",
      "Epoch 166/1500\n",
      "71/71 [==============================] - 0s 967us/step - loss: 0.1208 - accuracy: 0.9595\n",
      "Epoch 167/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9551\n",
      "Epoch 168/1500\n",
      "71/71 [==============================] - 0s 979us/step - loss: 0.1157 - accuracy: 0.9582\n",
      "Epoch 169/1500\n",
      "71/71 [==============================] - 0s 970us/step - loss: 0.1337 - accuracy: 0.9498\n",
      "Epoch 170/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9573\n",
      "Epoch 171/1500\n",
      "71/71 [==============================] - 0s 914us/step - loss: 0.1292 - accuracy: 0.9493\n",
      "Epoch 172/1500\n",
      "71/71 [==============================] - 0s 929us/step - loss: 0.1215 - accuracy: 0.9586\n",
      "Epoch 173/1500\n",
      "71/71 [==============================] - 0s 922us/step - loss: 0.1391 - accuracy: 0.9466\n",
      "Epoch 174/1500\n",
      "71/71 [==============================] - 0s 928us/step - loss: 0.1322 - accuracy: 0.9515\n",
      "Epoch 175/1500\n",
      "71/71 [==============================] - 0s 946us/step - loss: 0.1482 - accuracy: 0.9502\n",
      "Epoch 176/1500\n",
      "71/71 [==============================] - 0s 949us/step - loss: 0.1268 - accuracy: 0.9551\n",
      "Epoch 177/1500\n",
      "71/71 [==============================] - 0s 932us/step - loss: 0.1353 - accuracy: 0.9538\n",
      "Epoch 178/1500\n",
      "71/71 [==============================] - 0s 918us/step - loss: 0.1310 - accuracy: 0.9520\n",
      "Epoch 179/1500\n",
      "71/71 [==============================] - 0s 897us/step - loss: 0.1282 - accuracy: 0.9529\n",
      "Epoch 180/1500\n",
      "71/71 [==============================] - 0s 916us/step - loss: 0.1193 - accuracy: 0.9618\n",
      "Epoch 181/1500\n",
      "71/71 [==============================] - 0s 910us/step - loss: 0.1237 - accuracy: 0.9551\n",
      "Epoch 182/1500\n",
      "71/71 [==============================] - 0s 938us/step - loss: 0.1304 - accuracy: 0.9600\n",
      "Epoch 183/1500\n",
      "71/71 [==============================] - 0s 950us/step - loss: 0.1190 - accuracy: 0.9578\n",
      "Epoch 184/1500\n",
      "71/71 [==============================] - 0s 992us/step - loss: 0.1242 - accuracy: 0.9551\n",
      "Epoch 185/1500\n",
      "71/71 [==============================] - 0s 961us/step - loss: 0.1194 - accuracy: 0.9573\n",
      "Epoch 186/1500\n",
      "71/71 [==============================] - 0s 914us/step - loss: 0.1242 - accuracy: 0.9564\n",
      "Epoch 187/1500\n",
      "71/71 [==============================] - 0s 921us/step - loss: 0.1228 - accuracy: 0.9551\n",
      "Epoch 188/1500\n",
      "71/71 [==============================] - 0s 956us/step - loss: 0.1158 - accuracy: 0.9560\n",
      "Epoch 189/1500\n",
      "71/71 [==============================] - 0s 947us/step - loss: 0.1252 - accuracy: 0.9529\n",
      "Epoch 190/1500\n",
      "71/71 [==============================] - 0s 971us/step - loss: 0.1194 - accuracy: 0.9542\n",
      "Epoch 191/1500\n",
      "71/71 [==============================] - 0s 924us/step - loss: 0.1204 - accuracy: 0.9564\n",
      "Epoch 192/1500\n",
      "71/71 [==============================] - 0s 917us/step - loss: 0.1179 - accuracy: 0.9609\n",
      "Epoch 193/1500\n",
      "71/71 [==============================] - 0s 928us/step - loss: 0.1331 - accuracy: 0.9506\n",
      "Epoch 194/1500\n",
      "71/71 [==============================] - 0s 962us/step - loss: 0.1164 - accuracy: 0.9600\n",
      "Epoch 195/1500\n",
      "71/71 [==============================] - 0s 972us/step - loss: 0.1237 - accuracy: 0.9604\n",
      "Epoch 196/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9662\n",
      "Epoch 197/1500\n",
      "71/71 [==============================] - 0s 950us/step - loss: 0.1312 - accuracy: 0.9560\n",
      "Epoch 198/1500\n",
      "71/71 [==============================] - 0s 976us/step - loss: 0.1313 - accuracy: 0.9511\n",
      "Epoch 199/1500\n",
      "71/71 [==============================] - 0s 989us/step - loss: 0.1093 - accuracy: 0.9600\n",
      "Epoch 200/1500\n",
      "71/71 [==============================] - 0s 929us/step - loss: 0.1171 - accuracy: 0.9573\n",
      "Epoch 201/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9586\n",
      "Epoch 202/1500\n",
      "71/71 [==============================] - 0s 963us/step - loss: 0.1150 - accuracy: 0.9586\n",
      "Epoch 203/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9640\n",
      "Epoch 204/1500\n",
      "71/71 [==============================] - 0s 976us/step - loss: 0.1292 - accuracy: 0.9515\n",
      "Epoch 205/1500\n",
      "71/71 [==============================] - 0s 905us/step - loss: 0.1244 - accuracy: 0.9586\n",
      "Epoch 206/1500\n",
      "71/71 [==============================] - 0s 928us/step - loss: 0.1234 - accuracy: 0.9582\n",
      "Epoch 207/1500\n",
      "71/71 [==============================] - 0s 961us/step - loss: 0.1389 - accuracy: 0.9498\n",
      "Epoch 208/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9609\n",
      "Epoch 209/1500\n",
      "71/71 [==============================] - 0s 992us/step - loss: 0.1162 - accuracy: 0.9578\n",
      "Epoch 210/1500\n",
      "71/71 [==============================] - 0s 973us/step - loss: 0.1185 - accuracy: 0.9578\n",
      "Epoch 211/1500\n",
      "71/71 [==============================] - 0s 994us/step - loss: 0.1294 - accuracy: 0.9600\n",
      "Epoch 212/1500\n",
      "71/71 [==============================] - 0s 947us/step - loss: 0.1019 - accuracy: 0.9693\n",
      "Epoch 213/1500\n",
      "71/71 [==============================] - 0s 966us/step - loss: 0.1148 - accuracy: 0.9618\n",
      "Epoch 214/1500\n",
      "71/71 [==============================] - 0s 976us/step - loss: 0.1019 - accuracy: 0.9635\n",
      "Epoch 215/1500\n",
      "71/71 [==============================] - 0s 970us/step - loss: 0.1078 - accuracy: 0.9627\n",
      "Epoch 216/1500\n",
      "71/71 [==============================] - 0s 936us/step - loss: 0.1163 - accuracy: 0.9591\n",
      "Epoch 217/1500\n",
      "71/71 [==============================] - 0s 980us/step - loss: 0.1339 - accuracy: 0.9524\n",
      "Epoch 218/1500\n",
      "71/71 [==============================] - 0s 960us/step - loss: 0.0947 - accuracy: 0.9698\n",
      "Epoch 219/1500\n",
      "71/71 [==============================] - 0s 944us/step - loss: 0.1027 - accuracy: 0.9644\n",
      "Epoch 220/1500\n",
      "71/71 [==============================] - 0s 916us/step - loss: 0.0998 - accuracy: 0.9649\n",
      "Epoch 221/1500\n",
      "71/71 [==============================] - 0s 907us/step - loss: 0.1031 - accuracy: 0.9680\n",
      "Epoch 222/1500\n",
      "71/71 [==============================] - 0s 942us/step - loss: 0.1231 - accuracy: 0.9551\n",
      "Epoch 223/1500\n",
      "71/71 [==============================] - 0s 911us/step - loss: 0.1034 - accuracy: 0.9667\n",
      "Epoch 224/1500\n",
      "71/71 [==============================] - 0s 961us/step - loss: 0.1147 - accuracy: 0.9644\n",
      "Epoch 225/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9680\n",
      "Epoch 226/1500\n",
      "71/71 [==============================] - 0s 949us/step - loss: 0.1005 - accuracy: 0.9649\n",
      "Epoch 227/1500\n",
      "71/71 [==============================] - 0s 926us/step - loss: 0.0929 - accuracy: 0.9733\n",
      "Epoch 228/1500\n",
      "71/71 [==============================] - 0s 912us/step - loss: 0.1016 - accuracy: 0.9644\n",
      "Epoch 229/1500\n",
      "71/71 [==============================] - 0s 926us/step - loss: 0.1069 - accuracy: 0.9609\n",
      "Epoch 230/1500\n",
      "71/71 [==============================] - 0s 988us/step - loss: 0.1085 - accuracy: 0.9573\n",
      "Epoch 231/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1006 - accuracy: 0.9631\n",
      "Epoch 232/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9622\n",
      "Epoch 233/1500\n",
      "71/71 [==============================] - 0s 979us/step - loss: 0.1010 - accuracy: 0.9609\n",
      "Epoch 234/1500\n",
      "71/71 [==============================] - 0s 982us/step - loss: 0.1290 - accuracy: 0.9586\n",
      "Epoch 235/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9662\n",
      "Epoch 236/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9578\n",
      "Epoch 237/1500\n",
      "71/71 [==============================] - 0s 963us/step - loss: 0.0997 - accuracy: 0.9662\n",
      "Epoch 238/1500\n",
      "71/71 [==============================] - 0s 971us/step - loss: 0.1128 - accuracy: 0.9631\n",
      "Epoch 239/1500\n",
      "71/71 [==============================] - 0s 938us/step - loss: 0.1135 - accuracy: 0.9618\n",
      "Epoch 240/1500\n",
      "71/71 [==============================] - 0s 974us/step - loss: 0.1057 - accuracy: 0.9684\n",
      "Epoch 241/1500\n",
      "71/71 [==============================] - 0s 953us/step - loss: 0.1021 - accuracy: 0.9635\n",
      "Epoch 242/1500\n",
      "71/71 [==============================] - 0s 941us/step - loss: 0.0942 - accuracy: 0.9671\n",
      "Epoch 243/1500\n",
      "71/71 [==============================] - 0s 931us/step - loss: 0.0947 - accuracy: 0.9653\n",
      "Epoch 244/1500\n",
      "71/71 [==============================] - 0s 918us/step - loss: 0.0968 - accuracy: 0.9667\n",
      "Epoch 245/1500\n",
      "71/71 [==============================] - 0s 938us/step - loss: 0.1038 - accuracy: 0.9627\n",
      "Epoch 246/1500\n",
      "71/71 [==============================] - 0s 921us/step - loss: 0.1139 - accuracy: 0.9653\n",
      "Epoch 247/1500\n",
      "71/71 [==============================] - 0s 925us/step - loss: 0.0986 - accuracy: 0.9662\n",
      "Epoch 248/1500\n",
      "71/71 [==============================] - 0s 936us/step - loss: 0.1057 - accuracy: 0.9631\n",
      "Epoch 249/1500\n",
      "71/71 [==============================] - 0s 958us/step - loss: 0.0900 - accuracy: 0.9693\n",
      "Epoch 250/1500\n",
      "71/71 [==============================] - 0s 942us/step - loss: 0.1067 - accuracy: 0.9586\n",
      "Epoch 251/1500\n",
      "71/71 [==============================] - 0s 940us/step - loss: 0.0926 - accuracy: 0.9662\n",
      "Epoch 252/1500\n",
      "71/71 [==============================] - 0s 924us/step - loss: 0.0944 - accuracy: 0.9689\n",
      "Epoch 253/1500\n",
      "71/71 [==============================] - 0s 940us/step - loss: 0.0832 - accuracy: 0.9693\n",
      "Epoch 254/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9702\n",
      "Epoch 255/1500\n",
      "71/71 [==============================] - 0s 970us/step - loss: 0.1131 - accuracy: 0.9613\n",
      "Epoch 256/1500\n",
      "71/71 [==============================] - 0s 935us/step - loss: 0.0947 - accuracy: 0.9715\n",
      "Epoch 257/1500\n",
      "71/71 [==============================] - 0s 918us/step - loss: 0.1020 - accuracy: 0.9667\n",
      "Epoch 258/1500\n",
      "71/71 [==============================] - 0s 940us/step - loss: 0.0859 - accuracy: 0.9707\n",
      "Epoch 259/1500\n",
      "71/71 [==============================] - 0s 964us/step - loss: 0.0951 - accuracy: 0.9675\n",
      "Epoch 260/1500\n",
      "71/71 [==============================] - 0s 932us/step - loss: 0.1036 - accuracy: 0.9667\n",
      "Epoch 261/1500\n",
      "71/71 [==============================] - 0s 940us/step - loss: 0.1026 - accuracy: 0.9649\n",
      "Epoch 262/1500\n",
      "71/71 [==============================] - 0s 998us/step - loss: 0.0982 - accuracy: 0.9631\n",
      "Epoch 263/1500\n",
      "71/71 [==============================] - 0s 965us/step - loss: 0.1014 - accuracy: 0.9644\n",
      "Epoch 264/1500\n",
      "71/71 [==============================] - 0s 989us/step - loss: 0.1023 - accuracy: 0.9693\n",
      "Epoch 265/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9755\n",
      "Epoch 266/1500\n",
      "71/71 [==============================] - 0s 975us/step - loss: 0.0878 - accuracy: 0.9702\n",
      "Epoch 267/1500\n",
      "71/71 [==============================] - 0s 933us/step - loss: 0.0965 - accuracy: 0.9693\n",
      "Epoch 268/1500\n",
      "71/71 [==============================] - 0s 931us/step - loss: 0.0955 - accuracy: 0.9658\n",
      "Epoch 269/1500\n",
      "71/71 [==============================] - 0s 950us/step - loss: 0.0894 - accuracy: 0.9707\n",
      "Epoch 270/1500\n",
      "71/71 [==============================] - 0s 955us/step - loss: 0.1061 - accuracy: 0.9627\n",
      "Epoch 271/1500\n",
      "71/71 [==============================] - 0s 936us/step - loss: 0.0914 - accuracy: 0.9693\n",
      "Epoch 272/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0907 - accuracy: 0.9689\n",
      "Epoch 273/1500\n",
      "71/71 [==============================] - 0s 982us/step - loss: 0.1128 - accuracy: 0.9524\n",
      "Epoch 274/1500\n",
      "71/71 [==============================] - 0s 970us/step - loss: 0.1030 - accuracy: 0.9609\n",
      "Epoch 275/1500\n",
      "71/71 [==============================] - 0s 937us/step - loss: 0.0809 - accuracy: 0.9711\n",
      "Epoch 276/1500\n",
      "71/71 [==============================] - 0s 950us/step - loss: 0.0901 - accuracy: 0.9689\n",
      "Epoch 277/1500\n",
      "71/71 [==============================] - 0s 959us/step - loss: 0.0959 - accuracy: 0.9667\n",
      "Epoch 278/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9644\n",
      "Epoch 279/1500\n",
      "71/71 [==============================] - 0s 973us/step - loss: 0.0951 - accuracy: 0.9658\n",
      "Epoch 280/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.9622\n",
      "Epoch 281/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9707\n",
      "Epoch 282/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9729\n",
      "Epoch 283/1500\n",
      "71/71 [==============================] - 0s 977us/step - loss: 0.0951 - accuracy: 0.9689\n",
      "Epoch 284/1500\n",
      "71/71 [==============================] - 0s 961us/step - loss: 0.0961 - accuracy: 0.9693\n",
      "Epoch 285/1500\n",
      "71/71 [==============================] - 0s 976us/step - loss: 0.0859 - accuracy: 0.9715\n",
      "Epoch 286/1500\n",
      "71/71 [==============================] - 0s 933us/step - loss: 0.0975 - accuracy: 0.9635\n",
      "Epoch 287/1500\n",
      "71/71 [==============================] - 0s 933us/step - loss: 0.0978 - accuracy: 0.9658\n",
      "Epoch 288/1500\n",
      "71/71 [==============================] - 0s 934us/step - loss: 0.0870 - accuracy: 0.9698\n",
      "Epoch 289/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9658\n",
      "Epoch 290/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9702\n",
      "Epoch 291/1500\n",
      "71/71 [==============================] - 0s 973us/step - loss: 0.1043 - accuracy: 0.9604\n",
      "Epoch 292/1500\n",
      "71/71 [==============================] - 0s 963us/step - loss: 0.1057 - accuracy: 0.9680\n",
      "Epoch 293/1500\n",
      "71/71 [==============================] - 0s 994us/step - loss: 0.1022 - accuracy: 0.9653\n",
      "Epoch 294/1500\n",
      "71/71 [==============================] - 0s 966us/step - loss: 0.0924 - accuracy: 0.9667\n",
      "Epoch 295/1500\n",
      "54/71 [=====================>........] - ETA: 0s - loss: 0.1241 - accuracy: 0.9566Restoring model weights from the end of the best epoch: 265.\n",
      "71/71 [==============================] - 0s 981us/step - loss: 0.1230 - accuracy: 0.9578\n",
      "Epoch 295: early stopping\n",
      "8/8 [==============================] - 0s 756us/step - loss: 0.5602 - accuracy: 0.7939\n",
      "8/8 [==============================] - 0s 623us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.82 (23/28)\n",
      "Before appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "After appending - Cat IDs: 841, Predictions: 841, Actuals: 841, Gender: 841\n",
      "Final Test Results - Loss: 0.5601895451545715, Accuracy: 0.7938596606254578, Precision: 0.7601795030366459, Recall: 0.7866957628659756, F1 Score: 0.7718242412476543\n",
      "Confusion Matrix:\n",
      " [[128   4  24]\n",
      " [  3  22   0]\n",
      " [ 16   0  31]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6791114933666285\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8016891032457352\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7266953140497208\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6943025153959035\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.699318108370939\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[4]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90d80b-198d-4293-a1a0-73a65f6588d0",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9dd5399-64d4-435b-9e73-cb31f16d042d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 841, Predictions: 841, Actuals: 841, Gender: 841\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d9869f0-e23f-4453-a329-395fa44f1208",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a7f81185-7b51-497f-98cb-80c4b8f35388",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.78 (86/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "adbd2ca0-3dea-4b42-bfad-93138cb1ae30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b1ea4fe7-6ee1-4185-a009-b864d9aa6b85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[senior, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[senior, senior, adult, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, kit...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, kitten, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, s...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "65    059A  [senior, adult, senior, senior, senior, senior...        senior           senior                   True\n",
       "64    058A                            [senior, adult, senior]        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [adult, adult, senior, adult, adult, senior, s...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "56    051A  [adult, adult, adult, adult, senior, senior, a...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A             [kitten, adult, adult, kitten, kitten]        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "81    075A               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                    [senior, senior, adult, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                    [adult, senior]         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "92    097A  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "33    026C                                            [adult]         adult            adult                   True\n",
       "32    026B                                            [adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "10    009A                     [senior, adult, adult, senior]         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "25    023A        [adult, kitten, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, senior, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "18    016A  [senior, adult, senior, senior, senior, senior...        senior           senior                   True\n",
       "17    015A  [adult, adult, senior, adult, senior, adult, s...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "8     007A       [adult, senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "42    036A  [senior, senior, adult, senior, adult, adult, ...         adult            adult                   True\n",
       "41    035A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, kitten, adult, adult, adult, ki...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "7     006A                             [senior, adult, adult]         adult            adult                   True\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "102   108A      [senior, adult, adult, senior, adult, senior]         adult           senior                  False\n",
       "101   106A  [adult, senior, senior, senior, adult, adult, ...         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "47    041A                                    [adult, kitten]         adult           kitten                  False\n",
       "90    095A  [senior, adult, adult, senior, senior, senior,...        senior            adult                  False\n",
       "40    034A             [senior, senior, adult, senior, adult]        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "52    047A  [adult, adult, adult, adult, adult, adult, kit...         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "54    049A                                            [adult]         adult           kitten                  False\n",
       "57    051B  [adult, adult, adult, adult, senior, adult, ad...         adult           senior                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "60    054A                                    [adult, senior]         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "69    063A  [senior, senior, senior, kitten, adult, senior...        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "48    042A  [kitten, adult, kitten, kitten, adult, adult, ...         adult           kitten                  False\n",
       "109   117A  [senior, senior, adult, adult, adult, adult, a...         adult           senior                  False"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e03366c5-a807-4159-b0c1-8dc88c04d91e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     65\n",
      "kitten     8\n",
      "senior    13\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "0ae8f86b-0e19-49df-a07c-f64383bce76b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             65  89.041096\n",
      "1           kitten           15              8  53.333333\n",
      "2           senior           22             13  59.090909\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6350c1b2-050d-4bf3-98f1-5a80b3078d3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ea2c7-0827-4ffa-9777-ad2891e5f49b",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "94fbe612-c62e-4abe-8ec3-d75940a993cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          554            445  80.324910\n",
      "1           kitten          109             67  61.467890\n",
      "2           senior          178            100  56.179775\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "50e84f5a-909e-47ca-93ca-2e92abf7ded1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ac46e-4cc6-4237-925c-524d47e83b46",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1743599c-0d3f-4b10-a095-02c36218c679",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    218      169     77.52\n",
      "1          M    337      255     75.67\n",
      "2          X    286      188     65.73\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b8546267-aa7a-4e4b-b60e-810f836ebf8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613636d-22ae-4629-ac86-daf482925194",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
