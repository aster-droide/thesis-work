{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17da2043-9a5b-40fe-b3cb-f755b9a98deb",
   "metadata": {},
   "source": [
    "# VGGIsh Multi Seed Validation\n",
    "#### 5 Random (but reproducible) seeds are selected for 5 different runs of train/test\n",
    "#### Models have been optimised and verified on validation sets thoroughly, running a final train/test evaluation here\n",
    "#### The setup:\n",
    "\n",
    "- 4 fold StratifiedGroupKFold for stratification and ensuring each cat_id group only appears in one set at a time\n",
    "- Final scores averaged over the 4 folds\n",
    "- For each seed run we will explore the cat_id predictions through majority voting\n",
    "- For each run we will explore the potential impact of gender\n",
    "\n",
    "The dataset is highly unbalanced, resources for an unbiased estimate have been implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac1e09ae-387c-4347-b6f5-8d09dd1bf3f2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, concatenate\n",
    "from tensorflow.keras.optimizers import Adam, Adamax, SGD, RMSprop, AdamW\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GroupKFold, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from keras.regularizers import l2\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from focal_loss import SparseCategoricalFocalLoss\n",
    "import shap\n",
    "from keras.regularizers import l1, l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d005cfd2-3eda-44c4-bbb5-af343e979bc2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seeds: [7270  860 5390 5191 5734]\n"
     ]
    }
   ],
   "source": [
    "# Set an initial seed for reproducibility\n",
    "np.random.seed(42)  \n",
    "\n",
    "# Generate a list of 5 random seeds\n",
    "random_seeds = np.random.randint(0, 10000, size=5)\n",
    "print(\"Random Seeds:\", random_seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae122a-9f9c-47a3-8835-2d46d2f8e2f9",
   "metadata": {},
   "source": [
    "## Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5097e68-5153-440c-9294-dfa9e6061652",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def check_initial_group_split(groups_train, groups_test):\n",
    "    \"\"\"\n",
    "    Check if any group is present in both the train and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    - groups_train: Array of group identifiers for the train set\n",
    "    - groups_test: Array of group identifiers for the test set\n",
    "\n",
    "    Returns:\n",
    "    - Prints out any groups found in both sets and the count of such groups\n",
    "    \"\"\"\n",
    "    train_groups = set(groups_train)\n",
    "    test_groups = set(groups_test)\n",
    "    common_groups = train_groups.intersection(test_groups)\n",
    "\n",
    "    if common_groups:\n",
    "        print(f\"Warning: Found {len(common_groups)} common groups in both train/validation and test sets: {common_groups}\")\n",
    "    else:\n",
    "        print(\"No common groups found between train and test sets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "88dfe2de-bb1e-4d49-9260-e056c39627bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to perform the swaps based on cat_id, ensuring swaps within the same age_group\n",
    "def swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids):\n",
    "    for cat_id in specific_cat_ids:\n",
    "        # Check if the specific cat_id is not in the training set\n",
    "        if cat_id not in dataframe.iloc[train_val_idx]['cat_id'].values:\n",
    "            # Get the age_group of this cat_id\n",
    "            age_group = dataframe[dataframe['cat_id'] == cat_id]['age_group'].iloc[0]\n",
    "                \n",
    "            # Find a different cat_id within the same age_group in the train set that is not in the test set\n",
    "            other_cat_ids_in_age_group = dataframe[(dataframe['age_group'] == age_group) & \n",
    "                                                   (dataframe['cat_id'] != cat_id) &\n",
    "                                                   (~dataframe['cat_id'].isin(dataframe.iloc[test_idx]['cat_id']))]['cat_id'].unique()\n",
    "            \n",
    "            # Choose one other cat_id for swapping\n",
    "            if len(other_cat_ids_in_age_group) > 0:\n",
    "                other_cat_id = np.random.choice(other_cat_ids_in_age_group)\n",
    "\n",
    "                # Find all instances of the other_cat_id in the train set\n",
    "                other_cat_id_train_val_indices = train_val_idx[dataframe.iloc[train_val_idx]['cat_id'] == other_cat_id]\n",
    "                \n",
    "                # Find all instances of the specific cat_id in the test set\n",
    "                cat_id_test_indices = test_idx[dataframe.iloc[test_idx]['cat_id'] == cat_id]\n",
    "                \n",
    "                # Swap the indices\n",
    "                train_val_idx = np.setdiff1d(train_val_idx, other_cat_id_train_val_indices, assume_unique=True)\n",
    "                test_idx = np.setdiff1d(test_idx, cat_id_test_indices, assume_unique=True)\n",
    "\n",
    "                train_val_idx = np.concatenate((train_val_idx, cat_id_test_indices))\n",
    "                test_idx = np.concatenate((test_idx, other_cat_id_train_val_indices))\n",
    "            else:\n",
    "                print(f\"No alternative cat_id found in the same age_group as {cat_id} for swapping.\")\n",
    "                \n",
    "    return train_val_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2e6c2d87-cfab-493e-bca8-b3b8976a2bda",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Function to identify differences in groups\n",
    "def find_group_differences(original, new):\n",
    "    # Convert numpy arrays to sets for easy difference computation\n",
    "    original_set = set(original)\n",
    "    new_set = set(new)\n",
    "    # Find differences\n",
    "    moved_to_new = new_set - original_set\n",
    "    moved_to_original = original_set - new_set\n",
    "    return moved_to_new, moved_to_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6164b1c3-75dd-4549-aafd-cb6106297941",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# create custom logger function for local logs & stored in a .txt\n",
    "def logger(message, file=None):\n",
    "    print(message)\n",
    "    if file is not None:\n",
    "        with open(file, \"a\") as log_file:\n",
    "            log_file.write(message + \"\\n\")\n",
    "\n",
    "log_file_path = \"multi-seed-val-D13.txt\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "899fb535-0e25-4c7b-b6a5-13231e26c502",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set the aesthetic style of the plots\n",
    "sns.set(style=\"whitegrid\", palette=\"deep\")\n",
    "\n",
    "# Define a custom color palette\n",
    "colors = [\"#6aabd1\", \"#b6e2d3\", \"#dac292\"] \n",
    "sns.set_palette(sns.color_palette(colors))\n",
    "\n",
    "# Function to create bar plots with enhanced style\n",
    "def styled_barplot(data, x, y, title, xlabel, ylabel):\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    bar_plot = sns.barplot(x=x, y=y, data=data, errorbar=None, width=0.5)  \n",
    "    plt.title(title, fontsize=16, fontweight='bold', color=\"#333333\")\n",
    "    plt.xlabel(xlabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.ylabel(ylabel, fontsize=14, fontweight='bold', color=\"#333333\")\n",
    "    plt.xticks(fontsize=12, color=\"#333333\")\n",
    "    plt.yticks(fontsize=12, color=\"#333333\")\n",
    "    plt.ylim(0, 100) \n",
    "\n",
    "    # Adding value labels on top of each bar\n",
    "    for p in bar_plot.patches:\n",
    "        height = p.get_height()\n",
    "        # Annotate the height value on the bar\n",
    "        bar_plot.annotate(f'{height:.1f}', \n",
    "                          (p.get_x() + p.get_width() / 2., height), \n",
    "                          ha='center', va='center', \n",
    "                          xytext=(0, 9), \n",
    "                          textcoords='offset points', fontsize=12, color=\"#333333\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936a24a-13bd-4bc3-be13-0b210a09b5da",
   "metadata": {},
   "source": [
    "# RANDOM SEED 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70136a37-0507-4c2e-8307-c2dd905432e8",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14bb802b-f4bd-4464-a5fd-1977438428b1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[0])) \n",
    "np.random.seed(int(random_seeds[0]))\n",
    "tf.random.set_seed(int(random_seeds[0]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_1.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ce97c28-4210-4150-a60b-acdbf0e7716c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c26f17c3-4d21-4537-a090-9ca00f2be51a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f91a9c3-3474-4e83-8a5b-c5eb4a9a9223",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f4605c0b-f99d-48c3-8c2a-d87dd22c8946",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "        ..\n",
      "066A     1\n",
      "096A     1\n",
      "026C     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 87, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "055A    20\n",
      "001A    14\n",
      "025A    11\n",
      "016A    10\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "117A     7\n",
      "008A     6\n",
      "053A     6\n",
      "104A     4\n",
      "009A     4\n",
      "060A     3\n",
      "056A     3\n",
      "058A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "073A     1\n",
      "076A     1\n",
      "092A     1\n",
      "091A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    325\n",
      "M    253\n",
      "F    197\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    84\n",
      "F    55\n",
      "X    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [097A, 057A, 106A, 059A, 113A, 116A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 001A, 103A, 091A, 009A, 025A, 069A, 032...\n",
      "kitten                                         [045A, 110A]\n",
      "senior    [093A, 104A, 055A, 117A, 056A, 058A, 016A, 094...\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 14, 'senior': 13}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 2, 'senior': 9}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '033A' '034A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '044A' '046A' '047A' '048A' '049A' '050A' '051A'\n",
      " '051B' '052A' '054A' '057A' '059A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '068A' '070A' '071A' '072A' '074A' '075A' '087A' '088A'\n",
      " '090A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A' '105A'\n",
      " '106A' '108A' '109A' '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '009A' '015A' '016A' '024A' '025A' '032A' '045A' '053A'\n",
      " '055A' '056A' '058A' '060A' '069A' '073A' '076A' '091A' '092A' '093A'\n",
      " '094A' '103A' '104A' '110A' '117A']\n",
      "Length of X_train_val:\n",
      "775\n",
      "Length of y_train_val:\n",
      "775\n",
      "Length of groups_train_val:\n",
      "775\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     494\n",
      "kitten    161\n",
      "senior    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     94\n",
      "senior    58\n",
      "kitten    10\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 988, 1: 805, 2: 600})\n",
      "Epoch 1/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.9117 - accuracy: 0.6272\n",
      "Epoch 2/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6948 - accuracy: 0.7133\n",
      "Epoch 3/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6407 - accuracy: 0.7326\n",
      "Epoch 4/1500\n",
      "75/75 [==============================] - 0s 928us/step - loss: 0.5834 - accuracy: 0.7601\n",
      "Epoch 5/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5337 - accuracy: 0.7802\n",
      "Epoch 6/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5237 - accuracy: 0.7835\n",
      "Epoch 7/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4896 - accuracy: 0.7919\n",
      "Epoch 8/1500\n",
      "75/75 [==============================] - 0s 992us/step - loss: 0.4908 - accuracy: 0.8011\n",
      "Epoch 9/1500\n",
      "75/75 [==============================] - 0s 948us/step - loss: 0.4678 - accuracy: 0.7973\n",
      "Epoch 10/1500\n",
      "75/75 [==============================] - 0s 923us/step - loss: 0.4419 - accuracy: 0.8262\n",
      "Epoch 11/1500\n",
      "75/75 [==============================] - 0s 933us/step - loss: 0.4221 - accuracy: 0.8262\n",
      "Epoch 12/1500\n",
      "75/75 [==============================] - 0s 948us/step - loss: 0.4402 - accuracy: 0.8237\n",
      "Epoch 13/1500\n",
      "75/75 [==============================] - 0s 985us/step - loss: 0.4092 - accuracy: 0.8374\n",
      "Epoch 14/1500\n",
      "75/75 [==============================] - 0s 968us/step - loss: 0.3868 - accuracy: 0.8487\n",
      "Epoch 15/1500\n",
      "75/75 [==============================] - 0s 960us/step - loss: 0.3841 - accuracy: 0.8437\n",
      "Epoch 16/1500\n",
      "75/75 [==============================] - 0s 998us/step - loss: 0.4001 - accuracy: 0.8387\n",
      "Epoch 17/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3649 - accuracy: 0.8491\n",
      "Epoch 18/1500\n",
      "75/75 [==============================] - 0s 958us/step - loss: 0.3727 - accuracy: 0.8395\n",
      "Epoch 19/1500\n",
      "75/75 [==============================] - 0s 958us/step - loss: 0.3342 - accuracy: 0.8684\n",
      "Epoch 20/1500\n",
      "75/75 [==============================] - 0s 950us/step - loss: 0.3362 - accuracy: 0.8663\n",
      "Epoch 21/1500\n",
      "75/75 [==============================] - 0s 905us/step - loss: 0.3504 - accuracy: 0.8567\n",
      "Epoch 22/1500\n",
      "75/75 [==============================] - 0s 903us/step - loss: 0.3545 - accuracy: 0.8558\n",
      "Epoch 23/1500\n",
      "75/75 [==============================] - 0s 925us/step - loss: 0.3294 - accuracy: 0.8663\n",
      "Epoch 24/1500\n",
      "75/75 [==============================] - 0s 957us/step - loss: 0.3152 - accuracy: 0.8792\n",
      "Epoch 25/1500\n",
      "75/75 [==============================] - 0s 959us/step - loss: 0.3044 - accuracy: 0.8796\n",
      "Epoch 26/1500\n",
      "75/75 [==============================] - 0s 960us/step - loss: 0.2992 - accuracy: 0.8796\n",
      "Epoch 27/1500\n",
      "75/75 [==============================] - 0s 933us/step - loss: 0.2915 - accuracy: 0.8817\n",
      "Epoch 28/1500\n",
      "75/75 [==============================] - 0s 935us/step - loss: 0.3078 - accuracy: 0.8742\n",
      "Epoch 29/1500\n",
      "75/75 [==============================] - 0s 922us/step - loss: 0.2911 - accuracy: 0.8838\n",
      "Epoch 30/1500\n",
      "75/75 [==============================] - 0s 925us/step - loss: 0.2900 - accuracy: 0.8851\n",
      "Epoch 31/1500\n",
      "75/75 [==============================] - 0s 923us/step - loss: 0.2834 - accuracy: 0.8872\n",
      "Epoch 32/1500\n",
      "75/75 [==============================] - 0s 920us/step - loss: 0.2876 - accuracy: 0.8913\n",
      "Epoch 33/1500\n",
      "75/75 [==============================] - 0s 915us/step - loss: 0.2879 - accuracy: 0.8813\n",
      "Epoch 34/1500\n",
      "75/75 [==============================] - 0s 949us/step - loss: 0.2734 - accuracy: 0.8930\n",
      "Epoch 35/1500\n",
      "75/75 [==============================] - 0s 991us/step - loss: 0.2698 - accuracy: 0.8905\n",
      "Epoch 36/1500\n",
      "75/75 [==============================] - 0s 992us/step - loss: 0.2748 - accuracy: 0.8884\n",
      "Epoch 37/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2750 - accuracy: 0.8888\n",
      "Epoch 38/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2873 - accuracy: 0.8834\n",
      "Epoch 39/1500\n",
      "75/75 [==============================] - 0s 990us/step - loss: 0.2529 - accuracy: 0.8985\n",
      "Epoch 40/1500\n",
      "75/75 [==============================] - 0s 992us/step - loss: 0.2549 - accuracy: 0.9014\n",
      "Epoch 41/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2373 - accuracy: 0.9110\n",
      "Epoch 42/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2509 - accuracy: 0.9010\n",
      "Epoch 43/1500\n",
      "75/75 [==============================] - 0s 997us/step - loss: 0.2556 - accuracy: 0.8930\n",
      "Epoch 44/1500\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2411 - accuracy: 0.8997\n",
      "Epoch 45/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2406 - accuracy: 0.9106\n",
      "Epoch 46/1500\n",
      "75/75 [==============================] - 0s 964us/step - loss: 0.2335 - accuracy: 0.9110\n",
      "Epoch 47/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2278 - accuracy: 0.9131\n",
      "Epoch 48/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2272 - accuracy: 0.9110\n",
      "Epoch 49/1500\n",
      "75/75 [==============================] - 0s 962us/step - loss: 0.2421 - accuracy: 0.9068\n",
      "Epoch 50/1500\n",
      "75/75 [==============================] - 0s 994us/step - loss: 0.2216 - accuracy: 0.9060\n",
      "Epoch 51/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2220 - accuracy: 0.9127\n",
      "Epoch 52/1500\n",
      "75/75 [==============================] - 0s 925us/step - loss: 0.2198 - accuracy: 0.9164\n",
      "Epoch 53/1500\n",
      "75/75 [==============================] - 0s 939us/step - loss: 0.2207 - accuracy: 0.9173\n",
      "Epoch 54/1500\n",
      "75/75 [==============================] - 0s 993us/step - loss: 0.2380 - accuracy: 0.9081\n",
      "Epoch 55/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2251 - accuracy: 0.9127\n",
      "Epoch 56/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2145 - accuracy: 0.9173\n",
      "Epoch 57/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2111 - accuracy: 0.9164\n",
      "Epoch 58/1500\n",
      "75/75 [==============================] - 0s 964us/step - loss: 0.2079 - accuracy: 0.9206\n",
      "Epoch 59/1500\n",
      "75/75 [==============================] - 0s 942us/step - loss: 0.2121 - accuracy: 0.9148\n",
      "Epoch 60/1500\n",
      "75/75 [==============================] - 0s 950us/step - loss: 0.2267 - accuracy: 0.9131\n",
      "Epoch 61/1500\n",
      "75/75 [==============================] - 0s 923us/step - loss: 0.1901 - accuracy: 0.9231\n",
      "Epoch 62/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2117 - accuracy: 0.9206\n",
      "Epoch 63/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2061 - accuracy: 0.9152\n",
      "Epoch 64/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1923 - accuracy: 0.9298\n",
      "Epoch 65/1500\n",
      "75/75 [==============================] - 0s 986us/step - loss: 0.1941 - accuracy: 0.9302\n",
      "Epoch 66/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2078 - accuracy: 0.9156\n",
      "Epoch 67/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9344\n",
      "Epoch 68/1500\n",
      "75/75 [==============================] - 0s 994us/step - loss: 0.2000 - accuracy: 0.9231\n",
      "Epoch 69/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9256\n",
      "Epoch 70/1500\n",
      "75/75 [==============================] - 0s 989us/step - loss: 0.1686 - accuracy: 0.9365\n",
      "Epoch 71/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1945 - accuracy: 0.9294\n",
      "Epoch 72/1500\n",
      "75/75 [==============================] - 0s 946us/step - loss: 0.1794 - accuracy: 0.9331\n",
      "Epoch 73/1500\n",
      "75/75 [==============================] - 0s 959us/step - loss: 0.1883 - accuracy: 0.9298\n",
      "Epoch 74/1500\n",
      "75/75 [==============================] - 0s 910us/step - loss: 0.1895 - accuracy: 0.9244\n",
      "Epoch 75/1500\n",
      "75/75 [==============================] - 0s 962us/step - loss: 0.2090 - accuracy: 0.9244\n",
      "Epoch 76/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9277\n",
      "Epoch 77/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.9252\n",
      "Epoch 78/1500\n",
      "75/75 [==============================] - 0s 918us/step - loss: 0.1756 - accuracy: 0.9319\n",
      "Epoch 79/1500\n",
      "75/75 [==============================] - 0s 972us/step - loss: 0.1828 - accuracy: 0.9315\n",
      "Epoch 80/1500\n",
      "75/75 [==============================] - 0s 990us/step - loss: 0.1794 - accuracy: 0.9315\n",
      "Epoch 81/1500\n",
      "75/75 [==============================] - 0s 980us/step - loss: 0.1746 - accuracy: 0.9365\n",
      "Epoch 82/1500\n",
      "75/75 [==============================] - 0s 895us/step - loss: 0.1761 - accuracy: 0.9323\n",
      "Epoch 83/1500\n",
      "75/75 [==============================] - 0s 940us/step - loss: 0.1642 - accuracy: 0.9386\n",
      "Epoch 84/1500\n",
      "75/75 [==============================] - 0s 985us/step - loss: 0.1770 - accuracy: 0.9323\n",
      "Epoch 85/1500\n",
      "75/75 [==============================] - 0s 959us/step - loss: 0.1729 - accuracy: 0.9361\n",
      "Epoch 86/1500\n",
      "75/75 [==============================] - 0s 908us/step - loss: 0.1689 - accuracy: 0.9331\n",
      "Epoch 87/1500\n",
      "75/75 [==============================] - 0s 915us/step - loss: 0.1941 - accuracy: 0.9327\n",
      "Epoch 88/1500\n",
      "75/75 [==============================] - 0s 958us/step - loss: 0.1566 - accuracy: 0.9490\n",
      "Epoch 89/1500\n",
      "75/75 [==============================] - 0s 960us/step - loss: 0.1613 - accuracy: 0.9369\n",
      "Epoch 90/1500\n",
      "75/75 [==============================] - 0s 974us/step - loss: 0.1579 - accuracy: 0.9440\n",
      "Epoch 91/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9444\n",
      "Epoch 92/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9356\n",
      "Epoch 93/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1823 - accuracy: 0.9310\n",
      "Epoch 94/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1676 - accuracy: 0.9340\n",
      "Epoch 95/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9507\n",
      "Epoch 96/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9440\n",
      "Epoch 97/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9461\n",
      "Epoch 98/1500\n",
      "75/75 [==============================] - 0s 971us/step - loss: 0.1519 - accuracy: 0.9361\n",
      "Epoch 99/1500\n",
      "75/75 [==============================] - 0s 926us/step - loss: 0.1691 - accuracy: 0.9373\n",
      "Epoch 100/1500\n",
      "75/75 [==============================] - 0s 945us/step - loss: 0.1682 - accuracy: 0.9407\n",
      "Epoch 101/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1569 - accuracy: 0.9457\n",
      "Epoch 102/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1549 - accuracy: 0.9453\n",
      "Epoch 103/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9436\n",
      "Epoch 104/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9490\n",
      "Epoch 105/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.9507\n",
      "Epoch 106/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9432\n",
      "Epoch 107/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9461\n",
      "Epoch 108/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9478\n",
      "Epoch 109/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9432\n",
      "Epoch 110/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9469\n",
      "Epoch 111/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9511\n",
      "Epoch 112/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9524\n",
      "Epoch 113/1500\n",
      "75/75 [==============================] - 0s 949us/step - loss: 0.1619 - accuracy: 0.9436\n",
      "Epoch 114/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9545\n",
      "Epoch 115/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9570\n",
      "Epoch 116/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9545\n",
      "Epoch 117/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9423\n",
      "Epoch 118/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9394\n",
      "Epoch 119/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9461\n",
      "Epoch 120/1500\n",
      "75/75 [==============================] - 0s 938us/step - loss: 0.1503 - accuracy: 0.9423\n",
      "Epoch 121/1500\n",
      "75/75 [==============================] - 0s 994us/step - loss: 0.1513 - accuracy: 0.9453\n",
      "Epoch 122/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.9448\n",
      "Epoch 123/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1450 - accuracy: 0.9448\n",
      "Epoch 124/1500\n",
      "75/75 [==============================] - 0s 927us/step - loss: 0.1348 - accuracy: 0.9515\n",
      "Epoch 125/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9574\n",
      "Epoch 126/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9624\n",
      "Epoch 127/1500\n",
      "75/75 [==============================] - 0s 976us/step - loss: 0.1170 - accuracy: 0.9578\n",
      "Epoch 128/1500\n",
      "75/75 [==============================] - 0s 936us/step - loss: 0.1365 - accuracy: 0.9519\n",
      "Epoch 129/1500\n",
      "75/75 [==============================] - 0s 989us/step - loss: 0.1301 - accuracy: 0.9465\n",
      "Epoch 130/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9457\n",
      "Epoch 131/1500\n",
      "75/75 [==============================] - 0s 985us/step - loss: 0.1418 - accuracy: 0.9494\n",
      "Epoch 132/1500\n",
      "75/75 [==============================] - 0s 944us/step - loss: 0.1289 - accuracy: 0.9536\n",
      "Epoch 133/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.9461\n",
      "Epoch 134/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9469\n",
      "Epoch 135/1500\n",
      "75/75 [==============================] - 0s 920us/step - loss: 0.1578 - accuracy: 0.9432\n",
      "Epoch 136/1500\n",
      "75/75 [==============================] - 0s 983us/step - loss: 0.1383 - accuracy: 0.9490\n",
      "Epoch 137/1500\n",
      "75/75 [==============================] - 0s 994us/step - loss: 0.1318 - accuracy: 0.9578\n",
      "Epoch 138/1500\n",
      "75/75 [==============================] - 0s 962us/step - loss: 0.1390 - accuracy: 0.9503\n",
      "Epoch 139/1500\n",
      "75/75 [==============================] - 0s 905us/step - loss: 0.1458 - accuracy: 0.9461\n",
      "Epoch 140/1500\n",
      "75/75 [==============================] - 0s 966us/step - loss: 0.1240 - accuracy: 0.9561\n",
      "Epoch 141/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9532\n",
      "Epoch 142/1500\n",
      "75/75 [==============================] - 0s 942us/step - loss: 0.1353 - accuracy: 0.9503\n",
      "Epoch 143/1500\n",
      "75/75 [==============================] - 0s 927us/step - loss: 0.1341 - accuracy: 0.9494\n",
      "Epoch 144/1500\n",
      "75/75 [==============================] - 0s 972us/step - loss: 0.1311 - accuracy: 0.9536\n",
      "Epoch 145/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9536\n",
      "Epoch 146/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9586\n",
      "Epoch 147/1500\n",
      "75/75 [==============================] - 0s 965us/step - loss: 0.1396 - accuracy: 0.9494\n",
      "Epoch 148/1500\n",
      "75/75 [==============================] - 0s 951us/step - loss: 0.1436 - accuracy: 0.9478\n",
      "Epoch 149/1500\n",
      "75/75 [==============================] - 0s 943us/step - loss: 0.1222 - accuracy: 0.9578\n",
      "Epoch 150/1500\n",
      "75/75 [==============================] - 0s 932us/step - loss: 0.1325 - accuracy: 0.9532\n",
      "Epoch 151/1500\n",
      "75/75 [==============================] - 0s 900us/step - loss: 0.1312 - accuracy: 0.9570\n",
      "Epoch 152/1500\n",
      "75/75 [==============================] - 0s 898us/step - loss: 0.1257 - accuracy: 0.9607\n",
      "Epoch 153/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9590\n",
      "Epoch 154/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9653\n",
      "Epoch 155/1500\n",
      "75/75 [==============================] - 0s 975us/step - loss: 0.1148 - accuracy: 0.9578\n",
      "Epoch 156/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9557\n",
      "Epoch 157/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1207 - accuracy: 0.9561\n",
      "Epoch 158/1500\n",
      "75/75 [==============================] - 0s 990us/step - loss: 0.1222 - accuracy: 0.9565\n",
      "Epoch 159/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9611\n",
      "Epoch 160/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9465\n",
      "Epoch 161/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1058 - accuracy: 0.9624\n",
      "Epoch 162/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9586\n",
      "Epoch 163/1500\n",
      "75/75 [==============================] - 0s 996us/step - loss: 0.1275 - accuracy: 0.9519\n",
      "Epoch 164/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9561\n",
      "Epoch 165/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9557\n",
      "Epoch 166/1500\n",
      "75/75 [==============================] - 0s 906us/step - loss: 0.1168 - accuracy: 0.9574\n",
      "Epoch 167/1500\n",
      "75/75 [==============================] - 0s 992us/step - loss: 0.1275 - accuracy: 0.9590\n",
      "Epoch 168/1500\n",
      "75/75 [==============================] - 0s 982us/step - loss: 0.1242 - accuracy: 0.9582\n",
      "Epoch 169/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9511\n",
      "Epoch 170/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9687\n",
      "Epoch 171/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9636\n",
      "Epoch 172/1500\n",
      "75/75 [==============================] - 0s 996us/step - loss: 0.1142 - accuracy: 0.9603\n",
      "Epoch 173/1500\n",
      "75/75 [==============================] - 0s 991us/step - loss: 0.1120 - accuracy: 0.9570\n",
      "Epoch 174/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9582\n",
      "Epoch 175/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9586\n",
      "Epoch 176/1500\n",
      "75/75 [==============================] - 0s 989us/step - loss: 0.1199 - accuracy: 0.9582\n",
      "Epoch 177/1500\n",
      "75/75 [==============================] - 0s 991us/step - loss: 0.1212 - accuracy: 0.9553\n",
      "Epoch 178/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9557\n",
      "Epoch 179/1500\n",
      "75/75 [==============================] - 0s 960us/step - loss: 0.1107 - accuracy: 0.9578\n",
      "Epoch 180/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9545\n",
      "Epoch 181/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9616\n",
      "Epoch 182/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9620\n",
      "Epoch 183/1500\n",
      "75/75 [==============================] - 0s 922us/step - loss: 0.1044 - accuracy: 0.9611\n",
      "Epoch 184/1500\n",
      "75/75 [==============================] - 0s 952us/step - loss: 0.1155 - accuracy: 0.9586\n",
      "Epoch 185/1500\n",
      "75/75 [==============================] - 0s 993us/step - loss: 0.1168 - accuracy: 0.9582\n",
      "Epoch 186/1500\n",
      "75/75 [==============================] - 0s 975us/step - loss: 0.1032 - accuracy: 0.9645\n",
      "Epoch 187/1500\n",
      "75/75 [==============================] - 0s 972us/step - loss: 0.1158 - accuracy: 0.9574\n",
      "Epoch 188/1500\n",
      "75/75 [==============================] - 0s 945us/step - loss: 0.1241 - accuracy: 0.9549\n",
      "Epoch 189/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9553\n",
      "Epoch 190/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9620\n",
      "Epoch 191/1500\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.1157 - accuracy: 0.9624\n",
      "Epoch 192/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9553\n",
      "Epoch 193/1500\n",
      "75/75 [==============================] - 0s 993us/step - loss: 0.1009 - accuracy: 0.9674\n",
      "Epoch 194/1500\n",
      "75/75 [==============================] - 0s 975us/step - loss: 0.1053 - accuracy: 0.9599\n",
      "Epoch 195/1500\n",
      "75/75 [==============================] - 0s 961us/step - loss: 0.1203 - accuracy: 0.9590\n",
      "Epoch 196/1500\n",
      "75/75 [==============================] - 0s 966us/step - loss: 0.0986 - accuracy: 0.9674\n",
      "Epoch 197/1500\n",
      "75/75 [==============================] - 0s 953us/step - loss: 0.0992 - accuracy: 0.9674\n",
      "Epoch 198/1500\n",
      "75/75 [==============================] - 0s 944us/step - loss: 0.0994 - accuracy: 0.9657\n",
      "Epoch 199/1500\n",
      "75/75 [==============================] - 0s 936us/step - loss: 0.1136 - accuracy: 0.9574\n",
      "Epoch 200/1500\n",
      "75/75 [==============================] - 0s 928us/step - loss: 0.0910 - accuracy: 0.9716\n",
      "Epoch 201/1500\n",
      "75/75 [==============================] - 0s 946us/step - loss: 0.1116 - accuracy: 0.9557\n",
      "Epoch 202/1500\n",
      "75/75 [==============================] - 0s 920us/step - loss: 0.1011 - accuracy: 0.9670\n",
      "Epoch 203/1500\n",
      "75/75 [==============================] - 0s 916us/step - loss: 0.1168 - accuracy: 0.9603\n",
      "Epoch 204/1500\n",
      "75/75 [==============================] - 0s 946us/step - loss: 0.1129 - accuracy: 0.9574\n",
      "Epoch 205/1500\n",
      "75/75 [==============================] - 0s 949us/step - loss: 0.0819 - accuracy: 0.9691\n",
      "Epoch 206/1500\n",
      "75/75 [==============================] - 0s 934us/step - loss: 0.0940 - accuracy: 0.9662\n",
      "Epoch 207/1500\n",
      "75/75 [==============================] - 0s 922us/step - loss: 0.0961 - accuracy: 0.9645\n",
      "Epoch 208/1500\n",
      "75/75 [==============================] - 0s 953us/step - loss: 0.1035 - accuracy: 0.9649\n",
      "Epoch 209/1500\n",
      "75/75 [==============================] - 0s 973us/step - loss: 0.1279 - accuracy: 0.9499\n",
      "Epoch 210/1500\n",
      "75/75 [==============================] - 0s 968us/step - loss: 0.0997 - accuracy: 0.9657\n",
      "Epoch 211/1500\n",
      "75/75 [==============================] - 0s 957us/step - loss: 0.1074 - accuracy: 0.9599\n",
      "Epoch 212/1500\n",
      "75/75 [==============================] - 0s 970us/step - loss: 0.1030 - accuracy: 0.9624\n",
      "Epoch 213/1500\n",
      "75/75 [==============================] - 0s 969us/step - loss: 0.0958 - accuracy: 0.9682\n",
      "Epoch 214/1500\n",
      "75/75 [==============================] - 0s 983us/step - loss: 0.1047 - accuracy: 0.9641\n",
      "Epoch 215/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.9599\n",
      "Epoch 216/1500\n",
      "75/75 [==============================] - 0s 964us/step - loss: 0.0935 - accuracy: 0.9632\n",
      "Epoch 217/1500\n",
      "75/75 [==============================] - 0s 924us/step - loss: 0.0911 - accuracy: 0.9662\n",
      "Epoch 218/1500\n",
      "75/75 [==============================] - 0s 940us/step - loss: 0.0944 - accuracy: 0.9687\n",
      "Epoch 219/1500\n",
      "75/75 [==============================] - 0s 985us/step - loss: 0.0957 - accuracy: 0.9653\n",
      "Epoch 220/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.9682\n",
      "Epoch 221/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9616\n",
      "Epoch 222/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9611\n",
      "Epoch 223/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9703\n",
      "Epoch 224/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0977 - accuracy: 0.9657\n",
      "Epoch 225/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9666\n",
      "Epoch 226/1500\n",
      "75/75 [==============================] - 0s 981us/step - loss: 0.0950 - accuracy: 0.9649\n",
      "Epoch 227/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9657\n",
      "Epoch 228/1500\n",
      "75/75 [==============================] - 0s 990us/step - loss: 0.0980 - accuracy: 0.9716\n",
      "Epoch 229/1500\n",
      "75/75 [==============================] - 0s 962us/step - loss: 0.0967 - accuracy: 0.9653\n",
      "Epoch 230/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9620\n",
      "Epoch 231/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.9695\n",
      "Epoch 232/1500\n",
      "75/75 [==============================] - 0s 956us/step - loss: 0.0975 - accuracy: 0.9674\n",
      "Epoch 233/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9682\n",
      "Epoch 234/1500\n",
      "75/75 [==============================] - 0s 946us/step - loss: 0.1112 - accuracy: 0.9607\n",
      "Epoch 235/1500\n",
      "54/75 [====================>.........] - ETA: 0s - loss: 0.1007 - accuracy: 0.9670Restoring model weights from the end of the best epoch: 205.\n",
      "75/75 [==============================] - 0s 997us/step - loss: 0.0970 - accuracy: 0.9687\n",
      "Epoch 235: early stopping\n",
      "6/6 [==============================] - 0s 880us/step - loss: 1.2763 - accuracy: 0.6543\n",
      "6/6 [==============================] - 0s 577us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (18/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "Final Test Results - Loss: 1.2762565612792969, Accuracy: 0.654321014881134, Precision: 0.6574481074481074, Recall: 0.7232086084617265, F1 Score: 0.6844559359125589\n",
      "Confusion Matrix:\n",
      " [[61  4 29]\n",
      " [ 1  9  0]\n",
      " [22  0 36]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "        ..\n",
      "041A     1\n",
      "092A     1\n",
      "076A     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, Length: 83, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "057A    27\n",
      "000B    19\n",
      "029A    17\n",
      "106A    14\n",
      "028A    13\n",
      "039A    12\n",
      "036A    11\n",
      "068A    11\n",
      "071A    10\n",
      "005A    10\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "037A     6\n",
      "044A     5\n",
      "070A     5\n",
      "105A     4\n",
      "014A     3\n",
      "054A     2\n",
      "025B     2\n",
      "096A     1\n",
      "049A     1\n",
      "048A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    264\n",
      "X    198\n",
      "F    193\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    150\n",
      "M     73\n",
      "F     59\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 019...\n",
      "kitten    [014B, 111A, 040A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 116A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 028A, 022A, 029A, 095A, 005A, 039A, 013...\n",
      "kitten                             [044A, 046A, 049A, 048A]\n",
      "senior                             [057A, 106A, 051B, 054A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '047A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '046A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'047A'}\n",
      "Moved to Test Set:\n",
      "{'047A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A'\n",
      " '011A' '012A' '014B' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '023A' '023B' '024A' '025A' '025C' '026A' '026C' '027A' '031A' '032A'\n",
      " '033A' '034A' '035A' '038A' '040A' '041A' '042A' '043A' '045A' '046A'\n",
      " '050A' '051A' '052A' '053A' '055A' '056A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '069A' '072A' '073A' '074A'\n",
      " '075A' '076A' '087A' '090A' '091A' '092A' '093A' '094A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '108A' '109A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '005A' '010A' '013B' '014A' '022A' '025B' '026B' '028A' '029A'\n",
      " '036A' '037A' '039A' '044A' '047A' '048A' '049A' '051B' '054A' '057A'\n",
      " '068A' '070A' '071A' '088A' '095A' '096A' '100A' '105A' '106A']\n",
      "Length of X_train_val:\n",
      "690\n",
      "Length of y_train_val:\n",
      "690\n",
      "Length of groups_train_val:\n",
      "690\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    126\n",
      "kitten    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     70\n",
      "senior     52\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "kitten    136\n",
      "senior    126\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "senior     52\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 856, 1: 680, 2: 630})\n",
      "Epoch 1/1500\n",
      "68/68 [==============================] - 0s 981us/step - loss: 0.9449 - accuracy: 0.6066\n",
      "Epoch 2/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6981 - accuracy: 0.7133\n",
      "Epoch 3/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6087 - accuracy: 0.7604\n",
      "Epoch 4/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5885 - accuracy: 0.7669\n",
      "Epoch 5/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5454 - accuracy: 0.7821\n",
      "Epoch 6/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5087 - accuracy: 0.7964\n",
      "Epoch 7/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5104 - accuracy: 0.7895\n",
      "Epoch 8/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4543 - accuracy: 0.8250\n",
      "Epoch 9/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4402 - accuracy: 0.8218\n",
      "Epoch 10/1500\n",
      "68/68 [==============================] - 0s 992us/step - loss: 0.4260 - accuracy: 0.8324\n",
      "Epoch 11/1500\n",
      "68/68 [==============================] - 0s 988us/step - loss: 0.4091 - accuracy: 0.8366\n",
      "Epoch 12/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3930 - accuracy: 0.8416\n",
      "Epoch 13/1500\n",
      "68/68 [==============================] - 0s 939us/step - loss: 0.3757 - accuracy: 0.8495\n",
      "Epoch 14/1500\n",
      "68/68 [==============================] - 0s 978us/step - loss: 0.3724 - accuracy: 0.8550\n",
      "Epoch 15/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.8490\n",
      "Epoch 16/1500\n",
      "68/68 [==============================] - 0s 992us/step - loss: 0.3528 - accuracy: 0.8546\n",
      "Epoch 17/1500\n",
      "68/68 [==============================] - 0s 923us/step - loss: 0.3400 - accuracy: 0.8657\n",
      "Epoch 18/1500\n",
      "68/68 [==============================] - 0s 961us/step - loss: 0.3482 - accuracy: 0.8666\n",
      "Epoch 19/1500\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.3164 - accuracy: 0.8758\n",
      "Epoch 20/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.3266 - accuracy: 0.8712\n",
      "Epoch 21/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3228 - accuracy: 0.8689\n",
      "Epoch 22/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.3211 - accuracy: 0.8735\n",
      "Epoch 23/1500\n",
      "68/68 [==============================] - 0s 948us/step - loss: 0.2933 - accuracy: 0.8855\n",
      "Epoch 24/1500\n",
      "68/68 [==============================] - 0s 951us/step - loss: 0.3123 - accuracy: 0.8740\n",
      "Epoch 25/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2936 - accuracy: 0.8823\n",
      "Epoch 26/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2635 - accuracy: 0.8998\n",
      "Epoch 27/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2791 - accuracy: 0.8892\n",
      "Epoch 28/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2681 - accuracy: 0.8920\n",
      "Epoch 29/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2780 - accuracy: 0.8873\n",
      "Epoch 30/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2700 - accuracy: 0.8947\n",
      "Epoch 31/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2525 - accuracy: 0.9026\n",
      "Epoch 32/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2682 - accuracy: 0.8970\n",
      "Epoch 33/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2425 - accuracy: 0.9063\n",
      "Epoch 34/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.9063\n",
      "Epoch 35/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2569 - accuracy: 0.8994\n",
      "Epoch 36/1500\n",
      "68/68 [==============================] - 0s 986us/step - loss: 0.2244 - accuracy: 0.9183\n",
      "Epoch 37/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2480 - accuracy: 0.8989\n",
      "Epoch 38/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2364 - accuracy: 0.9100\n",
      "Epoch 39/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2328 - accuracy: 0.9160\n",
      "Epoch 40/1500\n",
      "68/68 [==============================] - 0s 979us/step - loss: 0.2092 - accuracy: 0.9197\n",
      "Epoch 41/1500\n",
      "68/68 [==============================] - 0s 933us/step - loss: 0.2163 - accuracy: 0.9155\n",
      "Epoch 42/1500\n",
      "68/68 [==============================] - 0s 936us/step - loss: 0.2230 - accuracy: 0.9137\n",
      "Epoch 43/1500\n",
      "68/68 [==============================] - 0s 900us/step - loss: 0.2278 - accuracy: 0.9211\n",
      "Epoch 44/1500\n",
      "68/68 [==============================] - 0s 937us/step - loss: 0.2194 - accuracy: 0.9155\n",
      "Epoch 45/1500\n",
      "68/68 [==============================] - 0s 951us/step - loss: 0.2224 - accuracy: 0.9058\n",
      "Epoch 46/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2057 - accuracy: 0.9201\n",
      "Epoch 47/1500\n",
      "68/68 [==============================] - 0s 959us/step - loss: 0.2309 - accuracy: 0.9151\n",
      "Epoch 48/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2131 - accuracy: 0.9137\n",
      "Epoch 49/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2080 - accuracy: 0.9132\n",
      "Epoch 50/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1913 - accuracy: 0.9303\n",
      "Epoch 51/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.9197\n",
      "Epoch 52/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.9280\n",
      "Epoch 53/1500\n",
      "68/68 [==============================] - 0s 994us/step - loss: 0.2029 - accuracy: 0.9224\n",
      "Epoch 54/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9280\n",
      "Epoch 55/1500\n",
      "68/68 [==============================] - 0s 930us/step - loss: 0.1882 - accuracy: 0.9340\n",
      "Epoch 56/1500\n",
      "68/68 [==============================] - 0s 925us/step - loss: 0.1922 - accuracy: 0.9266\n",
      "Epoch 57/1500\n",
      "68/68 [==============================] - 0s 929us/step - loss: 0.1639 - accuracy: 0.9446\n",
      "Epoch 58/1500\n",
      "68/68 [==============================] - 0s 944us/step - loss: 0.1782 - accuracy: 0.9418\n",
      "Epoch 59/1500\n",
      "68/68 [==============================] - 0s 961us/step - loss: 0.1811 - accuracy: 0.9317\n",
      "Epoch 60/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9284\n",
      "Epoch 61/1500\n",
      "68/68 [==============================] - 0s 953us/step - loss: 0.1706 - accuracy: 0.9367\n",
      "Epoch 62/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1706 - accuracy: 0.9344\n",
      "Epoch 63/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.9372\n",
      "Epoch 64/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.9266\n",
      "Epoch 65/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9386\n",
      "Epoch 66/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.9414\n",
      "Epoch 67/1500\n",
      "68/68 [==============================] - 0s 994us/step - loss: 0.1537 - accuracy: 0.9497\n",
      "Epoch 68/1500\n",
      "68/68 [==============================] - 0s 992us/step - loss: 0.1804 - accuracy: 0.9363\n",
      "Epoch 69/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9298\n",
      "Epoch 70/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9414\n",
      "Epoch 71/1500\n",
      "68/68 [==============================] - 0s 965us/step - loss: 0.1485 - accuracy: 0.9478\n",
      "Epoch 72/1500\n",
      "68/68 [==============================] - 0s 977us/step - loss: 0.1591 - accuracy: 0.9386\n",
      "Epoch 73/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9418\n",
      "Epoch 74/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1532 - accuracy: 0.9464\n",
      "Epoch 75/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9409\n",
      "Epoch 76/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9404\n",
      "Epoch 77/1500\n",
      "68/68 [==============================] - 0s 989us/step - loss: 0.1607 - accuracy: 0.9414\n",
      "Epoch 78/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1585 - accuracy: 0.9404\n",
      "Epoch 79/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1717 - accuracy: 0.9432\n",
      "Epoch 80/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1586 - accuracy: 0.9395\n",
      "Epoch 81/1500\n",
      "68/68 [==============================] - 0s 998us/step - loss: 0.1671 - accuracy: 0.9377\n",
      "Epoch 82/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9423\n",
      "Epoch 83/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9538\n",
      "Epoch 84/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1575 - accuracy: 0.9483\n",
      "Epoch 85/1500\n",
      "68/68 [==============================] - 0s 982us/step - loss: 0.1569 - accuracy: 0.9418\n",
      "Epoch 86/1500\n",
      "68/68 [==============================] - 0s 947us/step - loss: 0.1557 - accuracy: 0.9464\n",
      "Epoch 87/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9543\n",
      "Epoch 88/1500\n",
      "68/68 [==============================] - 0s 925us/step - loss: 0.1687 - accuracy: 0.9358\n",
      "Epoch 89/1500\n",
      "68/68 [==============================] - 0s 984us/step - loss: 0.1537 - accuracy: 0.9414\n",
      "Epoch 90/1500\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.1367 - accuracy: 0.9529\n",
      "Epoch 91/1500\n",
      "68/68 [==============================] - 0s 957us/step - loss: 0.1452 - accuracy: 0.9492\n",
      "Epoch 92/1500\n",
      "68/68 [==============================] - 0s 916us/step - loss: 0.1328 - accuracy: 0.9543\n",
      "Epoch 93/1500\n",
      "68/68 [==============================] - 0s 929us/step - loss: 0.1363 - accuracy: 0.9511\n",
      "Epoch 94/1500\n",
      "68/68 [==============================] - 0s 932us/step - loss: 0.1382 - accuracy: 0.9464\n",
      "Epoch 95/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9437\n",
      "Epoch 96/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.9455\n",
      "Epoch 97/1500\n",
      "68/68 [==============================] - 0s 949us/step - loss: 0.1379 - accuracy: 0.9497\n",
      "Epoch 98/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9552\n",
      "Epoch 99/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9575\n",
      "Epoch 100/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9543\n",
      "Epoch 101/1500\n",
      "68/68 [==============================] - 0s 930us/step - loss: 0.1272 - accuracy: 0.9580\n",
      "Epoch 102/1500\n",
      "68/68 [==============================] - 0s 946us/step - loss: 0.1149 - accuracy: 0.9635\n",
      "Epoch 103/1500\n",
      "68/68 [==============================] - 0s 945us/step - loss: 0.1348 - accuracy: 0.9515\n",
      "Epoch 104/1500\n",
      "68/68 [==============================] - 0s 928us/step - loss: 0.1287 - accuracy: 0.9557\n",
      "Epoch 105/1500\n",
      "68/68 [==============================] - 0s 1000us/step - loss: 0.1289 - accuracy: 0.9538\n",
      "Epoch 106/1500\n",
      "68/68 [==============================] - 0s 999us/step - loss: 0.1175 - accuracy: 0.9612\n",
      "Epoch 107/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9548\n",
      "Epoch 108/1500\n",
      "68/68 [==============================] - 0s 999us/step - loss: 0.1242 - accuracy: 0.9543\n",
      "Epoch 109/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9612\n",
      "Epoch 110/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9575\n",
      "Epoch 111/1500\n",
      "68/68 [==============================] - 0s 980us/step - loss: 0.1252 - accuracy: 0.9594\n",
      "Epoch 112/1500\n",
      "68/68 [==============================] - 0s 983us/step - loss: 0.1183 - accuracy: 0.9598\n",
      "Epoch 113/1500\n",
      "68/68 [==============================] - 0s 4ms/step - loss: 0.1110 - accuracy: 0.9663\n",
      "Epoch 114/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9571\n",
      "Epoch 115/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9584\n",
      "Epoch 116/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9548\n",
      "Epoch 117/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1053 - accuracy: 0.9626\n",
      "Epoch 118/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.9497\n",
      "Epoch 119/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9584\n",
      "Epoch 120/1500\n",
      "68/68 [==============================] - 0s 963us/step - loss: 0.1250 - accuracy: 0.9543\n",
      "Epoch 121/1500\n",
      "68/68 [==============================] - 0s 1000us/step - loss: 0.1131 - accuracy: 0.9589\n",
      "Epoch 122/1500\n",
      "68/68 [==============================] - 0s 968us/step - loss: 0.1040 - accuracy: 0.9621\n",
      "Epoch 123/1500\n",
      "68/68 [==============================] - 0s 987us/step - loss: 0.1208 - accuracy: 0.9561\n",
      "Epoch 124/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9501\n",
      "Epoch 125/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9584\n",
      "Epoch 126/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1001 - accuracy: 0.9677\n",
      "Epoch 127/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9621\n",
      "Epoch 128/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9566\n",
      "Epoch 129/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9557\n",
      "Epoch 130/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9617\n",
      "Epoch 131/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9598\n",
      "Epoch 132/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9677\n",
      "Epoch 133/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9594\n",
      "Epoch 134/1500\n",
      "68/68 [==============================] - 0s 990us/step - loss: 0.1157 - accuracy: 0.9571\n",
      "Epoch 135/1500\n",
      "68/68 [==============================] - 0s 976us/step - loss: 0.1040 - accuracy: 0.9681\n",
      "Epoch 136/1500\n",
      "68/68 [==============================] - 0s 937us/step - loss: 0.1163 - accuracy: 0.9575\n",
      "Epoch 137/1500\n",
      "68/68 [==============================] - 0s 990us/step - loss: 0.1154 - accuracy: 0.9571\n",
      "Epoch 138/1500\n",
      "68/68 [==============================] - 0s 984us/step - loss: 0.1019 - accuracy: 0.9663\n",
      "Epoch 139/1500\n",
      "68/68 [==============================] - 0s 953us/step - loss: 0.1174 - accuracy: 0.9515\n",
      "Epoch 140/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9571\n",
      "Epoch 141/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9603\n",
      "Epoch 142/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9663\n",
      "Epoch 143/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9626\n",
      "Epoch 144/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9584\n",
      "Epoch 145/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9603\n",
      "Epoch 146/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9603\n",
      "Epoch 147/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9612\n",
      "Epoch 148/1500\n",
      "68/68 [==============================] - 0s 980us/step - loss: 0.0971 - accuracy: 0.9668\n",
      "Epoch 149/1500\n",
      "68/68 [==============================] - 0s 984us/step - loss: 0.0827 - accuracy: 0.9700\n",
      "Epoch 150/1500\n",
      "68/68 [==============================] - 0s 946us/step - loss: 0.1084 - accuracy: 0.9589\n",
      "Epoch 151/1500\n",
      "68/68 [==============================] - 0s 923us/step - loss: 0.1069 - accuracy: 0.9635\n",
      "Epoch 152/1500\n",
      "68/68 [==============================] - 0s 948us/step - loss: 0.1112 - accuracy: 0.9584\n",
      "Epoch 153/1500\n",
      "68/68 [==============================] - 0s 942us/step - loss: 0.1011 - accuracy: 0.9663\n",
      "Epoch 154/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.1035 - accuracy: 0.9649\n",
      "Epoch 155/1500\n",
      "68/68 [==============================] - 0s 953us/step - loss: 0.0979 - accuracy: 0.9658\n",
      "Epoch 156/1500\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.0976 - accuracy: 0.9672\n",
      "Epoch 157/1500\n",
      "68/68 [==============================] - 0s 983us/step - loss: 0.0915 - accuracy: 0.9686\n",
      "Epoch 158/1500\n",
      "68/68 [==============================] - 0s 954us/step - loss: 0.1094 - accuracy: 0.9654\n",
      "Epoch 159/1500\n",
      "68/68 [==============================] - 0s 973us/step - loss: 0.1037 - accuracy: 0.9612\n",
      "Epoch 160/1500\n",
      "68/68 [==============================] - 0s 957us/step - loss: 0.1007 - accuracy: 0.9621\n",
      "Epoch 161/1500\n",
      "68/68 [==============================] - 0s 990us/step - loss: 0.0826 - accuracy: 0.9695\n",
      "Epoch 162/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9654\n",
      "Epoch 163/1500\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.1038 - accuracy: 0.9640\n",
      "Epoch 164/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9631\n",
      "Epoch 165/1500\n",
      "68/68 [==============================] - 0s 982us/step - loss: 0.0879 - accuracy: 0.9681\n",
      "Epoch 166/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9603\n",
      "Epoch 167/1500\n",
      "68/68 [==============================] - 0s 989us/step - loss: 0.0911 - accuracy: 0.9691\n",
      "Epoch 168/1500\n",
      "68/68 [==============================] - 0s 991us/step - loss: 0.0825 - accuracy: 0.9714\n",
      "Epoch 169/1500\n",
      "68/68 [==============================] - 0s 929us/step - loss: 0.0890 - accuracy: 0.9654\n",
      "Epoch 170/1500\n",
      "68/68 [==============================] - 0s 950us/step - loss: 0.0929 - accuracy: 0.9663\n",
      "Epoch 171/1500\n",
      "68/68 [==============================] - 0s 996us/step - loss: 0.0791 - accuracy: 0.9783\n",
      "Epoch 172/1500\n",
      "68/68 [==============================] - 0s 966us/step - loss: 0.0958 - accuracy: 0.9672\n",
      "Epoch 173/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.9695\n",
      "Epoch 174/1500\n",
      "68/68 [==============================] - 0s 990us/step - loss: 0.0777 - accuracy: 0.9755\n",
      "Epoch 175/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.0874 - accuracy: 0.9714\n",
      "Epoch 176/1500\n",
      "68/68 [==============================] - 0s 933us/step - loss: 0.0919 - accuracy: 0.9668\n",
      "Epoch 177/1500\n",
      "68/68 [==============================] - 0s 936us/step - loss: 0.0920 - accuracy: 0.9681\n",
      "Epoch 178/1500\n",
      "68/68 [==============================] - 0s 930us/step - loss: 0.0905 - accuracy: 0.9700\n",
      "Epoch 179/1500\n",
      "68/68 [==============================] - 0s 986us/step - loss: 0.0730 - accuracy: 0.9741\n",
      "Epoch 180/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9774\n",
      "Epoch 181/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9709\n",
      "Epoch 182/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9668\n",
      "Epoch 183/1500\n",
      "68/68 [==============================] - 0s 974us/step - loss: 0.0813 - accuracy: 0.9714\n",
      "Epoch 184/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9700\n",
      "Epoch 185/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9672\n",
      "Epoch 186/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9640\n",
      "Epoch 187/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9649\n",
      "Epoch 188/1500\n",
      "68/68 [==============================] - 0s 980us/step - loss: 0.0749 - accuracy: 0.9746\n",
      "Epoch 189/1500\n",
      "68/68 [==============================] - 0s 966us/step - loss: 0.0883 - accuracy: 0.9741\n",
      "Epoch 190/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9695\n",
      "Epoch 191/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.9718\n",
      "Epoch 192/1500\n",
      "68/68 [==============================] - 0s 999us/step - loss: 0.0955 - accuracy: 0.9691\n",
      "Epoch 193/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9677\n",
      "Epoch 194/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9741\n",
      "Epoch 195/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.9714\n",
      "Epoch 196/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9751\n",
      "Epoch 197/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9672\n",
      "Epoch 198/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9765\n",
      "Epoch 199/1500\n",
      "68/68 [==============================] - 0s 996us/step - loss: 0.0870 - accuracy: 0.9723\n",
      "Epoch 200/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.9705\n",
      "Epoch 201/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9695\n",
      "Epoch 202/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9691\n",
      "Epoch 203/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9746\n",
      "Epoch 204/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0788 - accuracy: 0.9709\n",
      "Epoch 205/1500\n",
      "68/68 [==============================] - 0s 967us/step - loss: 0.0926 - accuracy: 0.9691\n",
      "Epoch 206/1500\n",
      "68/68 [==============================] - 0s 959us/step - loss: 0.0741 - accuracy: 0.9774\n",
      "Epoch 207/1500\n",
      "68/68 [==============================] - 0s 945us/step - loss: 0.0771 - accuracy: 0.9728\n",
      "Epoch 208/1500\n",
      "68/68 [==============================] - 0s 961us/step - loss: 0.0835 - accuracy: 0.9691\n",
      "Epoch 209/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9774\n",
      "Epoch 210/1500\n",
      "68/68 [==============================] - 0s 970us/step - loss: 0.0767 - accuracy: 0.9746\n",
      "Epoch 211/1500\n",
      "68/68 [==============================] - 0s 953us/step - loss: 0.0827 - accuracy: 0.9691\n",
      "Epoch 212/1500\n",
      "68/68 [==============================] - 0s 938us/step - loss: 0.0812 - accuracy: 0.9760\n",
      "Epoch 213/1500\n",
      "68/68 [==============================] - 0s 945us/step - loss: 0.1006 - accuracy: 0.9672\n",
      "Epoch 214/1500\n",
      "68/68 [==============================] - 0s 937us/step - loss: 0.0933 - accuracy: 0.9714\n",
      "Epoch 215/1500\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0720 - accuracy: 0.9769\n",
      "Epoch 216/1500\n",
      "68/68 [==============================] - 0s 978us/step - loss: 0.0719 - accuracy: 0.9755\n",
      "Epoch 217/1500\n",
      "68/68 [==============================] - 0s 965us/step - loss: 0.0850 - accuracy: 0.9658\n",
      "Epoch 218/1500\n",
      "68/68 [==============================] - 0s 971us/step - loss: 0.0735 - accuracy: 0.9746\n",
      "Epoch 219/1500\n",
      "68/68 [==============================] - 0s 979us/step - loss: 0.0733 - accuracy: 0.9718\n",
      "Epoch 220/1500\n",
      "68/68 [==============================] - 0s 998us/step - loss: 0.0717 - accuracy: 0.9741\n",
      "Epoch 221/1500\n",
      "68/68 [==============================] - 0s 988us/step - loss: 0.0729 - accuracy: 0.9769\n",
      "Epoch 222/1500\n",
      "68/68 [==============================] - 0s 961us/step - loss: 0.0856 - accuracy: 0.9677\n",
      "Epoch 223/1500\n",
      "68/68 [==============================] - 0s 978us/step - loss: 0.0782 - accuracy: 0.9755\n",
      "Epoch 224/1500\n",
      "68/68 [==============================] - 0s 955us/step - loss: 0.0755 - accuracy: 0.9755\n",
      "Epoch 225/1500\n",
      "68/68 [==============================] - 0s 961us/step - loss: 0.0988 - accuracy: 0.9686\n",
      "Epoch 226/1500\n",
      "68/68 [==============================] - 0s 945us/step - loss: 0.0862 - accuracy: 0.9709\n",
      "Epoch 227/1500\n",
      "68/68 [==============================] - 0s 944us/step - loss: 0.0645 - accuracy: 0.9811\n",
      "Epoch 228/1500\n",
      "68/68 [==============================] - 0s 974us/step - loss: 0.0751 - accuracy: 0.9709\n",
      "Epoch 229/1500\n",
      "68/68 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9700\n",
      "Epoch 230/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9801\n",
      "Epoch 231/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9751\n",
      "Epoch 232/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9741\n",
      "Epoch 233/1500\n",
      "68/68 [==============================] - 0s 947us/step - loss: 0.0658 - accuracy: 0.9778\n",
      "Epoch 234/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0550 - accuracy: 0.9820\n",
      "Epoch 235/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.9723\n",
      "Epoch 236/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0595 - accuracy: 0.9806\n",
      "Epoch 237/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9783\n",
      "Epoch 238/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0711 - accuracy: 0.9746\n",
      "Epoch 239/1500\n",
      "68/68 [==============================] - 0s 990us/step - loss: 0.0864 - accuracy: 0.9751\n",
      "Epoch 240/1500\n",
      "68/68 [==============================] - 0s 905us/step - loss: 0.0695 - accuracy: 0.9765\n",
      "Epoch 241/1500\n",
      "68/68 [==============================] - 0s 973us/step - loss: 0.0806 - accuracy: 0.9723\n",
      "Epoch 242/1500\n",
      "68/68 [==============================] - 0s 985us/step - loss: 0.0773 - accuracy: 0.9714\n",
      "Epoch 243/1500\n",
      "68/68 [==============================] - 0s 976us/step - loss: 0.0672 - accuracy: 0.9774\n",
      "Epoch 244/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9700\n",
      "Epoch 245/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9695\n",
      "Epoch 246/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9783\n",
      "Epoch 247/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9718\n",
      "Epoch 248/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.9788\n",
      "Epoch 249/1500\n",
      "68/68 [==============================] - 0s 965us/step - loss: 0.0850 - accuracy: 0.9695\n",
      "Epoch 250/1500\n",
      "68/68 [==============================] - 0s 986us/step - loss: 0.0957 - accuracy: 0.9668\n",
      "Epoch 251/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9709\n",
      "Epoch 252/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0753 - accuracy: 0.9714\n",
      "Epoch 253/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.9829\n",
      "Epoch 254/1500\n",
      "68/68 [==============================] - 0s 951us/step - loss: 0.0729 - accuracy: 0.9778\n",
      "Epoch 255/1500\n",
      "68/68 [==============================] - 0s 975us/step - loss: 0.0818 - accuracy: 0.9705\n",
      "Epoch 256/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0728 - accuracy: 0.9746\n",
      "Epoch 257/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9760\n",
      "Epoch 258/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0588 - accuracy: 0.9811\n",
      "Epoch 259/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9686\n",
      "Epoch 260/1500\n",
      "68/68 [==============================] - 0s 936us/step - loss: 0.0716 - accuracy: 0.9760\n",
      "Epoch 261/1500\n",
      "68/68 [==============================] - 0s 942us/step - loss: 0.0709 - accuracy: 0.9746\n",
      "Epoch 262/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9769\n",
      "Epoch 263/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0680 - accuracy: 0.9746\n",
      "Epoch 264/1500\n",
      "49/68 [====================>.........] - ETA: 0s - loss: 0.0623 - accuracy: 0.9815Restoring model weights from the end of the best epoch: 234.\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0598 - accuracy: 0.9806\n",
      "Epoch 264: early stopping\n",
      "8/8 [==============================] - 0s 827us/step - loss: 1.2155 - accuracy: 0.6518\n",
      "8/8 [==============================] - 0s 725us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.66 (19/29)\n",
      "Before appending - Cat IDs: 162, Predictions: 162, Actuals: 162, Gender: 162\n",
      "After appending - Cat IDs: 409, Predictions: 409, Actuals: 409, Gender: 409\n",
      "Final Test Results - Loss: 1.215499997138977, Accuracy: 0.6518218517303467, Precision: 0.6243061641834649, Recall: 0.5856456043956043, F1 Score: 0.601875058015409\n",
      "Confusion Matrix:\n",
      " [[119   5  36]\n",
      " [ 13  22   0]\n",
      " [ 31   1  20]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "055A    20\n",
      "        ..\n",
      "096A     1\n",
      "043A     1\n",
      "073A     1\n",
      "041A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 82, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "067A    19\n",
      "019A    17\n",
      "097A    16\n",
      "059A    14\n",
      "097B    14\n",
      "002A    13\n",
      "116A    12\n",
      "051A    12\n",
      "072A     9\n",
      "033A     9\n",
      "027A     7\n",
      "099A     7\n",
      "050A     7\n",
      "023A     6\n",
      "034A     5\n",
      "025C     5\n",
      "075A     5\n",
      "052A     4\n",
      "003A     4\n",
      "012A     3\n",
      "006A     3\n",
      "018A     2\n",
      "026C     1\n",
      "019B     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    250\n",
      "F    202\n",
      "M    180\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    157\n",
      "X     98\n",
      "F     50\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 103A, 071A, 028A, 062A, 101...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 043...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 051B, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 097B, 019A, 074A, 067A, 020A, 002...\n",
      "kitten                                   [047A, 050A, 115A]\n",
      "senior                       [097A, 059A, 116A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 52, 'kitten': 13, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 22, 'kitten': 3, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '004A' '005A' '007A' '008A' '009A' '010A' '011A'\n",
      " '013B' '014A' '014B' '015A' '016A' '021A' '022A' '023B' '024A' '025A'\n",
      " '025B' '026A' '026B' '028A' '029A' '031A' '032A' '035A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '049A' '051B' '053A' '054A' '055A' '056A' '057A' '058A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '068A' '069A' '070A' '071A' '073A'\n",
      " '076A' '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '100A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '002B' '003A' '006A' '012A' '018A' '019A' '019B' '020A' '023A'\n",
      " '025C' '026C' '027A' '033A' '034A' '047A' '050A' '051A' '052A' '059A'\n",
      " '067A' '072A' '074A' '075A' '090A' '097A' '097B' '099A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "632\n",
      "Length of y_train_val:\n",
      "632\n",
      "Length of groups_train_val:\n",
      "632\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     374\n",
      "kitten    135\n",
      "senior    123\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     214\n",
      "senior     55\n",
      "kitten     36\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age group distribution: Counter({0: 748, 1: 675, 2: 615})\n",
      "Epoch 1/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.9083 - accuracy: 0.5996\n",
      "Epoch 2/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.6924 - accuracy: 0.7125\n",
      "Epoch 3/1500\n",
      "64/64 [==============================] - 0s 951us/step - loss: 0.6295 - accuracy: 0.7301\n",
      "Epoch 4/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5949 - accuracy: 0.7360\n",
      "Epoch 5/1500\n",
      "64/64 [==============================] - 0s 928us/step - loss: 0.5809 - accuracy: 0.7488\n",
      "Epoch 6/1500\n",
      "64/64 [==============================] - 0s 874us/step - loss: 0.5542 - accuracy: 0.7659\n",
      "Epoch 7/1500\n",
      "64/64 [==============================] - 0s 898us/step - loss: 0.5492 - accuracy: 0.7723\n",
      "Epoch 8/1500\n",
      "64/64 [==============================] - 0s 904us/step - loss: 0.5280 - accuracy: 0.7728\n",
      "Epoch 9/1500\n",
      "64/64 [==============================] - 0s 967us/step - loss: 0.5211 - accuracy: 0.7807\n",
      "Epoch 10/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5104 - accuracy: 0.7821\n",
      "Epoch 11/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4782 - accuracy: 0.8032\n",
      "Epoch 12/1500\n",
      "64/64 [==============================] - 0s 953us/step - loss: 0.4670 - accuracy: 0.7978\n",
      "Epoch 13/1500\n",
      "64/64 [==============================] - 0s 960us/step - loss: 0.4639 - accuracy: 0.8175\n",
      "Epoch 14/1500\n",
      "64/64 [==============================] - 0s 918us/step - loss: 0.4349 - accuracy: 0.8219\n",
      "Epoch 15/1500\n",
      "64/64 [==============================] - 0s 961us/step - loss: 0.4465 - accuracy: 0.8101\n",
      "Epoch 16/1500\n",
      "64/64 [==============================] - 0s 943us/step - loss: 0.4096 - accuracy: 0.8391\n",
      "Epoch 17/1500\n",
      "64/64 [==============================] - 0s 952us/step - loss: 0.4236 - accuracy: 0.8199\n",
      "Epoch 18/1500\n",
      "64/64 [==============================] - 0s 960us/step - loss: 0.3966 - accuracy: 0.8454\n",
      "Epoch 19/1500\n",
      "64/64 [==============================] - 0s 942us/step - loss: 0.4067 - accuracy: 0.8381\n",
      "Epoch 20/1500\n",
      "64/64 [==============================] - 0s 875us/step - loss: 0.4052 - accuracy: 0.8292\n",
      "Epoch 21/1500\n",
      "64/64 [==============================] - 0s 982us/step - loss: 0.3902 - accuracy: 0.8346\n",
      "Epoch 22/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3786 - accuracy: 0.8386\n",
      "Epoch 23/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3912 - accuracy: 0.8386\n",
      "Epoch 24/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3622 - accuracy: 0.8587\n",
      "Epoch 25/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3503 - accuracy: 0.8616\n",
      "Epoch 26/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3660 - accuracy: 0.8513\n",
      "Epoch 27/1500\n",
      "64/64 [==============================] - 0s 954us/step - loss: 0.3596 - accuracy: 0.8523\n",
      "Epoch 28/1500\n",
      "64/64 [==============================] - 0s 956us/step - loss: 0.3316 - accuracy: 0.8651\n",
      "Epoch 29/1500\n",
      "64/64 [==============================] - 0s 941us/step - loss: 0.3595 - accuracy: 0.8503\n",
      "Epoch 30/1500\n",
      "64/64 [==============================] - 0s 896us/step - loss: 0.3272 - accuracy: 0.8680\n",
      "Epoch 31/1500\n",
      "64/64 [==============================] - 0s 890us/step - loss: 0.3199 - accuracy: 0.8675\n",
      "Epoch 32/1500\n",
      "64/64 [==============================] - 0s 960us/step - loss: 0.3256 - accuracy: 0.8675\n",
      "Epoch 33/1500\n",
      "64/64 [==============================] - 0s 966us/step - loss: 0.3310 - accuracy: 0.8660\n",
      "Epoch 34/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3251 - accuracy: 0.8670\n",
      "Epoch 35/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3084 - accuracy: 0.8788\n",
      "Epoch 36/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8749\n",
      "Epoch 37/1500\n",
      "64/64 [==============================] - 0s 955us/step - loss: 0.3036 - accuracy: 0.8783\n",
      "Epoch 38/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3032 - accuracy: 0.8729\n",
      "Epoch 39/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8778\n",
      "Epoch 40/1500\n",
      "64/64 [==============================] - 0s 997us/step - loss: 0.2825 - accuracy: 0.8896\n",
      "Epoch 41/1500\n",
      "64/64 [==============================] - 0s 988us/step - loss: 0.2704 - accuracy: 0.8921\n",
      "Epoch 42/1500\n",
      "64/64 [==============================] - 0s 974us/step - loss: 0.3165 - accuracy: 0.8739\n",
      "Epoch 43/1500\n",
      "64/64 [==============================] - 0s 972us/step - loss: 0.3217 - accuracy: 0.8724\n",
      "Epoch 44/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2992 - accuracy: 0.8793\n",
      "Epoch 45/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2739 - accuracy: 0.8916\n",
      "Epoch 46/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2817 - accuracy: 0.9019\n",
      "Epoch 47/1500\n",
      "64/64 [==============================] - 0s 967us/step - loss: 0.2708 - accuracy: 0.8979\n",
      "Epoch 48/1500\n",
      "64/64 [==============================] - 0s 985us/step - loss: 0.2750 - accuracy: 0.8916\n",
      "Epoch 49/1500\n",
      "64/64 [==============================] - 0s 970us/step - loss: 0.2809 - accuracy: 0.8803\n",
      "Epoch 50/1500\n",
      "64/64 [==============================] - 0s 961us/step - loss: 0.2557 - accuracy: 0.8965\n",
      "Epoch 51/1500\n",
      "64/64 [==============================] - 0s 968us/step - loss: 0.2734 - accuracy: 0.8935\n",
      "Epoch 52/1500\n",
      "64/64 [==============================] - 0s 945us/step - loss: 0.2769 - accuracy: 0.8911\n",
      "Epoch 53/1500\n",
      "64/64 [==============================] - 0s 981us/step - loss: 0.2545 - accuracy: 0.9033\n",
      "Epoch 54/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2472 - accuracy: 0.9132\n",
      "Epoch 55/1500\n",
      "64/64 [==============================] - 0s 971us/step - loss: 0.2481 - accuracy: 0.9038\n",
      "Epoch 56/1500\n",
      "64/64 [==============================] - 0s 963us/step - loss: 0.2521 - accuracy: 0.9043\n",
      "Epoch 57/1500\n",
      "64/64 [==============================] - 0s 999us/step - loss: 0.2463 - accuracy: 0.9058\n",
      "Epoch 58/1500\n",
      "64/64 [==============================] - 0s 945us/step - loss: 0.2590 - accuracy: 0.9024\n",
      "Epoch 59/1500\n",
      "64/64 [==============================] - 0s 897us/step - loss: 0.2354 - accuracy: 0.9082\n",
      "Epoch 60/1500\n",
      "64/64 [==============================] - 0s 897us/step - loss: 0.2331 - accuracy: 0.9127\n",
      "Epoch 61/1500\n",
      "64/64 [==============================] - 0s 960us/step - loss: 0.2354 - accuracy: 0.9122\n",
      "Epoch 62/1500\n",
      "64/64 [==============================] - 0s 927us/step - loss: 0.2349 - accuracy: 0.9136\n",
      "Epoch 63/1500\n",
      "64/64 [==============================] - 0s 907us/step - loss: 0.2273 - accuracy: 0.9122\n",
      "Epoch 64/1500\n",
      "64/64 [==============================] - 0s 874us/step - loss: 0.2316 - accuracy: 0.9053\n",
      "Epoch 65/1500\n",
      "64/64 [==============================] - 0s 906us/step - loss: 0.2281 - accuracy: 0.9200\n",
      "Epoch 66/1500\n",
      "64/64 [==============================] - 0s 912us/step - loss: 0.2253 - accuracy: 0.9181\n",
      "Epoch 67/1500\n",
      "64/64 [==============================] - 0s 916us/step - loss: 0.2134 - accuracy: 0.9151\n",
      "Epoch 68/1500\n",
      "64/64 [==============================] - 0s 911us/step - loss: 0.2348 - accuracy: 0.9097\n",
      "Epoch 69/1500\n",
      "64/64 [==============================] - 0s 878us/step - loss: 0.2211 - accuracy: 0.9127\n",
      "Epoch 70/1500\n",
      "64/64 [==============================] - 0s 921us/step - loss: 0.2246 - accuracy: 0.9220\n",
      "Epoch 71/1500\n",
      "64/64 [==============================] - 0s 915us/step - loss: 0.2359 - accuracy: 0.9097\n",
      "Epoch 72/1500\n",
      "64/64 [==============================] - 0s 908us/step - loss: 0.2204 - accuracy: 0.9205\n",
      "Epoch 73/1500\n",
      "64/64 [==============================] - 0s 905us/step - loss: 0.2437 - accuracy: 0.9024\n",
      "Epoch 74/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2183 - accuracy: 0.9151\n",
      "Epoch 75/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2071 - accuracy: 0.9264\n",
      "Epoch 76/1500\n",
      "64/64 [==============================] - 0s 992us/step - loss: 0.2039 - accuracy: 0.9313\n",
      "Epoch 77/1500\n",
      "64/64 [==============================] - 0s 946us/step - loss: 0.1982 - accuracy: 0.9269\n",
      "Epoch 78/1500\n",
      "64/64 [==============================] - 0s 940us/step - loss: 0.1947 - accuracy: 0.9274\n",
      "Epoch 79/1500\n",
      "64/64 [==============================] - 0s 919us/step - loss: 0.2098 - accuracy: 0.9205\n",
      "Epoch 80/1500\n",
      "64/64 [==============================] - 0s 905us/step - loss: 0.2272 - accuracy: 0.9122\n",
      "Epoch 81/1500\n",
      "64/64 [==============================] - 0s 927us/step - loss: 0.2128 - accuracy: 0.9225\n",
      "Epoch 82/1500\n",
      "64/64 [==============================] - 0s 908us/step - loss: 0.1921 - accuracy: 0.9313\n",
      "Epoch 83/1500\n",
      "64/64 [==============================] - 0s 908us/step - loss: 0.2300 - accuracy: 0.9161\n",
      "Epoch 84/1500\n",
      "64/64 [==============================] - 0s 896us/step - loss: 0.2152 - accuracy: 0.9136\n",
      "Epoch 85/1500\n",
      "64/64 [==============================] - 0s 952us/step - loss: 0.1985 - accuracy: 0.9274\n",
      "Epoch 86/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.9269\n",
      "Epoch 87/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2037 - accuracy: 0.9176\n",
      "Epoch 88/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2051 - accuracy: 0.9215\n",
      "Epoch 89/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2114 - accuracy: 0.9122\n",
      "Epoch 90/1500\n",
      "64/64 [==============================] - 0s 978us/step - loss: 0.1844 - accuracy: 0.9342\n",
      "Epoch 91/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9352\n",
      "Epoch 92/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1987 - accuracy: 0.9269\n",
      "Epoch 93/1500\n",
      "64/64 [==============================] - 0s 998us/step - loss: 0.2009 - accuracy: 0.9269\n",
      "Epoch 94/1500\n",
      "64/64 [==============================] - 0s 988us/step - loss: 0.1939 - accuracy: 0.9269\n",
      "Epoch 95/1500\n",
      "64/64 [==============================] - 0s 935us/step - loss: 0.1889 - accuracy: 0.9259\n",
      "Epoch 96/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.9362\n",
      "Epoch 97/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1745 - accuracy: 0.9347\n",
      "Epoch 98/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.9347\n",
      "Epoch 99/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9225\n",
      "Epoch 100/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9284\n",
      "Epoch 101/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1846 - accuracy: 0.9328\n",
      "Epoch 102/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9289\n",
      "Epoch 103/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9387\n",
      "Epoch 104/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.9318\n",
      "Epoch 105/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.9308\n",
      "Epoch 106/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.9333\n",
      "Epoch 107/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9470\n",
      "Epoch 108/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9333\n",
      "Epoch 109/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9269\n",
      "Epoch 110/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1622 - accuracy: 0.9401\n",
      "Epoch 111/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9367\n",
      "Epoch 112/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9396\n",
      "Epoch 113/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.9352\n",
      "Epoch 114/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1648 - accuracy: 0.9396\n",
      "Epoch 115/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9455\n",
      "Epoch 116/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9333\n",
      "Epoch 117/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9362\n",
      "Epoch 118/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9563\n",
      "Epoch 119/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9450\n",
      "Epoch 120/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1635 - accuracy: 0.9401\n",
      "Epoch 121/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1697 - accuracy: 0.9313\n",
      "Epoch 122/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1719 - accuracy: 0.9387\n",
      "Epoch 123/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1810 - accuracy: 0.9303\n",
      "Epoch 124/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1595 - accuracy: 0.9387\n",
      "Epoch 125/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9387\n",
      "Epoch 126/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9441\n",
      "Epoch 127/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1592 - accuracy: 0.9406\n",
      "Epoch 128/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9431\n",
      "Epoch 129/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1707 - accuracy: 0.9323\n",
      "Epoch 130/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1551 - accuracy: 0.9392\n",
      "Epoch 131/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9436\n",
      "Epoch 132/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9524\n",
      "Epoch 133/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9377\n",
      "Epoch 134/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9455\n",
      "Epoch 135/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1625 - accuracy: 0.9421\n",
      "Epoch 136/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9431\n",
      "Epoch 137/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9460\n",
      "Epoch 138/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9519\n",
      "Epoch 139/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.9455\n",
      "Epoch 140/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9372\n",
      "Epoch 141/1500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.1294 - accuracy: 0.9558\n",
      "Epoch 142/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9475\n",
      "Epoch 143/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9465\n",
      "Epoch 144/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9524\n",
      "Epoch 145/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9514\n",
      "Epoch 146/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9495\n",
      "Epoch 147/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9406\n",
      "Epoch 148/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9519\n",
      "Epoch 149/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9504\n",
      "Epoch 150/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9446\n",
      "Epoch 151/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1416 - accuracy: 0.9485\n",
      "Epoch 152/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9431\n",
      "Epoch 153/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9514\n",
      "Epoch 154/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9509\n",
      "Epoch 155/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9573\n",
      "Epoch 156/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9519\n",
      "Epoch 157/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1382 - accuracy: 0.9524\n",
      "Epoch 158/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9450\n",
      "Epoch 159/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.9460\n",
      "Epoch 160/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9426\n",
      "Epoch 161/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.9593\n",
      "Epoch 162/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1530 - accuracy: 0.9426\n",
      "Epoch 163/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9446\n",
      "Epoch 164/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9553\n",
      "Epoch 165/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1429 - accuracy: 0.9426\n",
      "Epoch 166/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9524\n",
      "Epoch 167/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9509\n",
      "Epoch 168/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9485\n",
      "Epoch 169/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9519\n",
      "Epoch 170/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9455\n",
      "Epoch 171/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9431\n",
      "Epoch 172/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9544\n",
      "Epoch 173/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9426\n",
      "Epoch 174/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9470\n",
      "Epoch 175/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9627\n",
      "Epoch 176/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1166 - accuracy: 0.9622\n",
      "Epoch 177/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9544\n",
      "Epoch 178/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9553\n",
      "Epoch 179/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1521 - accuracy: 0.9387\n",
      "Epoch 180/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9612\n",
      "Epoch 181/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9495\n",
      "Epoch 182/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9652\n",
      "Epoch 183/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9632\n",
      "Epoch 184/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9603\n",
      "Epoch 185/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9475\n",
      "Epoch 186/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9470\n",
      "Epoch 187/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9583\n",
      "Epoch 188/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9573\n",
      "Epoch 189/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9450\n",
      "Epoch 190/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9490\n",
      "Epoch 191/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9617\n",
      "Epoch 192/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9450\n",
      "Epoch 193/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9549\n",
      "Epoch 194/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9666\n",
      "Epoch 195/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9612\n",
      "Epoch 196/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9622\n",
      "Epoch 197/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9544\n",
      "Epoch 198/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9544\n",
      "Epoch 199/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9627\n",
      "Epoch 200/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9534\n",
      "Epoch 201/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9632\n",
      "Epoch 202/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9534\n",
      "Epoch 203/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9603\n",
      "Epoch 204/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9642\n",
      "Epoch 205/1500\n",
      "44/64 [===================>..........] - ETA: 0s - loss: 0.1221 - accuracy: 0.9545Restoring model weights from the end of the best epoch: 175.\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9578\n",
      "Epoch 205: early stopping\n",
      "10/10 [==============================] - 0s 921us/step - loss: 0.7196 - accuracy: 0.7803\n",
      "10/10 [==============================] - 0s 801us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (25/30)\n",
      "Before appending - Cat IDs: 409, Predictions: 409, Actuals: 409, Gender: 409\n",
      "After appending - Cat IDs: 714, Predictions: 714, Actuals: 714, Gender: 714\n",
      "Final Test Results - Loss: 0.7195785045623779, Accuracy: 0.7803278565406799, Precision: 0.7324324324324324, Recall: 0.797287516913685, F1 Score: 0.7555926854172469\n",
      "Confusion Matrix:\n",
      " [[166   8  40]\n",
      " [  4  32   0]\n",
      " [ 15   0  40]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "        ..\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, Length: 84, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "065A     9\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "021A     5\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "113A     3\n",
      "064A     3\n",
      "087A     2\n",
      "038A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "043A     1\n",
      "041A     1\n",
      "066A     1\n",
      "004A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    314\n",
      "X    271\n",
      "F    164\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    88\n",
      "X    77\n",
      "M    23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 097B, 028...\n",
      "kitten    [044A, 046A, 047A, 050A, 049A, 045A, 048A, 115...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 062A, 101A, 065A, 063A, 038A, 007A, 087...\n",
      "kitten           [014B, 111A, 040A, 042A, 109A, 043A, 041A]\n",
      "senior                             [113A, 108A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 9, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 7, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'037A'}\n",
      "Moved to Test Set:\n",
      "{'037A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026B' '026C' '027A' '028A'\n",
      " '029A' '032A' '033A' '034A' '036A' '039A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '053A' '054A' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '067A' '068A' '069A' '070A' '071A' '072A'\n",
      " '073A' '074A' '075A' '076A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '095A' '096A' '097A' '097B' '099A' '100A' '103A' '104A' '105A' '106A'\n",
      " '110A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '007A' '011A' '014B' '021A' '023B' '026A' '031A' '035A' '037A'\n",
      " '038A' '040A' '041A' '042A' '043A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '087A' '101A' '102A' '108A' '109A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "782\n",
      "Length of y_train_val:\n",
      "782\n",
      "Length of groups_train_val:\n",
      "782\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "kitten     55\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     501\n",
      "senior    165\n",
      "kitten    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     87\n",
      "kitten    55\n",
      "senior    13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 1002, 2: 825, 1: 580})\n",
      "Epoch 1/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.9526 - accuracy: 0.5916\n",
      "Epoch 2/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.7510 - accuracy: 0.6855\n",
      "Epoch 3/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.7048 - accuracy: 0.7121\n",
      "Epoch 4/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.6604 - accuracy: 0.7283\n",
      "Epoch 5/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.6284 - accuracy: 0.7295\n",
      "Epoch 6/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5742 - accuracy: 0.7703\n",
      "Epoch 7/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5501 - accuracy: 0.7703\n",
      "Epoch 8/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5269 - accuracy: 0.7852\n",
      "Epoch 9/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5459 - accuracy: 0.7711\n",
      "Epoch 10/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5459 - accuracy: 0.7736\n",
      "Epoch 11/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4922 - accuracy: 0.7919\n",
      "Epoch 12/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4626 - accuracy: 0.8126\n",
      "Epoch 13/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4872 - accuracy: 0.7914\n",
      "Epoch 14/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4785 - accuracy: 0.7973\n",
      "Epoch 15/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4381 - accuracy: 0.8122\n",
      "Epoch 16/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4284 - accuracy: 0.8218\n",
      "Epoch 17/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4366 - accuracy: 0.8205\n",
      "Epoch 18/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4252 - accuracy: 0.8238\n",
      "Epoch 19/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4140 - accuracy: 0.8284\n",
      "Epoch 20/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4110 - accuracy: 0.8363\n",
      "Epoch 21/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3943 - accuracy: 0.8371\n",
      "Epoch 22/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3816 - accuracy: 0.8417\n",
      "Epoch 23/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3939 - accuracy: 0.8421\n",
      "Epoch 24/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3820 - accuracy: 0.8384\n",
      "Epoch 25/1500\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.3757 - accuracy: 0.8525\n",
      "Epoch 26/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3590 - accuracy: 0.8533\n",
      "Epoch 27/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3520 - accuracy: 0.8600\n",
      "Epoch 28/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3532 - accuracy: 0.8517\n",
      "Epoch 29/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3628 - accuracy: 0.8558\n",
      "Epoch 30/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3609 - accuracy: 0.8575\n",
      "Epoch 31/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3345 - accuracy: 0.8612\n",
      "Epoch 32/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3334 - accuracy: 0.8621\n",
      "Epoch 33/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3079 - accuracy: 0.8774\n",
      "Epoch 34/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3269 - accuracy: 0.8704\n",
      "Epoch 35/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3185 - accuracy: 0.8774\n",
      "Epoch 36/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3142 - accuracy: 0.8720\n",
      "Epoch 37/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3275 - accuracy: 0.8671\n",
      "Epoch 38/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3032 - accuracy: 0.8866\n",
      "Epoch 39/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8833\n",
      "Epoch 40/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3036 - accuracy: 0.8783\n",
      "Epoch 41/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2951 - accuracy: 0.8803\n",
      "Epoch 42/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2764 - accuracy: 0.9020\n",
      "Epoch 43/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2784 - accuracy: 0.8974\n",
      "Epoch 44/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2887 - accuracy: 0.8812\n",
      "Epoch 45/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2908 - accuracy: 0.8878\n",
      "Epoch 46/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2765 - accuracy: 0.8986\n",
      "Epoch 47/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2901 - accuracy: 0.8820\n",
      "Epoch 48/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2879 - accuracy: 0.8816\n",
      "Epoch 49/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2930 - accuracy: 0.8882\n",
      "Epoch 50/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2801 - accuracy: 0.8924\n",
      "Epoch 51/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2735 - accuracy: 0.8953\n",
      "Epoch 52/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2551 - accuracy: 0.9061\n",
      "Epoch 53/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2619 - accuracy: 0.8990\n",
      "Epoch 54/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2472 - accuracy: 0.9074\n",
      "Epoch 55/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2628 - accuracy: 0.9028\n",
      "Epoch 56/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.9020\n",
      "Epoch 57/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2572 - accuracy: 0.9003\n",
      "Epoch 58/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2524 - accuracy: 0.9061\n",
      "Epoch 59/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2437 - accuracy: 0.9086\n",
      "Epoch 60/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2545 - accuracy: 0.9090\n",
      "Epoch 61/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2562 - accuracy: 0.9011\n",
      "Epoch 62/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2297 - accuracy: 0.9144\n",
      "Epoch 63/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2534 - accuracy: 0.9069\n",
      "Epoch 64/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2397 - accuracy: 0.9015\n",
      "Epoch 65/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2289 - accuracy: 0.9169\n",
      "Epoch 66/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2577 - accuracy: 0.8999\n",
      "Epoch 67/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2501 - accuracy: 0.9061\n",
      "Epoch 68/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2259 - accuracy: 0.9128\n",
      "Epoch 69/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2467 - accuracy: 0.9078\n",
      "Epoch 70/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2279 - accuracy: 0.9086\n",
      "Epoch 71/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2211 - accuracy: 0.9165\n",
      "Epoch 72/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2210 - accuracy: 0.9140\n",
      "Epoch 73/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2306 - accuracy: 0.9069\n",
      "Epoch 74/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2180 - accuracy: 0.9152\n",
      "Epoch 75/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2281 - accuracy: 0.9173\n",
      "Epoch 76/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9177\n",
      "Epoch 77/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2214 - accuracy: 0.9198\n",
      "Epoch 78/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2213 - accuracy: 0.9177\n",
      "Epoch 79/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9236\n",
      "Epoch 80/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2039 - accuracy: 0.9244\n",
      "Epoch 81/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1948 - accuracy: 0.9240\n",
      "Epoch 82/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2206 - accuracy: 0.9186\n",
      "Epoch 83/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 0.9240\n",
      "Epoch 84/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9211\n",
      "Epoch 85/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2029 - accuracy: 0.9306\n",
      "Epoch 86/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9273\n",
      "Epoch 87/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2073 - accuracy: 0.9186\n",
      "Epoch 88/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9206\n",
      "Epoch 89/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2102 - accuracy: 0.9260\n",
      "Epoch 90/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9319\n",
      "Epoch 91/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1914 - accuracy: 0.9236\n",
      "Epoch 92/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2032 - accuracy: 0.9223\n",
      "Epoch 93/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9331\n",
      "Epoch 94/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9323\n",
      "Epoch 95/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1982 - accuracy: 0.9277\n",
      "Epoch 96/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1899 - accuracy: 0.9252\n",
      "Epoch 97/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1812 - accuracy: 0.9335\n",
      "Epoch 98/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9260\n",
      "Epoch 99/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.9327\n",
      "Epoch 100/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.9339\n",
      "Epoch 101/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9369\n",
      "Epoch 102/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9273\n",
      "Epoch 103/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1789 - accuracy: 0.9327\n",
      "Epoch 104/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.9385\n",
      "Epoch 105/1500\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.1973 - accuracy: 0.9182\n",
      "Epoch 106/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1771 - accuracy: 0.9389\n",
      "Epoch 107/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9356\n",
      "Epoch 108/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.9389\n",
      "Epoch 109/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9381\n",
      "Epoch 110/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1586 - accuracy: 0.9468\n",
      "Epoch 111/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1905 - accuracy: 0.9298\n",
      "Epoch 112/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9418\n",
      "Epoch 113/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.9310\n",
      "Epoch 114/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9364\n",
      "Epoch 115/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.9410\n",
      "Epoch 116/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1876 - accuracy: 0.9294\n",
      "Epoch 117/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.9285\n",
      "Epoch 118/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9381\n",
      "Epoch 119/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.9410\n",
      "Epoch 120/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9364\n",
      "Epoch 121/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1760 - accuracy: 0.9339\n",
      "Epoch 122/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.9360\n",
      "Epoch 123/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9402\n",
      "Epoch 124/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9468\n",
      "Epoch 125/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9327\n",
      "Epoch 126/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9410\n",
      "Epoch 127/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.9323\n",
      "Epoch 128/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9427\n",
      "Epoch 129/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9310\n",
      "Epoch 130/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.9423\n",
      "Epoch 131/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1548 - accuracy: 0.9410\n",
      "Epoch 132/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9427\n",
      "Epoch 133/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9452\n",
      "Epoch 134/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1518 - accuracy: 0.9439\n",
      "Epoch 135/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9435\n",
      "Epoch 136/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.9431\n",
      "Epoch 137/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1487 - accuracy: 0.9464\n",
      "Epoch 138/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9423\n",
      "Epoch 139/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9472\n",
      "Epoch 140/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9402\n",
      "Epoch 141/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9535\n",
      "Epoch 142/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9489\n",
      "Epoch 143/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9564\n",
      "Epoch 144/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1518 - accuracy: 0.9464\n",
      "Epoch 145/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9489\n",
      "Epoch 146/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.9464\n",
      "Epoch 147/1500\n",
      "76/76 [==============================] - 0s 2ms/step - loss: 0.1424 - accuracy: 0.9493\n",
      "Epoch 148/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.9526\n",
      "Epoch 149/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9377\n",
      "Epoch 150/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9497\n",
      "Epoch 151/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9493\n",
      "Epoch 152/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9464\n",
      "Epoch 153/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9535\n",
      "Epoch 154/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9543\n",
      "Epoch 155/1500\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.1715 - accuracy: 0.9369\n",
      "Epoch 156/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9439\n",
      "Epoch 157/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1410 - accuracy: 0.9406\n",
      "Epoch 158/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9518\n",
      "Epoch 159/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9472\n",
      "Epoch 160/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9568\n",
      "Epoch 161/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9510\n",
      "Epoch 162/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1420 - accuracy: 0.9472\n",
      "Epoch 163/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9501\n",
      "Epoch 164/1500\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.1402 - accuracy: 0.9518\n",
      "Epoch 165/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9572\n",
      "Epoch 166/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9468\n",
      "Epoch 167/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9555\n",
      "Epoch 168/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1373 - accuracy: 0.9526\n",
      "Epoch 169/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9481\n",
      "Epoch 170/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9506\n",
      "Epoch 171/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9464\n",
      "Epoch 172/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1370 - accuracy: 0.9481\n",
      "Epoch 173/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1497 - accuracy: 0.9435\n",
      "Epoch 174/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9531\n",
      "Epoch 175/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1317 - accuracy: 0.9568\n",
      "Epoch 176/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9576\n",
      "Epoch 177/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9477\n",
      "Epoch 178/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1336 - accuracy: 0.9518\n",
      "Epoch 179/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9531\n",
      "Epoch 180/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9626\n",
      "Epoch 181/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9443\n",
      "Epoch 182/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9560\n",
      "Epoch 183/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9601\n",
      "Epoch 184/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9535\n",
      "Epoch 185/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9522\n",
      "Epoch 186/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9572\n",
      "Epoch 187/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9518\n",
      "Epoch 188/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9597\n",
      "Epoch 189/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9497\n",
      "Epoch 190/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9543\n",
      "Epoch 191/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9564\n",
      "Epoch 192/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9589\n",
      "Epoch 193/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9526\n",
      "Epoch 194/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9568\n",
      "Epoch 195/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9547\n",
      "Epoch 196/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9609\n",
      "Epoch 197/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9497\n",
      "Epoch 198/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9526\n",
      "Epoch 199/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9531\n",
      "Epoch 200/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1279 - accuracy: 0.9526\n",
      "Epoch 201/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9481\n",
      "Epoch 202/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9576\n",
      "Epoch 203/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9618\n",
      "Epoch 204/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9693\n",
      "Epoch 205/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9597\n",
      "Epoch 206/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9560\n",
      "Epoch 207/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9551\n",
      "Epoch 208/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9551\n",
      "Epoch 209/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9555\n",
      "Epoch 210/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9564\n",
      "Epoch 211/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9589\n",
      "Epoch 212/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9601\n",
      "Epoch 213/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9580\n",
      "Epoch 214/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9597\n",
      "Epoch 215/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9539\n",
      "Epoch 216/1500\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.1150 - accuracy: 0.9634\n",
      "Epoch 217/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9647\n",
      "Epoch 218/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9651\n",
      "Epoch 219/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.9676\n",
      "Epoch 220/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9643\n",
      "Epoch 221/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9601\n",
      "Epoch 222/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9643\n",
      "Epoch 223/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9585\n",
      "Epoch 224/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9539\n",
      "Epoch 225/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9614\n",
      "Epoch 226/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9522\n",
      "Epoch 227/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9618\n",
      "Epoch 228/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9601\n",
      "Epoch 229/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9585\n",
      "Epoch 230/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9585\n",
      "Epoch 231/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9655\n",
      "Epoch 232/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9639\n",
      "Epoch 233/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9622\n",
      "Epoch 234/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9643\n",
      "Epoch 235/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9647\n",
      "Epoch 236/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9643\n",
      "Epoch 237/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9651\n",
      "Epoch 238/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9609\n",
      "Epoch 239/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9672\n",
      "Epoch 240/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9651\n",
      "Epoch 241/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9680\n",
      "Epoch 242/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9672\n",
      "Epoch 243/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9630\n",
      "Epoch 244/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9697\n",
      "Epoch 245/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9614\n",
      "Epoch 246/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9705\n",
      "Epoch 247/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9668\n",
      "Epoch 248/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9697\n",
      "Epoch 249/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9647\n",
      "Epoch 250/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9639\n",
      "Epoch 251/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0987 - accuracy: 0.9701\n",
      "Epoch 252/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9614\n",
      "Epoch 253/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9572\n",
      "Epoch 254/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9663\n",
      "Epoch 255/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9597\n",
      "Epoch 256/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9659\n",
      "Epoch 257/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.9751\n",
      "Epoch 258/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9676\n",
      "Epoch 259/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9676\n",
      "Epoch 260/1500\n",
      "76/76 [==============================] - 0s 2ms/step - loss: 0.0908 - accuracy: 0.9701\n",
      "Epoch 261/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9738\n",
      "Epoch 262/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9605\n",
      "Epoch 263/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9709\n",
      "Epoch 264/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0977 - accuracy: 0.9659\n",
      "Epoch 265/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9676\n",
      "Epoch 266/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9618\n",
      "Epoch 267/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1017 - accuracy: 0.9684\n",
      "Epoch 268/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9668\n",
      "Epoch 269/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9593\n",
      "Epoch 270/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9672\n",
      "Epoch 271/1500\n",
      "76/76 [==============================] - 0s 2ms/step - loss: 0.0887 - accuracy: 0.9676\n",
      "Epoch 272/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0980 - accuracy: 0.9651\n",
      "Epoch 273/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9601\n",
      "Epoch 274/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.9697\n",
      "Epoch 275/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9676\n",
      "Epoch 276/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0956 - accuracy: 0.9659\n",
      "Epoch 277/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0912 - accuracy: 0.9688\n",
      "Epoch 278/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9618\n",
      "Epoch 279/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9722\n",
      "Epoch 280/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9651\n",
      "Epoch 281/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9655\n",
      "Epoch 282/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9672\n",
      "Epoch 283/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9659\n",
      "Epoch 284/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9676\n",
      "Epoch 285/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1244 - accuracy: 0.9547\n",
      "Epoch 286/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9634\n",
      "Epoch 287/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9626\n",
      "Epoch 288/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9626\n",
      "Epoch 289/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9663\n",
      "Epoch 290/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9672\n",
      "Epoch 291/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9676\n",
      "Epoch 292/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9643\n",
      "Epoch 293/1500\n",
      "45/76 [================>.............] - ETA: 0s - loss: 0.1015 - accuracy: 0.9632Restoring model weights from the end of the best epoch: 263.\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9605\n",
      "Epoch 293: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.5505 - accuracy: 0.7935\n",
      "5/5 [==============================] - 0s 820us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.82 (23/28)\n",
      "Before appending - Cat IDs: 714, Predictions: 714, Actuals: 714, Gender: 714\n",
      "After appending - Cat IDs: 869, Predictions: 869, Actuals: 869, Gender: 869\n",
      "Final Test Results - Loss: 0.5505380034446716, Accuracy: 0.7935484051704407, Precision: 0.6723060164236635, Recall: 0.6850842643946092, F1 Score: 0.6755555711357747\n",
      "Confusion Matrix:\n",
      " [[71  4 12]\n",
      " [ 7 47  1]\n",
      " [ 7  1  5]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6793698126202474\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.9404682666063309\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7200047820806503\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.671623180121917\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6978064985414063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[0]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79e793-c694-4bc7-8cc6-05e124ddf71f",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2c3c239d-6215-4589-a583-b37a18ca22cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 869, Predictions: 869, Actuals: 869, Gender: 869\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99978976-e4a9-4c04-a6b2-6b14a3111277",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "189b812b-d37f-462f-a99f-6de8e4a7899f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (83/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "907d957f-7340-4a76-b3fa-7cf166f43049",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7016fa20-094f-409a-9a2f-b6d6793aed03",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, kitten, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, senior, kitten, adult, senior,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten, kitten, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, kitten, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, senior, senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, kitten,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, senior, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "48    042A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A              [adult, adult, senior, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, kitten, adult, adult, adult, ad...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, senior, kitten, adult, senior,...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [adult, adult, adult, adult, senior, senior, a...        senior           senior                   True\n",
       "64    058A                            [senior, senior, adult]        senior           senior                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "58    052A                     [adult, senior, senior, adult]         adult            adult                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, adult, adult, kitten, kitten, kitten, ...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A           [kitten, kitten, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, kitten, adult, adult, ad...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A        [adult, kitten, adult, adult, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, senior, ...         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                     [adult, senior, adult, senior]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "2     002A  [adult, adult, senior, adult, senior, adult, a...         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "109   117A  [senior, senior, senior, senior, adult, adult,...        senior           senior                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "41    035A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "43    037A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "40    034A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "102   108A      [senior, adult, adult, senior, adult, kitten]         adult           senior                  False\n",
       "101   106A  [adult, senior, adult, senior, adult, adult, a...         adult           senior                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "7     006A                            [senior, senior, adult]        senior            adult                  False\n",
       "42    036A  [senior, senior, senior, senior, adult, senior...        senior            adult                  False\n",
       "57    051B  [senior, adult, kitten, adult, senior, adult, ...         adult           senior                  False\n",
       "30    025C             [adult, senior, senior, senior, adult]        senior            adult                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "59    053A     [senior, senior, adult, senior, adult, senior]        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "90    095A  [senior, adult, adult, senior, senior, kitten,...        senior            adult                  False\n",
       "89    094A  [senior, adult, adult, adult, adult, senior, s...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, adult, adult, adult, adu...         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, senior, adult, s...         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "34    027A  [adult, senior, senior, senior, senior, senior...        senior            adult                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "994531fd-e6fb-4491-9eab-86e41ac1ed3c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     60\n",
      "kitten    12\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "229af3e2-6a64-4f3d-a594-e04108fce87f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             60  82.191781\n",
      "1           kitten           15             12  80.000000\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3960450a-db52-4464-a965-00f1e600bda5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bbf768-64c2-48ec-80e3-3a961b0b12a6",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fe16003e-4015-4d28-ae37-ca0c94952758",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          555            417  75.135135\n",
      "1           kitten          136            110  80.882353\n",
      "2           senior          178            101  56.741573\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5b5b8766-b4bd-4dd2-92e2-6513741b03ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3efa447-1e32-42df-8664-a3f1f0e0f812",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5eecd6a6-4094-4747-8029-f795a87f8ff0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      170     79.81\n",
      "1          M    337      220     65.28\n",
      "2          X    319      238     74.61\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "115592e9-47eb-42fe-8cb5-26beeb7328ba",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae066af0-659b-43f0-924c-2c50be0e6c40",
   "metadata": {},
   "source": [
    "# RANDOM SEED 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc76296a-4029-447e-b12c-41520160251a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[1]))\n",
    "np.random.seed(int(random_seeds[1]))\n",
    "tf.random.set_seed(int(random_seeds[1]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_1.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "093977a2-1813-4a02-9573-f757b3d2a567",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c89dbfa-4c9b-4e46-a0d9-f659db30532f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a62fdf-86b5-45e7-9249-73a55985936a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ee1b36c7-4a91-4071-a713-c904da913c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "106A    14\n",
      "097B    14\n",
      "001A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "005A    10\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "022A     9\n",
      "051B     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "033A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "027A     7\n",
      "117A     7\n",
      "031A     7\n",
      "109A     6\n",
      "108A     6\n",
      "053A     6\n",
      "008A     6\n",
      "023A     6\n",
      "037A     6\n",
      "007A     6\n",
      "034A     5\n",
      "025C     5\n",
      "070A     5\n",
      "023B     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "062A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "052A     4\n",
      "009A     4\n",
      "060A     3\n",
      "012A     3\n",
      "006A     3\n",
      "064A     3\n",
      "058A     3\n",
      "054A     2\n",
      "061A     2\n",
      "087A     2\n",
      "069A     2\n",
      "032A     2\n",
      "011A     2\n",
      "018A     2\n",
      "025B     2\n",
      "093A     2\n",
      "088A     1\n",
      "091A     1\n",
      "100A     1\n",
      "090A     1\n",
      "019B     1\n",
      "115A     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "073A     1\n",
      "026C     1\n",
      "076A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "014B    10\n",
      "015A     9\n",
      "099A     7\n",
      "050A     7\n",
      "021A     5\n",
      "003A     4\n",
      "056A     3\n",
      "014A     3\n",
      "113A     3\n",
      "038A     2\n",
      "102A     2\n",
      "096A     1\n",
      "110A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    233\n",
      "M    226\n",
      "F    210\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    115\n",
      "M    111\n",
      "F     42\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 040A, 046A, 109A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 059A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 103A, 074A, 002B, 101A, 038A, 099A, 014...\n",
      "kitten                 [014B, 111A, 047A, 042A, 050A, 110A]\n",
      "senior                       [097A, 113A, 056A, 051A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 60, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 14, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '004A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '016A' '018A' '019A' '019B' '020A' '022A'\n",
      " '023A' '023B' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A'\n",
      " '029A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '043A' '044A' '045A' '046A' '048A' '049A' '051B' '052A' '053A' '054A'\n",
      " '055A' '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A'\n",
      " '066A' '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A'\n",
      " '088A' '090A' '091A' '092A' '093A' '094A' '095A' '097B' '100A' '104A'\n",
      " '105A' '106A' '108A' '109A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '003A' '014A' '014B' '015A' '021A' '024A' '036A' '038A' '042A'\n",
      " '047A' '050A' '051A' '056A' '068A' '074A' '096A' '097A' '099A' '101A'\n",
      " '102A' '103A' '110A' '111A' '113A']\n",
      "Length of X_train_val:\n",
      "669\n",
      "Length of y_train_val:\n",
      "669\n",
      "Length of groups_train_val:\n",
      "669\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     428\n",
      "senior    143\n",
      "kitten     98\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     160\n",
      "kitten     73\n",
      "senior     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 856, 2: 715, 1: 490})\n",
      "Epoch 1/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.9772 - accuracy: 0.5769\n",
      "Epoch 2/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.7573 - accuracy: 0.6793\n",
      "Epoch 3/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.6725 - accuracy: 0.7118\n",
      "Epoch 4/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.6329 - accuracy: 0.7327\n",
      "Epoch 5/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.6073 - accuracy: 0.7501\n",
      "Epoch 6/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5503 - accuracy: 0.7744\n",
      "Epoch 7/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5508 - accuracy: 0.7695\n",
      "Epoch 8/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5311 - accuracy: 0.7846\n",
      "Epoch 9/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.5228 - accuracy: 0.7943\n",
      "Epoch 10/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4905 - accuracy: 0.8054\n",
      "Epoch 11/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4685 - accuracy: 0.8093\n",
      "Epoch 12/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4593 - accuracy: 0.8156\n",
      "Epoch 13/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4568 - accuracy: 0.8132\n",
      "Epoch 14/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4333 - accuracy: 0.8205\n",
      "Epoch 15/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4186 - accuracy: 0.8302\n",
      "Epoch 16/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4294 - accuracy: 0.8278\n",
      "Epoch 17/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4195 - accuracy: 0.8375\n",
      "Epoch 18/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.4159 - accuracy: 0.8418\n",
      "Epoch 19/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3958 - accuracy: 0.8379\n",
      "Epoch 20/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3783 - accuracy: 0.8515\n",
      "Epoch 21/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3857 - accuracy: 0.8389\n",
      "Epoch 22/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3698 - accuracy: 0.8559\n",
      "Epoch 23/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3496 - accuracy: 0.8627\n",
      "Epoch 24/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3430 - accuracy: 0.8598\n",
      "Epoch 25/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3513 - accuracy: 0.8607\n",
      "Epoch 26/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3337 - accuracy: 0.8656\n",
      "Epoch 27/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3408 - accuracy: 0.8574\n",
      "Epoch 28/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8680\n",
      "Epoch 29/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3353 - accuracy: 0.8574\n",
      "Epoch 30/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3245 - accuracy: 0.8685\n",
      "Epoch 31/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3211 - accuracy: 0.8753\n",
      "Epoch 32/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3130 - accuracy: 0.8768\n",
      "Epoch 33/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2963 - accuracy: 0.8845\n",
      "Epoch 34/1500\n",
      "65/65 [==============================] - 0s 974us/step - loss: 0.3228 - accuracy: 0.8680\n",
      "Epoch 35/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3110 - accuracy: 0.8772\n",
      "Epoch 36/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3082 - accuracy: 0.8797\n",
      "Epoch 37/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2873 - accuracy: 0.8928\n",
      "Epoch 38/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2711 - accuracy: 0.8869\n",
      "Epoch 39/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2946 - accuracy: 0.8811\n",
      "Epoch 40/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2893 - accuracy: 0.8874\n",
      "Epoch 41/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3064 - accuracy: 0.8768\n",
      "Epoch 42/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.3013 - accuracy: 0.8865\n",
      "Epoch 43/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2824 - accuracy: 0.8850\n",
      "Epoch 44/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2756 - accuracy: 0.8918\n",
      "Epoch 45/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2821 - accuracy: 0.8933\n",
      "Epoch 46/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2603 - accuracy: 0.9039\n",
      "Epoch 47/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2784 - accuracy: 0.8908\n",
      "Epoch 48/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2579 - accuracy: 0.8996\n",
      "Epoch 49/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2444 - accuracy: 0.9044\n",
      "Epoch 50/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2664 - accuracy: 0.8928\n",
      "Epoch 51/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2483 - accuracy: 0.9093\n",
      "Epoch 52/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2468 - accuracy: 0.9039\n",
      "Epoch 53/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2691 - accuracy: 0.9005\n",
      "Epoch 54/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.9098\n",
      "Epoch 55/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2623 - accuracy: 0.9000\n",
      "Epoch 56/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.9122\n",
      "Epoch 57/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2295 - accuracy: 0.9175\n",
      "Epoch 58/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2370 - accuracy: 0.9064\n",
      "Epoch 59/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2385 - accuracy: 0.9064\n",
      "Epoch 60/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2099 - accuracy: 0.9219\n",
      "Epoch 61/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2277 - accuracy: 0.9093\n",
      "Epoch 62/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2326 - accuracy: 0.9199\n",
      "Epoch 63/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2428 - accuracy: 0.9122\n",
      "Epoch 64/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2388 - accuracy: 0.9025\n",
      "Epoch 65/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2103 - accuracy: 0.9229\n",
      "Epoch 66/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2118 - accuracy: 0.9170\n",
      "Epoch 67/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9233\n",
      "Epoch 68/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9219\n",
      "Epoch 69/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2075 - accuracy: 0.9199\n",
      "Epoch 70/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2012 - accuracy: 0.9243\n",
      "Epoch 71/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2000 - accuracy: 0.9165\n",
      "Epoch 72/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2093 - accuracy: 0.9185\n",
      "Epoch 73/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9199\n",
      "Epoch 74/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9287\n",
      "Epoch 75/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2233 - accuracy: 0.9131\n",
      "Epoch 76/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2132 - accuracy: 0.9262\n",
      "Epoch 77/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2141 - accuracy: 0.9195\n",
      "Epoch 78/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1998 - accuracy: 0.9233\n",
      "Epoch 79/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2047 - accuracy: 0.9287\n",
      "Epoch 80/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9229\n",
      "Epoch 81/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1896 - accuracy: 0.9340\n",
      "Epoch 82/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.2132 - accuracy: 0.9209\n",
      "Epoch 83/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1965 - accuracy: 0.9306\n",
      "Epoch 84/1500\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1833 - accuracy: 0.9301\n",
      "Epoch 85/1500\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.1900 - accuracy: 0.9301\n",
      "Epoch 86/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9355\n",
      "Epoch 87/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9277\n",
      "Epoch 88/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1950 - accuracy: 0.9296\n",
      "Epoch 89/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9355\n",
      "Epoch 90/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9267\n",
      "Epoch 91/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1944 - accuracy: 0.9296\n",
      "Epoch 92/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1968 - accuracy: 0.9262\n",
      "Epoch 93/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1865 - accuracy: 0.9355\n",
      "Epoch 94/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1808 - accuracy: 0.9272\n",
      "Epoch 95/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9389\n",
      "Epoch 96/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.9311\n",
      "Epoch 97/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1729 - accuracy: 0.9398\n",
      "Epoch 98/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.9355\n",
      "Epoch 99/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9442\n",
      "Epoch 100/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1752 - accuracy: 0.9408\n",
      "Epoch 101/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9316\n",
      "Epoch 102/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9330\n",
      "Epoch 103/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1663 - accuracy: 0.9389\n",
      "Epoch 104/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1759 - accuracy: 0.9296\n",
      "Epoch 105/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1632 - accuracy: 0.9418\n",
      "Epoch 106/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9330\n",
      "Epoch 107/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1727 - accuracy: 0.9340\n",
      "Epoch 108/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1655 - accuracy: 0.9345\n",
      "Epoch 109/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9403\n",
      "Epoch 110/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1778 - accuracy: 0.9321\n",
      "Epoch 111/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.9340\n",
      "Epoch 112/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1727 - accuracy: 0.9418\n",
      "Epoch 113/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.9471\n",
      "Epoch 114/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1527 - accuracy: 0.9418\n",
      "Epoch 115/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1596 - accuracy: 0.9355\n",
      "Epoch 116/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9374\n",
      "Epoch 117/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.9452\n",
      "Epoch 118/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9427\n",
      "Epoch 119/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.9364\n",
      "Epoch 120/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1602 - accuracy: 0.9457\n",
      "Epoch 121/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9452\n",
      "Epoch 122/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9437\n",
      "Epoch 123/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9393\n",
      "Epoch 124/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1584 - accuracy: 0.9418\n",
      "Epoch 125/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.9413\n",
      "Epoch 126/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9466\n",
      "Epoch 127/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9481\n",
      "Epoch 128/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9461\n",
      "Epoch 129/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9418\n",
      "Epoch 130/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1533 - accuracy: 0.9442\n",
      "Epoch 131/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9452\n",
      "Epoch 132/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9350\n",
      "Epoch 133/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9452\n",
      "Epoch 134/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9544\n",
      "Epoch 135/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9408\n",
      "Epoch 136/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9447\n",
      "Epoch 137/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1443 - accuracy: 0.9408\n",
      "Epoch 138/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1566 - accuracy: 0.9447\n",
      "Epoch 139/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9520\n",
      "Epoch 140/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9510\n",
      "Epoch 141/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9457\n",
      "Epoch 142/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1655 - accuracy: 0.9461\n",
      "Epoch 143/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9515\n",
      "Epoch 144/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9505\n",
      "Epoch 145/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9544\n",
      "Epoch 146/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9520\n",
      "Epoch 147/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1525 - accuracy: 0.9491\n",
      "Epoch 148/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1592 - accuracy: 0.9413\n",
      "Epoch 149/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9520\n",
      "Epoch 150/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9476\n",
      "Epoch 151/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9544\n",
      "Epoch 152/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.9558\n",
      "Epoch 153/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9495\n",
      "Epoch 154/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9452\n",
      "Epoch 155/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9515\n",
      "Epoch 156/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9597\n",
      "Epoch 157/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9563\n",
      "Epoch 158/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9583\n",
      "Epoch 159/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9476\n",
      "Epoch 160/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.9457\n",
      "Epoch 161/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9466\n",
      "Epoch 162/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9432\n",
      "Epoch 163/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9583\n",
      "Epoch 164/1500\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1192 - accuracy: 0.9573\n",
      "Epoch 165/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9534\n",
      "Epoch 166/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9432\n",
      "Epoch 167/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9461\n",
      "Epoch 168/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9607\n",
      "Epoch 169/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9486\n",
      "Epoch 170/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9558\n",
      "Epoch 171/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9583\n",
      "Epoch 172/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9607\n",
      "Epoch 173/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1229 - accuracy: 0.9568\n",
      "Epoch 174/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1414 - accuracy: 0.9539\n",
      "Epoch 175/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9592\n",
      "Epoch 176/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9525\n",
      "Epoch 177/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9588\n",
      "Epoch 178/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9588\n",
      "Epoch 179/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9626\n",
      "Epoch 180/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9563\n",
      "Epoch 181/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9622\n",
      "Epoch 182/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9529\n",
      "Epoch 183/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9544\n",
      "Epoch 184/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9578\n",
      "Epoch 185/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9592\n",
      "Epoch 186/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9675\n",
      "Epoch 187/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9631\n",
      "Epoch 188/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9549\n",
      "Epoch 189/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9578\n",
      "Epoch 190/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9607\n",
      "Epoch 191/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9656\n",
      "Epoch 192/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9563\n",
      "Epoch 193/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9680\n",
      "Epoch 194/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9583\n",
      "Epoch 195/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9539\n",
      "Epoch 196/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9544\n",
      "Epoch 197/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9602\n",
      "Epoch 198/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9597\n",
      "Epoch 199/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9554\n",
      "Epoch 200/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9592\n",
      "Epoch 201/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9626\n",
      "Epoch 202/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9602\n",
      "Epoch 203/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9573\n",
      "Epoch 204/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9500\n",
      "Epoch 205/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9651\n",
      "Epoch 206/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9602\n",
      "Epoch 207/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9617\n",
      "Epoch 208/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9495\n",
      "Epoch 209/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9709\n",
      "Epoch 210/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9583\n",
      "Epoch 211/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9631\n",
      "Epoch 212/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9631\n",
      "Epoch 213/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9660\n",
      "Epoch 214/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9738\n",
      "Epoch 215/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9704\n",
      "Epoch 216/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9626\n",
      "Epoch 217/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9636\n",
      "Epoch 218/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9505\n",
      "Epoch 219/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9641\n",
      "Epoch 220/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9641\n",
      "Epoch 221/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9675\n",
      "Epoch 222/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9704\n",
      "Epoch 223/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9689\n",
      "Epoch 224/1500\n",
      "65/65 [==============================] - 0s 4ms/step - loss: 0.1044 - accuracy: 0.9680\n",
      "Epoch 225/1500\n",
      "65/65 [==============================] - 0s 2ms/step - loss: 0.1032 - accuracy: 0.9641\n",
      "Epoch 226/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9583\n",
      "Epoch 227/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9636\n",
      "Epoch 228/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9583\n",
      "Epoch 229/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9563\n",
      "Epoch 230/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9495\n",
      "Epoch 231/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9607\n",
      "Epoch 232/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9660\n",
      "Epoch 233/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9660\n",
      "Epoch 234/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9626\n",
      "Epoch 235/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9665\n",
      "Epoch 236/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9757\n",
      "Epoch 237/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9714\n",
      "Epoch 238/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.9689\n",
      "Epoch 239/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9636\n",
      "Epoch 240/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9685\n",
      "Epoch 241/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9641\n",
      "Epoch 242/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9631\n",
      "Epoch 243/1500\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9656\n",
      "Epoch 244/1500\n",
      "47/65 [====================>.........] - ETA: 0s - loss: 0.1102 - accuracy: 0.9634Restoring model weights from the end of the best epoch: 214.\n",
      "65/65 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.9646\n",
      "Epoch 244: early stopping\n",
      "9/9 [==============================] - 0s 893us/step - loss: 0.9118 - accuracy: 0.6828\n",
      "9/9 [==============================] - 0s 809us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (20/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "Final Test Results - Loss: 0.9117901921272278, Accuracy: 0.6828358173370361, Precision: 0.6818966058096493, Recall: 0.661619373776908, F1 Score: 0.6419510371446286\n",
      "Confusion Matrix:\n",
      " [[118   3  39]\n",
      " [ 32  41   0]\n",
      " [ 11   0  24]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "010A     8\n",
      "099A     7\n",
      "050A     7\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "053A     6\n",
      "108A     6\n",
      "023A     6\n",
      "007A     6\n",
      "025C     5\n",
      "044A     5\n",
      "023B     5\n",
      "021A     5\n",
      "070A     5\n",
      "034A     5\n",
      "003A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "104A     4\n",
      "058A     3\n",
      "012A     3\n",
      "006A     3\n",
      "014A     3\n",
      "113A     3\n",
      "056A     3\n",
      "018A     2\n",
      "038A     2\n",
      "025B     2\n",
      "054A     2\n",
      "032A     2\n",
      "069A     2\n",
      "102A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "073A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "063A    11\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "045A     9\n",
      "095A     8\n",
      "037A     6\n",
      "008A     6\n",
      "109A     6\n",
      "075A     5\n",
      "105A     4\n",
      "064A     3\n",
      "060A     3\n",
      "093A     2\n",
      "087A     2\n",
      "011A     2\n",
      "061A     2\n",
      "076A     1\n",
      "043A     1\n",
      "091A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    238\n",
      "F    229\n",
      "X    221\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    127\n",
      "M     99\n",
      "F     23\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 015A, 103A, 071A, 097B, 028A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 050A, 049...\n",
      "senior    [097A, 057A, 106A, 104A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 001A, 019A, 067A, 022A, 029A, 095A, 072...\n",
      "kitten                             [046A, 109A, 043A, 045A]\n",
      "senior                             [093A, 051B, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '040A' '041A'\n",
      " '042A' '044A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '043A'\n",
      " '045A' '046A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'040A'}\n",
      "Moved to Test Set:\n",
      "{'040A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '006A' '007A' '009A'\n",
      " '010A' '012A' '013B' '014A' '014B' '015A' '016A' '018A' '019B' '020A'\n",
      " '021A' '023A' '023B' '024A' '025A' '025B' '025C' '026A' '026C' '027A'\n",
      " '028A' '031A' '032A' '034A' '035A' '036A' '038A' '039A' '041A' '042A'\n",
      " '044A' '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A'\n",
      " '055A' '056A' '057A' '058A' '059A' '062A' '066A' '068A' '069A' '070A'\n",
      " '071A' '073A' '074A' '088A' '090A' '092A' '094A' '096A' '097A' '097B'\n",
      " '099A' '101A' '102A' '103A' '104A' '106A' '108A' '110A' '111A' '113A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '008A' '011A' '019A' '022A' '026B' '029A' '033A' '037A' '040A'\n",
      " '043A' '045A' '051B' '060A' '061A' '063A' '064A' '065A' '067A' '072A'\n",
      " '075A' '076A' '087A' '091A' '093A' '095A' '100A' '105A' '109A']\n",
      "Length of X_train_val:\n",
      "741\n",
      "Length of y_train_val:\n",
      "741\n",
      "Length of groups_train_val:\n",
      "741\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     155\n",
      "kitten     79\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     433\n",
      "senior    163\n",
      "kitten    145\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     155\n",
      "kitten     26\n",
      "senior     15\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 866, 2: 815, 1: 725})\n",
      "Epoch 1/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.8640 - accuracy: 0.6259\n",
      "Epoch 2/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.6863 - accuracy: 0.7178\n",
      "Epoch 3/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5963 - accuracy: 0.7577\n",
      "Epoch 4/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5642 - accuracy: 0.7772\n",
      "Epoch 5/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5213 - accuracy: 0.7922\n",
      "Epoch 6/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.5221 - accuracy: 0.7918\n",
      "Epoch 7/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4884 - accuracy: 0.8022\n",
      "Epoch 8/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4803 - accuracy: 0.8092\n",
      "Epoch 9/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4492 - accuracy: 0.8163\n",
      "Epoch 10/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4427 - accuracy: 0.8221\n",
      "Epoch 11/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4369 - accuracy: 0.8329\n",
      "Epoch 12/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4291 - accuracy: 0.8317\n",
      "Epoch 13/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.4038 - accuracy: 0.8379\n",
      "Epoch 14/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3901 - accuracy: 0.8541\n",
      "Epoch 15/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3953 - accuracy: 0.8470\n",
      "Epoch 16/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3787 - accuracy: 0.8491\n",
      "Epoch 17/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3651 - accuracy: 0.8616\n",
      "Epoch 18/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3812 - accuracy: 0.8470\n",
      "Epoch 19/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3716 - accuracy: 0.8495\n",
      "Epoch 20/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3800 - accuracy: 0.8516\n",
      "Epoch 21/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3624 - accuracy: 0.8628\n",
      "Epoch 22/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3614 - accuracy: 0.8495\n",
      "Epoch 23/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3456 - accuracy: 0.8591\n",
      "Epoch 24/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3289 - accuracy: 0.8707\n",
      "Epoch 25/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3267 - accuracy: 0.8753\n",
      "Epoch 26/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3186 - accuracy: 0.8745\n",
      "Epoch 27/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3181 - accuracy: 0.8707\n",
      "Epoch 28/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3251 - accuracy: 0.8732\n",
      "Epoch 29/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3072 - accuracy: 0.8815\n",
      "Epoch 30/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2870 - accuracy: 0.8899\n",
      "Epoch 31/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2939 - accuracy: 0.8853\n",
      "Epoch 32/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.8861\n",
      "Epoch 33/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2904 - accuracy: 0.8907\n",
      "Epoch 34/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.3092 - accuracy: 0.8720\n",
      "Epoch 35/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2866 - accuracy: 0.8890\n",
      "Epoch 36/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2865 - accuracy: 0.8886\n",
      "Epoch 37/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2838 - accuracy: 0.8882\n",
      "Epoch 38/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2700 - accuracy: 0.8924\n",
      "Epoch 39/1500\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.2759 - accuracy: 0.8948\n",
      "Epoch 40/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2666 - accuracy: 0.9027\n",
      "Epoch 41/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.9061\n",
      "Epoch 42/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2555 - accuracy: 0.9040\n",
      "Epoch 43/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2616 - accuracy: 0.8903\n",
      "Epoch 44/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2701 - accuracy: 0.8998\n",
      "Epoch 45/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.9040\n",
      "Epoch 46/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2526 - accuracy: 0.9081\n",
      "Epoch 47/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2357 - accuracy: 0.9152\n",
      "Epoch 48/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2403 - accuracy: 0.9094\n",
      "Epoch 49/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2381 - accuracy: 0.9123\n",
      "Epoch 50/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2543 - accuracy: 0.9123\n",
      "Epoch 51/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2323 - accuracy: 0.9090\n",
      "Epoch 52/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2350 - accuracy: 0.9119\n",
      "Epoch 53/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2398 - accuracy: 0.9102\n",
      "Epoch 54/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2289 - accuracy: 0.9102\n",
      "Epoch 55/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2266 - accuracy: 0.9173\n",
      "Epoch 56/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2473 - accuracy: 0.9023\n",
      "Epoch 57/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2223 - accuracy: 0.9148\n",
      "Epoch 58/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2405 - accuracy: 0.9048\n",
      "Epoch 59/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2254 - accuracy: 0.9190\n",
      "Epoch 60/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2095 - accuracy: 0.9219\n",
      "Epoch 61/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2155 - accuracy: 0.9194\n",
      "Epoch 62/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2317 - accuracy: 0.9131\n",
      "Epoch 63/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2056 - accuracy: 0.9239\n",
      "Epoch 64/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9190\n",
      "Epoch 65/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9323\n",
      "Epoch 66/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2006 - accuracy: 0.9256\n",
      "Epoch 67/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2026 - accuracy: 0.9235\n",
      "Epoch 68/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1988 - accuracy: 0.9264\n",
      "Epoch 69/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9223\n",
      "Epoch 70/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2126 - accuracy: 0.9227\n",
      "Epoch 71/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9273\n",
      "Epoch 72/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1943 - accuracy: 0.9293\n",
      "Epoch 73/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9318\n",
      "Epoch 74/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9323\n",
      "Epoch 75/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9298\n",
      "Epoch 76/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 0.9331\n",
      "Epoch 77/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2102 - accuracy: 0.9227\n",
      "Epoch 78/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9256\n",
      "Epoch 79/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9352\n",
      "Epoch 80/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9248\n",
      "Epoch 81/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9214\n",
      "Epoch 82/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1979 - accuracy: 0.9235\n",
      "Epoch 83/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1763 - accuracy: 0.9368\n",
      "Epoch 84/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1927 - accuracy: 0.9268\n",
      "Epoch 85/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1843 - accuracy: 0.9335\n",
      "Epoch 86/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1655 - accuracy: 0.9401\n",
      "Epoch 87/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.2006 - accuracy: 0.9268\n",
      "Epoch 88/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9360\n",
      "Epoch 89/1500\n",
      "76/76 [==============================] - 0s 945us/step - loss: 0.1722 - accuracy: 0.9327\n",
      "Epoch 90/1500\n",
      "76/76 [==============================] - 0s 930us/step - loss: 0.1699 - accuracy: 0.9401\n",
      "Epoch 91/1500\n",
      "76/76 [==============================] - 0s 963us/step - loss: 0.1722 - accuracy: 0.9393\n",
      "Epoch 92/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1781 - accuracy: 0.9281\n",
      "Epoch 93/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.9347\n",
      "Epoch 94/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.9443\n",
      "Epoch 95/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9406\n",
      "Epoch 96/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9447\n",
      "Epoch 97/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9514\n",
      "Epoch 98/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9468\n",
      "Epoch 99/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9364\n",
      "Epoch 100/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.9397\n",
      "Epoch 101/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1674 - accuracy: 0.9418\n",
      "Epoch 102/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9456\n",
      "Epoch 103/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.9360\n",
      "Epoch 104/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9534\n",
      "Epoch 105/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9397\n",
      "Epoch 106/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9493\n",
      "Epoch 107/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9422\n",
      "Epoch 108/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9422\n",
      "Epoch 109/1500\n",
      "76/76 [==============================] - 0s 3ms/step - loss: 0.1640 - accuracy: 0.9352\n",
      "Epoch 110/1500\n",
      "76/76 [==============================] - 0s 2ms/step - loss: 0.1542 - accuracy: 0.9439\n",
      "Epoch 111/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9510\n",
      "Epoch 112/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9510\n",
      "Epoch 113/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1494 - accuracy: 0.9426\n",
      "Epoch 114/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9377\n",
      "Epoch 115/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9493\n",
      "Epoch 116/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9472\n",
      "Epoch 117/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9451\n",
      "Epoch 118/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1535 - accuracy: 0.9489\n",
      "Epoch 119/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.9480\n",
      "Epoch 120/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9464\n",
      "Epoch 121/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.9464\n",
      "Epoch 122/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9489\n",
      "Epoch 123/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9485\n",
      "Epoch 124/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9401\n",
      "Epoch 125/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9460\n",
      "Epoch 126/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9497\n",
      "Epoch 127/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9439\n",
      "Epoch 128/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9630\n",
      "Epoch 129/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1277 - accuracy: 0.9522\n",
      "Epoch 130/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1351 - accuracy: 0.9468\n",
      "Epoch 131/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9472\n",
      "Epoch 132/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9522\n",
      "Epoch 133/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.9534\n",
      "Epoch 134/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9559\n",
      "Epoch 135/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9480\n",
      "Epoch 136/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9543\n",
      "Epoch 137/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9505\n",
      "Epoch 138/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9472\n",
      "Epoch 139/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9589\n",
      "Epoch 140/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9601\n",
      "Epoch 141/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9510\n",
      "Epoch 142/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9510\n",
      "Epoch 143/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9547\n",
      "Epoch 144/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9510\n",
      "Epoch 145/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9451\n",
      "Epoch 146/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9576\n",
      "Epoch 147/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9514\n",
      "Epoch 148/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9510\n",
      "Epoch 149/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9605\n",
      "Epoch 150/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9564\n",
      "Epoch 151/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9547\n",
      "Epoch 152/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9609\n",
      "Epoch 153/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9659\n",
      "Epoch 154/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9601\n",
      "Epoch 155/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9597\n",
      "Epoch 156/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9576\n",
      "Epoch 157/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1388 - accuracy: 0.9476\n",
      "Epoch 158/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9597\n",
      "Epoch 159/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9634\n",
      "Epoch 160/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9622\n",
      "Epoch 161/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9580\n",
      "Epoch 162/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9601\n",
      "Epoch 163/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9589\n",
      "Epoch 164/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9601\n",
      "Epoch 165/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1140 - accuracy: 0.9605\n",
      "Epoch 166/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9526\n",
      "Epoch 167/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9514\n",
      "Epoch 168/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9505\n",
      "Epoch 169/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9555\n",
      "Epoch 170/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9547\n",
      "Epoch 171/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9543\n",
      "Epoch 172/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9597\n",
      "Epoch 173/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9593\n",
      "Epoch 174/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9601\n",
      "Epoch 175/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9551\n",
      "Epoch 176/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9655\n",
      "Epoch 177/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1327 - accuracy: 0.9489\n",
      "Epoch 178/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9551\n",
      "Epoch 179/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9447\n",
      "Epoch 180/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9559\n",
      "Epoch 181/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9576\n",
      "Epoch 182/1500\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.1212 - accuracy: 0.9559\n",
      "Epoch 183/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9643\n",
      "Epoch 184/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1164 - accuracy: 0.9576\n",
      "Epoch 185/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9589\n",
      "Epoch 186/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9667\n",
      "Epoch 187/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9597\n",
      "Epoch 188/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9593\n",
      "Epoch 189/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1135 - accuracy: 0.9605\n",
      "Epoch 190/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9622\n",
      "Epoch 191/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1175 - accuracy: 0.9564\n",
      "Epoch 192/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9618\n",
      "Epoch 193/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9688\n",
      "Epoch 194/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9626\n",
      "Epoch 195/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9584\n",
      "Epoch 196/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9609\n",
      "Epoch 197/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9597\n",
      "Epoch 198/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9638\n",
      "Epoch 199/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9634\n",
      "Epoch 200/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1002 - accuracy: 0.9655\n",
      "Epoch 201/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9692\n",
      "Epoch 202/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9601\n",
      "Epoch 203/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9630\n",
      "Epoch 204/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9626\n",
      "Epoch 205/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9742\n",
      "Epoch 206/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9651\n",
      "Epoch 207/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9634\n",
      "Epoch 208/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9551\n",
      "Epoch 209/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9605\n",
      "Epoch 210/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9688\n",
      "Epoch 211/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.9680\n",
      "Epoch 212/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9705\n",
      "Epoch 213/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9630\n",
      "Epoch 214/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.9493\n",
      "Epoch 215/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9638\n",
      "Epoch 216/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9643\n",
      "Epoch 217/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.9722\n",
      "Epoch 218/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.9663\n",
      "Epoch 219/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9630\n",
      "Epoch 220/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9634\n",
      "Epoch 221/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9643\n",
      "Epoch 222/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9609\n",
      "Epoch 223/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9597\n",
      "Epoch 224/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9647\n",
      "Epoch 225/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9672\n",
      "Epoch 226/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9672\n",
      "Epoch 227/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9634\n",
      "Epoch 228/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9676\n",
      "Epoch 229/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9651\n",
      "Epoch 230/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9659\n",
      "Epoch 231/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9667\n",
      "Epoch 232/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9709\n",
      "Epoch 233/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9692\n",
      "Epoch 234/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9676\n",
      "Epoch 235/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9680\n",
      "Epoch 236/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9767\n",
      "Epoch 237/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9701\n",
      "Epoch 238/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9659\n",
      "Epoch 239/1500\n",
      "76/76 [==============================] - 0s 4ms/step - loss: 0.0809 - accuracy: 0.9717\n",
      "Epoch 240/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9680\n",
      "Epoch 241/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9692\n",
      "Epoch 242/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9742\n",
      "Epoch 243/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9717\n",
      "Epoch 244/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9663\n",
      "Epoch 245/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9738\n",
      "Epoch 246/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9697\n",
      "Epoch 247/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0960 - accuracy: 0.9647\n",
      "Epoch 248/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0929 - accuracy: 0.9655\n",
      "Epoch 249/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9593\n",
      "Epoch 250/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9692\n",
      "Epoch 251/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0749 - accuracy: 0.9746\n",
      "Epoch 252/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9763\n",
      "Epoch 253/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9701\n",
      "Epoch 254/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9763\n",
      "Epoch 255/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0810 - accuracy: 0.9709\n",
      "Epoch 256/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9655\n",
      "Epoch 257/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0818 - accuracy: 0.9746\n",
      "Epoch 258/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9722\n",
      "Epoch 259/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9659\n",
      "Epoch 260/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9663\n",
      "Epoch 261/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9780\n",
      "Epoch 262/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9663\n",
      "Epoch 263/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.9705\n",
      "Epoch 264/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9618\n",
      "Epoch 265/1500\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9684\n",
      "Epoch 266/1500\n",
      "48/76 [=================>............] - ETA: 0s - loss: 0.0784 - accuracy: 0.9772Restoring model weights from the end of the best epoch: 236.\n",
      "76/76 [==============================] - 0s 1ms/step - loss: 0.0816 - accuracy: 0.9751\n",
      "Epoch 266: early stopping\n",
      "7/7 [==============================] - 0s 856us/step - loss: 0.7008 - accuracy: 0.7194\n",
      "7/7 [==============================] - 0s 652us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.72 (21/29)\n",
      "Before appending - Cat IDs: 268, Predictions: 268, Actuals: 268, Gender: 268\n",
      "After appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "Final Test Results - Loss: 0.7007728219032288, Accuracy: 0.7193877696990967, Precision: 0.5776451602594829, Recall: 0.6891370278467054, F1 Score: 0.6025101568628313\n",
      "Confusion Matrix:\n",
      " [[111  13  31]\n",
      " [  3  23   0]\n",
      " [  8   0   7]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "042A    14\n",
      "097B    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "036A    11\n",
      "068A    11\n",
      "063A    11\n",
      "040A    10\n",
      "014B    10\n",
      "022A     9\n",
      "072A     9\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "015A     9\n",
      "094A     8\n",
      "095A     8\n",
      "117A     7\n",
      "050A     7\n",
      "099A     7\n",
      "027A     7\n",
      "031A     7\n",
      "008A     6\n",
      "109A     6\n",
      "053A     6\n",
      "037A     6\n",
      "108A     6\n",
      "023A     6\n",
      "023B     5\n",
      "075A     5\n",
      "021A     5\n",
      "105A     4\n",
      "026A     4\n",
      "062A     4\n",
      "035A     4\n",
      "052A     4\n",
      "003A     4\n",
      "056A     3\n",
      "113A     3\n",
      "064A     3\n",
      "014A     3\n",
      "060A     3\n",
      "012A     3\n",
      "058A     3\n",
      "061A     2\n",
      "011A     2\n",
      "102A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "069A     2\n",
      "041A     1\n",
      "019B     1\n",
      "024A     1\n",
      "100A     1\n",
      "110A     1\n",
      "096A     1\n",
      "076A     1\n",
      "088A     1\n",
      "092A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "073A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "000B    19\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "116A    12\n",
      "025A    11\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "010A     8\n",
      "013B     8\n",
      "007A     6\n",
      "070A     5\n",
      "044A     5\n",
      "034A     5\n",
      "025C     5\n",
      "104A     4\n",
      "009A     4\n",
      "006A     3\n",
      "018A     2\n",
      "054A     2\n",
      "025B     2\n",
      "032A     2\n",
      "049A     1\n",
      "026C     1\n",
      "066A     1\n",
      "115A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    322\n",
      "M    241\n",
      "F    159\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "F    93\n",
      "X    26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 001A, 103A, 097B, 019A, 074...\n",
      "kitten    [014B, 111A, 040A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 057A, 055A, 113A, 051B, 117A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 071A, 028A, 020A, 034A, 005A, 002A, 009...\n",
      "kitten                                   [044A, 049A, 115A]\n",
      "senior           [106A, 104A, 059A, 116A, 054A, 016A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 13, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 3, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '003A' '004A' '008A' '011A' '012A' '014A' '014B'\n",
      " '015A' '019A' '019B' '021A' '022A' '023A' '023B' '024A' '026A' '026B'\n",
      " '027A' '029A' '031A' '033A' '035A' '036A' '037A' '038A' '039A' '040A'\n",
      " '041A' '042A' '043A' '045A' '046A' '047A' '048A' '050A' '051A' '051B'\n",
      " '052A' '053A' '055A' '056A' '057A' '058A' '060A' '061A' '062A' '063A'\n",
      " '064A' '065A' '067A' '068A' '069A' '072A' '073A' '074A' '075A' '076A'\n",
      " '087A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A'\n",
      " '113A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '005A' '006A' '007A' '009A' '010A' '013B' '016A' '018A'\n",
      " '020A' '025A' '025B' '025C' '026C' '028A' '032A' '034A' '044A' '049A'\n",
      " '054A' '059A' '066A' '070A' '071A' '090A' '104A' '106A' '115A' '116A']\n",
      "Length of X_train_val:\n",
      "722\n",
      "Length of y_train_val:\n",
      "722\n",
      "Length of groups_train_val:\n",
      "722\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     437\n",
      "kitten    164\n",
      "senior    121\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     151\n",
      "senior     57\n",
      "kitten      7\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 874, 1: 820, 2: 605})\n",
      "Epoch 1/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.9527 - accuracy: 0.6107\n",
      "Epoch 2/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.7235 - accuracy: 0.7086\n",
      "Epoch 3/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.6298 - accuracy: 0.7495\n",
      "Epoch 4/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5735 - accuracy: 0.7638\n",
      "Epoch 5/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5647 - accuracy: 0.7764\n",
      "Epoch 6/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.5316 - accuracy: 0.7869\n",
      "Epoch 7/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4670 - accuracy: 0.8143\n",
      "Epoch 8/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4853 - accuracy: 0.8047\n",
      "Epoch 9/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4520 - accuracy: 0.8204\n",
      "Epoch 10/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4461 - accuracy: 0.8134\n",
      "Epoch 11/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4314 - accuracy: 0.8325\n",
      "Epoch 12/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4127 - accuracy: 0.8308\n",
      "Epoch 13/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.4085 - accuracy: 0.8360\n",
      "Epoch 14/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3942 - accuracy: 0.8330\n",
      "Epoch 15/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3924 - accuracy: 0.8478\n",
      "Epoch 16/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3569 - accuracy: 0.8538\n",
      "Epoch 17/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3572 - accuracy: 0.8499\n",
      "Epoch 18/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3641 - accuracy: 0.8534\n",
      "Epoch 19/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3657 - accuracy: 0.8486\n",
      "Epoch 20/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3331 - accuracy: 0.8669\n",
      "Epoch 21/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8665\n",
      "Epoch 22/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3104 - accuracy: 0.8860\n",
      "Epoch 23/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3423 - accuracy: 0.8647\n",
      "Epoch 24/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3115 - accuracy: 0.8630\n",
      "Epoch 25/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3062 - accuracy: 0.8826\n",
      "Epoch 26/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3123 - accuracy: 0.8778\n",
      "Epoch 27/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.3168 - accuracy: 0.8730\n",
      "Epoch 28/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2979 - accuracy: 0.8799\n",
      "Epoch 29/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2993 - accuracy: 0.8847\n",
      "Epoch 30/1500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.3036 - accuracy: 0.8886\n",
      "Epoch 31/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2974 - accuracy: 0.8795\n",
      "Epoch 32/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2859 - accuracy: 0.8847\n",
      "Epoch 33/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2876 - accuracy: 0.8882\n",
      "Epoch 34/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2682 - accuracy: 0.8908\n",
      "Epoch 35/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2702 - accuracy: 0.8987\n",
      "Epoch 36/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2634 - accuracy: 0.9078\n",
      "Epoch 37/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2782 - accuracy: 0.8891\n",
      "Epoch 38/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2578 - accuracy: 0.9004\n",
      "Epoch 39/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.9047\n",
      "Epoch 40/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2652 - accuracy: 0.8978\n",
      "Epoch 41/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2497 - accuracy: 0.9074\n",
      "Epoch 42/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2621 - accuracy: 0.9047\n",
      "Epoch 43/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.9060\n",
      "Epoch 44/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2559 - accuracy: 0.9065\n",
      "Epoch 45/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2400 - accuracy: 0.9026\n",
      "Epoch 46/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2371 - accuracy: 0.9065\n",
      "Epoch 47/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2488 - accuracy: 0.9078\n",
      "Epoch 48/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2209 - accuracy: 0.9161\n",
      "Epoch 49/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2351 - accuracy: 0.9065\n",
      "Epoch 50/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9100\n",
      "Epoch 51/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2340 - accuracy: 0.9104\n",
      "Epoch 52/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9078\n",
      "Epoch 53/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2477 - accuracy: 0.9082\n",
      "Epoch 54/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2240 - accuracy: 0.9130\n",
      "Epoch 55/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2361 - accuracy: 0.9143\n",
      "Epoch 56/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2268 - accuracy: 0.9165\n",
      "Epoch 57/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9274\n",
      "Epoch 58/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2259 - accuracy: 0.9152\n",
      "Epoch 59/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2022 - accuracy: 0.9239\n",
      "Epoch 60/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2082 - accuracy: 0.9234\n",
      "Epoch 61/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2013 - accuracy: 0.9265\n",
      "Epoch 62/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2150 - accuracy: 0.9200\n",
      "Epoch 63/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2075 - accuracy: 0.9204\n",
      "Epoch 64/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2284 - accuracy: 0.9143\n",
      "Epoch 65/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1939 - accuracy: 0.9308\n",
      "Epoch 66/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1826 - accuracy: 0.9374\n",
      "Epoch 67/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2006 - accuracy: 0.9282\n",
      "Epoch 68/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2010 - accuracy: 0.9287\n",
      "Epoch 69/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9330\n",
      "Epoch 70/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2026 - accuracy: 0.9208\n",
      "Epoch 71/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9387\n",
      "Epoch 72/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9239\n",
      "Epoch 73/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2030 - accuracy: 0.9265\n",
      "Epoch 74/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.9261\n",
      "Epoch 75/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9321\n",
      "Epoch 76/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9282\n",
      "Epoch 77/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1750 - accuracy: 0.9317\n",
      "Epoch 78/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.9382\n",
      "Epoch 79/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.2102 - accuracy: 0.9152\n",
      "Epoch 80/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9282\n",
      "Epoch 81/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1901 - accuracy: 0.9321\n",
      "Epoch 82/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1858 - accuracy: 0.9334\n",
      "Epoch 83/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.9274\n",
      "Epoch 84/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9348\n",
      "Epoch 85/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1778 - accuracy: 0.9378\n",
      "Epoch 86/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1666 - accuracy: 0.9435\n",
      "Epoch 87/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9343\n",
      "Epoch 88/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1575 - accuracy: 0.9435\n",
      "Epoch 89/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1677 - accuracy: 0.9430\n",
      "Epoch 90/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1678 - accuracy: 0.9361\n",
      "Epoch 91/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9282\n",
      "Epoch 92/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.9378\n",
      "Epoch 93/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1795 - accuracy: 0.9313\n",
      "Epoch 94/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1635 - accuracy: 0.9391\n",
      "Epoch 95/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9452\n",
      "Epoch 96/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9382\n",
      "Epoch 97/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1632 - accuracy: 0.9404\n",
      "Epoch 98/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1575 - accuracy: 0.9452\n",
      "Epoch 99/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9452\n",
      "Epoch 100/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9478\n",
      "Epoch 101/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1506 - accuracy: 0.9491\n",
      "Epoch 102/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9482\n",
      "Epoch 103/1500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1542 - accuracy: 0.9439\n",
      "Epoch 104/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1932 - accuracy: 0.9213\n",
      "Epoch 105/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1589 - accuracy: 0.9339\n",
      "Epoch 106/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9404\n",
      "Epoch 107/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1470 - accuracy: 0.9508\n",
      "Epoch 108/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9439\n",
      "Epoch 109/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9482\n",
      "Epoch 110/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1446 - accuracy: 0.9487\n",
      "Epoch 111/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.9491\n",
      "Epoch 112/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1582 - accuracy: 0.9491\n",
      "Epoch 113/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1699 - accuracy: 0.9391\n",
      "Epoch 114/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9430\n",
      "Epoch 115/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1520 - accuracy: 0.9430\n",
      "Epoch 116/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1643 - accuracy: 0.9378\n",
      "Epoch 117/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9491\n",
      "Epoch 118/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9478\n",
      "Epoch 119/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.9443\n",
      "Epoch 120/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9426\n",
      "Epoch 121/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9504\n",
      "Epoch 122/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1534 - accuracy: 0.9408\n",
      "Epoch 123/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9461\n",
      "Epoch 124/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9482\n",
      "Epoch 125/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1462 - accuracy: 0.9439\n",
      "Epoch 126/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1449 - accuracy: 0.9421\n",
      "Epoch 127/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9526\n",
      "Epoch 128/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9535\n",
      "Epoch 129/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1437 - accuracy: 0.9526\n",
      "Epoch 130/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9448\n",
      "Epoch 131/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9517\n",
      "Epoch 132/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9504\n",
      "Epoch 133/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1167 - accuracy: 0.9595\n",
      "Epoch 134/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9504\n",
      "Epoch 135/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9565\n",
      "Epoch 136/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9556\n",
      "Epoch 137/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9487\n",
      "Epoch 138/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9539\n",
      "Epoch 139/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9595\n",
      "Epoch 140/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9465\n",
      "Epoch 141/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9430\n",
      "Epoch 142/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1261 - accuracy: 0.9548\n",
      "Epoch 143/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1302 - accuracy: 0.9530\n",
      "Epoch 144/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9556\n",
      "Epoch 145/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9582\n",
      "Epoch 146/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9565\n",
      "Epoch 147/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9543\n",
      "Epoch 148/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9535\n",
      "Epoch 149/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9591\n",
      "Epoch 150/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9517\n",
      "Epoch 151/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9526\n",
      "Epoch 152/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9561\n",
      "Epoch 153/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9535\n",
      "Epoch 154/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9565\n",
      "Epoch 155/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1244 - accuracy: 0.9582\n",
      "Epoch 156/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9522\n",
      "Epoch 157/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9569\n",
      "Epoch 158/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9508\n",
      "Epoch 159/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9530\n",
      "Epoch 160/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9526\n",
      "Epoch 161/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9548\n",
      "Epoch 162/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9600\n",
      "Epoch 163/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9635\n",
      "Epoch 164/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9591\n",
      "Epoch 165/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9656\n",
      "Epoch 166/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9613\n",
      "Epoch 167/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9548\n",
      "Epoch 168/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9626\n",
      "Epoch 169/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9635\n",
      "Epoch 170/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9582\n",
      "Epoch 171/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9595\n",
      "Epoch 172/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9552\n",
      "Epoch 173/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9656\n",
      "Epoch 174/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9643\n",
      "Epoch 175/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9591\n",
      "Epoch 176/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9622\n",
      "Epoch 177/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9648\n",
      "Epoch 178/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9613\n",
      "Epoch 179/1500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.1096 - accuracy: 0.9604\n",
      "Epoch 180/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1217 - accuracy: 0.9574\n",
      "Epoch 181/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9604\n",
      "Epoch 182/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1149 - accuracy: 0.9643\n",
      "Epoch 183/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9639\n",
      "Epoch 184/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9617\n",
      "Epoch 185/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9626\n",
      "Epoch 186/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0914 - accuracy: 0.9687\n",
      "Epoch 187/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9626\n",
      "Epoch 188/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9604\n",
      "Epoch 189/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9639\n",
      "Epoch 190/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9665\n",
      "Epoch 191/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9517\n",
      "Epoch 192/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9561\n",
      "Epoch 193/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9561\n",
      "Epoch 194/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1050 - accuracy: 0.9600\n",
      "Epoch 195/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9735\n",
      "Epoch 196/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9613\n",
      "Epoch 197/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1048 - accuracy: 0.9604\n",
      "Epoch 198/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9600\n",
      "Epoch 199/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1052 - accuracy: 0.9639\n",
      "Epoch 200/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9665\n",
      "Epoch 201/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9582\n",
      "Epoch 202/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9630\n",
      "Epoch 203/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9591\n",
      "Epoch 204/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9600\n",
      "Epoch 205/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9669\n",
      "Epoch 206/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9682\n",
      "Epoch 207/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9639\n",
      "Epoch 208/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9639\n",
      "Epoch 209/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9595\n",
      "Epoch 210/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9656\n",
      "Epoch 211/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0977 - accuracy: 0.9665\n",
      "Epoch 212/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9756\n",
      "Epoch 213/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9682\n",
      "Epoch 214/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9691\n",
      "Epoch 215/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9613\n",
      "Epoch 216/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9569\n",
      "Epoch 217/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9648\n",
      "Epoch 218/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9622\n",
      "Epoch 219/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.9678\n",
      "Epoch 220/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9674\n",
      "Epoch 221/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.9656\n",
      "Epoch 222/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9643\n",
      "Epoch 223/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0877 - accuracy: 0.9735\n",
      "Epoch 224/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9643\n",
      "Epoch 225/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9622\n",
      "Epoch 226/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9665\n",
      "Epoch 227/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9674\n",
      "Epoch 228/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9587\n",
      "Epoch 229/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9600\n",
      "Epoch 230/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0883 - accuracy: 0.9678\n",
      "Epoch 231/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9682\n",
      "Epoch 232/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9682\n",
      "Epoch 233/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9669\n",
      "Epoch 234/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9682\n",
      "Epoch 235/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9669\n",
      "Epoch 236/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9717\n",
      "Epoch 237/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9622\n",
      "Epoch 238/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9739\n",
      "Epoch 239/1500\n",
      "72/72 [==============================] - 0s 4ms/step - loss: 0.0867 - accuracy: 0.9704\n",
      "Epoch 240/1500\n",
      "72/72 [==============================] - 0s 2ms/step - loss: 0.0968 - accuracy: 0.9652\n",
      "Epoch 241/1500\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9687\n",
      "Epoch 242/1500\n",
      "47/72 [==================>...........] - ETA: 0s - loss: 0.0862 - accuracy: 0.9674Restoring model weights from the end of the best epoch: 212.\n",
      "72/72 [==============================] - 0s 1ms/step - loss: 0.0875 - accuracy: 0.9669\n",
      "Epoch 242: early stopping\n",
      "7/7 [==============================] - 0s 941us/step - loss: 0.9659 - accuracy: 0.7442\n",
      "7/7 [==============================] - 0s 712us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.77 (23/30)\n",
      "Before appending - Cat IDs: 464, Predictions: 464, Actuals: 464, Gender: 464\n",
      "After appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "Final Test Results - Loss: 0.965858519077301, Accuracy: 0.7441860437393188, Precision: 0.6818112633181125, Recall: 0.745804909625056, F1 Score: 0.709639126305793\n",
      "Confusion Matrix:\n",
      " [[121   3  27]\n",
      " [  1   6   0]\n",
      " [ 24   0  33]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "001A    14\n",
      "042A    14\n",
      "059A    14\n",
      "028A    13\n",
      "111A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "025A    11\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "005A    10\n",
      "016A    10\n",
      "014B    10\n",
      "071A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "015A     9\n",
      "045A     9\n",
      "022A     9\n",
      "065A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "050A     7\n",
      "099A     7\n",
      "109A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "070A     5\n",
      "021A     5\n",
      "075A     5\n",
      "009A     4\n",
      "104A     4\n",
      "105A     4\n",
      "003A     4\n",
      "113A     3\n",
      "056A     3\n",
      "064A     3\n",
      "060A     3\n",
      "014A     3\n",
      "006A     3\n",
      "025B     2\n",
      "102A     2\n",
      "011A     2\n",
      "061A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "054A     2\n",
      "032A     2\n",
      "018A     2\n",
      "091A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "043A     1\n",
      "115A     1\n",
      "076A     1\n",
      "066A     1\n",
      "026C     1\n",
      "096A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "057A    27\n",
      "055A    20\n",
      "097B    14\n",
      "039A    12\n",
      "040A    10\n",
      "094A     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "023A     6\n",
      "053A     6\n",
      "108A     6\n",
      "023B     5\n",
      "026A     4\n",
      "062A     4\n",
      "052A     4\n",
      "035A     4\n",
      "058A     3\n",
      "012A     3\n",
      "069A     2\n",
      "048A     1\n",
      "088A     1\n",
      "004A     1\n",
      "073A     1\n",
      "041A     1\n",
      "092A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    306\n",
      "X    268\n",
      "F    158\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    94\n",
      "X    80\n",
      "M    31\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 015A, 001A, 103A, 071A, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 046A, 047A, 042A, 109A, 050...\n",
      "senior    [093A, 097A, 106A, 104A, 059A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 097B, 062A, 039A, 023A, 027A, 069A, 026...\n",
      "kitten                                   [040A, 041A, 048A]\n",
      "senior                 [057A, 055A, 117A, 058A, 094A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 13, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 3, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '020A'\n",
      " '021A' '022A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '004A' '012A' '019B' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'022A'}\n",
      "Moved to Test Set:\n",
      "{'022A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '005A' '006A' '007A' '008A'\n",
      " '009A' '010A' '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A'\n",
      " '020A' '021A' '024A' '025A' '025B' '025C' '026B' '026C' '028A' '029A'\n",
      " '032A' '033A' '034A' '036A' '037A' '038A' '042A' '043A' '044A' '045A'\n",
      " '046A' '047A' '049A' '050A' '051A' '051B' '054A' '056A' '059A' '060A'\n",
      " '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A' '072A'\n",
      " '074A' '075A' '076A' '087A' '090A' '091A' '093A' '095A' '096A' '097A'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A' '110A'\n",
      " '111A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['004A' '012A' '019B' '022A' '023A' '023B' '026A' '027A' '031A' '035A'\n",
      " '039A' '040A' '041A' '048A' '052A' '053A' '055A' '057A' '058A' '062A'\n",
      " '069A' '073A' '088A' '092A' '094A' '097B' '108A' '117A']\n",
      "Length of X_train_val:\n",
      "762\n",
      "Length of y_train_val:\n",
      "762\n",
      "Length of groups_train_val:\n",
      "762\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     466\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     122\n",
      "senior     71\n",
      "kitten     12\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     496\n",
      "kitten    159\n",
      "senior    107\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     92\n",
      "senior    71\n",
      "kitten    12\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 992, 1: 795, 2: 535})\n",
      "Epoch 1/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.9358 - accuracy: 0.6115\n",
      "Epoch 2/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6662 - accuracy: 0.7235\n",
      "Epoch 3/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5962 - accuracy: 0.7597\n",
      "Epoch 4/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5728 - accuracy: 0.7717\n",
      "Epoch 5/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5420 - accuracy: 0.7873\n",
      "Epoch 6/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5336 - accuracy: 0.7804\n",
      "Epoch 7/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4875 - accuracy: 0.8002\n",
      "Epoch 8/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4682 - accuracy: 0.8127\n",
      "Epoch 9/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4460 - accuracy: 0.8217\n",
      "Epoch 10/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4373 - accuracy: 0.8273\n",
      "Epoch 11/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4067 - accuracy: 0.8376\n",
      "Epoch 12/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4259 - accuracy: 0.8299\n",
      "Epoch 13/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8428\n",
      "Epoch 14/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3813 - accuracy: 0.8488\n",
      "Epoch 15/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3570 - accuracy: 0.8506\n",
      "Epoch 16/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3667 - accuracy: 0.8557\n",
      "Epoch 17/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3648 - accuracy: 0.8531\n",
      "Epoch 18/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3503 - accuracy: 0.8648\n",
      "Epoch 19/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3512 - accuracy: 0.8570\n",
      "Epoch 20/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3485 - accuracy: 0.8643\n",
      "Epoch 21/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3499 - accuracy: 0.8648\n",
      "Epoch 22/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8794\n",
      "Epoch 23/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3457 - accuracy: 0.8622\n",
      "Epoch 24/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3024 - accuracy: 0.8798\n",
      "Epoch 25/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8622\n",
      "Epoch 26/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3271 - accuracy: 0.8691\n",
      "Epoch 27/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3318 - accuracy: 0.8583\n",
      "Epoch 28/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3003 - accuracy: 0.8807\n",
      "Epoch 29/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3060 - accuracy: 0.8734\n",
      "Epoch 30/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2964 - accuracy: 0.8794\n",
      "Epoch 31/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2752 - accuracy: 0.8949\n",
      "Epoch 32/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2876 - accuracy: 0.8820\n",
      "Epoch 33/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2826 - accuracy: 0.8842\n",
      "Epoch 34/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2677 - accuracy: 0.8906\n",
      "Epoch 35/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2724 - accuracy: 0.8910\n",
      "Epoch 36/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2612 - accuracy: 0.8902\n",
      "Epoch 37/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2732 - accuracy: 0.8941\n",
      "Epoch 38/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2821 - accuracy: 0.8910\n",
      "Epoch 39/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2635 - accuracy: 0.8941\n",
      "Epoch 40/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2550 - accuracy: 0.9018\n",
      "Epoch 41/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2599 - accuracy: 0.9027\n",
      "Epoch 42/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2536 - accuracy: 0.8988\n",
      "Epoch 43/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2414 - accuracy: 0.9100\n",
      "Epoch 44/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2438 - accuracy: 0.9074\n",
      "Epoch 45/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2470 - accuracy: 0.9022\n",
      "Epoch 46/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2422 - accuracy: 0.9070\n",
      "Epoch 47/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2488 - accuracy: 0.9014\n",
      "Epoch 48/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2281 - accuracy: 0.9083\n",
      "Epoch 49/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2259 - accuracy: 0.9078\n",
      "Epoch 50/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2291 - accuracy: 0.9160\n",
      "Epoch 51/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2231 - accuracy: 0.9130\n",
      "Epoch 52/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2303 - accuracy: 0.9113\n",
      "Epoch 53/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2347 - accuracy: 0.9091\n",
      "Epoch 54/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2263 - accuracy: 0.9152\n",
      "Epoch 55/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2135 - accuracy: 0.9147\n",
      "Epoch 56/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2254 - accuracy: 0.9091\n",
      "Epoch 57/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2060 - accuracy: 0.9203\n",
      "Epoch 58/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2079 - accuracy: 0.9182\n",
      "Epoch 59/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2250 - accuracy: 0.9143\n",
      "Epoch 60/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2140 - accuracy: 0.9143\n",
      "Epoch 61/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2036 - accuracy: 0.9208\n",
      "Epoch 62/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2097 - accuracy: 0.9134\n",
      "Epoch 63/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1931 - accuracy: 0.9220\n",
      "Epoch 64/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2226 - accuracy: 0.9121\n",
      "Epoch 65/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2008 - accuracy: 0.9242\n",
      "Epoch 66/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1984 - accuracy: 0.9203\n",
      "Epoch 67/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2115 - accuracy: 0.9169\n",
      "Epoch 68/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1997 - accuracy: 0.9212\n",
      "Epoch 69/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2031 - accuracy: 0.9233\n",
      "Epoch 70/1500\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1920 - accuracy: 0.9289\n",
      "Epoch 71/1500\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1975 - accuracy: 0.9182\n",
      "Epoch 72/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1884 - accuracy: 0.9268\n",
      "Epoch 73/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9242\n",
      "Epoch 74/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2183 - accuracy: 0.9104\n",
      "Epoch 75/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1903 - accuracy: 0.9298\n",
      "Epoch 76/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.9264\n",
      "Epoch 77/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9337\n",
      "Epoch 78/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1972 - accuracy: 0.9268\n",
      "Epoch 79/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1875 - accuracy: 0.9255\n",
      "Epoch 80/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2009 - accuracy: 0.9199\n",
      "Epoch 81/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1946 - accuracy: 0.9285\n",
      "Epoch 82/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.9341\n",
      "Epoch 83/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1829 - accuracy: 0.9376\n",
      "Epoch 84/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1699 - accuracy: 0.9311\n",
      "Epoch 85/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1809 - accuracy: 0.9298\n",
      "Epoch 86/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1913 - accuracy: 0.9302\n",
      "Epoch 87/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9380\n",
      "Epoch 88/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9328\n",
      "Epoch 89/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9259\n",
      "Epoch 90/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1614 - accuracy: 0.9401\n",
      "Epoch 91/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.9345\n",
      "Epoch 92/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1751 - accuracy: 0.9311\n",
      "Epoch 93/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9328\n",
      "Epoch 94/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1832 - accuracy: 0.9285\n",
      "Epoch 95/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9380\n",
      "Epoch 96/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9332\n",
      "Epoch 97/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9358\n",
      "Epoch 98/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9453\n",
      "Epoch 99/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9388\n",
      "Epoch 100/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1692 - accuracy: 0.9341\n",
      "Epoch 101/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1607 - accuracy: 0.9380\n",
      "Epoch 102/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9371\n",
      "Epoch 103/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9406\n",
      "Epoch 104/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1658 - accuracy: 0.9337\n",
      "Epoch 105/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.9354\n",
      "Epoch 106/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.9363\n",
      "Epoch 107/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.9384\n",
      "Epoch 108/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9397\n",
      "Epoch 109/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1655 - accuracy: 0.9337\n",
      "Epoch 110/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9475\n",
      "Epoch 111/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9436\n",
      "Epoch 112/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9423\n",
      "Epoch 113/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9380\n",
      "Epoch 114/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1684 - accuracy: 0.9363\n",
      "Epoch 115/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9410\n",
      "Epoch 116/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9401\n",
      "Epoch 117/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9397\n",
      "Epoch 118/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9492\n",
      "Epoch 119/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9470\n",
      "Epoch 120/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.9388\n",
      "Epoch 121/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9363\n",
      "Epoch 122/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1494 - accuracy: 0.9475\n",
      "Epoch 123/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9358\n",
      "Epoch 124/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9488\n",
      "Epoch 125/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1488 - accuracy: 0.9423\n",
      "Epoch 126/1500\n",
      "73/73 [==============================] - 0s 996us/step - loss: 0.1423 - accuracy: 0.9457\n",
      "Epoch 127/1500\n",
      "73/73 [==============================] - 0s 944us/step - loss: 0.1623 - accuracy: 0.9371\n",
      "Epoch 128/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9526\n",
      "Epoch 129/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9522\n",
      "Epoch 130/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9522\n",
      "Epoch 131/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9522\n",
      "Epoch 132/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1388 - accuracy: 0.9492\n",
      "Epoch 133/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9414\n",
      "Epoch 134/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1491 - accuracy: 0.9488\n",
      "Epoch 135/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9552\n",
      "Epoch 136/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9440\n",
      "Epoch 137/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9565\n",
      "Epoch 138/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9500\n",
      "Epoch 139/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1486 - accuracy: 0.9457\n",
      "Epoch 140/1500\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1280 - accuracy: 0.9582\n",
      "Epoch 141/1500\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1206 - accuracy: 0.9556\n",
      "Epoch 142/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9444\n",
      "Epoch 143/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.9548\n",
      "Epoch 144/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9479\n",
      "Epoch 145/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9539\n",
      "Epoch 146/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9500\n",
      "Epoch 147/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1302 - accuracy: 0.9531\n",
      "Epoch 148/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1331 - accuracy: 0.9479\n",
      "Epoch 149/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9462\n",
      "Epoch 150/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9479\n",
      "Epoch 151/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1374 - accuracy: 0.9457\n",
      "Epoch 152/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.9440\n",
      "Epoch 153/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9522\n",
      "Epoch 154/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9500\n",
      "Epoch 155/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1388 - accuracy: 0.9483\n",
      "Epoch 156/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9548\n",
      "Epoch 157/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9591\n",
      "Epoch 158/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9466\n",
      "Epoch 159/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9587\n",
      "Epoch 160/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9535\n",
      "Epoch 161/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9539\n",
      "Epoch 162/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9513\n",
      "Epoch 163/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9582\n",
      "Epoch 164/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9630\n",
      "Epoch 165/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1200 - accuracy: 0.9561\n",
      "Epoch 166/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9556\n",
      "Epoch 167/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9505\n",
      "Epoch 168/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9552\n",
      "Epoch 169/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9548\n",
      "Epoch 170/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9574\n",
      "Epoch 171/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9591\n",
      "Epoch 172/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9621\n",
      "Epoch 173/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9638\n",
      "Epoch 174/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9574\n",
      "Epoch 175/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9634\n",
      "Epoch 176/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9582\n",
      "Epoch 177/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1364 - accuracy: 0.9475\n",
      "Epoch 178/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1285 - accuracy: 0.9539\n",
      "Epoch 179/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9595\n",
      "Epoch 180/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.9681\n",
      "Epoch 181/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9565\n",
      "Epoch 182/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9531\n",
      "Epoch 183/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9625\n",
      "Epoch 184/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9574\n",
      "Epoch 185/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9660\n",
      "Epoch 186/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9643\n",
      "Epoch 187/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9599\n",
      "Epoch 188/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9552\n",
      "Epoch 189/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9643\n",
      "Epoch 190/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9548\n",
      "Epoch 191/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9591\n",
      "Epoch 192/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9668\n",
      "Epoch 193/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9574\n",
      "Epoch 194/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9500\n",
      "Epoch 195/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9612\n",
      "Epoch 196/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9746\n",
      "Epoch 197/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9608\n",
      "Epoch 198/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9634\n",
      "Epoch 199/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9612\n",
      "Epoch 200/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9587\n",
      "Epoch 201/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9643\n",
      "Epoch 202/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9595\n",
      "Epoch 203/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9582\n",
      "Epoch 204/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9638\n",
      "Epoch 205/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1064 - accuracy: 0.9608\n",
      "Epoch 206/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9604\n",
      "Epoch 207/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9621\n",
      "Epoch 208/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9625\n",
      "Epoch 209/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9608\n",
      "Epoch 210/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9668\n",
      "Epoch 211/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1049 - accuracy: 0.9608\n",
      "Epoch 212/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9647\n",
      "Epoch 213/1500\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.1177 - accuracy: 0.9565\n",
      "Epoch 214/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9543\n",
      "Epoch 215/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9647\n",
      "Epoch 216/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9681\n",
      "Epoch 217/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9483\n",
      "Epoch 218/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9686\n",
      "Epoch 219/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0992 - accuracy: 0.9634\n",
      "Epoch 220/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9630\n",
      "Epoch 221/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9690\n",
      "Epoch 222/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9655\n",
      "Epoch 223/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0965 - accuracy: 0.9660\n",
      "Epoch 224/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9699\n",
      "Epoch 225/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9660\n",
      "Epoch 226/1500\n",
      "49/73 [===================>..........] - ETA: 0s - loss: 0.1094 - accuracy: 0.9598Restoring model weights from the end of the best epoch: 196.\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9574\n",
      "Epoch 226: early stopping\n",
      "6/6 [==============================] - 0s 893us/step - loss: 1.4796 - accuracy: 0.6114\n",
      "6/6 [==============================] - 0s 722us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.75 (21/28)\n",
      "Before appending - Cat IDs: 679, Predictions: 679, Actuals: 679, Gender: 679\n",
      "After appending - Cat IDs: 854, Predictions: 854, Actuals: 854, Gender: 854\n",
      "Final Test Results - Loss: 1.4796192646026611, Accuracy: 0.6114285588264465, Precision: 0.6553086419753087, Recall: 0.6705279989113424, F1 Score: 0.6176646543753739\n",
      "Confusion Matrix:\n",
      " [[80  3  9]\n",
      " [ 1 11  0]\n",
      " [54  1 16]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6429412436721567\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 1.0145101994276047\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.6894595474004745\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6491654178406383\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.691772327540003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[1]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2845ad-17c1-494a-8bd0-971aefca9f01",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3910db95-6772-4098-bef1-fb5857901e5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 854, Predictions: 854, Actuals: 854, Gender: 854\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d543c7c2-c11b-4511-ba5a-c52969a61a62",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6b6f2173-89a8-40ba-afa6-410005c2dcb1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (83/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "aa8d9ba5-9d96-4aee-b3e2-ba87553d1899",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0b4ff1f7-d22d-4409-98d4-49e9a82bcb58",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, senior, kitten, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, kitten, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, adult, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, kitten, adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[senior, adult, kitten, senior, adult, senior,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, adult, adult, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, kit...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, kitten, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[senior, adult, adult, senior, adult, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[adult, senior, senior, senior, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, kitten, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[senior, senior, senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, senior, kitten, senior, adult, adult, ...         adult            adult                   True\n",
       "65    059A  [adult, adult, adult, senior, senior, senior, ...        senior           senior                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, kitten, senior, senior, adult, ...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, a...         adult            adult                   True\n",
       "70    064A                             [kitten, adult, adult]         adult            adult                   True\n",
       "69    063A  [senior, senior, senior, adult, adult, adult, ...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, adult, senior, adult, senior, senior,...        senior           senior                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A            [kitten, kitten, adult, kitten, kitten]        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "77    071A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [senior, senior, adult]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                    [senior, senior, adult, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, s...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "94    099A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "93    097B  [senior, adult, kitten, senior, adult, senior,...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, adult, adult, s...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "81    075A               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "48    042A  [kitten, adult, kitten, kitten, adult, adult, ...        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "28    025A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, kit...         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, kitten, adult, ad...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, senior, senior, adult, senior, adult, ...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, sen...         adult            adult                   True\n",
       "10    009A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [adult, adult, senior]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, ad...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "43    037A       [adult, senior, adult, adult, adult, senior]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, kitten, adult, adult, ki...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "40    034A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [senior, adult, adult, senior, adult, adult, s...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "41    035A                      [kitten, adult, adult, adult]         adult            adult                   True\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "42    036A  [adult, senior, senior, senior, adult, senior,...        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "101   106A  [adult, senior, adult, senior, adult, adult, a...         adult           senior                  False\n",
       "102   108A        [adult, adult, adult, adult, adult, kitten]         adult           senior                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "52    047A  [adult, adult, adult, adult, adult, kitten, ki...         adult           kitten                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "103   109A      [adult, adult, kitten, kitten, kitten, adult]         adult           kitten                  False\n",
       "61    055A  [adult, adult, senior, adult, senior, adult, a...         adult           senior                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "30    025C            [senior, senior, senior, senior, adult]        senior            adult                  False\n",
       "90    095A  [senior, adult, adult, senior, senior, senior,...        senior            adult                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "87    092A                                           [senior]        senior            adult                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "18    016A  [adult, adult, adult, senior, senior, senior, ...         adult           senior                  False\n",
       "64    058A                              [adult, adult, adult]         adult           senior                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "109   117A  [adult, adult, adult, adult, adult, adult, adult]         adult           senior                  False"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5ec47984-bc63-4f3f-a7a4-d3979dbd4c52",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     62\n",
      "kitten    11\n",
      "senior    10\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "952d49d2-180d-4f36-85b0-6e2b96610559",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             62  84.931507\n",
      "1           kitten           15             11  73.333333\n",
      "2           senior           22             10  45.454545\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6a94de06-f9df-49f6-99e7-abc9024f3dc0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885dd133-e156-4b01-8e84-4d94546e0eca",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dd0127df-e0fb-4862-9ac4-2113bd346e78",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          558            430  77.060932\n",
      "1           kitten          118             81  68.644068\n",
      "2           senior          178             80  44.943820\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4a0df220-5e5c-4c46-8474-ac4ecf291071",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299a7528-860f-45b0-8c00-d0ddf10a0d8f",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2924eee7-fd8b-4cee-9e43-bb0352e4c43b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    222      160     72.07\n",
      "1          M    337      243     72.11\n",
      "2          X    295      188     63.73\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b7b2139e-f100-44e6-aff4-807f96753499",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df58f49-cac6-40b1-8422-e3d95576c453",
   "metadata": {},
   "source": [
    "# RANDOM SEED 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bf9d1b64-575e-47ca-bf37-e8916438d506",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[2]))\n",
    "np.random.seed(int(random_seeds[2]))\n",
    "tf.random.set_seed(int(random_seeds[2]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_1.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "91d5db1d-6c9f-4047-97b9-5a0eef4fbaaa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e435598c-cabc-4129-8504-68aac9cc06bc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e29b526-5098-4ce6-bc77-a93d1e6e1397",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "79a922e6-198c-4c43-a6f2-90dc580ae875",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "097A    16\n",
      "101A    15\n",
      "042A    14\n",
      "106A    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "063A    11\n",
      "025A    11\n",
      "071A    10\n",
      "016A    10\n",
      "040A    10\n",
      "014B    10\n",
      "033A     9\n",
      "072A     9\n",
      "015A     9\n",
      "022A     9\n",
      "051B     9\n",
      "065A     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "108A     6\n",
      "109A     6\n",
      "037A     6\n",
      "007A     6\n",
      "008A     6\n",
      "025C     5\n",
      "070A     5\n",
      "021A     5\n",
      "034A     5\n",
      "075A     5\n",
      "023B     5\n",
      "035A     4\n",
      "009A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "058A     3\n",
      "014A     3\n",
      "011A     2\n",
      "061A     2\n",
      "032A     2\n",
      "093A     2\n",
      "025B     2\n",
      "087A     2\n",
      "069A     2\n",
      "073A     1\n",
      "115A     1\n",
      "088A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "019B     1\n",
      "066A     1\n",
      "004A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "096A     1\n",
      "043A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "001A    14\n",
      "097B    14\n",
      "111A    13\n",
      "051A    12\n",
      "068A    11\n",
      "036A    11\n",
      "005A    10\n",
      "045A     9\n",
      "117A     7\n",
      "053A     6\n",
      "023A     6\n",
      "044A     5\n",
      "105A     4\n",
      "052A     4\n",
      "104A     4\n",
      "018A     2\n",
      "054A     2\n",
      "038A     2\n",
      "102A     2\n",
      "091A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    265\n",
      "M    256\n",
      "F    198\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    83\n",
      "M    81\n",
      "F    54\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 103A, 071A, 028A, 067...\n",
      "kitten    [014B, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [001A, 097B, 019A, 074A, 029A, 005A, 091A, 023...\n",
      "kitten                             [044A, 111A, 045A, 110A]\n",
      "senior                             [104A, 054A, 117A, 051A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002A' '002B' '003A' '004A' '006A' '007A' '008A' '009A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '015A' '016A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A'\n",
      " '028A' '031A' '032A' '033A' '034A' '035A' '037A' '039A' '040A' '041A'\n",
      " '042A' '043A' '046A' '047A' '048A' '049A' '050A' '051B' '055A' '056A'\n",
      " '057A' '058A' '059A' '060A' '061A' '062A' '063A' '064A' '065A' '066A'\n",
      " '067A' '069A' '070A' '071A' '072A' '073A' '075A' '076A' '087A' '088A'\n",
      " '090A' '092A' '093A' '094A' '095A' '096A' '097A' '099A' '100A' '101A'\n",
      " '103A' '106A' '108A' '109A' '113A' '115A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '005A' '018A' '019A' '023A' '029A' '036A' '038A' '044A'\n",
      " '045A' '051A' '052A' '053A' '054A' '068A' '074A' '091A' '097B' '102A'\n",
      " '104A' '105A' '110A' '111A' '117A']\n",
      "Length of X_train_val:\n",
      "719\n",
      "Length of y_train_val:\n",
      "719\n",
      "Length of groups_train_val:\n",
      "719\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     423\n",
      "senior    153\n",
      "kitten    143\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     165\n",
      "kitten     28\n",
      "senior     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 846, 2: 765, 1: 715})\n",
      "Epoch 1/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.9040 - accuracy: 0.6350\n",
      "Epoch 2/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.6644 - accuracy: 0.7218\n",
      "Epoch 3/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5897 - accuracy: 0.7674\n",
      "Epoch 4/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5627 - accuracy: 0.7747\n",
      "Epoch 5/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5425 - accuracy: 0.7794\n",
      "Epoch 6/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.5162 - accuracy: 0.7958\n",
      "Epoch 7/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4834 - accuracy: 0.8100\n",
      "Epoch 8/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4481 - accuracy: 0.8237\n",
      "Epoch 9/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4495 - accuracy: 0.8237\n",
      "Epoch 10/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4307 - accuracy: 0.8259\n",
      "Epoch 11/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4210 - accuracy: 0.8340\n",
      "Epoch 12/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4148 - accuracy: 0.8422\n",
      "Epoch 13/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.4011 - accuracy: 0.8418\n",
      "Epoch 14/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3898 - accuracy: 0.8452\n",
      "Epoch 15/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3878 - accuracy: 0.8517\n",
      "Epoch 16/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3884 - accuracy: 0.8521\n",
      "Epoch 17/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3487 - accuracy: 0.8573\n",
      "Epoch 18/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3441 - accuracy: 0.8676\n",
      "Epoch 19/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3304 - accuracy: 0.8775\n",
      "Epoch 20/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3354 - accuracy: 0.8672\n",
      "Epoch 21/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3283 - accuracy: 0.8801\n",
      "Epoch 22/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3074 - accuracy: 0.8895\n",
      "Epoch 23/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3194 - accuracy: 0.8745\n",
      "Epoch 24/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3135 - accuracy: 0.8822\n",
      "Epoch 25/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3072 - accuracy: 0.8839\n",
      "Epoch 26/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3115 - accuracy: 0.8818\n",
      "Epoch 27/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2921 - accuracy: 0.8891\n",
      "Epoch 28/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.3012 - accuracy: 0.8869\n",
      "Epoch 29/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2928 - accuracy: 0.8869\n",
      "Epoch 30/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2798 - accuracy: 0.8972\n",
      "Epoch 31/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2751 - accuracy: 0.8947\n",
      "Epoch 32/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2820 - accuracy: 0.8869\n",
      "Epoch 33/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2832 - accuracy: 0.8912\n",
      "Epoch 34/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2644 - accuracy: 0.9020\n",
      "Epoch 35/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2654 - accuracy: 0.9037\n",
      "Epoch 36/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2694 - accuracy: 0.8934\n",
      "Epoch 37/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2515 - accuracy: 0.9063\n",
      "Epoch 38/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2613 - accuracy: 0.8942\n",
      "Epoch 39/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.9033\n",
      "Epoch 40/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2466 - accuracy: 0.9046\n",
      "Epoch 41/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2505 - accuracy: 0.9041\n",
      "Epoch 42/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2387 - accuracy: 0.9080\n",
      "Epoch 43/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2432 - accuracy: 0.9114\n",
      "Epoch 44/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2450 - accuracy: 0.9037\n",
      "Epoch 45/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2330 - accuracy: 0.9114\n",
      "Epoch 46/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2241 - accuracy: 0.9127\n",
      "Epoch 47/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2184 - accuracy: 0.9196\n",
      "Epoch 48/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2359 - accuracy: 0.9136\n",
      "Epoch 49/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2176 - accuracy: 0.9192\n",
      "Epoch 50/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2219 - accuracy: 0.9192\n",
      "Epoch 51/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2245 - accuracy: 0.9170\n",
      "Epoch 52/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2416 - accuracy: 0.9101\n",
      "Epoch 53/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2299 - accuracy: 0.9140\n",
      "Epoch 54/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2314 - accuracy: 0.9114\n",
      "Epoch 55/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2278 - accuracy: 0.9127\n",
      "Epoch 56/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2153 - accuracy: 0.9273\n",
      "Epoch 57/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1934 - accuracy: 0.9347\n",
      "Epoch 58/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2039 - accuracy: 0.9256\n",
      "Epoch 59/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2069 - accuracy: 0.9166\n",
      "Epoch 60/1500\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.1889 - accuracy: 0.9299\n",
      "Epoch 61/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2131 - accuracy: 0.9265\n",
      "Epoch 62/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9248\n",
      "Epoch 63/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.2054 - accuracy: 0.9226\n",
      "Epoch 64/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1928 - accuracy: 0.9239\n",
      "Epoch 65/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1994 - accuracy: 0.9261\n",
      "Epoch 66/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1902 - accuracy: 0.9282\n",
      "Epoch 67/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1931 - accuracy: 0.9256\n",
      "Epoch 68/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1845 - accuracy: 0.9321\n",
      "Epoch 69/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9329\n",
      "Epoch 70/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9334\n",
      "Epoch 71/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1747 - accuracy: 0.9368\n",
      "Epoch 72/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9355\n",
      "Epoch 73/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1865 - accuracy: 0.9304\n",
      "Epoch 74/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9394\n",
      "Epoch 75/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9394\n",
      "Epoch 76/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9359\n",
      "Epoch 77/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9325\n",
      "Epoch 78/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1821 - accuracy: 0.9334\n",
      "Epoch 79/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1918 - accuracy: 0.9308\n",
      "Epoch 80/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9295\n",
      "Epoch 81/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1718 - accuracy: 0.9351\n",
      "Epoch 82/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9463\n",
      "Epoch 83/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9390\n",
      "Epoch 84/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1584 - accuracy: 0.9411\n",
      "Epoch 85/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9445\n",
      "Epoch 86/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1539 - accuracy: 0.9458\n",
      "Epoch 87/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.9411\n",
      "Epoch 88/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9390\n",
      "Epoch 89/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9372\n",
      "Epoch 90/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9407\n",
      "Epoch 91/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1616 - accuracy: 0.9437\n",
      "Epoch 92/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9359\n",
      "Epoch 93/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9433\n",
      "Epoch 94/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9355\n",
      "Epoch 95/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9484\n",
      "Epoch 96/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1546 - accuracy: 0.9458\n",
      "Epoch 97/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9454\n",
      "Epoch 98/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9527\n",
      "Epoch 99/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9398\n",
      "Epoch 100/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9359\n",
      "Epoch 101/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9475\n",
      "Epoch 102/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9484\n",
      "Epoch 103/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.9475\n",
      "Epoch 104/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9467\n",
      "Epoch 105/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1525 - accuracy: 0.9450\n",
      "Epoch 106/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1468 - accuracy: 0.9488\n",
      "Epoch 107/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9557\n",
      "Epoch 108/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9510\n",
      "Epoch 109/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9458\n",
      "Epoch 110/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9398\n",
      "Epoch 111/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9407\n",
      "Epoch 112/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9514\n",
      "Epoch 113/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9523\n",
      "Epoch 114/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9514\n",
      "Epoch 115/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9557\n",
      "Epoch 116/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9467\n",
      "Epoch 117/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9514\n",
      "Epoch 118/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9445\n",
      "Epoch 119/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1512 - accuracy: 0.9488\n",
      "Epoch 120/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1220 - accuracy: 0.9557\n",
      "Epoch 121/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9531\n",
      "Epoch 122/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9424\n",
      "Epoch 123/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9531\n",
      "Epoch 124/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9549\n",
      "Epoch 125/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9536\n",
      "Epoch 126/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9600\n",
      "Epoch 127/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1357 - accuracy: 0.9501\n",
      "Epoch 128/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9574\n",
      "Epoch 129/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9617\n",
      "Epoch 130/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9514\n",
      "Epoch 131/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9579\n",
      "Epoch 132/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9497\n",
      "Epoch 133/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9536\n",
      "Epoch 134/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9549\n",
      "Epoch 135/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9574\n",
      "Epoch 136/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9540\n",
      "Epoch 137/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9531\n",
      "Epoch 138/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9622\n",
      "Epoch 139/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9574\n",
      "Epoch 140/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9561\n",
      "Epoch 141/1500\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.1234 - accuracy: 0.9549\n",
      "Epoch 142/1500\n",
      "73/73 [==============================] - 0s 2ms/step - loss: 0.1268 - accuracy: 0.9561\n",
      "Epoch 143/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9557\n",
      "Epoch 144/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9587\n",
      "Epoch 145/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9570\n",
      "Epoch 146/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9622\n",
      "Epoch 147/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9553\n",
      "Epoch 148/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1245 - accuracy: 0.9604\n",
      "Epoch 149/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9609\n",
      "Epoch 150/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9553\n",
      "Epoch 151/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9574\n",
      "Epoch 152/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9622\n",
      "Epoch 153/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9596\n",
      "Epoch 154/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9609\n",
      "Epoch 155/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9652\n",
      "Epoch 156/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9600\n",
      "Epoch 157/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9647\n",
      "Epoch 158/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9596\n",
      "Epoch 159/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1194 - accuracy: 0.9536\n",
      "Epoch 160/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9604\n",
      "Epoch 161/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9643\n",
      "Epoch 162/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9596\n",
      "Epoch 163/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9647\n",
      "Epoch 164/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9574\n",
      "Epoch 165/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0905 - accuracy: 0.9695\n",
      "Epoch 166/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0908 - accuracy: 0.9660\n",
      "Epoch 167/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9544\n",
      "Epoch 168/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9604\n",
      "Epoch 169/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9587\n",
      "Epoch 170/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9609\n",
      "Epoch 171/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.9678\n",
      "Epoch 172/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9669\n",
      "Epoch 173/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1124 - accuracy: 0.9647\n",
      "Epoch 174/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1120 - accuracy: 0.9592\n",
      "Epoch 175/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9596\n",
      "Epoch 176/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9609\n",
      "Epoch 177/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9699\n",
      "Epoch 178/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9643\n",
      "Epoch 179/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9652\n",
      "Epoch 180/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9587\n",
      "Epoch 181/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9660\n",
      "Epoch 182/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9635\n",
      "Epoch 183/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9574\n",
      "Epoch 184/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.9673\n",
      "Epoch 185/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1046 - accuracy: 0.9592\n",
      "Epoch 186/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9721\n",
      "Epoch 187/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9721\n",
      "Epoch 188/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0993 - accuracy: 0.9690\n",
      "Epoch 189/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9643\n",
      "Epoch 190/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9686\n",
      "Epoch 191/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9609\n",
      "Epoch 192/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9617\n",
      "Epoch 193/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1172 - accuracy: 0.9566\n",
      "Epoch 194/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9656\n",
      "Epoch 195/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9609\n",
      "Epoch 196/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9592\n",
      "Epoch 197/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9656\n",
      "Epoch 198/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9652\n",
      "Epoch 199/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9613\n",
      "Epoch 200/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.9673\n",
      "Epoch 201/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9626\n",
      "Epoch 202/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0842 - accuracy: 0.9721\n",
      "Epoch 203/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.9708\n",
      "Epoch 204/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9660\n",
      "Epoch 205/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9699\n",
      "Epoch 206/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9695\n",
      "Epoch 207/1500\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.1091 - accuracy: 0.9652\n",
      "Epoch 208/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9579\n",
      "Epoch 209/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.9703\n",
      "Epoch 210/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9609\n",
      "Epoch 211/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9690\n",
      "Epoch 212/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9665\n",
      "Epoch 213/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9686\n",
      "Epoch 214/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9643\n",
      "Epoch 215/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9647\n",
      "Epoch 216/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0878 - accuracy: 0.9712\n",
      "Epoch 217/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0841 - accuracy: 0.9712\n",
      "Epoch 218/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0945 - accuracy: 0.9647\n",
      "Epoch 219/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9678\n",
      "Epoch 220/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9678\n",
      "Epoch 221/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9695\n",
      "Epoch 222/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0834 - accuracy: 0.9733\n",
      "Epoch 223/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9729\n",
      "Epoch 224/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9669\n",
      "Epoch 225/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9600\n",
      "Epoch 226/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9703\n",
      "Epoch 227/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9690\n",
      "Epoch 228/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0948 - accuracy: 0.9660\n",
      "Epoch 229/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9639\n",
      "Epoch 230/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9652\n",
      "Epoch 231/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0850 - accuracy: 0.9695\n",
      "Epoch 232/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9639\n",
      "Epoch 233/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9678\n",
      "Epoch 234/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9690\n",
      "Epoch 235/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9665\n",
      "Epoch 236/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0966 - accuracy: 0.9643\n",
      "Epoch 237/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0798 - accuracy: 0.9729\n",
      "Epoch 238/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9725\n",
      "Epoch 239/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.9733\n",
      "Epoch 240/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.9716\n",
      "Epoch 241/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9665\n",
      "Epoch 242/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9592\n",
      "Epoch 243/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.9746\n",
      "Epoch 244/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9738\n",
      "Epoch 245/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9643\n",
      "Epoch 246/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9721\n",
      "Epoch 247/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9673\n",
      "Epoch 248/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9639\n",
      "Epoch 249/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9712\n",
      "Epoch 250/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0872 - accuracy: 0.9721\n",
      "Epoch 251/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0815 - accuracy: 0.9746\n",
      "Epoch 252/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9686\n",
      "Epoch 253/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0866 - accuracy: 0.9690\n",
      "Epoch 254/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9733\n",
      "Epoch 255/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9716\n",
      "Epoch 256/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0783 - accuracy: 0.9746\n",
      "Epoch 257/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0881 - accuracy: 0.9716\n",
      "Epoch 258/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9703\n",
      "Epoch 259/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9738\n",
      "Epoch 260/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.9725\n",
      "Epoch 261/1500\n",
      "73/73 [==============================] - 0s 3ms/step - loss: 0.0810 - accuracy: 0.9725\n",
      "Epoch 262/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0940 - accuracy: 0.9669\n",
      "Epoch 263/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9690\n",
      "Epoch 264/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9772\n",
      "Epoch 265/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.9669\n",
      "Epoch 266/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9686\n",
      "Epoch 267/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.9751\n",
      "Epoch 268/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9733\n",
      "Epoch 269/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1081 - accuracy: 0.9604\n",
      "Epoch 270/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9733\n",
      "Epoch 271/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0825 - accuracy: 0.9725\n",
      "Epoch 272/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0715 - accuracy: 0.9785\n",
      "Epoch 273/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9759\n",
      "Epoch 274/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9721\n",
      "Epoch 275/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0787 - accuracy: 0.9703\n",
      "Epoch 276/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9716\n",
      "Epoch 277/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0722 - accuracy: 0.9789\n",
      "Epoch 278/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0666 - accuracy: 0.9811\n",
      "Epoch 279/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0770 - accuracy: 0.9768\n",
      "Epoch 280/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0748 - accuracy: 0.9746\n",
      "Epoch 281/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9678\n",
      "Epoch 282/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0800 - accuracy: 0.9738\n",
      "Epoch 283/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9764\n",
      "Epoch 284/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.9781\n",
      "Epoch 285/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.9708\n",
      "Epoch 286/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9682\n",
      "Epoch 287/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9678\n",
      "Epoch 288/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9742\n",
      "Epoch 289/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9665\n",
      "Epoch 290/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9764\n",
      "Epoch 291/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9678\n",
      "Epoch 292/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0646 - accuracy: 0.9772\n",
      "Epoch 293/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9776\n",
      "Epoch 294/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9746\n",
      "Epoch 295/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9695\n",
      "Epoch 296/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9703\n",
      "Epoch 297/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.9768\n",
      "Epoch 298/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9751\n",
      "Epoch 299/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0901 - accuracy: 0.9682\n",
      "Epoch 300/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0738 - accuracy: 0.9746\n",
      "Epoch 301/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0750 - accuracy: 0.9738\n",
      "Epoch 302/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.9789\n",
      "Epoch 303/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9690\n",
      "Epoch 304/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0697 - accuracy: 0.9794\n",
      "Epoch 305/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9746\n",
      "Epoch 306/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.9764\n",
      "Epoch 307/1500\n",
      "73/73 [==============================] - 0s 4ms/step - loss: 0.0855 - accuracy: 0.9721\n",
      "Epoch 308/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9746\n",
      "Epoch 309/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9733\n",
      "Epoch 310/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0678 - accuracy: 0.9764\n",
      "Epoch 311/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9695\n",
      "Epoch 312/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9600\n",
      "Epoch 313/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9673\n",
      "Epoch 314/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0828 - accuracy: 0.9746\n",
      "Epoch 315/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.9772\n",
      "Epoch 316/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9695\n",
      "Epoch 317/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0758 - accuracy: 0.9733\n",
      "Epoch 318/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9643\n",
      "Epoch 319/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9712\n",
      "Epoch 320/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0663 - accuracy: 0.9755\n",
      "Epoch 321/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0650 - accuracy: 0.9798\n",
      "Epoch 322/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9699\n",
      "Epoch 323/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9742\n",
      "Epoch 324/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9785\n",
      "Epoch 325/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.9794\n",
      "Epoch 326/1500\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0632 - accuracy: 0.9794\n",
      "Epoch 327/1500\n",
      "44/73 [=================>............] - ETA: 0s - loss: 0.0815 - accuracy: 0.9766Restoring model weights from the end of the best epoch: 297.\n",
      "73/73 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9751\n",
      "Epoch 327: early stopping\n",
      "7/7 [==============================] - 0s 877us/step - loss: 1.5491 - accuracy: 0.6284\n",
      "7/7 [==============================] - 0s 669us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.64 (16/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "Final Test Results - Loss: 1.5491094589233398, Accuracy: 0.6284403800964355, Precision: 0.6491250361246474, Recall: 0.7260894660894661, F1 Score: 0.631685168460839\n",
      "Confusion Matrix:\n",
      " [[94  5 66]\n",
      " [ 2 26  0]\n",
      " [ 8  0 17]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "072A     9\n",
      "033A     9\n",
      "045A     9\n",
      "022A     9\n",
      "094A     8\n",
      "013B     8\n",
      "010A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "099A     7\n",
      "027A     7\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "023A     6\n",
      "007A     6\n",
      "037A     6\n",
      "053A     6\n",
      "044A     5\n",
      "025C     5\n",
      "034A     5\n",
      "021A     5\n",
      "035A     4\n",
      "003A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "009A     4\n",
      "105A     4\n",
      "058A     3\n",
      "064A     3\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "014A     3\n",
      "093A     2\n",
      "025B     2\n",
      "038A     2\n",
      "087A     2\n",
      "102A     2\n",
      "032A     2\n",
      "054A     2\n",
      "018A     2\n",
      "115A     1\n",
      "100A     1\n",
      "090A     1\n",
      "024A     1\n",
      "110A     1\n",
      "073A     1\n",
      "066A     1\n",
      "091A     1\n",
      "088A     1\n",
      "048A     1\n",
      "026C     1\n",
      "041A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "042A    14\n",
      "106A    14\n",
      "116A    12\n",
      "039A    12\n",
      "063A    11\n",
      "040A    10\n",
      "016A    10\n",
      "071A    10\n",
      "014B    10\n",
      "065A     9\n",
      "015A     9\n",
      "051B     9\n",
      "095A     8\n",
      "070A     5\n",
      "075A     5\n",
      "023B     5\n",
      "062A     4\n",
      "060A     3\n",
      "011A     2\n",
      "069A     2\n",
      "061A     2\n",
      "043A     1\n",
      "092A     1\n",
      "076A     1\n",
      "004A     1\n",
      "019B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    241\n",
      "F    181\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    96\n",
      "X    75\n",
      "F    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 001A, 103A, 097B, 028A, 019A, 074...\n",
      "kitten    [044A, 111A, 046A, 047A, 109A, 050A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 104A, 055A, 059A, 113A, 054...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 071A, 062A, 002B, 095A, 065A, 039...\n",
      "kitten                             [014B, 040A, 042A, 043A]\n",
      "senior                 [106A, 116A, 051B, 016A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 12, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 4, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A' '010A'\n",
      " '012A' '013B' '014A' '018A' '019A' '020A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '004A' '011A' '014B' '015A' '016A' '019B' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'020A'}\n",
      "Moved to Test Set:\n",
      "{'020A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '012A' '013B' '014A' '018A' '019A' '021A' '022A' '023A' '024A'\n",
      " '025A' '025B' '025C' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '041A' '044A' '045A'\n",
      " '046A' '047A' '048A' '049A' '050A' '051A' '052A' '053A' '054A' '055A'\n",
      " '056A' '057A' '058A' '059A' '064A' '066A' '067A' '068A' '072A' '073A'\n",
      " '074A' '087A' '088A' '090A' '091A' '093A' '094A' '096A' '097A' '097B'\n",
      " '099A' '100A' '101A' '102A' '103A' '104A' '105A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '011A' '014B' '015A' '016A' '019B' '020A' '023B' '039A'\n",
      " '040A' '042A' '043A' '051B' '060A' '061A' '062A' '063A' '065A' '069A'\n",
      " '070A' '071A' '075A' '076A' '092A' '095A' '106A' '116A']\n",
      "Length of X_train_val:\n",
      "711\n",
      "Length of y_train_val:\n",
      "711\n",
      "Length of groups_train_val:\n",
      "711\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     430\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     158\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     446\n",
      "kitten    136\n",
      "senior    129\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     142\n",
      "senior     49\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 892, 1: 680, 2: 645})\n",
      "Epoch 1/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.9587 - accuracy: 0.5918\n",
      "Epoch 2/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.7220 - accuracy: 0.6973\n",
      "Epoch 3/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6522 - accuracy: 0.7339\n",
      "Epoch 4/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6110 - accuracy: 0.7510\n",
      "Epoch 5/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5868 - accuracy: 0.7600\n",
      "Epoch 6/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5511 - accuracy: 0.7709\n",
      "Epoch 7/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5054 - accuracy: 0.7975\n",
      "Epoch 8/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4834 - accuracy: 0.8097\n",
      "Epoch 9/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4822 - accuracy: 0.8011\n",
      "Epoch 10/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.8169\n",
      "Epoch 11/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4523 - accuracy: 0.8133\n",
      "Epoch 12/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4474 - accuracy: 0.8214\n",
      "Epoch 13/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4188 - accuracy: 0.8286\n",
      "Epoch 14/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4065 - accuracy: 0.8372\n",
      "Epoch 15/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4064 - accuracy: 0.8358\n",
      "Epoch 16/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3812 - accuracy: 0.8426\n",
      "Epoch 17/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4083 - accuracy: 0.8318\n",
      "Epoch 18/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3825 - accuracy: 0.8498\n",
      "Epoch 19/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3638 - accuracy: 0.8570\n",
      "Epoch 20/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3424 - accuracy: 0.8660\n",
      "Epoch 21/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3402 - accuracy: 0.8710\n",
      "Epoch 22/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3334 - accuracy: 0.8651\n",
      "Epoch 23/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3308 - accuracy: 0.8678\n",
      "Epoch 24/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3306 - accuracy: 0.8719\n",
      "Epoch 25/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3487 - accuracy: 0.8597\n",
      "Epoch 26/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3108 - accuracy: 0.8751\n",
      "Epoch 27/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8737\n",
      "Epoch 28/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2984 - accuracy: 0.8805\n",
      "Epoch 29/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2847 - accuracy: 0.8881\n",
      "Epoch 30/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2905 - accuracy: 0.8850\n",
      "Epoch 31/1500\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.2958 - accuracy: 0.8818\n",
      "Epoch 32/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2754 - accuracy: 0.8954\n",
      "Epoch 33/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2667 - accuracy: 0.8967\n",
      "Epoch 34/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2791 - accuracy: 0.8836\n",
      "Epoch 35/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.9017\n",
      "Epoch 36/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2633 - accuracy: 0.8954\n",
      "Epoch 37/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2704 - accuracy: 0.8999\n",
      "Epoch 38/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2902 - accuracy: 0.8863\n",
      "Epoch 39/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2532 - accuracy: 0.9026\n",
      "Epoch 40/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2444 - accuracy: 0.9084\n",
      "Epoch 41/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2394 - accuracy: 0.9120\n",
      "Epoch 42/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2522 - accuracy: 0.8981\n",
      "Epoch 43/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2542 - accuracy: 0.9048\n",
      "Epoch 44/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2378 - accuracy: 0.9089\n",
      "Epoch 45/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2426 - accuracy: 0.9102\n",
      "Epoch 46/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9062\n",
      "Epoch 47/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2372 - accuracy: 0.9066\n",
      "Epoch 48/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2120 - accuracy: 0.9206\n",
      "Epoch 49/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2303 - accuracy: 0.9175\n",
      "Epoch 50/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2124 - accuracy: 0.9193\n",
      "Epoch 51/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2277 - accuracy: 0.9111\n",
      "Epoch 52/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2074 - accuracy: 0.9283\n",
      "Epoch 53/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2174 - accuracy: 0.9125\n",
      "Epoch 54/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2140 - accuracy: 0.9220\n",
      "Epoch 55/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2145 - accuracy: 0.9229\n",
      "Epoch 56/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2096 - accuracy: 0.9166\n",
      "Epoch 57/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1960 - accuracy: 0.9238\n",
      "Epoch 58/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2068 - accuracy: 0.9220\n",
      "Epoch 59/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1804 - accuracy: 0.9350\n",
      "Epoch 60/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2118 - accuracy: 0.9220\n",
      "Epoch 61/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2059 - accuracy: 0.9233\n",
      "Epoch 62/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1989 - accuracy: 0.9265\n",
      "Epoch 63/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1837 - accuracy: 0.9314\n",
      "Epoch 64/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1937 - accuracy: 0.9292\n",
      "Epoch 65/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.9341\n",
      "Epoch 66/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9319\n",
      "Epoch 67/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9369\n",
      "Epoch 68/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.9296\n",
      "Epoch 69/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1868 - accuracy: 0.9260\n",
      "Epoch 70/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.9337\n",
      "Epoch 71/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9323\n",
      "Epoch 72/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9364\n",
      "Epoch 73/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1746 - accuracy: 0.9400\n",
      "Epoch 74/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1860 - accuracy: 0.9305\n",
      "Epoch 75/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1677 - accuracy: 0.9355\n",
      "Epoch 76/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9274\n",
      "Epoch 77/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.9287\n",
      "Epoch 78/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1752 - accuracy: 0.9359\n",
      "Epoch 79/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.9355\n",
      "Epoch 80/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1661 - accuracy: 0.9355\n",
      "Epoch 81/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.9373\n",
      "Epoch 82/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1629 - accuracy: 0.9364\n",
      "Epoch 83/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1584 - accuracy: 0.9405\n",
      "Epoch 84/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1594 - accuracy: 0.9423\n",
      "Epoch 85/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1581 - accuracy: 0.9369\n",
      "Epoch 86/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1691 - accuracy: 0.9359\n",
      "Epoch 87/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.9454\n",
      "Epoch 88/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1617 - accuracy: 0.9418\n",
      "Epoch 89/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9450\n",
      "Epoch 90/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9441\n",
      "Epoch 91/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.9441\n",
      "Epoch 92/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1556 - accuracy: 0.9423\n",
      "Epoch 93/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9481\n",
      "Epoch 94/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1756 - accuracy: 0.9323\n",
      "Epoch 95/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9504\n",
      "Epoch 96/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9526\n",
      "Epoch 97/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1310 - accuracy: 0.9526\n",
      "Epoch 98/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9522\n",
      "Epoch 99/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9526\n",
      "Epoch 100/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9459\n",
      "Epoch 101/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9531\n",
      "Epoch 102/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1518 - accuracy: 0.9499\n",
      "Epoch 103/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9522\n",
      "Epoch 104/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9459\n",
      "Epoch 105/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9549\n",
      "Epoch 106/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9549\n",
      "Epoch 107/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9585\n",
      "Epoch 108/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9490\n",
      "Epoch 109/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9517\n",
      "Epoch 110/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9562\n",
      "Epoch 111/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1356 - accuracy: 0.9463\n",
      "Epoch 112/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9630\n",
      "Epoch 113/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1319 - accuracy: 0.9549\n",
      "Epoch 114/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1361 - accuracy: 0.9508\n",
      "Epoch 115/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9581\n",
      "Epoch 116/1500\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.1297 - accuracy: 0.9535\n",
      "Epoch 117/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9581\n",
      "Epoch 118/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9590\n",
      "Epoch 119/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9531\n",
      "Epoch 120/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9558\n",
      "Epoch 121/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9581\n",
      "Epoch 122/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1414 - accuracy: 0.9463\n",
      "Epoch 123/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1241 - accuracy: 0.9531\n",
      "Epoch 124/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9562\n",
      "Epoch 125/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9621\n",
      "Epoch 126/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9603\n",
      "Epoch 127/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1122 - accuracy: 0.9644\n",
      "Epoch 128/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9635\n",
      "Epoch 129/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9540\n",
      "Epoch 130/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9535\n",
      "Epoch 131/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9653\n",
      "Epoch 132/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1390 - accuracy: 0.9463\n",
      "Epoch 133/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9571\n",
      "Epoch 134/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9617\n",
      "Epoch 135/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9540\n",
      "Epoch 136/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9599\n",
      "Epoch 137/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9576\n",
      "Epoch 138/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9531\n",
      "Epoch 139/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1201 - accuracy: 0.9535\n",
      "Epoch 140/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9626\n",
      "Epoch 141/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9657\n",
      "Epoch 142/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9630\n",
      "Epoch 143/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9603\n",
      "Epoch 144/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0975 - accuracy: 0.9675\n",
      "Epoch 145/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1057 - accuracy: 0.9657\n",
      "Epoch 146/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9693\n",
      "Epoch 147/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9639\n",
      "Epoch 148/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1131 - accuracy: 0.9576\n",
      "Epoch 149/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9648\n",
      "Epoch 150/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9648\n",
      "Epoch 151/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9626\n",
      "Epoch 152/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9707\n",
      "Epoch 153/1500\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.1112 - accuracy: 0.9626\n",
      "Epoch 154/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9635\n",
      "Epoch 155/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1026 - accuracy: 0.9648\n",
      "Epoch 156/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9603\n",
      "Epoch 157/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9571\n",
      "Epoch 158/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1027 - accuracy: 0.9644\n",
      "Epoch 159/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1014 - accuracy: 0.9626\n",
      "Epoch 160/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9680\n",
      "Epoch 161/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9657\n",
      "Epoch 162/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1435 - accuracy: 0.9531\n",
      "Epoch 163/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0970 - accuracy: 0.9666\n",
      "Epoch 164/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9662\n",
      "Epoch 165/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0984 - accuracy: 0.9698\n",
      "Epoch 166/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9689\n",
      "Epoch 167/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0918 - accuracy: 0.9693\n",
      "Epoch 168/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9626\n",
      "Epoch 169/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9644\n",
      "Epoch 170/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9581\n",
      "Epoch 171/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9571\n",
      "Epoch 172/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9666\n",
      "Epoch 173/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0995 - accuracy: 0.9675\n",
      "Epoch 174/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9666\n",
      "Epoch 175/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1029 - accuracy: 0.9653\n",
      "Epoch 176/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9725\n",
      "Epoch 177/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0969 - accuracy: 0.9653\n",
      "Epoch 178/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9770\n",
      "Epoch 179/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0986 - accuracy: 0.9671\n",
      "Epoch 180/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9621\n",
      "Epoch 181/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9729\n",
      "Epoch 182/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9644\n",
      "Epoch 183/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9644\n",
      "Epoch 184/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9689\n",
      "Epoch 185/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9662\n",
      "Epoch 186/1500\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0985 - accuracy: 0.9630\n",
      "Epoch 187/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.9662\n",
      "Epoch 188/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9680\n",
      "Epoch 189/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9608\n",
      "Epoch 190/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9702\n",
      "Epoch 191/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0797 - accuracy: 0.9743\n",
      "Epoch 192/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9666\n",
      "Epoch 193/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9671\n",
      "Epoch 194/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0831 - accuracy: 0.9720\n",
      "Epoch 195/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0917 - accuracy: 0.9720\n",
      "Epoch 196/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9639\n",
      "Epoch 197/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.9761\n",
      "Epoch 198/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9774\n",
      "Epoch 199/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9738\n",
      "Epoch 200/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9702\n",
      "Epoch 201/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9774\n",
      "Epoch 202/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9711\n",
      "Epoch 203/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0734 - accuracy: 0.9802\n",
      "Epoch 204/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.9797\n",
      "Epoch 205/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9720\n",
      "Epoch 206/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0935 - accuracy: 0.9671\n",
      "Epoch 207/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9635\n",
      "Epoch 208/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0884 - accuracy: 0.9666\n",
      "Epoch 209/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9716\n",
      "Epoch 210/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9648\n",
      "Epoch 211/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.9720\n",
      "Epoch 212/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0698 - accuracy: 0.9783\n",
      "Epoch 213/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9666\n",
      "Epoch 214/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.9693\n",
      "Epoch 215/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9639\n",
      "Epoch 216/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9684\n",
      "Epoch 217/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0718 - accuracy: 0.9783\n",
      "Epoch 218/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9666\n",
      "Epoch 219/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0804 - accuracy: 0.9720\n",
      "Epoch 220/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9747\n",
      "Epoch 221/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9725\n",
      "Epoch 222/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9707\n",
      "Epoch 223/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0774 - accuracy: 0.9743\n",
      "Epoch 224/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9738\n",
      "Epoch 225/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9725\n",
      "Epoch 226/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0695 - accuracy: 0.9779\n",
      "Epoch 227/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9711\n",
      "Epoch 228/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0768 - accuracy: 0.9793\n",
      "Epoch 229/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.9765\n",
      "Epoch 230/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.9738\n",
      "Epoch 231/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9729\n",
      "Epoch 232/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9797\n",
      "Epoch 233/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9734\n",
      "Epoch 234/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9725\n",
      "Epoch 235/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0909 - accuracy: 0.9684\n",
      "Epoch 236/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0824 - accuracy: 0.9702\n",
      "Epoch 237/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0731 - accuracy: 0.9765\n",
      "Epoch 238/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0778 - accuracy: 0.9734\n",
      "Epoch 239/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0833 - accuracy: 0.9707\n",
      "Epoch 240/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0614 - accuracy: 0.9824\n",
      "Epoch 241/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0756 - accuracy: 0.9761\n",
      "Epoch 242/1500\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0723 - accuracy: 0.9779\n",
      "Epoch 243/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0858 - accuracy: 0.9716\n",
      "Epoch 244/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9734\n",
      "Epoch 245/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9648\n",
      "Epoch 246/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9738\n",
      "Epoch 247/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0593 - accuracy: 0.9865\n",
      "Epoch 248/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0696 - accuracy: 0.9761\n",
      "Epoch 249/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.9756\n",
      "Epoch 250/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9756\n",
      "Epoch 251/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9734\n",
      "Epoch 252/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9738\n",
      "Epoch 253/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0773 - accuracy: 0.9756\n",
      "Epoch 254/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9756\n",
      "Epoch 255/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0843 - accuracy: 0.9684\n",
      "Epoch 256/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9788\n",
      "Epoch 257/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0755 - accuracy: 0.9752\n",
      "Epoch 258/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9707\n",
      "Epoch 259/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.9806\n",
      "Epoch 260/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0740 - accuracy: 0.9765\n",
      "Epoch 261/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0729 - accuracy: 0.9756\n",
      "Epoch 262/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0816 - accuracy: 0.9707\n",
      "Epoch 263/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.9783\n",
      "Epoch 264/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.9761\n",
      "Epoch 265/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9738\n",
      "Epoch 266/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0676 - accuracy: 0.9788\n",
      "Epoch 267/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.9761\n",
      "Epoch 268/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9720\n",
      "Epoch 269/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.9788\n",
      "Epoch 270/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0508 - accuracy: 0.9865\n",
      "Epoch 271/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0636 - accuracy: 0.9774\n",
      "Epoch 272/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9765\n",
      "Epoch 273/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0583 - accuracy: 0.9815\n",
      "Epoch 274/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9729\n",
      "Epoch 275/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9698\n",
      "Epoch 276/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0643 - accuracy: 0.9793\n",
      "Epoch 277/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9802\n",
      "Epoch 278/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0762 - accuracy: 0.9756\n",
      "Epoch 279/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9793\n",
      "Epoch 280/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0650 - accuracy: 0.9802\n",
      "Epoch 281/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.9793\n",
      "Epoch 282/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0570 - accuracy: 0.9851\n",
      "Epoch 283/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.9752\n",
      "Epoch 284/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0626 - accuracy: 0.9829\n",
      "Epoch 285/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9761\n",
      "Epoch 286/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0848 - accuracy: 0.9675\n",
      "Epoch 287/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0647 - accuracy: 0.9788\n",
      "Epoch 288/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0706 - accuracy: 0.9770\n",
      "Epoch 289/1500\n",
      "70/70 [==============================] - 0s 4ms/step - loss: 0.0674 - accuracy: 0.9811\n",
      "Epoch 290/1500\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0692 - accuracy: 0.9770\n",
      "Epoch 291/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.9797\n",
      "Epoch 292/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0549 - accuracy: 0.9847\n",
      "Epoch 293/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0702 - accuracy: 0.9774\n",
      "Epoch 294/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9716\n",
      "Epoch 295/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9793\n",
      "Epoch 296/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0547 - accuracy: 0.9806\n",
      "Epoch 297/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0658 - accuracy: 0.9788\n",
      "Epoch 298/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0596 - accuracy: 0.9815\n",
      "Epoch 299/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0631 - accuracy: 0.9783\n",
      "Epoch 300/1500\n",
      "45/70 [==================>...........] - ETA: 0s - loss: 0.0657 - accuracy: 0.9778Restoring model weights from the end of the best epoch: 270.\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.9797\n",
      "Epoch 300: early stopping\n",
      "8/8 [==============================] - 0s 812us/step - loss: 1.0103 - accuracy: 0.7920\n",
      "8/8 [==============================] - 0s 672us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (22/28)\n",
      "Before appending - Cat IDs: 218, Predictions: 218, Actuals: 218, Gender: 218\n",
      "After appending - Cat IDs: 444, Predictions: 444, Actuals: 444, Gender: 444\n",
      "Final Test Results - Loss: 1.0102852582931519, Accuracy: 0.7920354008674622, Precision: 0.7634560735231877, Recall: 0.7567596052505509, F1 Score: 0.7578585187759287\n",
      "Confusion Matrix:\n",
      " [[123   5  14]\n",
      " [  1  32   2]\n",
      " [ 25   0  24]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "055A    20\n",
      "000B    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "001A    14\n",
      "097B    14\n",
      "042A    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "040A    10\n",
      "071A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "033A     9\n",
      "065A     9\n",
      "015A     9\n",
      "045A     9\n",
      "095A     8\n",
      "117A     7\n",
      "099A     7\n",
      "031A     7\n",
      "023A     6\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "021A     5\n",
      "023B     5\n",
      "025C     5\n",
      "070A     5\n",
      "034A     5\n",
      "044A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "012A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "060A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "038A     2\n",
      "093A     2\n",
      "018A     2\n",
      "054A     2\n",
      "088A     1\n",
      "090A     1\n",
      "110A     1\n",
      "091A     1\n",
      "019B     1\n",
      "092A     1\n",
      "004A     1\n",
      "049A     1\n",
      "076A     1\n",
      "043A     1\n",
      "026C     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "020A    23\n",
      "067A    19\n",
      "097A    16\n",
      "059A    14\n",
      "022A     9\n",
      "072A     9\n",
      "010A     8\n",
      "094A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "037A     6\n",
      "109A     6\n",
      "008A     6\n",
      "009A     4\n",
      "003A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "025B     2\n",
      "087A     2\n",
      "032A     2\n",
      "066A     1\n",
      "048A     1\n",
      "041A     1\n",
      "115A     1\n",
      "096A     1\n",
      "100A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    243\n",
      "X    229\n",
      "F    226\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    119\n",
      "M     94\n",
      "F     26\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [044A, 014B, 111A, 040A, 047A, 042A, 043A, 049...\n",
      "senior    [093A, 057A, 106A, 104A, 055A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [067A, 020A, 022A, 072A, 009A, 027A, 013B, 014...\n",
      "kitten                 [046A, 109A, 050A, 041A, 048A, 115A]\n",
      "senior                       [097A, 059A, 058A, 094A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '044A' '045A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '046A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'044A'}\n",
      "Moved to Test Set:\n",
      "{'044A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '005A' '006A' '007A' '011A'\n",
      " '012A' '014B' '015A' '016A' '018A' '019A' '019B' '021A' '023A' '023B'\n",
      " '025A' '025C' '026A' '026B' '026C' '028A' '029A' '031A' '033A' '034A'\n",
      " '035A' '036A' '038A' '039A' '040A' '042A' '043A' '045A' '046A' '047A'\n",
      " '049A' '051A' '051B' '052A' '053A' '054A' '055A' '056A' '057A' '060A'\n",
      " '061A' '062A' '063A' '065A' '068A' '069A' '070A' '071A' '073A' '074A'\n",
      " '075A' '076A' '088A' '090A' '091A' '092A' '093A' '095A' '097B' '099A'\n",
      " '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A' '113A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '008A' '009A' '010A' '013B' '014A' '020A' '022A' '024A' '025B'\n",
      " '027A' '032A' '037A' '041A' '044A' '048A' '050A' '058A' '059A' '064A'\n",
      " '066A' '067A' '072A' '087A' '094A' '096A' '097A' '100A' '109A' '115A']\n",
      "Length of X_train_val:\n",
      "756\n",
      "Length of y_train_val:\n",
      "756\n",
      "Length of groups_train_val:\n",
      "756\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     470\n",
      "senior    136\n",
      "kitten     92\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     118\n",
      "kitten     79\n",
      "senior     42\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     470\n",
      "kitten    150\n",
      "senior    136\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     118\n",
      "senior     42\n",
      "kitten     21\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 940, 1: 750, 2: 680})\n",
      "Epoch 1/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.8811 - accuracy: 0.6152\n",
      "Epoch 2/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6940 - accuracy: 0.7127\n",
      "Epoch 3/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6352 - accuracy: 0.7325\n",
      "Epoch 4/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6091 - accuracy: 0.7384\n",
      "Epoch 5/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5598 - accuracy: 0.7734\n",
      "Epoch 6/1500\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5548 - accuracy: 0.7726\n",
      "Epoch 7/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5143 - accuracy: 0.7831\n",
      "Epoch 8/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5082 - accuracy: 0.7899\n",
      "Epoch 9/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4841 - accuracy: 0.7941\n",
      "Epoch 10/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4932 - accuracy: 0.7983\n",
      "Epoch 11/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4772 - accuracy: 0.8059\n",
      "Epoch 12/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4645 - accuracy: 0.8105\n",
      "Epoch 13/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4458 - accuracy: 0.8236\n",
      "Epoch 14/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4314 - accuracy: 0.8224\n",
      "Epoch 15/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4298 - accuracy: 0.8257\n",
      "Epoch 16/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4256 - accuracy: 0.8228\n",
      "Epoch 17/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4304 - accuracy: 0.8219\n",
      "Epoch 18/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4181 - accuracy: 0.8274\n",
      "Epoch 19/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4175 - accuracy: 0.8308\n",
      "Epoch 20/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4172 - accuracy: 0.8304\n",
      "Epoch 21/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4015 - accuracy: 0.8414\n",
      "Epoch 22/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3924 - accuracy: 0.8443\n",
      "Epoch 23/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3931 - accuracy: 0.8405\n",
      "Epoch 24/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3816 - accuracy: 0.8447\n",
      "Epoch 25/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4034 - accuracy: 0.8371\n",
      "Epoch 26/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3792 - accuracy: 0.8502\n",
      "Epoch 27/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3713 - accuracy: 0.8405\n",
      "Epoch 28/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3588 - accuracy: 0.8570\n",
      "Epoch 29/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3615 - accuracy: 0.8498\n",
      "Epoch 30/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3477 - accuracy: 0.8629\n",
      "Epoch 31/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3620 - accuracy: 0.8553\n",
      "Epoch 32/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3531 - accuracy: 0.8616\n",
      "Epoch 33/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3458 - accuracy: 0.8540\n",
      "Epoch 34/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3446 - accuracy: 0.8570\n",
      "Epoch 35/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3361 - accuracy: 0.8700\n",
      "Epoch 36/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3411 - accuracy: 0.8692\n",
      "Epoch 37/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3276 - accuracy: 0.8738\n",
      "Epoch 38/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3393 - accuracy: 0.8637\n",
      "Epoch 39/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3309 - accuracy: 0.8675\n",
      "Epoch 40/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8646\n",
      "Epoch 41/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3175 - accuracy: 0.8713\n",
      "Epoch 42/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3199 - accuracy: 0.8755\n",
      "Epoch 43/1500\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.3100 - accuracy: 0.8722\n",
      "Epoch 44/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3199 - accuracy: 0.8713\n",
      "Epoch 45/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8696\n",
      "Epoch 46/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3050 - accuracy: 0.8844\n",
      "Epoch 47/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3157 - accuracy: 0.8667\n",
      "Epoch 48/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3048 - accuracy: 0.8679\n",
      "Epoch 49/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3077 - accuracy: 0.8751\n",
      "Epoch 50/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3077 - accuracy: 0.8785\n",
      "Epoch 51/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3053 - accuracy: 0.8654\n",
      "Epoch 52/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3011 - accuracy: 0.8785\n",
      "Epoch 53/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2937 - accuracy: 0.8802\n",
      "Epoch 54/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2781 - accuracy: 0.8983\n",
      "Epoch 55/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3126 - accuracy: 0.8722\n",
      "Epoch 56/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2843 - accuracy: 0.8882\n",
      "Epoch 57/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2676 - accuracy: 0.8920\n",
      "Epoch 58/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2805 - accuracy: 0.8899\n",
      "Epoch 59/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2907 - accuracy: 0.8852\n",
      "Epoch 60/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2711 - accuracy: 0.8996\n",
      "Epoch 61/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2701 - accuracy: 0.8983\n",
      "Epoch 62/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2794 - accuracy: 0.8873\n",
      "Epoch 63/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2665 - accuracy: 0.8996\n",
      "Epoch 64/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2690 - accuracy: 0.8975\n",
      "Epoch 65/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.8987\n",
      "Epoch 66/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2658 - accuracy: 0.8962\n",
      "Epoch 67/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2671 - accuracy: 0.8949\n",
      "Epoch 68/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2652 - accuracy: 0.9004\n",
      "Epoch 69/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2735 - accuracy: 0.8992\n",
      "Epoch 70/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2584 - accuracy: 0.8945\n",
      "Epoch 71/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2556 - accuracy: 0.9013\n",
      "Epoch 72/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9131\n",
      "Epoch 73/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2471 - accuracy: 0.9004\n",
      "Epoch 74/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2625 - accuracy: 0.9017\n",
      "Epoch 75/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2463 - accuracy: 0.9055\n",
      "Epoch 76/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2597 - accuracy: 0.9038\n",
      "Epoch 77/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2451 - accuracy: 0.9110\n",
      "Epoch 78/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2459 - accuracy: 0.9084\n",
      "Epoch 79/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2303 - accuracy: 0.9110\n",
      "Epoch 80/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2517 - accuracy: 0.9046\n",
      "Epoch 81/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2482 - accuracy: 0.9055\n",
      "Epoch 82/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2536 - accuracy: 0.9025\n",
      "Epoch 83/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2396 - accuracy: 0.9093\n",
      "Epoch 84/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2399 - accuracy: 0.9105\n",
      "Epoch 85/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2312 - accuracy: 0.9135\n",
      "Epoch 86/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2283 - accuracy: 0.9110\n",
      "Epoch 87/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2469 - accuracy: 0.9063\n",
      "Epoch 88/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2255 - accuracy: 0.9114\n",
      "Epoch 89/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2361 - accuracy: 0.9089\n",
      "Epoch 90/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2411 - accuracy: 0.9135\n",
      "Epoch 91/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2328 - accuracy: 0.9076\n",
      "Epoch 92/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2379 - accuracy: 0.9101\n",
      "Epoch 93/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2365 - accuracy: 0.9059\n",
      "Epoch 94/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2283 - accuracy: 0.9148\n",
      "Epoch 95/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2476 - accuracy: 0.8996\n",
      "Epoch 96/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2195 - accuracy: 0.9236\n",
      "Epoch 97/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9148\n",
      "Epoch 98/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.9008\n",
      "Epoch 99/1500\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2241 - accuracy: 0.9139\n",
      "Epoch 100/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2271 - accuracy: 0.9148\n",
      "Epoch 101/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2162 - accuracy: 0.9143\n",
      "Epoch 102/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2376 - accuracy: 0.9068\n",
      "Epoch 103/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2193 - accuracy: 0.9177\n",
      "Epoch 104/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2003 - accuracy: 0.9232\n",
      "Epoch 105/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1931 - accuracy: 0.9278\n",
      "Epoch 106/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2185 - accuracy: 0.9152\n",
      "Epoch 107/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9165\n",
      "Epoch 108/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2012 - accuracy: 0.9253\n",
      "Epoch 109/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2275 - accuracy: 0.9169\n",
      "Epoch 110/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2007 - accuracy: 0.9219\n",
      "Epoch 111/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2114 - accuracy: 0.9152\n",
      "Epoch 112/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2103 - accuracy: 0.9249\n",
      "Epoch 113/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2149 - accuracy: 0.9249\n",
      "Epoch 114/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2132 - accuracy: 0.9211\n",
      "Epoch 115/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2002 - accuracy: 0.9274\n",
      "Epoch 116/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9165\n",
      "Epoch 117/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9321\n",
      "Epoch 118/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2028 - accuracy: 0.9270\n",
      "Epoch 119/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9152\n",
      "Epoch 120/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2064 - accuracy: 0.9173\n",
      "Epoch 121/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2083 - accuracy: 0.9211\n",
      "Epoch 122/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2009 - accuracy: 0.9219\n",
      "Epoch 123/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1873 - accuracy: 0.9283\n",
      "Epoch 124/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1943 - accuracy: 0.9211\n",
      "Epoch 125/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.9160\n",
      "Epoch 126/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1998 - accuracy: 0.9228\n",
      "Epoch 127/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.9397\n",
      "Epoch 128/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9241\n",
      "Epoch 129/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1916 - accuracy: 0.9266\n",
      "Epoch 130/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9253\n",
      "Epoch 131/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2018 - accuracy: 0.9215\n",
      "Epoch 132/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2021 - accuracy: 0.9241\n",
      "Epoch 133/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1913 - accuracy: 0.9333\n",
      "Epoch 134/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.9333\n",
      "Epoch 135/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9325\n",
      "Epoch 136/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1790 - accuracy: 0.9316\n",
      "Epoch 137/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1898 - accuracy: 0.9316\n",
      "Epoch 138/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1841 - accuracy: 0.9346\n",
      "Epoch 139/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9422\n",
      "Epoch 140/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9321\n",
      "Epoch 141/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1857 - accuracy: 0.9354\n",
      "Epoch 142/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.9308\n",
      "Epoch 143/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1812 - accuracy: 0.9312\n",
      "Epoch 144/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1770 - accuracy: 0.9316\n",
      "Epoch 145/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9342\n",
      "Epoch 146/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9367\n",
      "Epoch 147/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1698 - accuracy: 0.9380\n",
      "Epoch 148/1500\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1736 - accuracy: 0.9426\n",
      "Epoch 149/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1801 - accuracy: 0.9333\n",
      "Epoch 150/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1657 - accuracy: 0.9409\n",
      "Epoch 151/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1798 - accuracy: 0.9338\n",
      "Epoch 152/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9392\n",
      "Epoch 153/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1807 - accuracy: 0.9316\n",
      "Epoch 154/1500\n",
      "75/75 [==============================] - 0s 984us/step - loss: 0.1824 - accuracy: 0.9376\n",
      "Epoch 155/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1794 - accuracy: 0.9287\n",
      "Epoch 156/1500\n",
      "75/75 [==============================] - 0s 976us/step - loss: 0.1675 - accuracy: 0.9384\n",
      "Epoch 157/1500\n",
      "75/75 [==============================] - 0s 965us/step - loss: 0.1738 - accuracy: 0.9304\n",
      "Epoch 158/1500\n",
      "75/75 [==============================] - 0s 955us/step - loss: 0.1519 - accuracy: 0.9515\n",
      "Epoch 159/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1676 - accuracy: 0.9367\n",
      "Epoch 160/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1695 - accuracy: 0.9380\n",
      "Epoch 161/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.9338\n",
      "Epoch 162/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1889 - accuracy: 0.9245\n",
      "Epoch 163/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1726 - accuracy: 0.9392\n",
      "Epoch 164/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.9405\n",
      "Epoch 165/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9392\n",
      "Epoch 166/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1757 - accuracy: 0.9354\n",
      "Epoch 167/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1575 - accuracy: 0.9422\n",
      "Epoch 168/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9380\n",
      "Epoch 169/1500\n",
      "48/75 [==================>...........] - ETA: 0s - loss: 0.1605 - accuracy: 0.9349Restoring model weights from the end of the best epoch: 139.\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9363\n",
      "Epoch 169: early stopping\n",
      "6/6 [==============================] - 0s 979us/step - loss: 0.4220 - accuracy: 0.8011\n",
      "6/6 [==============================] - 0s 791us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (24/30)\n",
      "Before appending - Cat IDs: 444, Predictions: 444, Actuals: 444, Gender: 444\n",
      "After appending - Cat IDs: 625, Predictions: 625, Actuals: 625, Gender: 625\n",
      "Final Test Results - Loss: 0.4219590425491333, Accuracy: 0.8011049628257751, Precision: 0.7966189185701381, Recall: 0.714420231369384, F1 Score: 0.74565616094354\n",
      "Confusion Matrix:\n",
      " [[104   2  12]\n",
      " [  9  12   0]\n",
      " [ 13   0  29]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "002B    32\n",
      "074A    25\n",
      "020A    23\n",
      "000B    19\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "059A    14\n",
      "042A    14\n",
      "111A    13\n",
      "039A    12\n",
      "051A    12\n",
      "116A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "016A    10\n",
      "005A    10\n",
      "040A    10\n",
      "014B    10\n",
      "071A    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "010A     8\n",
      "094A     8\n",
      "027A     7\n",
      "050A     7\n",
      "117A     7\n",
      "037A     6\n",
      "053A     6\n",
      "008A     6\n",
      "109A     6\n",
      "023A     6\n",
      "044A     5\n",
      "023B     5\n",
      "070A     5\n",
      "075A     5\n",
      "009A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "062A     4\n",
      "014A     3\n",
      "058A     3\n",
      "064A     3\n",
      "060A     3\n",
      "061A     2\n",
      "102A     2\n",
      "011A     2\n",
      "025B     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "032A     2\n",
      "018A     2\n",
      "069A     2\n",
      "092A     1\n",
      "100A     1\n",
      "096A     1\n",
      "110A     1\n",
      "115A     1\n",
      "019B     1\n",
      "041A     1\n",
      "004A     1\n",
      "043A     1\n",
      "048A     1\n",
      "066A     1\n",
      "091A     1\n",
      "076A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "055A    20\n",
      "101A    15\n",
      "028A    13\n",
      "002A    13\n",
      "025A    11\n",
      "033A     9\n",
      "031A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "034A     5\n",
      "025C     5\n",
      "021A     5\n",
      "026A     4\n",
      "035A     4\n",
      "012A     3\n",
      "006A     3\n",
      "113A     3\n",
      "056A     3\n",
      "093A     2\n",
      "049A     1\n",
      "026C     1\n",
      "073A     1\n",
      "088A     1\n",
      "090A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    277\n",
      "M    271\n",
      "F    151\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    101\n",
      "X     71\n",
      "M     66\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 015A, 001A, 071A, 097B, 019A, 074A, 067...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 042A, 109A, 050...\n",
      "senior    [097A, 106A, 104A, 059A, 116A, 051B, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 033A, 103A, 028A, 101A, 034A, 002A, 099...\n",
      "kitten                                         [047A, 049A]\n",
      "senior           [093A, 057A, 055A, 113A, 056A, 108A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 14, 'senior': 15}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 2, 'senior': 7}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002B' '003A' '004A' '005A' '008A' '009A' '010A'\n",
      " '011A' '013B' '014A' '014B' '015A' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023A' '023B' '024A' '025B' '027A' '029A' '032A' '036A' '037A'\n",
      " '038A' '039A' '040A' '041A' '042A' '043A' '044A' '045A' '046A' '048A'\n",
      " '050A' '051A' '051B' '052A' '053A' '054A' '058A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '070A' '071A'\n",
      " '072A' '074A' '075A' '076A' '087A' '091A' '092A' '094A' '095A' '096A'\n",
      " '097A' '097B' '100A' '102A' '104A' '105A' '106A' '109A' '110A' '111A'\n",
      " '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002A' '006A' '007A' '012A' '021A' '025A' '025C' '026A' '026B' '026C'\n",
      " '028A' '031A' '033A' '034A' '035A' '047A' '049A' '055A' '056A' '057A'\n",
      " '073A' '088A' '090A' '093A' '099A' '101A' '103A' '108A' '113A']\n",
      "Length of X_train_val:\n",
      "699\n",
      "Length of y_train_val:\n",
      "699\n",
      "Length of groups_train_val:\n",
      "699\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     441\n",
      "kitten    142\n",
      "senior    116\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     147\n",
      "senior     62\n",
      "kitten     29\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 882, 1: 710, 2: 580})\n",
      "Epoch 1/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.9659 - accuracy: 0.5921\n",
      "Epoch 2/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7488 - accuracy: 0.6966\n",
      "Epoch 3/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6438 - accuracy: 0.7353\n",
      "Epoch 4/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5908 - accuracy: 0.7615\n",
      "Epoch 5/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5653 - accuracy: 0.7638\n",
      "Epoch 6/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5563 - accuracy: 0.7680\n",
      "Epoch 7/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5276 - accuracy: 0.7804\n",
      "Epoch 8/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4967 - accuracy: 0.7901\n",
      "Epoch 9/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4840 - accuracy: 0.8085\n",
      "Epoch 10/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4717 - accuracy: 0.8066\n",
      "Epoch 11/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4357 - accuracy: 0.8287\n",
      "Epoch 12/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4493 - accuracy: 0.8140\n",
      "Epoch 13/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4132 - accuracy: 0.8310\n",
      "Epoch 14/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4007 - accuracy: 0.8370\n",
      "Epoch 15/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4012 - accuracy: 0.8287\n",
      "Epoch 16/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3908 - accuracy: 0.8430\n",
      "Epoch 17/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3700 - accuracy: 0.8504\n",
      "Epoch 18/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3589 - accuracy: 0.8458\n",
      "Epoch 19/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3787 - accuracy: 0.8370\n",
      "Epoch 20/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3663 - accuracy: 0.8513\n",
      "Epoch 21/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3586 - accuracy: 0.8582\n",
      "Epoch 22/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3497 - accuracy: 0.8642\n",
      "Epoch 23/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3472 - accuracy: 0.8591\n",
      "Epoch 24/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3360 - accuracy: 0.8688\n",
      "Epoch 25/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3331 - accuracy: 0.8669\n",
      "Epoch 26/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3440 - accuracy: 0.8669\n",
      "Epoch 27/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3279 - accuracy: 0.8715\n",
      "Epoch 28/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2990 - accuracy: 0.8812\n",
      "Epoch 29/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3158 - accuracy: 0.8757\n",
      "Epoch 30/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3167 - accuracy: 0.8748\n",
      "Epoch 31/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2879 - accuracy: 0.8840\n",
      "Epoch 32/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2854 - accuracy: 0.8840\n",
      "Epoch 33/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2904 - accuracy: 0.8794\n",
      "Epoch 34/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.8817\n",
      "Epoch 35/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2996 - accuracy: 0.8785\n",
      "Epoch 36/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2965 - accuracy: 0.8766\n",
      "Epoch 37/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2907 - accuracy: 0.8877\n",
      "Epoch 38/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2856 - accuracy: 0.8900\n",
      "Epoch 39/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2590 - accuracy: 0.8890\n",
      "Epoch 40/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2726 - accuracy: 0.8918\n",
      "Epoch 41/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.8964\n",
      "Epoch 42/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2705 - accuracy: 0.8996\n",
      "Epoch 43/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2637 - accuracy: 0.8959\n",
      "Epoch 44/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2256 - accuracy: 0.9093\n",
      "Epoch 45/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2336 - accuracy: 0.9098\n",
      "Epoch 46/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2401 - accuracy: 0.9042\n",
      "Epoch 47/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2506 - accuracy: 0.9038\n",
      "Epoch 48/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2304 - accuracy: 0.9116\n",
      "Epoch 49/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2330 - accuracy: 0.9121\n",
      "Epoch 50/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2330 - accuracy: 0.9116\n",
      "Epoch 51/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2413 - accuracy: 0.9056\n",
      "Epoch 52/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2183 - accuracy: 0.9162\n",
      "Epoch 53/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2168 - accuracy: 0.9107\n",
      "Epoch 54/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9162\n",
      "Epoch 55/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2274 - accuracy: 0.9130\n",
      "Epoch 56/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2055 - accuracy: 0.9259\n",
      "Epoch 57/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2203 - accuracy: 0.9125\n",
      "Epoch 58/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2255 - accuracy: 0.9061\n",
      "Epoch 59/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2208 - accuracy: 0.9144\n",
      "Epoch 60/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2103 - accuracy: 0.9231\n",
      "Epoch 61/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2042 - accuracy: 0.9208\n",
      "Epoch 62/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2154 - accuracy: 0.9144\n",
      "Epoch 63/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2134 - accuracy: 0.9176\n",
      "Epoch 64/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2100 - accuracy: 0.9213\n",
      "Epoch 65/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2039 - accuracy: 0.9194\n",
      "Epoch 66/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2187 - accuracy: 0.9116\n",
      "Epoch 67/1500\n",
      "68/68 [==============================] - 0s 4ms/step - loss: 0.2080 - accuracy: 0.9208\n",
      "Epoch 68/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.2107 - accuracy: 0.9203\n",
      "Epoch 69/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2040 - accuracy: 0.9167\n",
      "Epoch 70/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2123 - accuracy: 0.9190\n",
      "Epoch 71/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1894 - accuracy: 0.9263\n",
      "Epoch 72/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1880 - accuracy: 0.9259\n",
      "Epoch 73/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2068 - accuracy: 0.9199\n",
      "Epoch 74/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2104 - accuracy: 0.9199\n",
      "Epoch 75/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1820 - accuracy: 0.9328\n",
      "Epoch 76/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1848 - accuracy: 0.9355\n",
      "Epoch 77/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1944 - accuracy: 0.9185\n",
      "Epoch 78/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1819 - accuracy: 0.9319\n",
      "Epoch 79/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.9277\n",
      "Epoch 80/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9397\n",
      "Epoch 81/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1806 - accuracy: 0.9309\n",
      "Epoch 82/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9323\n",
      "Epoch 83/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9346\n",
      "Epoch 84/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9365\n",
      "Epoch 85/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9401\n",
      "Epoch 86/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1777 - accuracy: 0.9328\n",
      "Epoch 87/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9383\n",
      "Epoch 88/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1796 - accuracy: 0.9314\n",
      "Epoch 89/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1521 - accuracy: 0.9424\n",
      "Epoch 90/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9406\n",
      "Epoch 91/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1657 - accuracy: 0.9337\n",
      "Epoch 92/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1542 - accuracy: 0.9438\n",
      "Epoch 93/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.9337\n",
      "Epoch 94/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1741 - accuracy: 0.9360\n",
      "Epoch 95/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1592 - accuracy: 0.9429\n",
      "Epoch 96/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1753 - accuracy: 0.9337\n",
      "Epoch 97/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1667 - accuracy: 0.9369\n",
      "Epoch 98/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9378\n",
      "Epoch 99/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1679 - accuracy: 0.9328\n",
      "Epoch 100/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9397\n",
      "Epoch 101/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9415\n",
      "Epoch 102/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1497 - accuracy: 0.9411\n",
      "Epoch 103/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1457 - accuracy: 0.9448\n",
      "Epoch 104/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.9438\n",
      "Epoch 105/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9383\n",
      "Epoch 106/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9461\n",
      "Epoch 107/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1566 - accuracy: 0.9411\n",
      "Epoch 108/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1546 - accuracy: 0.9415\n",
      "Epoch 109/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9512\n",
      "Epoch 110/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9388\n",
      "Epoch 111/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9480\n",
      "Epoch 112/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9494\n",
      "Epoch 113/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9457\n",
      "Epoch 114/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1626 - accuracy: 0.9346\n",
      "Epoch 115/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1626 - accuracy: 0.9415\n",
      "Epoch 116/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1468 - accuracy: 0.9415\n",
      "Epoch 117/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1515 - accuracy: 0.9452\n",
      "Epoch 118/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9401\n",
      "Epoch 119/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9498\n",
      "Epoch 120/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9484\n",
      "Epoch 121/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1385 - accuracy: 0.9494\n",
      "Epoch 122/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9535\n",
      "Epoch 123/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1272 - accuracy: 0.9572\n",
      "Epoch 124/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9471\n",
      "Epoch 125/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1569 - accuracy: 0.9448\n",
      "Epoch 126/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9471\n",
      "Epoch 127/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9466\n",
      "Epoch 128/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9567\n",
      "Epoch 129/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9544\n",
      "Epoch 130/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1359 - accuracy: 0.9517\n",
      "Epoch 131/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9503\n",
      "Epoch 132/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9461\n",
      "Epoch 133/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9530\n",
      "Epoch 134/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1291 - accuracy: 0.9544\n",
      "Epoch 135/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9503\n",
      "Epoch 136/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9503\n",
      "Epoch 137/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9540\n",
      "Epoch 138/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.9517\n",
      "Epoch 139/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1402 - accuracy: 0.9521\n",
      "Epoch 140/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1197 - accuracy: 0.9581\n",
      "Epoch 141/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9466\n",
      "Epoch 142/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9526\n",
      "Epoch 143/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9517\n",
      "Epoch 144/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9553\n",
      "Epoch 145/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9471\n",
      "Epoch 146/1500\n",
      "68/68 [==============================] - 0s 4ms/step - loss: 0.1217 - accuracy: 0.9549\n",
      "Epoch 147/1500\n",
      "68/68 [==============================] - 0s 2ms/step - loss: 0.1378 - accuracy: 0.9503\n",
      "Epoch 148/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1355 - accuracy: 0.9517\n",
      "Epoch 149/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9572\n",
      "Epoch 150/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9572\n",
      "Epoch 151/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9507\n",
      "Epoch 152/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9558\n",
      "Epoch 153/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.9535\n",
      "Epoch 154/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9581\n",
      "Epoch 155/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9604\n",
      "Epoch 156/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9567\n",
      "Epoch 157/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9627\n",
      "Epoch 158/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9613\n",
      "Epoch 159/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9521\n",
      "Epoch 160/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9530\n",
      "Epoch 161/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1301 - accuracy: 0.9590\n",
      "Epoch 162/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9549\n",
      "Epoch 163/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1070 - accuracy: 0.9627\n",
      "Epoch 164/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9609\n",
      "Epoch 165/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9618\n",
      "Epoch 166/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9655\n",
      "Epoch 167/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9503\n",
      "Epoch 168/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9535\n",
      "Epoch 169/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9645\n",
      "Epoch 170/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9632\n",
      "Epoch 171/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9572\n",
      "Epoch 172/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9549\n",
      "Epoch 173/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9540\n",
      "Epoch 174/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9599\n",
      "Epoch 175/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9586\n",
      "Epoch 176/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9599\n",
      "Epoch 177/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9572\n",
      "Epoch 178/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9618\n",
      "Epoch 179/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9581\n",
      "Epoch 180/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9572\n",
      "Epoch 181/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1138 - accuracy: 0.9599\n",
      "Epoch 182/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9553\n",
      "Epoch 183/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9673\n",
      "Epoch 184/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9572\n",
      "Epoch 185/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9650\n",
      "Epoch 186/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9599\n",
      "Epoch 187/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9655\n",
      "Epoch 188/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9609\n",
      "Epoch 189/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1022 - accuracy: 0.9632\n",
      "Epoch 190/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9650\n",
      "Epoch 191/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1015 - accuracy: 0.9622\n",
      "Epoch 192/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9581\n",
      "Epoch 193/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9609\n",
      "Epoch 194/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9622\n",
      "Epoch 195/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9705\n",
      "Epoch 196/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0988 - accuracy: 0.9622\n",
      "Epoch 197/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9604\n",
      "Epoch 198/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9632\n",
      "Epoch 199/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9710\n",
      "Epoch 200/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9687\n",
      "Epoch 201/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9632\n",
      "Epoch 202/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0962 - accuracy: 0.9673\n",
      "Epoch 203/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9705\n",
      "Epoch 204/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9701\n",
      "Epoch 205/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9622\n",
      "Epoch 206/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9627\n",
      "Epoch 207/1500\n",
      "68/68 [==============================] - 0s 4ms/step - loss: 0.1063 - accuracy: 0.9632\n",
      "Epoch 208/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9572\n",
      "Epoch 209/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9701\n",
      "Epoch 210/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0947 - accuracy: 0.9655\n",
      "Epoch 211/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9604\n",
      "Epoch 212/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1263 - accuracy: 0.9517\n",
      "Epoch 213/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9705\n",
      "Epoch 214/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9567\n",
      "Epoch 215/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9687\n",
      "Epoch 216/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9613\n",
      "Epoch 217/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1000 - accuracy: 0.9645\n",
      "Epoch 218/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9581\n",
      "Epoch 219/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9678\n",
      "Epoch 220/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9733\n",
      "Epoch 221/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9678\n",
      "Epoch 222/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0952 - accuracy: 0.9687\n",
      "Epoch 223/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9618\n",
      "Epoch 224/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1035 - accuracy: 0.9632\n",
      "Epoch 225/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9687\n",
      "Epoch 226/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9645\n",
      "Epoch 227/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0937 - accuracy: 0.9655\n",
      "Epoch 228/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9705\n",
      "Epoch 229/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9724\n",
      "Epoch 230/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0869 - accuracy: 0.9692\n",
      "Epoch 231/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0840 - accuracy: 0.9742\n",
      "Epoch 232/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9590\n",
      "Epoch 233/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9701\n",
      "Epoch 234/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9710\n",
      "Epoch 235/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9733\n",
      "Epoch 236/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0838 - accuracy: 0.9692\n",
      "Epoch 237/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9701\n",
      "Epoch 238/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9705\n",
      "Epoch 239/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.9692\n",
      "Epoch 240/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9742\n",
      "Epoch 241/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9687\n",
      "Epoch 242/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9692\n",
      "Epoch 243/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9678\n",
      "Epoch 244/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9682\n",
      "Epoch 245/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9724\n",
      "Epoch 246/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9682\n",
      "Epoch 247/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9761\n",
      "Epoch 248/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9590\n",
      "Epoch 249/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9673\n",
      "Epoch 250/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0934 - accuracy: 0.9673\n",
      "Epoch 251/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9705\n",
      "Epoch 252/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0839 - accuracy: 0.9692\n",
      "Epoch 253/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9632\n",
      "Epoch 254/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9705\n",
      "Epoch 255/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9618\n",
      "Epoch 256/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0820 - accuracy: 0.9751\n",
      "Epoch 257/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9673\n",
      "Epoch 258/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9733\n",
      "Epoch 259/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0652 - accuracy: 0.9779\n",
      "Epoch 260/1500\n",
      "68/68 [==============================] - 0s 4ms/step - loss: 0.0708 - accuracy: 0.9779\n",
      "Epoch 261/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.9765\n",
      "Epoch 262/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9747\n",
      "Epoch 263/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9664\n",
      "Epoch 264/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0921 - accuracy: 0.9696\n",
      "Epoch 265/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1003 - accuracy: 0.9609\n",
      "Epoch 266/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0859 - accuracy: 0.9678\n",
      "Epoch 267/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0683 - accuracy: 0.9784\n",
      "Epoch 268/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9756\n",
      "Epoch 269/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9719\n",
      "Epoch 270/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0794 - accuracy: 0.9751\n",
      "Epoch 271/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0922 - accuracy: 0.9682\n",
      "Epoch 272/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9696\n",
      "Epoch 273/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0747 - accuracy: 0.9751\n",
      "Epoch 274/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9673\n",
      "Epoch 275/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9678\n",
      "Epoch 276/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0688 - accuracy: 0.9774\n",
      "Epoch 277/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 0.9751\n",
      "Epoch 278/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9719\n",
      "Epoch 279/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0871 - accuracy: 0.9692\n",
      "Epoch 280/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0659 - accuracy: 0.9774\n",
      "Epoch 281/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0837 - accuracy: 0.9742\n",
      "Epoch 282/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0736 - accuracy: 0.9742\n",
      "Epoch 283/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9747\n",
      "Epoch 284/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9696\n",
      "Epoch 285/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9733\n",
      "Epoch 286/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.9742\n",
      "Epoch 287/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.9751\n",
      "Epoch 288/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0685 - accuracy: 0.9742\n",
      "Epoch 289/1500\n",
      "48/68 [====================>.........] - ETA: 0s - loss: 0.0560 - accuracy: 0.9811Restoring model weights from the end of the best epoch: 259.\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.9765\n",
      "Epoch 289: early stopping\n",
      "8/8 [==============================] - 0s 845us/step - loss: 1.0705 - accuracy: 0.6597\n",
      "8/8 [==============================] - 0s 693us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (22/29)\n",
      "Before appending - Cat IDs: 625, Predictions: 625, Actuals: 625, Gender: 625\n",
      "After appending - Cat IDs: 863, Predictions: 863, Actuals: 863, Gender: 863\n",
      "Final Test Results - Loss: 1.0704662799835205, Accuracy: 0.6596638560295105, Precision: 0.6723484848484849, Recall: 0.5964008888687103, F1 Score: 0.6222910216718266\n",
      "Confusion Matrix:\n",
      " [[121   1  25]\n",
      " [  8  21   0]\n",
      " [ 47   0  15]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6893727174630336\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 1.0129550099372864\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7203111499547958\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.7203871282666146\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.6984175478945278\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[2]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be692ae4-6d3f-4353-b5bf-554d20da4df3",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8da9a092-ed2e-4397-a6c8-2c4888735265",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 863, Predictions: 863, Actuals: 863, Gender: 863\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "51cf386a-c49e-4716-ba15-aa3b7930419a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8d43ac5-d50e-430d-98a1-ff4f45006bae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.75 (83/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ccc9acb7-bb1b-42a6-bb25-cdf5a3356315",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95e69b27-cae1-4a3a-ba70-5244a11aadf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, adult...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[senior, adult, senior, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, senior, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten, kitten, ...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, adult,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[senior, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, adult, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, senior, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, senior, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, senior, senior, adult, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, sen...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, kitten, senior, adult, senior, ...         adult            adult                   True\n",
       "65    059A  [senior, senior, senior, senior, senior, adult...        senior           senior                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, kitten, adult, adult, senior, ...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "69    063A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "60    054A                                   [senior, senior]        senior           senior                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "59    053A      [senior, adult, senior, senior, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, adult, senior, adult, senior, adult, ...        senior           senior                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, adult, adult, kitten, kitten, kitten, ...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "50    044A  [kitten, adult, kitten, kitten, kitten, adult,...        kitten           kitten                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                            [senior, senior, adult]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "104   110A                                           [kitten]        kitten           kitten                   True\n",
       "102   108A    [senior, senior, senior, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                     [senior, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "81    075A               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, kitten, senior, adult, senior, ...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, senior, senior,...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, adult, senior, senior...        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "48    042A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "30    025C               [adult, adult, adult, senior, adult]         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "17    015A  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, senior, kitten, kitte...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A       [adult, senior, adult, adult, senior, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                     [adult, senior, adult, senior]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "24    022A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "40    034A               [adult, senior, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "32    026B                                            [adult]         adult            adult                   True\n",
       "43    037A        [adult, adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "34    027A  [adult, senior, adult, senior, adult, senior, ...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "41    035A                      [kitten, adult, adult, adult]         adult            adult                   True\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "58    052A                   [senior, senior, senior, senior]        senior            adult                  False\n",
       "97    102A                                   [senior, senior]        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "101   106A  [adult, senior, adult, senior, adult, adult, a...         adult           senior                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "13    012A                            [senior, adult, senior]        senior            adult                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "42    036A  [senior, senior, senior, senior, adult, senior...        senior            adult                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "61    055A  [adult, senior, senior, adult, senior, adult, ...         adult           senior                  False\n",
       "36    029A  [senior, senior, senior, senior, adult, adult,...        senior            adult                  False\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, s...        senior            adult                  False\n",
       "62    056A                              [adult, adult, adult]         adult           senior                  False\n",
       "18    016A  [senior, adult, adult, adult, senior, senior, ...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "87    092A                                           [senior]        senior            adult                  False\n",
       "86    091A                                           [senior]        senior            adult                  False\n",
       "85    090A                                            [adult]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, adult, adult, adult, sen...         adult           senior                  False\n",
       "64    058A                             [senior, adult, adult]         adult           senior                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "109   117A  [senior, senior, adult, adult, adult, adult, a...         adult           senior                  False"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d36b3c54-3377-4249-a774-6d31557e36da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     60\n",
      "kitten    13\n",
      "senior    10\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b36eb8a4-57f3-48c0-b92c-4a8e5a52c59e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             60  82.191781\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22             10  45.454545\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1750e2da-df8c-4f00-b860-539dd822864f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded6b34e-2368-4956-8241-8e6ab6e8cf81",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fdefd9be-6e6d-4903-a0ec-7824da4313d9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          572            442  77.272727\n",
      "1           kitten          113             91  80.530973\n",
      "2           senior          178             85  47.752809\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "57576e25-349c-46e0-b57b-8bf3cee3b5de",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b69785-d4c1-49ab-ba75-4a733326cdc1",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "15c5b6e5-d7b0-4089-ae78-3dafe1cead3e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      151     70.89\n",
      "1          M    360      263     73.06\n",
      "2          X    290      204     70.34\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b6001f1b-3f01-42e9-8761-4253acbed5e1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    213      151     70.89\n",
      "1          M    360      263     73.06\n",
      "2          X    290      204     70.34\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b5d44-f489-4de0-8785-4bfa52d09797",
   "metadata": {},
   "source": [
    "# RANDOM SEED 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c4439fc4-670c-4009-8ca9-23c419ee99ec",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[3])) \n",
    "np.random.seed(int(random_seeds[3]))\n",
    "tf.random.set_seed(int(random_seeds[3]))\n",
    "\n",
    "## Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_1.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5fe04e3d-fd90-43c1-97af-5eac75e35f85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2ed5261d-2aeb-412e-a017-65b461a2cf5d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f70aa6-1716-400a-be48-cccb3511542e",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fb9d0248-481e-4807-85dd-407fa88963f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "059A    14\n",
      "042A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "111A    13\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "116A    12\n",
      "036A    11\n",
      "068A    11\n",
      "025A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "072A     9\n",
      "015A     9\n",
      "051B     9\n",
      "022A     9\n",
      "033A     9\n",
      "045A     9\n",
      "095A     8\n",
      "013B     8\n",
      "117A     7\n",
      "031A     7\n",
      "027A     7\n",
      "007A     6\n",
      "108A     6\n",
      "053A     6\n",
      "109A     6\n",
      "023A     6\n",
      "037A     6\n",
      "075A     5\n",
      "070A     5\n",
      "025C     5\n",
      "021A     5\n",
      "034A     5\n",
      "044A     5\n",
      "023B     5\n",
      "003A     4\n",
      "105A     4\n",
      "035A     4\n",
      "026A     4\n",
      "052A     4\n",
      "062A     4\n",
      "012A     3\n",
      "113A     3\n",
      "058A     3\n",
      "060A     3\n",
      "064A     3\n",
      "006A     3\n",
      "025B     2\n",
      "032A     2\n",
      "093A     2\n",
      "054A     2\n",
      "069A     2\n",
      "087A     2\n",
      "038A     2\n",
      "073A     1\n",
      "004A     1\n",
      "090A     1\n",
      "110A     1\n",
      "115A     1\n",
      "091A     1\n",
      "019B     1\n",
      "066A     1\n",
      "048A     1\n",
      "092A     1\n",
      "026C     1\n",
      "076A     1\n",
      "043A     1\n",
      "041A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "019A    17\n",
      "101A    15\n",
      "039A    12\n",
      "063A    11\n",
      "071A    10\n",
      "005A    10\n",
      "065A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "099A     7\n",
      "008A     6\n",
      "009A     4\n",
      "104A     4\n",
      "014A     3\n",
      "056A     3\n",
      "018A     2\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "096A     1\n",
      "049A     1\n",
      "088A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    296\n",
      "X    286\n",
      "F    208\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    62\n",
      "F    44\n",
      "M    41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 097B, 028...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [093A, 097A, 057A, 106A, 055A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 019A, 101A, 005A, 065A, 039A, 009A, 063...\n",
      "kitten                                         [050A, 049A]\n",
      "senior                       [104A, 056A, 094A, 011A, 061A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 14, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 2, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '006A' '007A' '012A'\n",
      " '013B' '014B' '015A' '016A' '019B' '020A' '021A' '022A' '023A' '023B'\n",
      " '024A' '025A' '025B' '025C' '026A' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '033A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '042A'\n",
      " '043A' '044A' '045A' '046A' '047A' '048A' '051A' '051B' '052A' '053A'\n",
      " '054A' '055A' '057A' '058A' '059A' '060A' '062A' '064A' '066A' '067A'\n",
      " '068A' '069A' '070A' '072A' '073A' '074A' '075A' '076A' '087A' '090A'\n",
      " '091A' '092A' '093A' '095A' '097A' '097B' '103A' '105A' '106A' '108A'\n",
      " '109A' '110A' '111A' '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['005A' '008A' '009A' '010A' '011A' '014A' '018A' '019A' '026B' '039A'\n",
      " '049A' '050A' '056A' '061A' '063A' '065A' '071A' '088A' '094A' '096A'\n",
      " '099A' '100A' '101A' '102A' '104A']\n",
      "Length of X_train_val:\n",
      "790\n",
      "Length of y_train_val:\n",
      "790\n",
      "Length of groups_train_val:\n",
      "790\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     468\n",
      "kitten    163\n",
      "senior    159\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     120\n",
      "senior     19\n",
      "kitten      8\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 936, 1: 815, 2: 795})\n",
      "Epoch 1/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.9586 - accuracy: 0.6068\n",
      "Epoch 2/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.7380 - accuracy: 0.6936\n",
      "Epoch 3/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.6380 - accuracy: 0.7313\n",
      "Epoch 4/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.6015 - accuracy: 0.7431\n",
      "Epoch 5/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5438 - accuracy: 0.7742\n",
      "Epoch 6/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5493 - accuracy: 0.7655\n",
      "Epoch 7/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5129 - accuracy: 0.7852\n",
      "Epoch 8/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5117 - accuracy: 0.7824\n",
      "Epoch 9/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.5049 - accuracy: 0.7907\n",
      "Epoch 10/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4705 - accuracy: 0.8068\n",
      "Epoch 11/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4522 - accuracy: 0.8166\n",
      "Epoch 12/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4328 - accuracy: 0.8225\n",
      "Epoch 13/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4161 - accuracy: 0.8284\n",
      "Epoch 14/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4316 - accuracy: 0.8240\n",
      "Epoch 15/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4101 - accuracy: 0.8413\n",
      "Epoch 16/1500\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4084 - accuracy: 0.8350\n",
      "Epoch 17/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3803 - accuracy: 0.8413\n",
      "Epoch 18/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3861 - accuracy: 0.8386\n",
      "Epoch 19/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3980 - accuracy: 0.8417\n",
      "Epoch 20/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3828 - accuracy: 0.8449\n",
      "Epoch 21/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3530 - accuracy: 0.8555\n",
      "Epoch 22/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3743 - accuracy: 0.8539\n",
      "Epoch 23/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3378 - accuracy: 0.8598\n",
      "Epoch 24/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3553 - accuracy: 0.8511\n",
      "Epoch 25/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3654 - accuracy: 0.8574\n",
      "Epoch 26/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3364 - accuracy: 0.8590\n",
      "Epoch 27/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8649\n",
      "Epoch 28/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3332 - accuracy: 0.8708\n",
      "Epoch 29/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3302 - accuracy: 0.8676\n",
      "Epoch 30/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3256 - accuracy: 0.8657\n",
      "Epoch 31/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3190 - accuracy: 0.8731\n",
      "Epoch 32/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3184 - accuracy: 0.8637\n",
      "Epoch 33/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3066 - accuracy: 0.8755\n",
      "Epoch 34/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2894 - accuracy: 0.8814\n",
      "Epoch 35/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3086 - accuracy: 0.8720\n",
      "Epoch 36/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2963 - accuracy: 0.8712\n",
      "Epoch 37/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2989 - accuracy: 0.8802\n",
      "Epoch 38/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2927 - accuracy: 0.8865\n",
      "Epoch 39/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2915 - accuracy: 0.8908\n",
      "Epoch 40/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2814 - accuracy: 0.8904\n",
      "Epoch 41/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2886 - accuracy: 0.8920\n",
      "Epoch 42/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2724 - accuracy: 0.8928\n",
      "Epoch 43/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2687 - accuracy: 0.8896\n",
      "Epoch 44/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2669 - accuracy: 0.8967\n",
      "Epoch 45/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2757 - accuracy: 0.8916\n",
      "Epoch 46/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2741 - accuracy: 0.8975\n",
      "Epoch 47/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2668 - accuracy: 0.8888\n",
      "Epoch 48/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2633 - accuracy: 0.9006\n",
      "Epoch 49/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2764 - accuracy: 0.8928\n",
      "Epoch 50/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2740 - accuracy: 0.8904\n",
      "Epoch 51/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2474 - accuracy: 0.9049\n",
      "Epoch 52/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2480 - accuracy: 0.9006\n",
      "Epoch 53/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2512 - accuracy: 0.9034\n",
      "Epoch 54/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2497 - accuracy: 0.9010\n",
      "Epoch 55/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2412 - accuracy: 0.9065\n",
      "Epoch 56/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2514 - accuracy: 0.9077\n",
      "Epoch 57/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2616 - accuracy: 0.8928\n",
      "Epoch 58/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9042\n",
      "Epoch 59/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2381 - accuracy: 0.9073\n",
      "Epoch 60/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2179 - accuracy: 0.9136\n",
      "Epoch 61/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2285 - accuracy: 0.9159\n",
      "Epoch 62/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2065 - accuracy: 0.9313\n",
      "Epoch 63/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2228 - accuracy: 0.9140\n",
      "Epoch 64/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2233 - accuracy: 0.9140\n",
      "Epoch 65/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2254 - accuracy: 0.9152\n",
      "Epoch 66/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2350 - accuracy: 0.9077\n",
      "Epoch 67/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2205 - accuracy: 0.9226\n",
      "Epoch 68/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2206 - accuracy: 0.9159\n",
      "Epoch 69/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2417 - accuracy: 0.9077\n",
      "Epoch 70/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2233 - accuracy: 0.9116\n",
      "Epoch 71/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2180 - accuracy: 0.9175\n",
      "Epoch 72/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9152\n",
      "Epoch 73/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2103 - accuracy: 0.9199\n",
      "Epoch 74/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2235 - accuracy: 0.9116\n",
      "Epoch 75/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2233 - accuracy: 0.9167\n",
      "Epoch 76/1500\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.1993 - accuracy: 0.9269\n",
      "Epoch 77/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2097 - accuracy: 0.9191\n",
      "Epoch 78/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1967 - accuracy: 0.9258\n",
      "Epoch 79/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1959 - accuracy: 0.9281\n",
      "Epoch 80/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9081\n",
      "Epoch 81/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9269\n",
      "Epoch 82/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2105 - accuracy: 0.9187\n",
      "Epoch 83/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2148 - accuracy: 0.9163\n",
      "Epoch 84/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2045 - accuracy: 0.9207\n",
      "Epoch 85/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2197 - accuracy: 0.9144\n",
      "Epoch 86/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9340\n",
      "Epoch 87/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1942 - accuracy: 0.9297\n",
      "Epoch 88/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.9297\n",
      "Epoch 89/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1844 - accuracy: 0.9360\n",
      "Epoch 90/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9281\n",
      "Epoch 91/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1895 - accuracy: 0.9313\n",
      "Epoch 92/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9305\n",
      "Epoch 93/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.9281\n",
      "Epoch 94/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1938 - accuracy: 0.9254\n",
      "Epoch 95/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1970 - accuracy: 0.9258\n",
      "Epoch 96/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1831 - accuracy: 0.9289\n",
      "Epoch 97/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1808 - accuracy: 0.9368\n",
      "Epoch 98/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1855 - accuracy: 0.9321\n",
      "Epoch 99/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1986 - accuracy: 0.9273\n",
      "Epoch 100/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1851 - accuracy: 0.9273\n",
      "Epoch 101/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.9328\n",
      "Epoch 102/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1915 - accuracy: 0.9277\n",
      "Epoch 103/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1662 - accuracy: 0.9419\n",
      "Epoch 104/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1700 - accuracy: 0.9360\n",
      "Epoch 105/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.9301\n",
      "Epoch 106/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1740 - accuracy: 0.9344\n",
      "Epoch 107/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1724 - accuracy: 0.9360\n",
      "Epoch 108/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9379\n",
      "Epoch 109/1500\n",
      "80/80 [==============================] - 0s 978us/step - loss: 0.1749 - accuracy: 0.9391\n",
      "Epoch 110/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9379\n",
      "Epoch 111/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1817 - accuracy: 0.9344\n",
      "Epoch 112/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1661 - accuracy: 0.9411\n",
      "Epoch 113/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1795 - accuracy: 0.9395\n",
      "Epoch 114/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1640 - accuracy: 0.9403\n",
      "Epoch 115/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1644 - accuracy: 0.9423\n",
      "Epoch 116/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1828 - accuracy: 0.9340\n",
      "Epoch 117/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1660 - accuracy: 0.9403\n",
      "Epoch 118/1500\n",
      "80/80 [==============================] - 0s 994us/step - loss: 0.1505 - accuracy: 0.9474\n",
      "Epoch 119/1500\n",
      "80/80 [==============================] - 0s 967us/step - loss: 0.1539 - accuracy: 0.9423\n",
      "Epoch 120/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1555 - accuracy: 0.9450\n",
      "Epoch 121/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.9348\n",
      "Epoch 122/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9313\n",
      "Epoch 123/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9391\n",
      "Epoch 124/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1572 - accuracy: 0.9411\n",
      "Epoch 125/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9419\n",
      "Epoch 126/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9399\n",
      "Epoch 127/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9403\n",
      "Epoch 128/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1599 - accuracy: 0.9403\n",
      "Epoch 129/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1699 - accuracy: 0.9344\n",
      "Epoch 130/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9430\n",
      "Epoch 131/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9454\n",
      "Epoch 132/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1568 - accuracy: 0.9427\n",
      "Epoch 133/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1673 - accuracy: 0.9352\n",
      "Epoch 134/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1723 - accuracy: 0.9372\n",
      "Epoch 135/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9289\n",
      "Epoch 136/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.9521\n",
      "Epoch 137/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9458\n",
      "Epoch 138/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9356\n",
      "Epoch 139/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1607 - accuracy: 0.9430\n",
      "Epoch 140/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1628 - accuracy: 0.9442\n",
      "Epoch 141/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9423\n",
      "Epoch 142/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1557 - accuracy: 0.9446\n",
      "Epoch 143/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9403\n",
      "Epoch 144/1500\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.1571 - accuracy: 0.9427\n",
      "Epoch 145/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1475 - accuracy: 0.9450\n",
      "Epoch 146/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9430\n",
      "Epoch 147/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9454\n",
      "Epoch 148/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1461 - accuracy: 0.9493\n",
      "Epoch 149/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1454 - accuracy: 0.9497\n",
      "Epoch 150/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1464 - accuracy: 0.9493\n",
      "Epoch 151/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1518 - accuracy: 0.9442\n",
      "Epoch 152/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9438\n",
      "Epoch 153/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9415\n",
      "Epoch 154/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1544 - accuracy: 0.9411\n",
      "Epoch 155/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1566 - accuracy: 0.9462\n",
      "Epoch 156/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9434\n",
      "Epoch 157/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9442\n",
      "Epoch 158/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.9493\n",
      "Epoch 159/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9537\n",
      "Epoch 160/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1387 - accuracy: 0.9513\n",
      "Epoch 161/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1458 - accuracy: 0.9482\n",
      "Epoch 162/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9434\n",
      "Epoch 163/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9513\n",
      "Epoch 164/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9501\n",
      "Epoch 165/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9509\n",
      "Epoch 166/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.9537\n",
      "Epoch 167/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1371 - accuracy: 0.9478\n",
      "Epoch 168/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9438\n",
      "Epoch 169/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9501\n",
      "Epoch 170/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9427\n",
      "Epoch 171/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9552\n",
      "Epoch 172/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9513\n",
      "Epoch 173/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9599\n",
      "Epoch 174/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.9521\n",
      "Epoch 175/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1568 - accuracy: 0.9364\n",
      "Epoch 176/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9521\n",
      "Epoch 177/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9564\n",
      "Epoch 178/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9564\n",
      "Epoch 179/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9442\n",
      "Epoch 180/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9529\n",
      "Epoch 181/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1366 - accuracy: 0.9482\n",
      "Epoch 182/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9599\n",
      "Epoch 183/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1314 - accuracy: 0.9501\n",
      "Epoch 184/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1269 - accuracy: 0.9584\n",
      "Epoch 185/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9540\n",
      "Epoch 186/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.9615\n",
      "Epoch 187/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9615\n",
      "Epoch 188/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9548\n",
      "Epoch 189/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9521\n",
      "Epoch 190/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9470\n",
      "Epoch 191/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9560\n",
      "Epoch 192/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1213 - accuracy: 0.9533\n",
      "Epoch 193/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9478\n",
      "Epoch 194/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9509\n",
      "Epoch 195/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9592\n",
      "Epoch 196/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9517\n",
      "Epoch 197/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9442\n",
      "Epoch 198/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9517\n",
      "Epoch 199/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9513\n",
      "Epoch 200/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9623\n",
      "Epoch 201/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.9501\n",
      "Epoch 202/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9537\n",
      "Epoch 203/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9572\n",
      "Epoch 204/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1242 - accuracy: 0.9580\n",
      "Epoch 205/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9576\n",
      "Epoch 206/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1233 - accuracy: 0.9548\n",
      "Epoch 207/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1210 - accuracy: 0.9564\n",
      "Epoch 208/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1098 - accuracy: 0.9576\n",
      "Epoch 209/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1239 - accuracy: 0.9552\n",
      "Epoch 210/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1190 - accuracy: 0.9595\n",
      "Epoch 211/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1283 - accuracy: 0.9548\n",
      "Epoch 212/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9537\n",
      "Epoch 213/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9521\n",
      "Epoch 214/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1270 - accuracy: 0.9580\n",
      "Epoch 215/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9529\n",
      "Epoch 216/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9544\n",
      "Epoch 217/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9599\n",
      "Epoch 218/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9666\n",
      "Epoch 219/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9454\n",
      "Epoch 220/1500\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.1157 - accuracy: 0.9580\n",
      "Epoch 221/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9662\n",
      "Epoch 222/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9548\n",
      "Epoch 223/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9588\n",
      "Epoch 224/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1196 - accuracy: 0.9537\n",
      "Epoch 225/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9619\n",
      "Epoch 226/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9650\n",
      "Epoch 227/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9607\n",
      "Epoch 228/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1148 - accuracy: 0.9580\n",
      "Epoch 229/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9639\n",
      "Epoch 230/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9643\n",
      "Epoch 231/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0989 - accuracy: 0.9701\n",
      "Epoch 232/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9548\n",
      "Epoch 233/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.9643\n",
      "Epoch 234/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9588\n",
      "Epoch 235/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9568\n",
      "Epoch 236/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9537\n",
      "Epoch 237/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9654\n",
      "Epoch 238/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9568\n",
      "Epoch 239/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1188 - accuracy: 0.9540\n",
      "Epoch 240/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9611\n",
      "Epoch 241/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9607\n",
      "Epoch 242/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9627\n",
      "Epoch 243/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9607\n",
      "Epoch 244/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9643\n",
      "Epoch 245/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0954 - accuracy: 0.9678\n",
      "Epoch 246/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9635\n",
      "Epoch 247/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9627\n",
      "Epoch 248/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9690\n",
      "Epoch 249/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.9694\n",
      "Epoch 250/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9627\n",
      "Epoch 251/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9603\n",
      "Epoch 252/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9588\n",
      "Epoch 253/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9654\n",
      "Epoch 254/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9576\n",
      "Epoch 255/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9627\n",
      "Epoch 256/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.9619\n",
      "Epoch 257/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9615\n",
      "Epoch 258/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1179 - accuracy: 0.9588\n",
      "Epoch 259/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9619\n",
      "Epoch 260/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9564\n",
      "Epoch 261/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1212 - accuracy: 0.9580\n",
      "Epoch 262/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9623\n",
      "Epoch 263/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9525\n",
      "Epoch 264/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9580\n",
      "Epoch 265/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1088 - accuracy: 0.9619\n",
      "Epoch 266/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1019 - accuracy: 0.9595\n",
      "Epoch 267/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9623\n",
      "Epoch 268/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9643\n",
      "Epoch 269/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9635\n",
      "Epoch 270/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9588\n",
      "Epoch 271/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9603\n",
      "Epoch 272/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9635\n",
      "Epoch 273/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9635\n",
      "Epoch 274/1500\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9615\n",
      "Epoch 275/1500\n",
      "50/80 [=================>............] - ETA: 0s - loss: 0.1152 - accuracy: 0.9569Restoring model weights from the end of the best epoch: 245.\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9521\n",
      "Epoch 275: early stopping\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.9854 - accuracy: 0.7279\n",
      "5/5 [==============================] - 0s 882us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (20/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "Final Test Results - Loss: 0.9854212403297424, Accuracy: 0.7278911471366882, Precision: 0.5940921823274765, Recall: 0.8298245614035088, F1 Score: 0.6464985287936108\n",
      "Confusion Matrix:\n",
      " [[84  9 27]\n",
      " [ 0  8  0]\n",
      " [ 4  0 15]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "000B    19\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "106A    14\n",
      "042A    14\n",
      "059A    14\n",
      "001A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "025A    11\n",
      "036A    11\n",
      "040A    10\n",
      "016A    10\n",
      "005A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "065A     9\n",
      "051B     9\n",
      "072A     9\n",
      "010A     8\n",
      "013B     8\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "027A     7\n",
      "099A     7\n",
      "008A     6\n",
      "108A     6\n",
      "007A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "026A     4\n",
      "009A     4\n",
      "105A     4\n",
      "035A     4\n",
      "003A     4\n",
      "104A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "113A     3\n",
      "014A     3\n",
      "012A     3\n",
      "011A     2\n",
      "061A     2\n",
      "102A     2\n",
      "069A     2\n",
      "018A     2\n",
      "032A     2\n",
      "093A     2\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "048A     1\n",
      "096A     1\n",
      "088A     1\n",
      "076A     1\n",
      "091A     1\n",
      "115A     1\n",
      "110A     1\n",
      "100A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "002B    32\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "097B    14\n",
      "068A    11\n",
      "015A     9\n",
      "045A     9\n",
      "033A     9\n",
      "109A     6\n",
      "053A     6\n",
      "075A     5\n",
      "025C     5\n",
      "062A     4\n",
      "060A     3\n",
      "025B     2\n",
      "087A     2\n",
      "038A     2\n",
      "054A     2\n",
      "041A     1\n",
      "043A     1\n",
      "026C     1\n",
      "066A     1\n",
      "004A     1\n",
      "019B     1\n",
      "090A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    299\n",
      "F    216\n",
      "M    214\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    123\n",
      "X     49\n",
      "F     36\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 071A, 028A, 019A, 074...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 050...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 059A, 113A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 097B, 067A, 020A, 062A, 002B, 029...\n",
      "kitten                             [109A, 043A, 041A, 045A]\n",
      "senior                             [055A, 054A, 090A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '005A' '006A' '007A' '008A' '009A'\n",
      " '010A' '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '027A' '028A' '031A' '032A'\n",
      " '034A' '035A' '036A' '037A' '039A' '040A' '042A' '044A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '051B' '052A' '056A' '057A' '058A' '059A'\n",
      " '061A' '063A' '064A' '065A' '069A' '070A' '071A' '072A' '073A' '074A'\n",
      " '076A' '088A' '091A' '092A' '093A' '094A' '095A' '096A' '097A' '099A'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '110A' '111A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '004A' '015A' '019B' '020A' '024A' '025B' '025C' '026C' '029A'\n",
      " '033A' '038A' '041A' '043A' '045A' '053A' '054A' '055A' '060A' '062A'\n",
      " '066A' '067A' '068A' '075A' '087A' '090A' '097B' '109A']\n",
      "Length of X_train_val:\n",
      "729\n",
      "Length of y_train_val:\n",
      "729\n",
      "Length of groups_train_val:\n",
      "729\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     421\n",
      "kitten    154\n",
      "senior    154\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     167\n",
      "senior     24\n",
      "kitten     17\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 842, 1: 770, 2: 770})\n",
      "Epoch 1/1500\n",
      "75/75 [==============================] - 1s 2ms/step - loss: 0.8921 - accuracy: 0.6138\n",
      "Epoch 2/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.7069 - accuracy: 0.7032\n",
      "Epoch 3/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6452 - accuracy: 0.7330\n",
      "Epoch 4/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.6027 - accuracy: 0.7452\n",
      "Epoch 5/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5723 - accuracy: 0.7586\n",
      "Epoch 6/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5726 - accuracy: 0.7519\n",
      "Epoch 7/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5120 - accuracy: 0.7964\n",
      "Epoch 8/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.5038 - accuracy: 0.7834\n",
      "Epoch 9/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4787 - accuracy: 0.7964\n",
      "Epoch 10/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4600 - accuracy: 0.8136\n",
      "Epoch 11/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4408 - accuracy: 0.8102\n",
      "Epoch 12/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4493 - accuracy: 0.8073\n",
      "Epoch 13/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4357 - accuracy: 0.8182\n",
      "Epoch 14/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4196 - accuracy: 0.8275\n",
      "Epoch 15/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4043 - accuracy: 0.8354\n",
      "Epoch 16/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3975 - accuracy: 0.8367\n",
      "Epoch 17/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.4065 - accuracy: 0.8287\n",
      "Epoch 18/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3670 - accuracy: 0.8447\n",
      "Epoch 19/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3942 - accuracy: 0.8367\n",
      "Epoch 20/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3682 - accuracy: 0.8451\n",
      "Epoch 21/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3577 - accuracy: 0.8480\n",
      "Epoch 22/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3534 - accuracy: 0.8568\n",
      "Epoch 23/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3532 - accuracy: 0.8476\n",
      "Epoch 24/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3653 - accuracy: 0.8459\n",
      "Epoch 25/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3376 - accuracy: 0.8640\n",
      "Epoch 26/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3700 - accuracy: 0.8480\n",
      "Epoch 27/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3330 - accuracy: 0.8673\n",
      "Epoch 28/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3142 - accuracy: 0.8648\n",
      "Epoch 29/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3410 - accuracy: 0.8619\n",
      "Epoch 30/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3353 - accuracy: 0.8564\n",
      "Epoch 31/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3177 - accuracy: 0.8732\n",
      "Epoch 32/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3169 - accuracy: 0.8657\n",
      "Epoch 33/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3103 - accuracy: 0.8778\n",
      "Epoch 34/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2824 - accuracy: 0.8846\n",
      "Epoch 35/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2932 - accuracy: 0.8778\n",
      "Epoch 36/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2812 - accuracy: 0.8866\n",
      "Epoch 37/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3074 - accuracy: 0.8699\n",
      "Epoch 38/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2900 - accuracy: 0.8875\n",
      "Epoch 39/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.3052 - accuracy: 0.8770\n",
      "Epoch 40/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2822 - accuracy: 0.8837\n",
      "Epoch 41/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2715 - accuracy: 0.8921\n",
      "Epoch 42/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2886 - accuracy: 0.8757\n",
      "Epoch 43/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2802 - accuracy: 0.8883\n",
      "Epoch 44/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2693 - accuracy: 0.8896\n",
      "Epoch 45/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2527 - accuracy: 0.9085\n",
      "Epoch 46/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2515 - accuracy: 0.8917\n",
      "Epoch 47/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2572 - accuracy: 0.8967\n",
      "Epoch 48/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2622 - accuracy: 0.8925\n",
      "Epoch 49/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2840 - accuracy: 0.8841\n",
      "Epoch 50/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2503 - accuracy: 0.9034\n",
      "Epoch 51/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2591 - accuracy: 0.8992\n",
      "Epoch 52/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2407 - accuracy: 0.8992\n",
      "Epoch 53/1500\n",
      "75/75 [==============================] - 0s 999us/step - loss: 0.2601 - accuracy: 0.8938\n",
      "Epoch 54/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2375 - accuracy: 0.9068\n",
      "Epoch 55/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2385 - accuracy: 0.9022\n",
      "Epoch 56/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2404 - accuracy: 0.9022\n",
      "Epoch 57/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2322 - accuracy: 0.9135\n",
      "Epoch 58/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2291 - accuracy: 0.9085\n",
      "Epoch 59/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2358 - accuracy: 0.9034\n",
      "Epoch 60/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2382 - accuracy: 0.9102\n",
      "Epoch 61/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2298 - accuracy: 0.9118\n",
      "Epoch 62/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2341 - accuracy: 0.9085\n",
      "Epoch 63/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2283 - accuracy: 0.9131\n",
      "Epoch 64/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2278 - accuracy: 0.9148\n",
      "Epoch 65/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.9114\n",
      "Epoch 66/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2209 - accuracy: 0.9110\n",
      "Epoch 67/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2262 - accuracy: 0.9072\n",
      "Epoch 68/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2090 - accuracy: 0.9169\n",
      "Epoch 69/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2249 - accuracy: 0.9127\n",
      "Epoch 70/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2206 - accuracy: 0.9139\n",
      "Epoch 71/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2088 - accuracy: 0.9173\n",
      "Epoch 72/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2371 - accuracy: 0.9060\n",
      "Epoch 73/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2128 - accuracy: 0.9148\n",
      "Epoch 74/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9249\n",
      "Epoch 75/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2087 - accuracy: 0.9156\n",
      "Epoch 76/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1928 - accuracy: 0.9261\n",
      "Epoch 77/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.9270\n",
      "Epoch 78/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2059 - accuracy: 0.9219\n",
      "Epoch 79/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2149 - accuracy: 0.9165\n",
      "Epoch 80/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9202\n",
      "Epoch 81/1500\n",
      "75/75 [==============================] - 0s 981us/step - loss: 0.1932 - accuracy: 0.9274\n",
      "Epoch 82/1500\n",
      "75/75 [==============================] - 0s 949us/step - loss: 0.2054 - accuracy: 0.9207\n",
      "Epoch 83/1500\n",
      "75/75 [==============================] - 0s 973us/step - loss: 0.2044 - accuracy: 0.9169\n",
      "Epoch 84/1500\n",
      "75/75 [==============================] - 0s 946us/step - loss: 0.1845 - accuracy: 0.9215\n",
      "Epoch 85/1500\n",
      "75/75 [==============================] - 0s 943us/step - loss: 0.1875 - accuracy: 0.9253\n",
      "Epoch 86/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.9328\n",
      "Epoch 87/1500\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2004 - accuracy: 0.9240\n",
      "Epoch 88/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.2105 - accuracy: 0.9186\n",
      "Epoch 89/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.9270\n",
      "Epoch 90/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1786 - accuracy: 0.9307\n",
      "Epoch 91/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.9312\n",
      "Epoch 92/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1784 - accuracy: 0.9291\n",
      "Epoch 93/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1818 - accuracy: 0.9324\n",
      "Epoch 94/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1898 - accuracy: 0.9265\n",
      "Epoch 95/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1805 - accuracy: 0.9257\n",
      "Epoch 96/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1917 - accuracy: 0.9278\n",
      "Epoch 97/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1706 - accuracy: 0.9337\n",
      "Epoch 98/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1621 - accuracy: 0.9395\n",
      "Epoch 99/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.9265\n",
      "Epoch 100/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1726 - accuracy: 0.9345\n",
      "Epoch 101/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1772 - accuracy: 0.9341\n",
      "Epoch 102/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9370\n",
      "Epoch 103/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9395\n",
      "Epoch 104/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9374\n",
      "Epoch 105/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1706 - accuracy: 0.9366\n",
      "Epoch 106/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1833 - accuracy: 0.9316\n",
      "Epoch 107/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.9320\n",
      "Epoch 108/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1661 - accuracy: 0.9395\n",
      "Epoch 109/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1698 - accuracy: 0.9316\n",
      "Epoch 110/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.9362\n",
      "Epoch 111/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1643 - accuracy: 0.9374\n",
      "Epoch 112/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9416\n",
      "Epoch 113/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9345\n",
      "Epoch 114/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1650 - accuracy: 0.9362\n",
      "Epoch 115/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1552 - accuracy: 0.9492\n",
      "Epoch 116/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9433\n",
      "Epoch 117/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9425\n",
      "Epoch 118/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.9312\n",
      "Epoch 119/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9383\n",
      "Epoch 120/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1797 - accuracy: 0.9282\n",
      "Epoch 121/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1782 - accuracy: 0.9358\n",
      "Epoch 122/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9500\n",
      "Epoch 123/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9463\n",
      "Epoch 124/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1511 - accuracy: 0.9446\n",
      "Epoch 125/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1612 - accuracy: 0.9345\n",
      "Epoch 126/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9404\n",
      "Epoch 127/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9463\n",
      "Epoch 128/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9450\n",
      "Epoch 129/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9421\n",
      "Epoch 130/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1300 - accuracy: 0.9500\n",
      "Epoch 131/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1471 - accuracy: 0.9454\n",
      "Epoch 132/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9345\n",
      "Epoch 133/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9425\n",
      "Epoch 134/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1391 - accuracy: 0.9458\n",
      "Epoch 135/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1365 - accuracy: 0.9542\n",
      "Epoch 136/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1557 - accuracy: 0.9408\n",
      "Epoch 137/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1545 - accuracy: 0.9450\n",
      "Epoch 138/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1421 - accuracy: 0.9479\n",
      "Epoch 139/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9488\n",
      "Epoch 140/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1407 - accuracy: 0.9446\n",
      "Epoch 141/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9496\n",
      "Epoch 142/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9551\n",
      "Epoch 143/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9534\n",
      "Epoch 144/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1365 - accuracy: 0.9412\n",
      "Epoch 145/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9559\n",
      "Epoch 146/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9479\n",
      "Epoch 147/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9488\n",
      "Epoch 148/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9509\n",
      "Epoch 149/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9513\n",
      "Epoch 150/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9479\n",
      "Epoch 151/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1342 - accuracy: 0.9488\n",
      "Epoch 152/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1398 - accuracy: 0.9517\n",
      "Epoch 153/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.9395\n",
      "Epoch 154/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1441 - accuracy: 0.9496\n",
      "Epoch 155/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9614\n",
      "Epoch 156/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9433\n",
      "Epoch 157/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1583 - accuracy: 0.9425\n",
      "Epoch 158/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9479\n",
      "Epoch 159/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1353 - accuracy: 0.9446\n",
      "Epoch 160/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1372 - accuracy: 0.9484\n",
      "Epoch 161/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9513\n",
      "Epoch 162/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9538\n",
      "Epoch 163/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1400 - accuracy: 0.9492\n",
      "Epoch 164/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1255 - accuracy: 0.9542\n",
      "Epoch 165/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1497 - accuracy: 0.9433\n",
      "Epoch 166/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9589\n",
      "Epoch 167/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9551\n",
      "Epoch 168/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9463\n",
      "Epoch 169/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1278 - accuracy: 0.9517\n",
      "Epoch 170/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1315 - accuracy: 0.9517\n",
      "Epoch 171/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9509\n",
      "Epoch 172/1500\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.1280 - accuracy: 0.9526\n",
      "Epoch 173/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9530\n",
      "Epoch 174/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9521\n",
      "Epoch 175/1500\n",
      "75/75 [==============================] - 0s 999us/step - loss: 0.1247 - accuracy: 0.9555\n",
      "Epoch 176/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9559\n",
      "Epoch 177/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9618\n",
      "Epoch 178/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9568\n",
      "Epoch 179/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1337 - accuracy: 0.9517\n",
      "Epoch 180/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1466 - accuracy: 0.9425\n",
      "Epoch 181/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9584\n",
      "Epoch 182/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9580\n",
      "Epoch 183/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9559\n",
      "Epoch 184/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9576\n",
      "Epoch 185/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1145 - accuracy: 0.9605\n",
      "Epoch 186/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1431 - accuracy: 0.9496\n",
      "Epoch 187/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9563\n",
      "Epoch 188/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9521\n",
      "Epoch 189/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9551\n",
      "Epoch 190/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9475\n",
      "Epoch 191/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9513\n",
      "Epoch 192/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.9601\n",
      "Epoch 193/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9555\n",
      "Epoch 194/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1115 - accuracy: 0.9618\n",
      "Epoch 195/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9576\n",
      "Epoch 196/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9647\n",
      "Epoch 197/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9635\n",
      "Epoch 198/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9664\n",
      "Epoch 199/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9593\n",
      "Epoch 200/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9584\n",
      "Epoch 201/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.9597\n",
      "Epoch 202/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9601\n",
      "Epoch 203/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1159 - accuracy: 0.9584\n",
      "Epoch 204/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9568\n",
      "Epoch 205/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9576\n",
      "Epoch 206/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9513\n",
      "Epoch 207/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1232 - accuracy: 0.9584\n",
      "Epoch 208/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9555\n",
      "Epoch 209/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9568\n",
      "Epoch 210/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9538\n",
      "Epoch 211/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1168 - accuracy: 0.9601\n",
      "Epoch 212/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.9555\n",
      "Epoch 213/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1078 - accuracy: 0.9614\n",
      "Epoch 214/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9542\n",
      "Epoch 215/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9610\n",
      "Epoch 216/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0895 - accuracy: 0.9706\n",
      "Epoch 217/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9618\n",
      "Epoch 218/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9580\n",
      "Epoch 219/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9547\n",
      "Epoch 220/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9538\n",
      "Epoch 221/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9593\n",
      "Epoch 222/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9635\n",
      "Epoch 223/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1065 - accuracy: 0.9610\n",
      "Epoch 224/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9656\n",
      "Epoch 225/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1093 - accuracy: 0.9614\n",
      "Epoch 226/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1097 - accuracy: 0.9568\n",
      "Epoch 227/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1077 - accuracy: 0.9610\n",
      "Epoch 228/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9664\n",
      "Epoch 229/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9622\n",
      "Epoch 230/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1079 - accuracy: 0.9618\n",
      "Epoch 231/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9635\n",
      "Epoch 232/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9580\n",
      "Epoch 233/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1258 - accuracy: 0.9509\n",
      "Epoch 234/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0910 - accuracy: 0.9656\n",
      "Epoch 235/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9559\n",
      "Epoch 236/1500\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1054 - accuracy: 0.9618\n",
      "Epoch 237/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9563\n",
      "Epoch 238/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9631\n",
      "Epoch 239/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0973 - accuracy: 0.9673\n",
      "Epoch 240/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1119 - accuracy: 0.9589\n",
      "Epoch 241/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9635\n",
      "Epoch 242/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9576\n",
      "Epoch 243/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9631\n",
      "Epoch 244/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1195 - accuracy: 0.9538\n",
      "Epoch 245/1500\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9597\n",
      "Epoch 246/1500\n",
      "51/75 [===================>..........] - ETA: 0s - loss: 0.0945 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 216.\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.9689\n",
      "Epoch 246: early stopping\n",
      "7/7 [==============================] - 0s 911us/step - loss: 0.6507 - accuracy: 0.7837\n",
      "7/7 [==============================] - 0s 683us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.82 (23/28)\n",
      "Before appending - Cat IDs: 147, Predictions: 147, Actuals: 147, Gender: 147\n",
      "After appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "Final Test Results - Loss: 0.6506680250167847, Accuracy: 0.7836538553237915, Precision: 0.66475935828877, Recall: 0.8449962819459121, F1 Score: 0.7149121409990976\n",
      "Confusion Matrix:\n",
      " [[127   9  31]\n",
      " [  1  16   0]\n",
      " [  4   0  20]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "002B    32\n",
      "047A    28\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "097A    16\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "051A    12\n",
      "063A    11\n",
      "036A    11\n",
      "068A    11\n",
      "005A    10\n",
      "016A    10\n",
      "071A    10\n",
      "065A     9\n",
      "045A     9\n",
      "022A     9\n",
      "015A     9\n",
      "033A     9\n",
      "010A     8\n",
      "094A     8\n",
      "050A     7\n",
      "031A     7\n",
      "117A     7\n",
      "099A     7\n",
      "007A     6\n",
      "108A     6\n",
      "109A     6\n",
      "008A     6\n",
      "053A     6\n",
      "075A     5\n",
      "044A     5\n",
      "021A     5\n",
      "025C     5\n",
      "034A     5\n",
      "023B     5\n",
      "009A     4\n",
      "003A     4\n",
      "104A     4\n",
      "062A     4\n",
      "113A     3\n",
      "014A     3\n",
      "056A     3\n",
      "060A     3\n",
      "064A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "018A     2\n",
      "054A     2\n",
      "087A     2\n",
      "038A     2\n",
      "093A     2\n",
      "011A     2\n",
      "100A     1\n",
      "090A     1\n",
      "115A     1\n",
      "088A     1\n",
      "024A     1\n",
      "019B     1\n",
      "096A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "073A     1\n",
      "043A     1\n",
      "091A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "057A    27\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "116A    12\n",
      "025A    11\n",
      "040A    10\n",
      "014B    10\n",
      "072A     9\n",
      "051B     9\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "037A     6\n",
      "023A     6\n",
      "070A     5\n",
      "052A     4\n",
      "035A     4\n",
      "026A     4\n",
      "105A     4\n",
      "012A     3\n",
      "006A     3\n",
      "058A     3\n",
      "032A     2\n",
      "076A     1\n",
      "110A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    235\n",
      "X    186\n",
      "F    169\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    162\n",
      "M    102\n",
      "F     83\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [033A, 015A, 071A, 097B, 028A, 019A, 074A, 067...\n",
      "kitten    [044A, 047A, 109A, 050A, 043A, 049A, 041A, 045...\n",
      "senior    [093A, 097A, 104A, 055A, 059A, 113A, 054A, 117...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 001A, 103A, 095A, 072A, 023A, 027...\n",
      "kitten                 [014B, 111A, 040A, 046A, 042A, 110A]\n",
      "senior                       [057A, 106A, 116A, 051B, 058A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 55, 'kitten': 10, 'senior': 17}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 19, 'kitten': 6, 'senior': 5}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A' '010A'\n",
      " '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '031A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '047A' '048A'\n",
      " '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A' '061A'\n",
      " '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A' '073A'\n",
      " '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A' '096A'\n",
      " '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '109A' '113A'\n",
      " '115A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A'\n",
      " '032A' '035A' '037A' '040A' '042A' '046A' '051B' '052A' '057A' '058A'\n",
      " '070A' '072A' '076A' '095A' '103A' '105A' '106A' '110A' '111A' '116A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'000A', '046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'109A', '031A'}\n",
      "Moved to Test Set:\n",
      "{'109A', '031A'}\n",
      "Removed from Test Set\n",
      "{'000A', '046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '002A' '002B' '003A' '004A' '005A' '007A' '008A' '009A'\n",
      " '010A' '011A' '014A' '015A' '016A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026B' '026C' '028A' '029A' '033A'\n",
      " '034A' '036A' '038A' '039A' '041A' '043A' '044A' '045A' '046A' '047A'\n",
      " '048A' '049A' '050A' '051A' '053A' '054A' '055A' '056A' '059A' '060A'\n",
      " '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A' '069A' '071A'\n",
      " '073A' '074A' '075A' '087A' '088A' '090A' '091A' '092A' '093A' '094A'\n",
      " '096A' '097A' '097B' '099A' '100A' '101A' '102A' '104A' '108A' '113A'\n",
      " '115A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['001A' '006A' '012A' '013B' '014B' '023A' '025A' '026A' '027A' '031A'\n",
      " '032A' '035A' '037A' '040A' '042A' '051B' '052A' '057A' '058A' '070A'\n",
      " '072A' '076A' '095A' '103A' '105A' '106A' '109A' '110A' '111A' '116A']\n",
      "Length of X_train_val:\n",
      "679\n",
      "Length of y_train_val:\n",
      "679\n",
      "Length of groups_train_val:\n",
      "679\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     417\n",
      "senior    113\n",
      "kitten     60\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     171\n",
      "kitten    111\n",
      "senior     65\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     449\n",
      "kitten    117\n",
      "senior    113\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     139\n",
      "senior     65\n",
      "kitten     54\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 898, 1: 585, 2: 565})\n",
      "Epoch 1/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.9799 - accuracy: 0.5864\n",
      "Epoch 2/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.7310 - accuracy: 0.7031\n",
      "Epoch 3/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.6557 - accuracy: 0.7261\n",
      "Epoch 4/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.6035 - accuracy: 0.7578\n",
      "Epoch 5/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5754 - accuracy: 0.7788\n",
      "Epoch 6/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5255 - accuracy: 0.7822\n",
      "Epoch 7/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.5089 - accuracy: 0.7964\n",
      "Epoch 8/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4812 - accuracy: 0.8013\n",
      "Epoch 9/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4813 - accuracy: 0.8081\n",
      "Epoch 10/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4464 - accuracy: 0.8228\n",
      "Epoch 11/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4252 - accuracy: 0.8335\n",
      "Epoch 12/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4290 - accuracy: 0.8335\n",
      "Epoch 13/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3996 - accuracy: 0.8418\n",
      "Epoch 14/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3963 - accuracy: 0.8481\n",
      "Epoch 15/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.4027 - accuracy: 0.8394\n",
      "Epoch 16/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3855 - accuracy: 0.8467\n",
      "Epoch 17/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3797 - accuracy: 0.8550\n",
      "Epoch 18/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3580 - accuracy: 0.8667\n",
      "Epoch 19/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3460 - accuracy: 0.8667\n",
      "Epoch 20/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3431 - accuracy: 0.8623\n",
      "Epoch 21/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3375 - accuracy: 0.8643\n",
      "Epoch 22/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3263 - accuracy: 0.8711\n",
      "Epoch 23/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3403 - accuracy: 0.8687\n",
      "Epoch 24/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3391 - accuracy: 0.8745\n",
      "Epoch 25/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3251 - accuracy: 0.8721\n",
      "Epoch 26/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3128 - accuracy: 0.8794\n",
      "Epoch 27/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3263 - accuracy: 0.8750\n",
      "Epoch 28/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3104 - accuracy: 0.8701\n",
      "Epoch 29/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.3205 - accuracy: 0.8774\n",
      "Epoch 30/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2904 - accuracy: 0.8926\n",
      "Epoch 31/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2765 - accuracy: 0.8965\n",
      "Epoch 32/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2975 - accuracy: 0.8916\n",
      "Epoch 33/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2765 - accuracy: 0.8945\n",
      "Epoch 34/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.8921\n",
      "Epoch 35/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2840 - accuracy: 0.8877\n",
      "Epoch 36/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2791 - accuracy: 0.8960\n",
      "Epoch 37/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2696 - accuracy: 0.8994\n",
      "Epoch 38/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2647 - accuracy: 0.9048\n",
      "Epoch 39/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2673 - accuracy: 0.8965\n",
      "Epoch 40/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2469 - accuracy: 0.9062\n",
      "Epoch 41/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2485 - accuracy: 0.9097\n",
      "Epoch 42/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2377 - accuracy: 0.9097\n",
      "Epoch 43/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2411 - accuracy: 0.9077\n",
      "Epoch 44/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2398 - accuracy: 0.9102\n",
      "Epoch 45/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2393 - accuracy: 0.9141\n",
      "Epoch 46/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2400 - accuracy: 0.9165\n",
      "Epoch 47/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2257 - accuracy: 0.9175\n",
      "Epoch 48/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2395 - accuracy: 0.9116\n",
      "Epoch 49/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2265 - accuracy: 0.9185\n",
      "Epoch 50/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.9175\n",
      "Epoch 51/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2333 - accuracy: 0.9048\n",
      "Epoch 52/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2352 - accuracy: 0.9072\n",
      "Epoch 53/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2203 - accuracy: 0.9214\n",
      "Epoch 54/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2076 - accuracy: 0.9243\n",
      "Epoch 55/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2181 - accuracy: 0.9141\n",
      "Epoch 56/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2065 - accuracy: 0.9209\n",
      "Epoch 57/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2122 - accuracy: 0.9214\n",
      "Epoch 58/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2020 - accuracy: 0.9224\n",
      "Epoch 59/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1944 - accuracy: 0.9331\n",
      "Epoch 60/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2041 - accuracy: 0.9277\n",
      "Epoch 61/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2001 - accuracy: 0.9312\n",
      "Epoch 62/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2090 - accuracy: 0.9199\n",
      "Epoch 63/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1949 - accuracy: 0.9263\n",
      "Epoch 64/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2200 - accuracy: 0.9170\n",
      "Epoch 65/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2136 - accuracy: 0.9189\n",
      "Epoch 66/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9199\n",
      "Epoch 67/1500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.1952 - accuracy: 0.9258\n",
      "Epoch 68/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9316\n",
      "Epoch 69/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1900 - accuracy: 0.9263\n",
      "Epoch 70/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1933 - accuracy: 0.9302\n",
      "Epoch 71/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1871 - accuracy: 0.9321\n",
      "Epoch 72/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9238\n",
      "Epoch 73/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1754 - accuracy: 0.9409\n",
      "Epoch 74/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1806 - accuracy: 0.9287\n",
      "Epoch 75/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1813 - accuracy: 0.9355\n",
      "Epoch 76/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1626 - accuracy: 0.9399\n",
      "Epoch 77/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1747 - accuracy: 0.9419\n",
      "Epoch 78/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9463\n",
      "Epoch 79/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1764 - accuracy: 0.9370\n",
      "Epoch 80/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.9399\n",
      "Epoch 81/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.9331\n",
      "Epoch 82/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1636 - accuracy: 0.9380\n",
      "Epoch 83/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1680 - accuracy: 0.9365\n",
      "Epoch 84/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1681 - accuracy: 0.9409\n",
      "Epoch 85/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9419\n",
      "Epoch 86/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1800 - accuracy: 0.9292\n",
      "Epoch 87/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1808 - accuracy: 0.9297\n",
      "Epoch 88/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9429\n",
      "Epoch 89/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9458\n",
      "Epoch 90/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1543 - accuracy: 0.9448\n",
      "Epoch 91/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9321\n",
      "Epoch 92/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1566 - accuracy: 0.9399\n",
      "Epoch 93/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1653 - accuracy: 0.9385\n",
      "Epoch 94/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1604 - accuracy: 0.9453\n",
      "Epoch 95/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1729 - accuracy: 0.9360\n",
      "Epoch 96/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9463\n",
      "Epoch 97/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1608 - accuracy: 0.9409\n",
      "Epoch 98/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1568 - accuracy: 0.9414\n",
      "Epoch 99/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9482\n",
      "Epoch 100/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.9492\n",
      "Epoch 101/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9473\n",
      "Epoch 102/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9448\n",
      "Epoch 103/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9487\n",
      "Epoch 104/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9546\n",
      "Epoch 105/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1504 - accuracy: 0.9453\n",
      "Epoch 106/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9517\n",
      "Epoch 107/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 0.9585\n",
      "Epoch 108/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9575\n",
      "Epoch 109/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9434\n",
      "Epoch 110/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1627 - accuracy: 0.9448\n",
      "Epoch 111/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9409\n",
      "Epoch 112/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9561\n",
      "Epoch 113/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1223 - accuracy: 0.9634\n",
      "Epoch 114/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9604\n",
      "Epoch 115/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9512\n",
      "Epoch 116/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1503 - accuracy: 0.9502\n",
      "Epoch 117/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9463\n",
      "Epoch 118/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9595\n",
      "Epoch 119/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9629\n",
      "Epoch 120/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1386 - accuracy: 0.9487\n",
      "Epoch 121/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1378 - accuracy: 0.9468\n",
      "Epoch 122/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1377 - accuracy: 0.9512\n",
      "Epoch 123/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1253 - accuracy: 0.9546\n",
      "Epoch 124/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9536\n",
      "Epoch 125/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9561\n",
      "Epoch 126/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9580\n",
      "Epoch 127/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9551\n",
      "Epoch 128/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9507\n",
      "Epoch 129/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1228 - accuracy: 0.9629\n",
      "Epoch 130/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1201 - accuracy: 0.9570\n",
      "Epoch 131/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9468\n",
      "Epoch 132/1500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.1213 - accuracy: 0.9580\n",
      "Epoch 133/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.1319 - accuracy: 0.9546\n",
      "Epoch 134/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9502\n",
      "Epoch 135/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9585\n",
      "Epoch 136/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9590\n",
      "Epoch 137/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9624\n",
      "Epoch 138/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1094 - accuracy: 0.9634\n",
      "Epoch 139/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9609\n",
      "Epoch 140/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1150 - accuracy: 0.9644\n",
      "Epoch 141/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9570\n",
      "Epoch 142/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1157 - accuracy: 0.9658\n",
      "Epoch 143/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1153 - accuracy: 0.9575\n",
      "Epoch 144/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9595\n",
      "Epoch 145/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9648\n",
      "Epoch 146/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1184 - accuracy: 0.9595\n",
      "Epoch 147/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1151 - accuracy: 0.9595\n",
      "Epoch 148/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9634\n",
      "Epoch 149/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9624\n",
      "Epoch 150/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1311 - accuracy: 0.9526\n",
      "Epoch 151/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9639\n",
      "Epoch 152/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9634\n",
      "Epoch 153/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9624\n",
      "Epoch 154/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9697\n",
      "Epoch 155/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9609\n",
      "Epoch 156/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1218 - accuracy: 0.9624\n",
      "Epoch 157/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1189 - accuracy: 0.9614\n",
      "Epoch 158/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1039 - accuracy: 0.9624\n",
      "Epoch 159/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1086 - accuracy: 0.9644\n",
      "Epoch 160/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1012 - accuracy: 0.9639\n",
      "Epoch 161/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9683\n",
      "Epoch 162/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9673\n",
      "Epoch 163/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0978 - accuracy: 0.9712\n",
      "Epoch 164/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9634\n",
      "Epoch 165/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9590\n",
      "Epoch 166/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9692\n",
      "Epoch 167/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9619\n",
      "Epoch 168/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1041 - accuracy: 0.9629\n",
      "Epoch 169/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9653\n",
      "Epoch 170/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0971 - accuracy: 0.9678\n",
      "Epoch 171/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9570\n",
      "Epoch 172/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9678\n",
      "Epoch 173/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0860 - accuracy: 0.9741\n",
      "Epoch 174/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9624\n",
      "Epoch 175/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1009 - accuracy: 0.9697\n",
      "Epoch 176/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0961 - accuracy: 0.9678\n",
      "Epoch 177/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9624\n",
      "Epoch 178/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0981 - accuracy: 0.9663\n",
      "Epoch 179/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1135 - accuracy: 0.9604\n",
      "Epoch 180/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0876 - accuracy: 0.9683\n",
      "Epoch 181/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1017 - accuracy: 0.9653\n",
      "Epoch 182/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1023 - accuracy: 0.9658\n",
      "Epoch 183/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0832 - accuracy: 0.9722\n",
      "Epoch 184/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9644\n",
      "Epoch 185/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0990 - accuracy: 0.9678\n",
      "Epoch 186/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9634\n",
      "Epoch 187/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9658\n",
      "Epoch 188/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0913 - accuracy: 0.9688\n",
      "Epoch 189/1500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0995 - accuracy: 0.9658\n",
      "Epoch 190/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9644\n",
      "Epoch 191/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0924 - accuracy: 0.9702\n",
      "Epoch 192/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9624\n",
      "Epoch 193/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9727\n",
      "Epoch 194/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0930 - accuracy: 0.9683\n",
      "Epoch 195/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9604\n",
      "Epoch 196/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9658\n",
      "Epoch 197/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0888 - accuracy: 0.9683\n",
      "Epoch 198/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9727\n",
      "Epoch 199/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9717\n",
      "Epoch 200/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9678\n",
      "Epoch 201/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0812 - accuracy: 0.9756\n",
      "Epoch 202/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9702\n",
      "Epoch 203/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0927 - accuracy: 0.9673\n",
      "Epoch 204/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1004 - accuracy: 0.9609\n",
      "Epoch 205/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1059 - accuracy: 0.9658\n",
      "Epoch 206/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9702\n",
      "Epoch 207/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0865 - accuracy: 0.9741\n",
      "Epoch 208/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9697\n",
      "Epoch 209/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0856 - accuracy: 0.9707\n",
      "Epoch 210/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1037 - accuracy: 0.9683\n",
      "Epoch 211/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9736\n",
      "Epoch 212/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9771\n",
      "Epoch 213/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0741 - accuracy: 0.9751\n",
      "Epoch 214/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0829 - accuracy: 0.9727\n",
      "Epoch 215/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0854 - accuracy: 0.9727\n",
      "Epoch 216/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9727\n",
      "Epoch 217/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9629\n",
      "Epoch 218/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0963 - accuracy: 0.9663\n",
      "Epoch 219/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0790 - accuracy: 0.9756\n",
      "Epoch 220/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0808 - accuracy: 0.9751\n",
      "Epoch 221/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0898 - accuracy: 0.9722\n",
      "Epoch 222/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0724 - accuracy: 0.9746\n",
      "Epoch 223/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9790\n",
      "Epoch 224/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9702\n",
      "Epoch 225/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0805 - accuracy: 0.9731\n",
      "Epoch 226/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0760 - accuracy: 0.9785\n",
      "Epoch 227/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0780 - accuracy: 0.9751\n",
      "Epoch 228/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.9746\n",
      "Epoch 229/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9736\n",
      "Epoch 230/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0887 - accuracy: 0.9707\n",
      "Epoch 231/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9668\n",
      "Epoch 232/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0972 - accuracy: 0.9663\n",
      "Epoch 233/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0836 - accuracy: 0.9717\n",
      "Epoch 234/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.9771\n",
      "Epoch 235/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0764 - accuracy: 0.9707\n",
      "Epoch 236/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.9756\n",
      "Epoch 237/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0608 - accuracy: 0.9805\n",
      "Epoch 238/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9746\n",
      "Epoch 239/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0853 - accuracy: 0.9727\n",
      "Epoch 240/1500\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 0.0823 - accuracy: 0.9717\n",
      "Epoch 241/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0779 - accuracy: 0.9751\n",
      "Epoch 242/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0700 - accuracy: 0.9800\n",
      "Epoch 243/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9771\n",
      "Epoch 244/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0712 - accuracy: 0.9790\n",
      "Epoch 245/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0873 - accuracy: 0.9678\n",
      "Epoch 246/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0776 - accuracy: 0.9775\n",
      "Epoch 247/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0789 - accuracy: 0.9785\n",
      "Epoch 248/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9727\n",
      "Epoch 249/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0791 - accuracy: 0.9712\n",
      "Epoch 250/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0754 - accuracy: 0.9795\n",
      "Epoch 251/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0693 - accuracy: 0.9775\n",
      "Epoch 252/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0686 - accuracy: 0.9790\n",
      "Epoch 253/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9683\n",
      "Epoch 254/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0957 - accuracy: 0.9653\n",
      "Epoch 255/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0744 - accuracy: 0.9746\n",
      "Epoch 256/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.9702\n",
      "Epoch 257/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0746 - accuracy: 0.9741\n",
      "Epoch 258/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0705 - accuracy: 0.9771\n",
      "Epoch 259/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0896 - accuracy: 0.9673\n",
      "Epoch 260/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0890 - accuracy: 0.9722\n",
      "Epoch 261/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0727 - accuracy: 0.9746\n",
      "Epoch 262/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0674 - accuracy: 0.9814\n",
      "Epoch 263/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0716 - accuracy: 0.9766\n",
      "Epoch 264/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0655 - accuracy: 0.9805\n",
      "Epoch 265/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9746\n",
      "Epoch 266/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9805\n",
      "Epoch 267/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0569 - accuracy: 0.9814\n",
      "Epoch 268/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0554 - accuracy: 0.9834\n",
      "Epoch 269/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0527 - accuracy: 0.9834\n",
      "Epoch 270/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0687 - accuracy: 0.9790\n",
      "Epoch 271/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0785 - accuracy: 0.9761\n",
      "Epoch 272/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9722\n",
      "Epoch 273/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0799 - accuracy: 0.9722\n",
      "Epoch 274/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0599 - accuracy: 0.9839\n",
      "Epoch 275/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0769 - accuracy: 0.9731\n",
      "Epoch 276/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.9702\n",
      "Epoch 277/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0668 - accuracy: 0.9805\n",
      "Epoch 278/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0819 - accuracy: 0.9731\n",
      "Epoch 279/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0855 - accuracy: 0.9702\n",
      "Epoch 280/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0642 - accuracy: 0.9795\n",
      "Epoch 281/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0737 - accuracy: 0.9751\n",
      "Epoch 282/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0717 - accuracy: 0.9751\n",
      "Epoch 283/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0844 - accuracy: 0.9692\n",
      "Epoch 284/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.9746\n",
      "Epoch 285/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.9795\n",
      "Epoch 286/1500\n",
      "64/64 [==============================] - 0s 5ms/step - loss: 0.0709 - accuracy: 0.9800\n",
      "Epoch 287/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0576 - accuracy: 0.9829\n",
      "Epoch 288/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9702\n",
      "Epoch 289/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0733 - accuracy: 0.9702\n",
      "Epoch 290/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0640 - accuracy: 0.9790\n",
      "Epoch 291/1500\n",
      "64/64 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9795\n",
      "Epoch 292/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0677 - accuracy: 0.9795\n",
      "Epoch 293/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0689 - accuracy: 0.9766\n",
      "Epoch 294/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0771 - accuracy: 0.9717\n",
      "Epoch 295/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0792 - accuracy: 0.9722\n",
      "Epoch 296/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9756\n",
      "Epoch 297/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0625 - accuracy: 0.9785\n",
      "Epoch 298/1500\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0641 - accuracy: 0.9800\n",
      "Epoch 299/1500\n",
      "48/64 [=====================>........] - ETA: 0s - loss: 0.0672 - accuracy: 0.9792Restoring model weights from the end of the best epoch: 269.\n",
      "64/64 [==============================] - 0s 1ms/step - loss: 0.0664 - accuracy: 0.9800\n",
      "Epoch 299: early stopping\n",
      "9/9 [==============================] - 0s 843us/step - loss: 1.0459 - accuracy: 0.7326\n",
      "9/9 [==============================] - 0s 698us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.83 (25/30)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before appending - Cat IDs: 355, Predictions: 355, Actuals: 355, Gender: 355\n",
      "After appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "Final Test Results - Loss: 1.0459216833114624, Accuracy: 0.7325581312179565, Precision: 0.7488644304682041, Recall: 0.6974557106451592, F1 Score: 0.7176937257998421\n",
      "Confusion Matrix:\n",
      " [[116   2  21]\n",
      " [  9  43   2]\n",
      " [ 35   0  30]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "057A    27\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "019A    17\n",
      "029A    17\n",
      "101A    15\n",
      "097B    14\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "111A    13\n",
      "039A    12\n",
      "116A    12\n",
      "068A    11\n",
      "025A    11\n",
      "063A    11\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "005A    10\n",
      "072A     9\n",
      "065A     9\n",
      "015A     9\n",
      "033A     9\n",
      "051B     9\n",
      "045A     9\n",
      "094A     8\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "050A     7\n",
      "027A     7\n",
      "099A     7\n",
      "053A     6\n",
      "109A     6\n",
      "008A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "035A     4\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "104A     4\n",
      "062A     4\n",
      "009A     4\n",
      "012A     3\n",
      "058A     3\n",
      "060A     3\n",
      "006A     3\n",
      "056A     3\n",
      "014A     3\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "054A     2\n",
      "032A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "066A     1\n",
      "026C     1\n",
      "076A     1\n",
      "096A     1\n",
      "041A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "047A    28\n",
      "074A    25\n",
      "000B    19\n",
      "097A    16\n",
      "059A    14\n",
      "028A    13\n",
      "002A    13\n",
      "051A    12\n",
      "036A    11\n",
      "016A    10\n",
      "022A     9\n",
      "117A     7\n",
      "031A     7\n",
      "108A     6\n",
      "007A     6\n",
      "023B     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "003A     4\n",
      "064A     3\n",
      "113A     3\n",
      "093A     2\n",
      "069A     2\n",
      "073A     1\n",
      "091A     1\n",
      "092A     1\n",
      "048A     1\n",
      "115A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    273\n",
      "M    266\n",
      "F    163\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "F    89\n",
      "X    75\n",
      "M    71\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 071A, 097...\n",
      "kitten    [014B, 111A, 040A, 046A, 042A, 109A, 050A, 043...\n",
      "senior    [057A, 106A, 104A, 055A, 116A, 051B, 054A, 056...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [028A, 074A, 022A, 034A, 091A, 002A, 007A, 069...\n",
      "kitten                             [044A, 047A, 048A, 115A]\n",
      "senior     [093A, 097A, 059A, 113A, 117A, 051A, 016A, 108A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 57, 'kitten': 12, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 17, 'kitten': 4, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '001A' '002B' '004A' '005A' '006A' '008A' '009A' '010A' '011A'\n",
      " '012A' '013B' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '023A'\n",
      " '024A' '025A' '025B' '025C' '026A' '026B' '026C' '027A' '029A' '032A'\n",
      " '033A' '035A' '037A' '038A' '039A' '040A' '041A' '042A' '043A' '045A'\n",
      " '046A' '049A' '050A' '051B' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '060A' '061A' '062A' '063A' '065A' '066A' '067A' '068A' '070A'\n",
      " '071A' '072A' '075A' '076A' '087A' '088A' '090A' '094A' '095A' '096A'\n",
      " '097B' '099A' '100A' '101A' '102A' '103A' '104A' '105A' '106A' '109A'\n",
      " '110A' '111A' '116A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '002A' '003A' '007A' '016A' '021A' '022A' '023B' '028A' '031A'\n",
      " '034A' '036A' '044A' '047A' '048A' '051A' '059A' '064A' '069A' '073A'\n",
      " '074A' '091A' '092A' '093A' '097A' '108A' '113A' '115A' '117A']\n",
      "Length of X_train_val:\n",
      "702\n",
      "Length of y_train_val:\n",
      "702\n",
      "Length of groups_train_val:\n",
      "702\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     458\n",
      "kitten    136\n",
      "senior    108\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     130\n",
      "senior     70\n",
      "kitten     35\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 916, 1: 680, 2: 540})\n",
      "Epoch 1/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.9297 - accuracy: 0.5983\n",
      "Epoch 2/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.7451 - accuracy: 0.6816\n",
      "Epoch 3/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.6655 - accuracy: 0.7200\n",
      "Epoch 4/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.6190 - accuracy: 0.7434\n",
      "Epoch 5/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.6046 - accuracy: 0.7453\n",
      "Epoch 6/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.5465 - accuracy: 0.7711\n",
      "Epoch 7/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.5573 - accuracy: 0.7645\n",
      "Epoch 8/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4909 - accuracy: 0.8015\n",
      "Epoch 9/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4942 - accuracy: 0.7954\n",
      "Epoch 10/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4999 - accuracy: 0.7968\n",
      "Epoch 11/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4652 - accuracy: 0.8081\n",
      "Epoch 12/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4560 - accuracy: 0.8118\n",
      "Epoch 13/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.4448 - accuracy: 0.8235\n",
      "Epoch 14/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4329 - accuracy: 0.8296\n",
      "Epoch 15/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4418 - accuracy: 0.8193\n",
      "Epoch 16/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3917 - accuracy: 0.8427\n",
      "Epoch 17/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.8366\n",
      "Epoch 18/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4026 - accuracy: 0.8366\n",
      "Epoch 19/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.4045 - accuracy: 0.8390\n",
      "Epoch 20/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3794 - accuracy: 0.8418\n",
      "Epoch 21/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3694 - accuracy: 0.8493\n",
      "Epoch 22/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3624 - accuracy: 0.8549\n",
      "Epoch 23/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3761 - accuracy: 0.8427\n",
      "Epoch 24/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3617 - accuracy: 0.8511\n",
      "Epoch 25/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3397 - accuracy: 0.8628\n",
      "Epoch 26/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3608 - accuracy: 0.8591\n",
      "Epoch 27/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3524 - accuracy: 0.8666\n",
      "Epoch 28/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3296 - accuracy: 0.8675\n",
      "Epoch 29/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3137 - accuracy: 0.8713\n",
      "Epoch 30/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.3207 - accuracy: 0.8769\n",
      "Epoch 31/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3089 - accuracy: 0.8820\n",
      "Epoch 32/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3126 - accuracy: 0.8708\n",
      "Epoch 33/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3096 - accuracy: 0.8759\n",
      "Epoch 34/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3263 - accuracy: 0.8727\n",
      "Epoch 35/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2809 - accuracy: 0.8919\n",
      "Epoch 36/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2846 - accuracy: 0.8853\n",
      "Epoch 37/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2948 - accuracy: 0.8904\n",
      "Epoch 38/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.3012 - accuracy: 0.8797\n",
      "Epoch 39/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2896 - accuracy: 0.8895\n",
      "Epoch 40/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2902 - accuracy: 0.8853\n",
      "Epoch 41/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2750 - accuracy: 0.8867\n",
      "Epoch 42/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2818 - accuracy: 0.8904\n",
      "Epoch 43/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2699 - accuracy: 0.9007\n",
      "Epoch 44/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2575 - accuracy: 0.8989\n",
      "Epoch 45/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2677 - accuracy: 0.8975\n",
      "Epoch 46/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2665 - accuracy: 0.8993\n",
      "Epoch 47/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2582 - accuracy: 0.9017\n",
      "Epoch 48/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2595 - accuracy: 0.8947\n",
      "Epoch 49/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2671 - accuracy: 0.9017\n",
      "Epoch 50/1500\n",
      "67/67 [==============================] - 0s 6ms/step - loss: 0.2372 - accuracy: 0.9036\n",
      "Epoch 51/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2351 - accuracy: 0.9101\n",
      "Epoch 52/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2315 - accuracy: 0.9157\n",
      "Epoch 53/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2571 - accuracy: 0.8979\n",
      "Epoch 54/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2645 - accuracy: 0.9017\n",
      "Epoch 55/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2294 - accuracy: 0.9106\n",
      "Epoch 56/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2438 - accuracy: 0.9110\n",
      "Epoch 57/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2411 - accuracy: 0.9007\n",
      "Epoch 58/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2331 - accuracy: 0.9129\n",
      "Epoch 59/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2319 - accuracy: 0.9096\n",
      "Epoch 60/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9064\n",
      "Epoch 61/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2319 - accuracy: 0.9092\n",
      "Epoch 62/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2088 - accuracy: 0.9246\n",
      "Epoch 63/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2520 - accuracy: 0.9045\n",
      "Epoch 64/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2548 - accuracy: 0.8965\n",
      "Epoch 65/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2211 - accuracy: 0.9139\n",
      "Epoch 66/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2474 - accuracy: 0.9017\n",
      "Epoch 67/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2156 - accuracy: 0.9195\n",
      "Epoch 68/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1983 - accuracy: 0.9288\n",
      "Epoch 69/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2163 - accuracy: 0.9218\n",
      "Epoch 70/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2302 - accuracy: 0.9139\n",
      "Epoch 71/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2094 - accuracy: 0.9256\n",
      "Epoch 72/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.9242\n",
      "Epoch 73/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1975 - accuracy: 0.9265\n",
      "Epoch 74/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.2122 - accuracy: 0.9209\n",
      "Epoch 75/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1941 - accuracy: 0.9251\n",
      "Epoch 76/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2050 - accuracy: 0.9270\n",
      "Epoch 77/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.2027 - accuracy: 0.9246\n",
      "Epoch 78/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1991 - accuracy: 0.9265\n",
      "Epoch 79/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1907 - accuracy: 0.9307\n",
      "Epoch 80/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1802 - accuracy: 0.9331\n",
      "Epoch 81/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1966 - accuracy: 0.9265\n",
      "Epoch 82/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1705 - accuracy: 0.9429\n",
      "Epoch 83/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9419\n",
      "Epoch 84/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.9284\n",
      "Epoch 85/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1990 - accuracy: 0.9284\n",
      "Epoch 86/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1978 - accuracy: 0.9256\n",
      "Epoch 87/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1971 - accuracy: 0.9242\n",
      "Epoch 88/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.9326\n",
      "Epoch 89/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9312\n",
      "Epoch 90/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9368\n",
      "Epoch 91/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1606 - accuracy: 0.9424\n",
      "Epoch 92/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9387\n",
      "Epoch 93/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9415\n",
      "Epoch 94/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1842 - accuracy: 0.9260\n",
      "Epoch 95/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9307\n",
      "Epoch 96/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1735 - accuracy: 0.9302\n",
      "Epoch 97/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1651 - accuracy: 0.9434\n",
      "Epoch 98/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1690 - accuracy: 0.9359\n",
      "Epoch 99/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1816 - accuracy: 0.9326\n",
      "Epoch 100/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9504\n",
      "Epoch 101/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1607 - accuracy: 0.9443\n",
      "Epoch 102/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1956 - accuracy: 0.9251\n",
      "Epoch 103/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9405\n",
      "Epoch 104/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9419\n",
      "Epoch 105/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1762 - accuracy: 0.9391\n",
      "Epoch 106/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1771 - accuracy: 0.9335\n",
      "Epoch 107/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1826 - accuracy: 0.9270\n",
      "Epoch 108/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1647 - accuracy: 0.9434\n",
      "Epoch 109/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1638 - accuracy: 0.9419\n",
      "Epoch 110/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1715 - accuracy: 0.9396\n",
      "Epoch 111/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.9368\n",
      "Epoch 112/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9448\n",
      "Epoch 113/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1406 - accuracy: 0.9537\n",
      "Epoch 114/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1648 - accuracy: 0.9424\n",
      "Epoch 115/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1648 - accuracy: 0.9410\n",
      "Epoch 116/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1549 - accuracy: 0.9452\n",
      "Epoch 117/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1535 - accuracy: 0.9462\n",
      "Epoch 118/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1593 - accuracy: 0.9448\n",
      "Epoch 119/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.9429\n",
      "Epoch 120/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1481 - accuracy: 0.9466\n",
      "Epoch 121/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1540 - accuracy: 0.9471\n",
      "Epoch 122/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1430 - accuracy: 0.9527\n",
      "Epoch 123/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1510 - accuracy: 0.9476\n",
      "Epoch 124/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9499\n",
      "Epoch 125/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9490\n",
      "Epoch 126/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1602 - accuracy: 0.9382\n",
      "Epoch 127/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1563 - accuracy: 0.9434\n",
      "Epoch 128/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9471\n",
      "Epoch 129/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1401 - accuracy: 0.9480\n",
      "Epoch 130/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9429\n",
      "Epoch 131/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1547 - accuracy: 0.9424\n",
      "Epoch 132/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9466\n",
      "Epoch 133/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1467 - accuracy: 0.9490\n",
      "Epoch 134/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9391\n",
      "Epoch 135/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1689 - accuracy: 0.9391\n",
      "Epoch 136/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9410\n",
      "Epoch 137/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1637 - accuracy: 0.9415\n",
      "Epoch 138/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9541\n",
      "Epoch 139/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1489 - accuracy: 0.9391\n",
      "Epoch 140/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1323 - accuracy: 0.9541\n",
      "Epoch 141/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1267 - accuracy: 0.9508\n",
      "Epoch 142/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9574\n",
      "Epoch 143/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1456 - accuracy: 0.9480\n",
      "Epoch 144/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1469 - accuracy: 0.9485\n",
      "Epoch 145/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1347 - accuracy: 0.9532\n",
      "Epoch 146/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9569\n",
      "Epoch 147/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9513\n",
      "Epoch 148/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9466\n",
      "Epoch 149/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1570 - accuracy: 0.9419\n",
      "Epoch 150/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1418 - accuracy: 0.9480\n",
      "Epoch 151/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9518\n",
      "Epoch 152/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9565\n",
      "Epoch 153/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9574\n",
      "Epoch 154/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9583\n",
      "Epoch 155/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9508\n",
      "Epoch 156/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1396 - accuracy: 0.9457\n",
      "Epoch 157/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1335 - accuracy: 0.9537\n",
      "Epoch 158/1500\n",
      "67/67 [==============================] - 0s 4ms/step - loss: 0.1234 - accuracy: 0.9565\n",
      "Epoch 159/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1216 - accuracy: 0.9551\n",
      "Epoch 160/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1299 - accuracy: 0.9546\n",
      "Epoch 161/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9546\n",
      "Epoch 162/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1326 - accuracy: 0.9494\n",
      "Epoch 163/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9532\n",
      "Epoch 164/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1154 - accuracy: 0.9640\n",
      "Epoch 165/1500\n",
      "67/67 [==============================] - 0s 3ms/step - loss: 0.1167 - accuracy: 0.9579\n",
      "Epoch 166/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1114 - accuracy: 0.9630\n",
      "Epoch 167/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9466\n",
      "Epoch 168/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.1180 - accuracy: 0.9565\n",
      "Epoch 169/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1262 - accuracy: 0.9565\n",
      "Epoch 170/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9452\n",
      "Epoch 171/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9569\n",
      "Epoch 172/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.1233 - accuracy: 0.9565\n",
      "Epoch 173/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.1281 - accuracy: 0.9555\n",
      "Epoch 174/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.1121 - accuracy: 0.9644\n",
      "Epoch 175/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.1275 - accuracy: 0.9518\n",
      "Epoch 176/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.1351 - accuracy: 0.9480\n",
      "Epoch 177/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.1287 - accuracy: 0.9551\n",
      "Epoch 178/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.1222 - accuracy: 0.9597\n",
      "Epoch 179/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1208 - accuracy: 0.9597\n",
      "Epoch 180/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9649\n",
      "Epoch 181/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9640\n",
      "Epoch 182/1500\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.1097 - accuracy: 0.9630\n",
      "Epoch 183/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9644\n",
      "Epoch 184/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1271 - accuracy: 0.9518\n",
      "Epoch 185/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1199 - accuracy: 0.9508\n",
      "Epoch 186/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9616\n",
      "Epoch 187/1500\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.1235 - accuracy: 0.9574\n",
      "Epoch 188/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1171 - accuracy: 0.9597\n",
      "Epoch 189/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9616\n",
      "Epoch 190/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1137 - accuracy: 0.9579\n",
      "Epoch 191/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9658\n",
      "Epoch 192/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9654\n",
      "Epoch 193/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9527\n",
      "Epoch 194/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9588\n",
      "Epoch 195/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9677\n",
      "Epoch 196/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0943 - accuracy: 0.9658\n",
      "Epoch 197/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1038 - accuracy: 0.9602\n",
      "Epoch 198/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0886 - accuracy: 0.9733\n",
      "Epoch 199/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1297 - accuracy: 0.9588\n",
      "Epoch 200/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9560\n",
      "Epoch 201/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1134 - accuracy: 0.9611\n",
      "Epoch 202/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1033 - accuracy: 0.9616\n",
      "Epoch 203/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9625\n",
      "Epoch 204/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9583\n",
      "Epoch 205/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1215 - accuracy: 0.9551\n",
      "Epoch 206/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1075 - accuracy: 0.9625\n",
      "Epoch 207/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9691\n",
      "Epoch 208/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9616\n",
      "Epoch 209/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1127 - accuracy: 0.9625\n",
      "Epoch 210/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1030 - accuracy: 0.9616\n",
      "Epoch 211/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9607\n",
      "Epoch 212/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9611\n",
      "Epoch 213/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1069 - accuracy: 0.9686\n",
      "Epoch 214/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9654\n",
      "Epoch 215/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9583\n",
      "Epoch 216/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0955 - accuracy: 0.9672\n",
      "Epoch 217/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0939 - accuracy: 0.9663\n",
      "Epoch 218/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9644\n",
      "Epoch 219/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9677\n",
      "Epoch 220/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9700\n",
      "Epoch 221/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0999 - accuracy: 0.9602\n",
      "Epoch 222/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0920 - accuracy: 0.9668\n",
      "Epoch 223/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0906 - accuracy: 0.9696\n",
      "Epoch 224/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9668\n",
      "Epoch 225/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1044 - accuracy: 0.9668\n",
      "Epoch 226/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9686\n",
      "Epoch 227/1500\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9602\n",
      "Epoch 228/1500\n",
      "42/67 [=================>............] - ETA: 0s - loss: 0.0876 - accuracy: 0.9702Restoring model weights from the end of the best epoch: 198.\n",
      "67/67 [==============================] - 0s 1ms/step - loss: 0.0889 - accuracy: 0.9696\n",
      "Epoch 228: early stopping\n",
      "8/8 [==============================] - 0s 825us/step - loss: 0.6838 - accuracy: 0.7319\n",
      "8/8 [==============================] - 0s 664us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.79 (23/29)\n",
      "Before appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "After appending - Cat IDs: 848, Predictions: 848, Actuals: 848, Gender: 848\n",
      "Final Test Results - Loss: 0.6837711334228516, Accuracy: 0.7319148778915405, Precision: 0.7520430107526881, Recall: 0.6893772893772893, F1 Score: 0.7113585245164192\n",
      "Confusion Matrix:\n",
      " [[111   6  13]\n",
      " [ 11  24   0]\n",
      " [ 33   0  37]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.6976157300272425\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8414455205202103\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7440045028924942\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.6899397454592846\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7654134608429674\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[3]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0522ad-89f9-479c-b073-c5e720fc6221",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ecac477e-680c-4a82-abcb-2b3a2dbfbea4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 848, Predictions: 848, Actuals: 848, Gender: 848\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce33a298-d7a3-40ec-8f78-00c109118230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "23d083a1-d61a-4fce-9b56-667a9930bfe0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.82 (90/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "431e9ecf-be90-466a-a1b7-dd98f7fb2a9a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "42e4a5c0-56dd-4771-820f-bcecaf391f0c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, senior, kitten, senior, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, senior, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, senior, adult, senior, adult, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, kitten, kitten, kitten, kitten,...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, adult, adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, kitten, adult, adult, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, senior, adult, kitten, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, senior, senior, senior, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, senior, adult, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, adult, adult, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, senior, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, senior, adult, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, adult, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, senior, adult, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, se...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, kitten, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, senior, kitten, senior, adult, adult, ...         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "80    074A  [adult, adult, adult, kitten, adult, adult, ad...         adult            adult                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A               [adult, adult, adult, adult, senior]         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, senior, senior, adult, s...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, k...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "62    056A                           [senior, senior, senior]        senior           senior                   True\n",
       "61    055A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "59    053A       [adult, adult, senior, senior, adult, adult]         adult            adult                   True\n",
       "58    052A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "57    051B  [senior, adult, senior, adult, senior, adult, ...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, senior, a...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "52    047A  [adult, adult, kitten, kitten, kitten, kitten,...        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "81    075A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "96    101A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "103   109A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "102   108A     [senior, senior, adult, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                   [senior, senior, senior, senior]        senior           senior                   True\n",
       "98    103A  [adult, senior, adult, adult, senior, senior, ...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [adult, adult, adult, senior, adult, senior, a...         adult            adult                   True\n",
       "92    097A  [adult, senior, senior, adult, adult, senior, ...        senior           senior                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "87    092A                                            [adult]         adult            adult                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "50    044A             [kitten, adult, adult, kitten, kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "48    042A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "47    041A                                           [kitten]        kitten           kitten                   True\n",
       "26    023B              [adult, adult, adult, kitten, kitten]         adult            adult                   True\n",
       "25    023A        [adult, kitten, adult, adult, adult, adult]         adult            adult                   True\n",
       "24    022A  [adult, kitten, adult, adult, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, senior, adult, kitten, adult, a...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "18    016A  [senior, adult, senior, senior, senior, senior...        senior           senior                   True\n",
       "17    015A  [adult, senior, adult, adult, senior, adult, s...         adult            adult                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "29    025B                                    [senior, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                              [adult, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "9     008A        [adult, adult, adult, kitten, adult, adult]         adult            adult                   True\n",
       "8     007A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                              [adult, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, senior, adult, adult, se...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "27    024A                                           [senior]        senior           senior                   True\n",
       "39    033A  [adult, adult, adult, kitten, kitten, adult, k...         adult            adult                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "42    036A  [senior, senior, senior, adult, adult, adult, ...         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "41    035A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "40    034A              [adult, senior, senior, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "35    028A  [adult, senior, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "46    040A  [kitten, kitten, senior, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "106   113A                             [adult, senior, adult]         adult           senior                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "101   106A  [adult, senior, adult, senior, adult, adult, a...         adult           senior                  False\n",
       "56    051A  [senior, senior, adult, adult, adult, senior, ...         adult           senior                  False\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "30    025C             [adult, senior, adult, senior, senior]        senior            adult                  False\n",
       "12    011A                                     [adult, adult]         adult           senior                  False\n",
       "60    054A                                     [adult, adult]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, se...         adult           senior                  False\n",
       "64    058A                             [senior, adult, adult]         adult           senior                  False\n",
       "65    059A  [adult, adult, adult, adult, senior, senior, a...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "66    060A                           [kitten, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                     [adult, adult]         adult           senior                  False\n",
       "69    063A  [senior, senior, senior, kitten, senior, senio...        senior            adult                  False\n",
       "33    026C                                           [senior]        senior            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "109   117A  [adult, senior, adult, adult, adult, adult, ad...         adult           senior                  False"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bc2e5ff6-b761-4b71-b7cd-9eb24aae0ebc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     66\n",
      "kitten    13\n",
      "senior    11\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ff925f9d-efd5-46a0-89a0-1fb3da66a46a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             66  90.410959\n",
      "1           kitten           15             13  86.666667\n",
      "2           senior           22             11  50.000000\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d8f08817-c78c-4654-b195-911f2a9ccb2b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea6f284-fe35-4071-8054-5e5a086b4ed9",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4394c6ec-6da8-42de-aaaa-bc1b17bbf74b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          556            438  78.776978\n",
      "1           kitten          114             91  79.824561\n",
      "2           senior          178            102  57.303371\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8d541c77-590d-4b79-b526-85520ea5c4a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics (Overall)\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Overall Accuracy by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c98d75-f483-4819-b292-975bd229cfd2",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c7325b11-ab06-45bd-8e77-d498737b7552",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    220      170     77.27\n",
      "1          M    337      253     75.07\n",
      "2          X    291      208     71.48\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "777ae0eb-dab5-47c0-86a7-0d3c6b01b9c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGUCAYAAAA72JSTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLOElEQVR4nO3deXxM9/7H8fckIqslliBi3/ddQ6kQW+1a271oa9erVa7b5aJoy0+rbdpya6mWS2hRJbZWkcYudLMTW0OIvYQsSGR+f3jk3EwTxGRiJub1fDzyeGTO+Z5zPpM47Xu++Z7v12Q2m80CAAAAnISLvQsAAAAAHicCMAAAAJwKARgAAABOhQAMAAAAp0IABgAAgFMhAAMAAMCpEIABAADgVAjAAAAAcCoEYAAAADiVPPYuAMCTLSkpSe3bt1dCQoIkqUqVKlq8eLGdq0JsbKy6dOlivP7ll1/sWI108eJFrV27Vlu3btWFCxcUFxcnd3d3FS9eXHXq1FG3bt1UvXp1u9b4IA0bNjS+X716tfz9/e1YDYCHIQADyFEbN240wq8kRUVF6dChQ6pRo4Ydq4IjWb16tT7++GOLfyeSlJKSopMnT+rkyZNauXKl+vTpo3/+858ymUx2qhTAk4IADCBHrVq1KsO2lStXEoAhSVq0aJE+/fRT43WBAgX01FNPqUiRIrpy5Yp27typ+Ph4mc1mffPNN/L19dXAgQPtVzCAJwIBGECOiY6O1r59+yRJ+fPn140bNyRJGzZs0OjRo+Xt7W3P8mBnBw4c0IwZM4zXzz77rN566y2Lfxfx8fF64403tGfPHknSvHnz1KtXL/n4+Dz2egE8OQjAAHJM+t7fnj17KjIyUocOHVJiYqLWr1+v559//r7HHj16VKGhofrtt990/fp1FSpUSBUqVFCfPn3UtGnTDO3j4+O1ePFiRURE6OzZs3Jzc5O/v7/atm2rnj17ysvLy2g7adIkrV27VpI0ZMgQDRs2zNj3yy+/aPjw4ZKkEiVKaM2aNca+tHGehQsX1ty5czVp0iQdOXJE+fPn1xtvvKHg4GDduXNHixcv1saNGxUTE6Pbt2/L29tb5cqV0/PPP6+OHTtaXfvAgQO1f/9+SdKoUaPUr18/i/N88803+vjjjyVJzZo1s+hZfZg7d+5o/vz5WrNmjf78808FBASoS5cu6tOnj/Lkufe/inHjxunHH3+UJPXq1UtvvPGGxTk2b96sf/3rX5KkChUqaOnSpQ+85uzZs3X37l1JUo0aNTRp0iS5urpatPHx8dE777yjcePGqUyZMqpQoYJSUlIs2qSmpiosLExhYWE6deqUXF1dVbZsWXXs2FHPPfecUX+a9L/HH3/8UWFhYVq2bJlOnz6tfPnyqWXLlho2bJgKFixocdzdu3e1ZMkSrVq1SmfPnlWhQoXUuXNnDRgw4IHv88qVK5o3b562bdumK1euKH/+/Kpdu7ZefPFF1axZ06LtnDlzNHfuXEnSW2+9pRs3bujrr79WUlKSqlevbuwDkD0EYAA5IiUlRevWrTNed+7cWcWLF9ehQ4ck3RsGcb8AvHbtWr333ntGOJLuPSR18eJF7dy5U6+88opeeuklY9+FCxf08ssvKyYmxth269YtRUVFKSoqSuHh4Zo9e7ZFCM6OW7du6ZVXXlFsbKwk6erVq6pcubJSU1M1btw4RUREWLS/efOm9u/fr/379+vs2bMWgftRau/SpYsRgDds2JAhAG/cuNH4vlOnTo/0nkaNGmX0skrSqVOn9Omnn2rfvn2aNm2aTCaTunbtagTg8PBw/etf/5KLy/8mE3qU68fFxennn382Xvft2zdD+E1TtGhRffHFF5nuS0lJ0ZtvvqktW7ZYbD906JAOHTqkLVu26JNPPlHevHkzPf7999/X8uXLjde3b9/Wt99+q4MHD2r+/PlGeDabzXrrrbcsfrcXLlzQ3Llzjd9JZk6cOKERI0bo6tWrxrarV68qIiJCW7Zs0dixY9WtW7dMj12xYoWOHTtmvC5evPh9rwPg0TANGoAcsW3bNv3555+SpHr16ikgIEBt27aVp6enpHs9vEeOHMlw3KlTpzRlyhQj/FaqVEk9e/ZUYGCg0eY///mPoqKijNfjxo0zAqSPj486deqkrl27Gn9KP3z4sGbNmmWz95aQkKDY2Fg1b95c3bt311NPPaVSpUpp+/btRkDy9vZW165d1adPH1WuXNk49uuvv5bZbLaq9rZt2xoh/vDhwzp79qxxngsXLujAgQOS7g03eeaZZx7pPe3Zs0fVqlVTz549VbVqVWN7RESE0ZPfqFEjlSxZUtK9EPfrr78a7W7fvq1t27ZJklxdXfXss88+8HpRUVFKTU01XtetW/eR6k3z3//+1wi/efLkUdu2bdW9e3flz59fkrR79+779ppevXpVy5cvV+XKlTP8no4cOWIxM8aqVasswm+VKlWMn9Xu3bszPX9aOE8LvyVKlFCPHj309NNPS7rXc/3+++/rxIkTmR5/7NgxFSlSRL169VL9+vXVrl27rP5YADwEPcAAckT64Q+dO3eWdC8Utm7d2hhWsGLFCo0bN87iuG+++UbJycmSpKCgIL3//vtGL9zkyZMVFhYmb29v7dmzR1WqVNG+ffuMccbe3t5atGiRAgICjOsOHjxYrq6uOnTokFJTUy16LLOjZcuW+vDDDy225c2bV926ddPx48c1fPhwNWnSRNK9Ht02bdooKSlJCQkJun79unx9fR+5di8vL7Vu3VqrV6+WdK8XOO2BsE2bNhnBum3btvft8byfNm3aaMqUKXJxcVFqaqrefvtto7d3xYoV6tatm0wmkzp37qzZs2cb12/UqJEkaceOHUpMTJQk4yG2B0n7cJSmUKFCFq/DwsI0efLkTI9NG7aSnJxsMaXeJ598YvzMX3zxRf39739XYmKili1bpkGDBsnDwyPDuZo1a6aQkBC5uLjo1q1b6t69uy5fvizp3oextA9eK1asMI5p2bKl3n//fbm6umb4WaW3efNmnT59WpJUunRpLVq0yPgAs3DhQk2fPl0pKSlasmSJxo8fn+l7nTFjhipVqpTpPgDWowcYgM1dunRJu3btkiR5enqqdevWxr6uXbsa32/YsMEITWnS97r16tXLYvzmiBEjFBYWps2bN6t///4Z2j/zzDNGgJTu9SouWrRIW7du1bx582wWfiVl2hsXGBio8ePHa8GCBWrSpIlu376tvXv3KjQ01KLX9/bt21bX/tefX5pNmzYZ3z/q8AdJGjBggHENFxcXvfDCC8a+qKgo40NJp06djHY//fSTMR43/fCHtA88D+Lu7m7x+q/jerPi6NGjunnzpiSpZMmSRviVpICAANWvX1/SvR77gwcPZnqOPn36GO/Hw8PDYnaStH+bycnJFn9xSPtgImX8WaWXfkhJhw4dLIbgpJ+D+X49yOXLlyf8AjmEHmAANrdmzRpjCIOrq6vxYFQak8kks9mshIQE/fjjj+revbux79KlS8b3JUqUsDjO19dXvr6+Ftse1F6SxZ/zsyJ9UH2QzK4l3RuKsGLFCkVGRioqKspiHHOatD/9W1N7nTp1VLZsWUVHR+vEiRP6448/5OnpaQS8smXLZniwKitKly5t8bps2bLG93fv3lVcXJyKFCmi4sWLKzAwUDt37lRcXJx2796tBg0aaPv27ZKkfPnyZWn4hZ+fn8XrixcvqkyZMsbrSpUq6cUXXzRer1+/XhcvXrQ45sKFC8b3586ds1iM4q+io6Mz3f/XcbXpQ2ra7y4uLs7i95i+TsnyZ3W/+mbPnm30nP/V+fPndevWrQw91Pf7NwYg+wjAAGzKbDYbf6KX7s1wkL4n7K9WrlxpEYDTyyw8PsijtpcyBt60ns6HyWwKt3379unVV19VYmKiTCaT6tatq/r166t27dqaPHmy8af1zDxK7V27dtVnn30m6V4vcPrQZk3vr3TvfacPYH+tJ/0Dal26dNHOnTuN6yclJSkpKUnSvaEUf+3dzUyFChXk5eVl9LL+8ssvFsGyRo0aFr2xBw4cyBCA09eYJ08eFShQ4L7Xu18P81+HimTlrwR/Pdf9zp1+jLO3t3emQzDSJCYmZtjPNIFAziEAA7CpX3/9VefOncty+8OHDysqKkpVqlSRdK9nMO2hsOjoaIvetTNnzui7775T+fLlVaVKFVWtWtWiJzFtvGV6s2bNUr58+VShQgXVq1dPHh4eFiHn1q1bFu2vX7+epbrd3NwybAsJCTEC3Xvvvaf27dsb+zILSdbULkkdO3bU559/rpSUFG3YsMEISi4uLurQoUOW6v+r48ePG0MGpHs/6zTu7u7GQ2WS1KJFCxUsWFDXr1/X5s2bjfmdpawNf5DuDTdo0aKFfvjhB0n3xn537tz5vmOXM+uZT//z8/f3txinK90LyPebWeJRFCxYUHnz5tWdO3ck3fvZpF+W+Y8//sj0uKJFixrfv/TSSxbTpWVlPHpm/8YA2AZjgAHYVFhYmPF9nz599Msvv2T61bhxY6Nd+uDSoEED4/tly5ZZ9MguW7ZMixcv1nvvvaevvvoqQ/tdu3bp5MmTxuujR4/qq6++0qeffqpRo0YZASZ9mDt16pRF/eHh4Vl6n5ktx3v8+HHj+/RzyO7atUvXrl0zXqf1DFpTu3TvgbHmzZtLuhecDx8+LElq3LhxhqEFWTVv3jwjpJvNZi1YsMDYV7NmTYsg6ebmZgTthIQEY/aH0qVLq1atWlm+5oABA4ze4ujoaL311lvGmN408fHxCgkJ0d69ezMcX716daP3+8yZM8YwDOne3LutWrXSc889p9dff/2Bve8PkydPHov3lX5Md0pKir788stMj0v/+129erXi4+ON18uWLVOLFi304osv3ndoBEs+AzmHHmAANnPz5k2LqaLSP/z2V+3atTOGRqxfv16jRo2Sp6en+vTpo7Vr1yolJUV79uzR3/72NzVq1Ejnzp0z/uwuSb1795Z072Gx2rVra//+/bp9+7YGDBigFi1ayMPDw+LBrA4dOhjBN/2DRTt37tTUqVNVpUoVbdmyRTt27LD6/RcpUsSYG3js2LFq27atrl69qq1bt1q0S3sIzpra03Tt2jXDfMPWDn+QpMjISPXr108NGzbUwYMHLR4a69WrV4b2Xbt21ddff52t65cvX16vvfaapk2bJknaunWrunTpoiZNmqhIkSK6ePGiIiMjlZCQYHFcWo+3h4eHnnvuOS1atEiSNGbMGD3zzDPy8/PTli1blJCQoISEBOXLl8+iN9Yaffr0MaZ927hxo86fP68aNWro999/t5irN73WrVtr1qxZunjxomJiYtSzZ081b95ciYmJ2rRpk1JSUnTo0KEs95oDsB16gAHYzA8//GCEu6JFi6pOnTr3bduqVSvjT7xpD8NJUsWKFfXvf//b6HGMjo7Wt99+axF+BwwYYPFA0+TJk435aRMTE/XDDz9o5cqVRo9b+fLlNWrUKItrp7WXpO+++07/93//px07dqhnz55Wv/+0mSkk6caNG1q+fLkiIiJ09+5di6V70y968ai1p2nSpIlFqPP29lZQUJBVdVeuXFn169fXiRMntGTJEovw26VLFwUHB2c4pkKFChYP21k7/KJXr16aOnWq0ZN78+ZNbdiwQV9//bXCw8Mtwm+RIkX0xhtvqG/fvsa24cOHGz2td+/eVUREhJYuXWo8gFasWDFNmTLlkev6q5YtW1os3HLw4EEtXbpUx44dU/369S3mEE7j4eGhDz74wAjsly9f1ooVK7R+/Xqjt/3ZZ5/Vc889l+36ADwaeoAB2Ez6uX9btWr1wD/h5suXT02bNjUWMVi5cqWxIlbXrl1VqVIli6WQvb29jYUa/hr0/P39FRoaqkWLFikiIsLohQ0ICFBwcLD69+9vLMAh3Zua7csvv9T06dO1a9cu3bp1SxUrVlSfPn3UsmVLffvtt1a9/549e8rX11cLFy5UdHS0zGazKlSooN69e+v27dvGvLbh4eHGe3jU2tO4urqqRo0a2rx5s6R7vY0PesjqQfLmzav//Oc/mj9/vtatW6crV64oICBAvXr1euBy1bVq1TLCcsOGDa1eqaxNmzaqX7++Vq1apV27dunUqVOKj4+Xl5eXihYtqlq1aqlJkyYKCgrKsKyxh4eHPv/8cyNYnjp1SsnJySpRooSaN2+ufv36qXDhwlbV9VdvvfWWqlatqqVLl+rMmTMqXLiwOnbsqIEDB2ro0KGZHlOzZk0tXbpUCxYs0K5du3T58mV5enqqTJkyeu655/Tss8/adHo+AFljMmd1zh8AgMM4c+aM+vTpY4wNnjNnjsWY05x2/fp19ezZ0xjbPGnSpGwNwQCAx4keYADIJc6fP69ly5bp7t27Wr9+vRF+K1So8FjCb1JSkmbNmiVXV1f99NNPRvj19fV94HhvAHA0DhuAL168qN69e+ujjz6yGOsXExOjkJAQ/f7773J1dVXr1q316quvWoyvS0xM1IwZM/TTTz8pMTFR9erV0z//+c/7TlYOALmByWRSaGioxTY3Nze9/vrrj+X67u7uWrZsmcWUbiaTSf/85z+tHn4BAPbgkAH4woULevXVVy2mjJHuPRwxfPhwFS5cWJMmTdK1a9c0ffp0xcbGasaMGUa7cePG6eDBgxo5cqS8vb01d+5cDR8+XMuWLcvwJDUA5BZFixZVqVKldOnSJXl4eKhKlSoaOHDgA1dAsyUXFxfVqlVLR44ckZubm8qVK6d+/fqpVatWj+X6AGArDhWAU1NTtW7dOn366aeZ7l++fLni4uK0ePFiY45NPz8/vfbaa9q7d6/q1q2r/fv3a9u2bfrss8/09NNPS5Lq1aunLl266Ntvv9WgQYMe07sBANtydXXVypUr7VrD3Llz7Xp9ALAFh3r09Pjx45o6dao6duyod955J8P+Xbt2qV69ehYTzAcGBsrb29uYu3PXrl3y9PRUYGCg0cbX11f169fP1vyeAAAAeDI4VAAuXry4Vq5ced/xZNHR0SpdurTFNldXV/n7+xvLiEZHR6tkyZIZlr8sVapUpkuNAgAAwLk41BCIAgUKqECBAvfdHx8fb0wonp6Xl5cxWXpW2jyqqKgo41jWZgcAAHBMycnJMplMqlev3gPbOVQAfpjU1NT77kubSDwrbayRNl1y2rRDAAAAyJ1yVQD28fFRYmJihu0JCQny8/Mz2vz555+Ztkk/VdqjqFKlig4cOCCz2ayKFStadQ4AAADkrBMnTjxwFdI0uSoAlylTRjExMRbb7t69q9jYWLVs2dJoExkZqdTUVIse35iYmGzPA2wymYz16gEAAOBYshJ+JQd7CO5hAgMD9dtvvxmrD0lSZGSkEhMTjVkfAgMDlZCQoF27dhltrl27pt9//91iZggAAAA4p1wVgHv06CF3d3eNGDFCERERCgsL09tvv62mTZuqTp06kqT69eurQYMGevvttxUWFqaIiAj94x//UL58+dSjRw87vwMAAADYW64aAuHr66vZs2crJCRE48ePl7e3t4KDgzVq1CiLdh9++KE++eQTffbZZ0pNTVWdOnU0depUVoEDAACATOa06Q3wQAcOHJAk1apVy86VAAAAIDNZzWu5aggEAAAAkF0EYAAAADgVAjAAAACcCgEYAAAAToUADAAAAKdCAAYAAIBTIQADAADAqRCAAQAA4FQIwAAAAHAqBGAAAAA4FQIwAAAAnAoBGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMAAAABwKgRgAAAAOBUCMAAAAJwKARgAAABOhQAMAAAAp0IABgAAgFMhAAMAAMCpEIABAADgVAjAAAAAcCoEYAAAADgVAjAAAACcCgEYAAAAToUADAAAAKdCAAYAAIBTIQADAADAqRCAAQAA4FQIwAAAAHAqBGAAAAA4FQIwAAAAnAoBGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMAAAABwKgRgAAAAOBUCMAAAAJxKHnsXAEjSL7/8ouHDh993/9ChQ/XFF1/cd3+DBg00Z86c++5fs2aNQkNDde7cORUrVky9evVS7969ZTKZslU3AADIfQjAcAhVq1bV/PnzM2yfNWuWDh06pHbt2qlJkyYZ9v/0008KDQ3V888/f99zh4WFafLkyXrhhRcUGBiogwcP6pNPPlFiYqIGDhxo0/cBAAAcHwEYDsHHx0e1atWy2LZlyxbt2bNH77//vsqUKZPhmAsXLigsLEw9e/ZU27Zt73vu+fPnKzg4WCNHjpQkNW7cWGfOnNHSpUsJwAAAOCECMBzSrVu39OGHH6pZs2Zq3bp1pm0+/fRTubu7a8SIEQ88V1q79Nzc3HTnzh2b1QsAAHIPAjAc0pIlS3T58mXNmjUr0/0HDhzQpk2bNHHiRPn4+DzwXOXKlZMkmc1m3bhxQxEREVq3bp369u1r87oBAIDjIwDD4SQnJ+ubb75R27ZtVapUqUzbLFy4UP7+/nr22WezfN4DBw4YQx6qV6+ufv362aReAACQu+TKadBWrlypXr16qVmzZurRo4eWLVsms9ls7I+JidHo0aMVFBSk4OBgTZ06VfHx8XasGI8iPDxcV69eVf/+/TPdf/HiRW3ZskV/+9vflCdP1j/DlShRQnPmzNHEiRN15coVDRw4ULdu3bJV2QAAIJfIdT3AYWFhmjJlinr37q0WLVro999/14cffqg7d+6oX79+unnzpoYPH67ChQtr0qRJunbtmqZPn67Y2FjNmDHD3uUjC8LDw1W+fHlVrlw50/0REREymUwPfPAtM0WLFlXRokXVoEEDlSxZUkOHDtWmTZvUqVMnW5QN2EVWphAcOnSoBg0apH379mXYv3DhQlWvXv2h17l48aJ69+6tjz76SA0bNsxWzQBgb7kuAK9evVp169bV66+/LuneE/2nT5/WsmXL1K9fPy1fvlxxcXFavHixChYsKEny8/PTa6+9pr1796pu3br2Kx4PlZKSol27dunFF1+8b5tt27apXr16Kly48EPPl5iYqK1bt6pGjRoWwymqVq0qSbpy5Ur2iwbsKCtTCJrNZp04cUJ9+/bN8FBp2hj5B7lw4YJeffVV/pIG4ImR6wLw7du3VaRIEYttBQoUUFxcnCRp165dqlevnhF+JSkwMFDe3t7asWMHAdjBnThxQrdu3VKdOnUy3W82m3Xo0CH17t07S+dzdXXVe++9pw4dOmjcuHHG9sjISElSxYoVs180YEdZmUIwJiZGCQkJevrppzO0fZDU1FStW7dOn376qY2rBgD7ynVjgP/2t78pMjJS33//veLj47Vr1y6tW7dOHTp0kCRFR0erdOnSFse4urrK399fp0+ftkfJeAQnTpyQJJUvXz7T/RcuXFB8fPwDe60OHDigs2fPSpLc3d01YMAAhYWFadasWfr555+1ePFivfvuu2rcuLGefvpp278JwI4ym0IwKipKku47rOh+jh8/rqlTp6pjx4565513bF4rANhLrusBbteunX799VdNmDDB2NakSRONGTNGkhQfHy9vb+8Mx3l5eSkhISFb1zabzUpMTMzWOfBgFy5ckHTvQ0tmP+tz585Juhds7/e7GDBggNq3b6+xY8dKuvehydvbWytWrFBoaKgKFiyoLl26aMCAAUpKSsqhdwLYx6JFi3T58mWFhIQY98ihQ4fk6empjz/+WDt37lRSUpLq1aunV199NUOHQXoFChTQ119/LT8/P/3++++S7v0Vjv8OAnBUZrNZJpPpoe1yXQAeM2aM9u7dq5EjR6pGjRo6ceKEvvjiC7355pv66KOPlJqaet9jXVyy1+GdnJysI0eOZOsceLB69eppzpw5OnXqVKb7TSaT5syZI0n3/V1ktr9y5cp66623LNrd7xpAbpWSkqIlS5aoQYMGunnzpnEP7N27V0lJSbpz546GDBmiq1evat26dXr55Zc1fvx4iyFjmbl69arxF7TTp0/Lw8Mjp98KAFgtb968D22TqwLwvn37tHPnTo0fP17dunWTJOOJ/lGjRmn79u3y8fHJtHciISFBfn5+2bq+m5sbY0YBOKyNGzfqxo0bGj58uMV/q0aPHq34+HiLZyDatWun/v37a+/evXr55Zcfeu60KQPLlCmjatWq2bx2ALCFtKGUD5OrAvD58+clKcMDUvXr15cknTx50njgI727d+8qNjZWLVu2zNb1TSaTvLy8snUOAMgp27dvV/ny5VW7dm2L7X99Ld17ALRcuXKKjo7O0n/X0pYTd3d357+DABxWVoY/SLnsIbiyZctKkjEWLU3a3JYBAQEKDAzUb7/9pmvXrhn7IyMjlZiYqMDAwMdWKwA8TmlTCLZp0ybD9rVr12r//v0Zjrl169ZDhz8AwJMoV/UAV61aVa1atdInn3yiGzduqGbNmjp16pS++OILVatWTUFBQWrQoIGWLl2qESNGaMiQIYqLi9P06dPVtGnT+06tBQC53f2mEMyTJ4/mzp2rIkWK6KuvvjK2Hz16VGfPnn3gnNsA8KTKVQFYkqZMmaKvvvpKK1as0Jw5c1S8eHF17txZQ4YMUZ48eeTr66vZs2crJCRE48ePl7e3t4KDgzVq1Ch7lw4AOeZBUwgOGTJEkyZN0oQJE9ShQwdduHBBs2fPVuXKlY2VEO/cuaOoqCj5+fmpWLFij7V2AHjccl0AdnNz0/Dhwx+49GfFihU1c+bMx1gVANjX1atXJUn58uXLsK9Tp05yd3fXwoUL9a9//Uuenp4KCgrSK6+8IldXV0n3VkUcMGCAhgwZomHDhj3W2gF7yOoy4mlSUlI0ePBgNWnSJEv3SIcOHXTp0qUM2zdt2sTQIwdgMpvNZnsXkRscOHBAkh5pFSUAAOCY4uPj9ccff2TYnraM+MKFC1WmTBlJ9+a/njhxojZt2pSlD4nXr19X69at9dprr2VYgbZatWrKkyfX9T/mGlnNa/wGnFSq2SyXLD4piceP3w8A5KysLCMu3Xvwftq0aZn25t5P2uqLLVu2VEBAgO2Khs0QgJ2Ui8mkJZHHdOkGKzo5Gr/8XuoT+GhL1gIAsiezZcQl6Z///Kfq1q2rkJAQde7cOUvnOnbsmLy9vVWyZMmcKhfZRAB2YpduJCr2WvaWhwYA4EmwZMkSXb58WbNmzbLYPnfu3EdeBOvYsWPKnz+/3njjDe3Zs0epqalq1qyZxowZoyJFitiybFgpV80DDAAAYGvJycn65ptv1LZtW5UqVcpinzUrwEZFRenSpUuqVq2aPv30U40ePVq//fabhg4dqqSkJFuVjWygBxgAADi18PBwXb16Vf3797fJ+caPHy9XV1fVqFFDklSvXj2VL19egwcP1rp169SjRw+bXAfWIwADAACnFh4ervLly6tyZds8f5HZ8uN169aVj4+Pjh07ZpNrIHsYAgEAAJzW/ZYRt1Z8fLxWrVplLE6TJjU1VcnJyfL19bXJdZA9BGAAeASpTJ3usPjdwBr3W0bcWm5ubpo2bZr++9//WmzfunWrbt++rYYNG9rkOsgehkAAwCNgCkHHxPSBsNaDlhHPqgMHDsjX11cBAQFyd3fXSy+9pDlz5qhQoUJ6+umndeLECX3xxRdq0aKFGjVqZKvSkQ0EYAB4REwhCDw5HrSMeFYNGDBAnTp10qRJkyRJgwYNkq+vr5YtW6bvvvtOBQoU0PPPP2+xtDLsiwAMAACc1osvvqgXX3wxS21/+eWXLG13cXFRjx49mO3BgTEGGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMAAAABwKgRgAAAAOBUCMAAAyHEsVe24nPF3w0IYAAAgx7GMuGNy1mXECcAAAOCxYBlxOAqGQAAAAMCpEIABAADgVAjAAAAAcCrZGgN89uxZXbx4UdeuXVOePHlUsGBBlS9fXvnz57dVfQAAAIBNPXIAPnjwoFauXKnIyEhdvnw50zalS5dW8+bN1blzZ5UvXz7bRQIAAAC2kuUAvHfvXk2fPl0HDx6UJJkfMGfc6dOndebMGS1evFh169bVqFGjVL169exXCwAAAGRTlgLwlClTtHr1aqWmpkqSypYtq1q1aqlSpUoqWrSovL29JUk3btzQ5cuXdfz4cR09elSnTp3S77//rgEDBqhDhw6aOHFizr0TAAAAIAuyFIDDwsLk5+en5557Tq1bt1aZMmWydPKrV69q06ZNWrFihdatW0cABgAAgN1lKQBPmzZNLVq0kIvLo00aUbhwYfXu3Vu9e/dWZGSkVQUCAAAAtpSlANyyZctsXygwMDDb5wAAAACyK9tLIcfHx2vWrFnavn27rl69Kj8/P7Vv314DBgyQm5ubLWoEAAAAbCbbAfjdd99VRESE8TomJkZffvmlkpKS9Nprr2X39AAAAIBNZSsAJycna8uWLWrVqpX69++vggULKj4+XqtWrdKPP/5IAAYAAIDDydJTbVOmTNGVK1cybL99+7ZSU1NVvnx51ahRQwEBAapatapq1Kih27dv27xYAAAAILuyPA3aDz/8oF69eumll14yljr28fFRpUqV9NVXX2nx4sXKly+fEhMTlZCQoBYtWuRo4QAAAIA1stQD/M4776hw4cIKDQ1V165dNX/+fN26dcvYV7ZsWSUlJenSpUuKj49X7dq19frrr+do4QAAAIA1stQD3KFDB7Vt21YrVqzQvHnzNHPmTC1dulSDBw9W9+7dtXTpUp0/f15//vmn/Pz85Ofnl9N1AwAAAFbJ8soWefLkUa9evRQWFqaXX35Zd+7c0bRp09SjRw/9+OOP8vf3V82aNQm/AAAAcGiPtrSbJA8PDw0cOFCrVq1S//79dfnyZU2YMEF///vftWPHjpyoEQAAALCZLAfgq1evat26dQoNDdWPP/4ok8mkV199VWFhYerevbv++OMPjR49WkOHDtX+/ftzsmYAAADAalkaA/zLL79ozJgxSkpKMrb5+vpqzpw5Klu2rP7973+rf//+mjVrljZu3KjBgwerWbNmCgkJybHCAQAAAGtkqQd4+vTpypMnj55++mm1a9dOLVq0UJ48eTRz5kyjTUBAgKZMmaJFixapSZMm2r59e44VDQAAAFgrSz3A0dHRmj59uurWrWtsu3nzpgYPHpyhbeXKlfXZZ59p7969tqoRAAAAsJksBeDixYvrvffeU9OmTeXj46OkpCTt3btXJUqUuO8x6cMyAAAA4CiyFIAHDhyoiRMnasmSJTKZTDKbzXJzc7MYAgEAAADkBlkKwO3bt1e5cuW0ZcsWY7GLtm3bKiAgIKfrAwAAAGwqSwFYkqpUqaIqVarkZC0AAABAjsvSLBBjxozRnj17rL7I4cOHNX78eKuP/6sDBw5o2LBhatasmdq2bauJEyfqzz//NPbHxMRo9OjRCgoKUnBwsKZOnar4+HibXR8AAAC5V5Z6gLdt26Zt27YpICBAwcHBCgoKUrVq1eTiknl+TklJ0b59+7Rnzx5t27ZNJ06ckCRNnjw52wUfOXJEw4cPV+PGjfXRRx/p8uXL+s9//qOYmBjNmzdPN2/e1PDhw1W4cGFNmjRJ165d0/Tp0xUbG6sZM2Zk+/oAAADI3bIUgOfOnasPPvhAx48f14IFC7RgwQK5ubmpXLlyKlq0qLy9vWUymZSYmKgLFy7ozJkzun37tiTJbDaratWqGjNmjE0Knj59uqpUqaKPP/7YCODe3t76+OOPde7cOW3YsEFxcXFavHixChYsKEny8/PTa6+9pr179zI7BQAAgJPLUgCuU6eOFi1apPDwcIWGhurIkSO6c+eOoqKidOzYMYu2ZrNZkmQymdS4cWM9//zzCgoKkslkynax169f16+//qpJkyZZ9D63atVKrVq1kiTt2rVL9erVM8KvJAUGBsrb21s7duwgAAMAADi5LD8E5+LiojZt2qhNmzaKjY3Vzp07tW/fPl2+fNkYf1uoUCEFBASobt26atSokYoVK2bTYk+cOKHU1FT5+vpq/Pjx2rp1q8xms1q2bKnXX39d+fLlU3R0tNq0aWNxnKurq/z9/XX69OlsXd9sNisxMTFb53AEJpNJnp6e9i4DD5GUlGR8oIRj4N5xfNw3jol7x/E9KfeO2WzOUqdrlgNwev7+/urRo4d69OhhzeFWu3btmiTp3XffVdOmTfXRRx/pzJkz+vzzz3Xu3Dl9+eWXio+Pl7e3d4Zjvby8lJCQkK3rJycn68iRI9k6hyPw9PRU9erV7V0GHuKPP/5QUlKSvctAOtw7jo/7xjFx7zi+J+neyZs370PbWBWA7SU5OVmSVLVqVb399tuSpMaNGytfvnwaN26cdu/erdTU1Psef7+H9rLKzc1NFStWzNY5HIEthqMg55UrV+6J+DT+JOHecXzcN46Je8fxPSn3TtrECw+TqwKwl5eXJKl58+YW25s2bSpJOnr0qHx8fDIdppCQkCA/P79sXd9kMhk1ADmNPxcCj477BrDOk3LvZPXDVva6RB+z0qVLS5Lu3LljsT0lJUWS5OHhoTJlyigmJsZi/927dxUbG6uyZcs+ljoBAADguHJVAC5Xrpz8/f21YcMGi276LVu2SJLq1q2rwMBA/fbbb8Z4YUmKjIxUYmKiAgMDH3vNAAAAcCy5KgCbTCaNHDlSBw4c0NixY7V7924tWbJEISEhatWqlapWraoePXrI3d1dI0aMUEREhMLCwvT222+radOmqlOnjr3fAgAAAOzMqjHABw8eVM2aNW1dS5a0bt1a7u7umjt3rkaPHq38+fPr+eef18svvyxJ8vX11ezZsxUSEqLx48fL29tbwcHBGjVqlF3qBQAAgGOxKgAPGDBA5cqVU8eOHdWhQwcVLVrU1nU9UPPmzTM8CJdexYoVNXPmzMdYEQAAAHILq4dAREdH6/PPP1enTp30yiuv6McffzSWPwYAAAAclVU9wC+++KLCw8N19uxZmc1m7dmzR3v27JGXl5fatGmjjh07suQwAAAAHJJVAfiVV17RK6+8oqioKG3atEnh4eGKiYlRQkKCVq1apVWrVsnf31+dOnVSp06dVLx4cVvXDQAAAFglW7NAVKlSRSNGjNCKFSu0ePFide3aVWazWWazWbGxsfriiy/UrVs3ffjhhw9coQ0AAAB4XLK9EtzNmzcVHh6ujRs36tdff5XJZDJCsHRvEYpvv/1W+fPn17Bhw7JdMAAAAJAdVgXgxMREbd68WRs2bNCePXuMldjMZrNcXFz01FNPqUuXLjKZTJoxY4ZiY2O1fv16AjAAAADszqoA3KZNGyUnJ0uS0dPr7++vzp07Zxjz6+fnp0GDBunSpUs2KBcAAADIHqsC8J07dyRJefPmVatWrdS1a1c1bNgw07b+/v6SpHz58llZIgAAAGA7VgXgatWqqUuXLmrfvr18fHwe2NbT01Off/65SpYsaVWBAAAAgC1ZFYAXLlwo6d5Y4OTkZLm5uUmSTp8+rSJFisjb29to6+3trcaNG9ugVAAAACD7rJ4GbdWqVerUqZMOHDhgbFu0aJGeffZZrV692ibFAQAAALZmVQDesWOHJk+erPj4eJ04ccLYHh0draSkJE2ePFl79uyxWZEAAACArVgVgBcvXixJKlGihCpUqGBs79u3r0qVKiWz2azQ0FDbVAgAAADYkFVjgE+ePCmTyaQJEyaoQYMGxvagoCAVKFBAQ4cO1fHjx21WJAAAAGArVvUAx8fHS5J8fX0z7Eub7uzmzZvZKAsAAADIGVYF4GLFikmSVqxYYbHdbDZryZIlFm0AAAAAR2LVEIigoCCFhoZq2bJlioyMVKVKlZSSkqJjx47p/PnzMplMatGiha1rBQAAALLNqgA8cOBAbd68WTExMTpz5ozOnDlj7DObzSpVqpQGDRpksyIBAAAAW7FqCISPj4/mz5+vbt26ycfHR2azWWazWd7e3urWrZvmzZv30BXiAAAAAHuwqgdYkgoUKKBx48Zp7Nixun79usxms3x9fWUymWxZHwAAAGBTVq8El8ZkMsnX11eFChUywm9qaqp27tyZ7eIAAAAAW7OqB9hsNmvevHnaunWrbty4odTUVGNfSkqKrl+/rpSUFO3evdtmhQIAAAC2YFUAXrp0qWbPni2TySSz2WyxL20bQyEAAADgiKwaArFu3TpJkqenp0qVKiWTyaQaNWqoXLlyRvh98803bVooAAAAYAtWBeCzZ8/KZDLpgw8+0NSpU2U2mzVs2DAtW7ZMf//732U2mxUdHW3jUgEAAIDssyoA3759W5JUunRpVa5cWV5eXjp48KAkqXv37pKkHTt22KhEAAAAwHasCsCFChWSJEVFRclkMqlSpUpG4D179qwk6dKlSzYqEQAAALAdqwJwnTp1ZDab9fbbbysmJkb16tXT4cOH1atXL40dO1bS/0IyAAAA4EisCsCDBw9W/vz5lZycrKJFi6pdu3YymUyKjo5WUlKSTCaTWrdubetaAQAAgGyzKgCXK1dOoaGhGjJkiDw8PFSxYkVNnDhRxYoVU/78+dW1a1cNGzbM1rUCAAAA2WbVPMA7duxQ7dq1NXjwYGNbhw4d1KFDB5sVBgAAAOQEq3qAJ0yYoPbt22vr1q22rgcAAADIUVYF4Fu3bik5OVlly5a1cTkAAABAzrIqAAcHB0uSIiIibFoMAAAAkNOsGgNcuXJlbd++XZ9//rlWrFih8uXLy8fHR3ny/O90JpNJEyZMsFmhAAAAgC1YFYA/++wzmUwmSdL58+d1/vz5TNsRgAEAAOBorArAkmQ2mx+4Py0gAwAAAI7EqgC8evVqW9cBAAAAPBZWBeASJUrYug4AAADgsbAqAP/2229Zale/fn1rTg8AAADkGKsC8LBhwx46xtdkMmn37t1WFQUAAADklBx7CA4AAABwRFYF4CFDhli8NpvNunPnji5cuKCIiAhVrVpVAwcOtEmBAAAAgC1ZFYCHDh16332bNm3S2LFjdfPmTauLAgAAAHKKVUshP0irVq0kSd98842tTw0AAABkm80D8M8//yyz2ayTJ0/a+tQAAABAtlk1BGL48OEZtqWmpio+Pl6nTp2SJBUqVCh7lQEAAAA5wKoA/Ouvv953GrS02SE6depkfVUAAABADrHpNGhubm4qWrSo2rVrp8GDB2ersKx6/fXXdfToUa1Zs8bYFhMTo5CQEP3+++9ydXVV69at9eqrr8rHx+ex1AQAAADHZVUA/vnnn21dh1W+//57RUREWCzNfPPmTQ0fPlyFCxfWpEmTdO3aNU2fPl2xsbGaMWOGHasFAACAI7C6BzgzycnJcnNzs+Up7+vy5cv66KOPVKxYMYvty5cvV1xcnBYvXqyCBQtKkvz8/PTaa69p7969qlu37mOpDwAAAI7J6lkgoqKi9I9//ENHjx41tk2fPl2DBw/W8ePHbVLcg7z33nt66qmn1KhRI4vtu3btUr169YzwK0mBgYHy9vbWjh07crwuAAAAODarAvCpU6c0bNgw/fLLLxZhNzo6Wvv27dPQoUMVHR1tqxozCAsL09GjR/Xmm29m2BcdHa3SpUtbbHN1dZW/v79Onz6dYzUBAAAgd7BqCMS8efOUkJCgvHnzWswGUa1aNf32229KSEjQf//7X02aNMlWdRrOnz+vTz75RBMmTLDo5U0THx8vb2/vDNu9vLyUkJCQrWubzWYlJiZm6xyOwGQyydPT095l4CGSkpIyfdgU9sO94/i4bxwT947je1LuHbPZfN+ZytKzKgDv3btXJpNJ48eP17PPPmts/8c//qGKFStq3Lhx+v3336059QOZzWa9++67atq0qYKDgzNtk5qaet/jXVyyt+5HcnKyjhw5kq1zOAJPT09Vr17d3mXgIf744w8lJSXZuwykw73j+LhvHBP3juN7ku6dvHnzPrSNVQH4zz//lCTVrFkzw74qVapIkq5cuWLNqR9o2bJlOn78uJYsWaKUlBRJ/5uOLSUlRS4uLvLx8cm0lzYhIUF+fn7Zur6bm5sqVqyYrXM4gqx8MoL9lStX7on4NP4k4d5xfNw3jol7x/E9KffOiRMnstTOqgBcoEABXb16VT///LNKlSplsW/nzp2SpHz58llz6gcKDw/X9evX1b59+wz7AgMDNWTIEJUpU0YxMTEW++7evavY2Fi1bNkyW9c3mUzy8vLK1jmArOLPhcCj474BrPOk3DtZ/bBlVQBu2LCh1q9fr48//lhHjhxRlSpVlJKSosOHD2vjxo0ymUwZZmewhbFjx2bo3Z07d66OHDmikJAQFS1aVC4uLlq4cKGuXbsmX19fSVJkZKQSExMVGBho85oAAACQu1gVgAcPHqytW7cqKSlJq1atsthnNpvl6empQYMG2aTA9MqWLZthW4ECBeTm5maMLerRo4eWLl2qESNGaMiQIYqLi9P06dPVtGlT1alTx+Y1AQAAIHex6qmwMmXKaMaMGSpdurTMZrPFV+nSpTVjxoxMw+rj4Ovrq9mzZ6tgwYIaP368Zs6cqeDgYE2dOtUu9QAAAMCxWL0SXO3atbV8+XJFRUUpJiZGZrNZpUqVUpUqVR7rYPfMplqrWLGiZs6c+dhqAAAAQO6RraWQExMTVb58eWPmh9OnTysxMTHTeXgBAAAAR2D1xLirVq1Sp06ddODAAWPbokWL9Oyzz2r16tU2KQ4AAACwNasC8I4dOzR58mTFx8dbzLcWHR2tpKQkTZ48WXv27LFZkQAAAICtWBWAFy9eLEkqUaKEKlSoYGzv27evSpUqJbPZrNDQUNtUCAAAANiQVWOAT548KZPJpAkTJqhBgwbG9qCgIBUoUEBDhw7V8ePHbVYkAAAAYCtW9QDHx8dLkrHQRHppK8DdvHkzG2UBAAAAOcOqAFysWDFJ0ooVKyy2m81mLVmyxKINAAAA4EisGgIRFBSk0NBQLVu2TJGRkapUqZJSUlJ07NgxnT9/XiaTSS1atLB1rQAAAEC2WRWABw4cqM2bNysmJkZnzpzRmTNnjH1pC2LkxFLIAAAAQHZZNQTCx8dH8+fPV7du3eTj42Msg+zt7a1u3bpp3rx58vHxsXWtAAAAQLZZvRJcgQIFNG7cOI0dO1bXr1+X2WyWr6/vY10GGQAAAHhUVq8El8ZkMsnX11eFChWSyWRSUlKSVq5cqRdeeMEW9QEAAAA2ZXUP8F8dOXJEK1as0IYNG5SUlGSr0wIAAAA2la0AnJiYqB9++EFhYWGKiooytpvNZoZCAAAAwCFZFYAPHTqklStXauPGjUZvr9lsliS5urqqRYsWev75521XJQAAAGAjWQ7ACQkJ+uGHH7Ry5UpjmeO00JvGZDJp7dq1KlKkiG2rBAAAAGwkSwH43Xff1aZNm3Tr1i2L0Ovl5aVWrVqpePHi+vLLLyWJ8AsAAACHlqUAvGbNGplMJpnNZuXJk0eBgYF69tln1aJFC7m7u2vXrl05XScAAABgE480DZrJZJKfn59q1qyp6tWry93dPafqAgAAAHJElnqA69atq71790qSzp8/rzlz5mjOnDmqXr262rdvz6pvAAAAyDWyFIDnzp2rM2fOKCwsTN9//72uXr0qSTp8+LAOHz5s0fbu3btydXW1faUAAACADWR5CETp0qU1cuRIrVu3Th9++KGaNWtmjAtOP+9v+/bt9emnn+rkyZM5VjQAAABgrUeeB9jV1VVBQUEKCgrSlStXtHr1aq1Zs0Znz56VJMXFxenrr7/WN998o927d9u8YAAAACA7HukhuL8qUqSIBg4cqJUrV2rWrFlq37693NzcjF5hAAAAwNFkaynk9Bo2bKiGDRvqzTff1Pfff6/Vq1fb6tQAAACAzdgsAKfx8fFRr1691KtXL1ufGgAAAMi2bA2BAAAAAHIbAjAAAACcCgEYAAAAToUADAAAAKdCAAYAAIBTIQADAADAqRCAAQAA4FQIwAAAAHAqBGAAAAA4FQIwAAAAnAoBGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMAAAABwKgRgAAAAOBUCMAAAAJwKARgAAABOhQAMAAAAp0IABgAAgFMhAAMAAMCpEIABAADgVPLYu4BHlZqaqhUrVmj58uU6d+6cChUqpGeeeUbDhg2Tj4+PJCkmJkYhISH6/fff5erqqtatW+vVV1819gMAAMB55boAvHDhQs2aNUv9+/dXo0aNdObMGc2ePVsnT57U559/rvj4eA0fPlyFCxfWpEmTdO3aNU2fPl2xsbGaMWOGvcsHAACAneWqAJyamqoFCxboueee0yuvvCJJeuqpp1SgQAGNHTtWR44c0e7duxUXF6fFixerYMGCkiQ/Pz+99tpr2rt3r+rWrWu/NwAAAAC7y1VjgBMSEtShQwe1a9fOYnvZsmUlSWfPntWuXbtUr149I/xKUmBgoLy9vbVjx47HWC0AAAAcUa7qAc6XL59ef/31DNs3b94sSSpfvryio6PVpk0bi/2urq7y9/fX6dOnH0eZAAAAcGC5KgBn5uDBg1qwYIGaN2+uihUrKj4+Xt7e3hnaeXl5KSEhIVvXMpvNSkxMzNY5HIHJZJKnp6e9y8BDJCUlyWw227sMpMO94/i4bxwT947je1LuHbPZLJPJ9NB2uToA7927V6NHj5a/v78mTpwo6d444ftxccneiI/k5GQdOXIkW+dwBJ6enqpevbq9y8BD/PHHH0pKSrJ3GUiHe8fxcd84Ju4dx/ck3Tt58+Z9aJtcG4A3bNigd955R6VLl9aMGTOMMb8+Pj6Z9tImJCTIz88vW9d0c3NTxYoVs3UOR5CVT0awv3Llyj0Rn8afJNw7jo/7xjFx7zi+J+XeOXHiRJba5coAHBoaqunTp6tBgwb66KOPLOb3LVOmjGJiYiza3717V7GxsWrZsmW2rmsymeTl5ZWtcwBZxZ8LgUfHfQNY50m5d7L6YStXzQIhSd99950+++wztW7dWjNmzMiwuEVgYKB+++03Xbt2zdgWGRmpxMREBQYGPu5yAQAA4GByVQ/wlStXFBISIn9/f/Xu3VtHjx612B8QEKAePXpo6dKlGjFihIYMGaK4uDhNnz5dTZs2VZ06dexUOQAAABxFrgrAO3bs0O3btxUbG6vBgwdn2D9x4kR17txZs2fPVkhIiMaPHy9vb28FBwdr1KhRj79gAAAAOJxcFYC7du2qrl27PrRdxYoVNXPmzMdQEQAAAHKbXDcGGAAAAMgOAjAAAACcCgEYAAAAToUADAAAAKdCAAYAAIBTIQADAADAqRCAAQAA4FQIwAAAAHAqBGAAAAA4FQIwAAAAnAoBGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMAAAABwKgRgAAAAOBUCMAAAAJwKARgAAABOhQAMAAAAp0IABgAAgFMhAAMAAMCpEIABAADgVAjAAAAAcCoEYAAAADgVAjAAAACcCgEYAAAAToUADAAAAKdCAAYAAIBTIQADAADAqRCAAQAA4FQIwAAAAHAqBGAAAAA4FQIwAAAAnAoBGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMAAAABwKgRgAAAAOBUCMAAAAJwKARgAAABOhQAMAAAAp0IABgAAgFMhAAMAAMCpEIABAADgVAjAAAAAcCpPdACOjIzUCy+8oKefflpdunRRaGiozGazvcsCAACAHT2xAfjAgQMaNWqUypQpow8//FDt27fX9OnTtWDBAnuXBgAAADvKY+8CcsqcOXNUpUoVvffee5Kkpk2bKiUlRfPnz1efPn3k4eFh5woBAABgD09kD/CdO3f066+/qmXLlhbbg4ODlZCQoL1799qnMAAAANjdExmAz507p+TkZJUuXdpie6lSpSRJp0+ftkdZAAAAcABP5BCI+Ph4SZK3t7fFdi8vL0lSQkLCI50vKipKd+7ckSTt37/fBhXan8lkUuNCqbpbkKEgjsbVJVUHDhzggU0Hxb3jmLhvHB/3jmN60u6d5ORkmUymh7Z7IgNwamrqA/e7uDx6x3faDzMrP9Tcwtvdzd4l4AGepH9rTxruHcfFfePYuHcc15Ny75hMJucNwD4+PpKkxMREi+1pPb9p+7OqSpUqtikMAAAAdvdEjgEOCAiQq6urYmJiLLanvS5btqwdqgIAAIAjeCIDsLu7u+rVq6eIiAiLMS0//fSTfHx8VLNmTTtWBwAAAHt6IgOwJA0aNEgHDx7UW2+9pR07dmjWrFkKDQ3VgAEDmAMYAADAiZnMT8pjf5mIiIjQnDlzdPr0afn5+alnz57q16+fvcsCAACAHT3RARgAAAD4qyd2CAQAAACQGQIwAAAAnAoBGAAAAE6FAAwAAACnQgAGAACAUyEAAwAAwKkQgAEAAOBUCMDIlSZNmqSGDRve92vTpk32LhFwKEOHDlXDhg01cODA+7b597//rYYNG2rSpEmPrzDAwV25ckXBwcHq06eP7ty5k2H/kiVL1KhRI23fvt0O1cFaeexdAGCtwoUL66OPPsp0X+nSpR9zNYDjc3Fx0YEDB3Tx4kUVK1bMYl9SUpK2bdtmp8oAx1WkSBGNGzdOb7zxhmbOnKlRo0YZ+w4fPqzPPvtMffv2VbNmzexXJB4ZARi5Vt68eVWrVi17lwHkGlWrVtXJkye1adMm9e3b12Lf1q1b5enpqfz589upOsBxtWrVSp07d9bixYvVrFkzNWzYUDdv3tS///1vVapUSa+88oq9S8QjYggEADgJDw8PNWvWTOHh4Rn2bdy4UcHBwXJ1dbVDZYDje/311+Xv76+JEycqPj5eU6ZMUVxcnKZOnao8eehPzG0IwMjVUlJSMnyZzWZ7lwU4rDZt2hjDINLEx8dr586dateunR0rAxybl5eX3nvvPV25ckXDhg3Tpk2bNH78eJUsWdLepcEKBGDkWufPn1dgYGCGrwULFti7NMBhNWvWTJ6enhYPim7evFm+vr6qW7eu/QoDcoHatWurT58+ioqKUlBQkFq3bm3vkmAl+uyRaxUpUkQhISEZtvv5+dmhGiB38PDwUPPmzRUeHm6MA96wYYPatm0rk8lk5+oAx3br1i3t2LFDJpNJP//8s86ePauAgAB7lwUr0AOMXMvNzU3Vq1fP8FWkSBF7lwY4tPTDIK5fv67du3erbdu29i4LcHgffPCBzp49qw8//FB3797VhAkTdPfuXXuXBSsQgAHAyTRt2lReXl4KDw9XRESESpYsqWrVqtm7LMChrV+/XmvWrNHLL7+soKAgjRo1Svv379eXX35p79JgBYZAAICTyZs3r4KCghQeHi53d3cefgMe4uzZs5o6daoaNWqk/v37S5J69Oihbdu2ad68eWrSpIlq165t5yrxKOgBBgAn1KZNG+3fv1+//vorARh4gOTkZI0dO1Z58uTRO++8IxeX/0Wnt99+W/ny5dPbb7+thIQEO1aJR0UABgAnFBgYqHz58qlChQoqW7asvcsBHNaMGTN0+PBhjR07NsND1mmrxJ07d07Tpk2zU4WwhsnMpKkAAABwIvQAAwAAwKkQgAEAAOBUCMAAAABwKgRgAAAAOBUCMAAAAJwKARgAAABOhQAMAAAAp8JSyADgALZv3661a9fq0KFD+vPPPyVJxYoVU926ddW7d29VqVLFrvVdvHhRHTt2lCR16tRJkyZNsms9AJAdBGAAsKPExERNnjxZGzZsyLDvzJkzOnPmjNauXas33nhDPXr0sEOFAPDkIQADgB29++672rRpkySpdu3aeuGFF1ShQgXduHFDa9eu1bfffqvU1FRNmzZNVatWVc2aNe1cMQDkfgRgALCTiIgII/w2bdpUISEhypPnf/9ZrlGjhjw9PbVw4UKlpqbq66+/1v/93//Zq1wAeGIQgAHATlasWGF8P2bMGIvwm+aFF15Qvnz5VK1aNVWvXt3YfunSJc2ZM0c7duxQXFycihYtqpYtW2rw4MHKly+f0W7SpElau3atChQooFWrVmnmzJkKDw/XzZs3VbFiRQ0fPlxNmza1uObBgwc1a9Ys7d+/X3ny5FFQUJD69Olz3/dx8OBBzZ07V/v27VNycrLKlCmjLl26qFevXnJx+d+z1g0bNpQk9e3bV5K0cuVKmUwmjRw5Us8///wj/vQAwHoms9lstncRAOCMmjVrplu3bsnf31+rV6/O8nHnzp3TwIEDdfXq1Qz7ypUrp/nz58vHx0fS/wKwt7e3SpYsqWPHjlm0d3V11bJly1SmTBlJ0m+//aYRI0YoOTnZol3RokV1+fJlSZYPwW3ZskVvvvmmUlJSMtTSvn17TZ482XidFoDz5cunmzdvGtuXLFmiihUrZvn9A0B2MQ0aANjB9evXdevWLUlSkSJFLPbdvXtXFy9ezPRLkqZNm6arV6/K3d1dkyZN0ooVKzR58mR5eHjojz/+0OzZszNcLyEhQTdv3tT06dO1fPlyPfXUU8a1vv/+e6PdRx99ZITfF154QcuWLdO0adMyDbi3bt3S5MmTlZKSooCAAP3nP//R8uXLNXjwYEnS+vXrFRERkeG4mzdvqlevXvruu+/0/vvvE34BPHYMgQAAO0g/NODu3bsW+2JjY9W9e/dMj/vpp5+0a9cuSdIzzzyjRo0aSZLq1aunVq1a6fvvv9f333+vMWPGyGQyWRw7atQoY7jDiBEjtHv3bkkyepIvX75s9BDXrVtXI0eOlCSVL19ecXFxmjJlisX5IiMjde3aNUlS7969Va5cOUlS9+7d9eOPPyomJkZr165Vy5YtLY5zd3fXyJEj5eHhYfQ8A8DjRAAGADvInz+/PD09lZSUpPPnz2f5uJiYGKWmpkqSNm7cqI0bN2Zoc+PGDZ07d04BAQEW28uXL2987+vra3yf1rt74cIFY9tfZ5uoVatWhuucOXPG+P7jjz/Wxx9/nKHN0aNHM2wrWbKkPDw8MmwHgMeFIRAAYCeNGzeWJP355586dOiQsb1UqVL65ZdfjK8SJUoY+1xdXbN07rSe2fTc3d2N79P3QKdJ32OcFrIf1D4rtWRWR9r4ZACwF3qAAcBOunbtqi1btkiSQkJCNHPmTIuQKknJycm6c+eO8Tp9r2737t01btw44/XJkyfl7e2t4sWLW1VPyZIlje/TB3JJ2rdvX4b2pUqVMr6fPHmy2rdvb7w+ePCgSpUqpQIFCmQ4LrPZLgDgcaIHGADs5JlnnlHbtm0l3QuYgwYN0k8//aSzZ8/q2LFjWrJkiXr16mUx24OPj4+aN28uSVq7dq2+++47nTlzRtu2bdPAgQPVqVMn9e/fX9ZM8OPr66v69esb9XzyySc6ceKENm3apM8//zxD+8aNG6tw4cKSpJkzZ2rbtm06e/asFi1apJdeeknBwcH65JNPHrkOAMhpfAwHADuaMGGC3N3dtWbNGh09elRvvPFGpu18fHw0bNgwSdLIkSO1f/9+xcXFaerUqRbt3N3d9eqrr2Z4AC6rXn/9dQ0ePFgJCQlavHixFi9eLEkqXbq07ty5o8TERKOth4eHRo8erQkTJig2NlajR4+2OJe/v7/69etnVR0AkJMIwABgRx4eHpo4caK6du2qNWvWaN++fbp8+bJSUlJUuHBhVatWTU2aNFG7du3k6ekp6d5cvwsXLtSXX36pPXv26OrVqypYsKBq166tgQMHqmrVqlbXU6lSJc2bN08zZszQr7/+qrx58+qZZ57RK6+8ol69emVo3759exUtWlShoaE6cOCAEhMT5efnp2bNmmnAgAEZpngDAEfAQhgAAABwKowBBgAAgFMhAAMAAMCpEIABAADgVAjAAAAAcCoEYAAAADgVAjAAAACcCgEYAAAAToUADAAAAKdCAAYAAIBTIQADAADAqRCAAQAA4FQIwAAAAHAqBGAAAAA4lf8Hf5HokEqNFjoAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40270db-853d-46a1-acb4-8dab8a4f9c81",
   "metadata": {},
   "source": [
    "# RANDOM SEED 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "588e6cfb-6ee5-4b51-a9ee-79dcbfbb208c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adult     588\n",
      "senior    178\n",
      "kitten    171\n",
      "Name: age_group, dtype: int64\n",
      "senior    712\n",
      "kitten    684\n",
      "adult     588\n",
      "Name: age_group, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Set a fixed random seed for reproducibility\n",
    "random.seed(int(random_seeds[4]))\n",
    "np.random.seed(int(random_seeds[4]))\n",
    "tf.random.set_seed(int(random_seeds[4]))\n",
    "\n",
    "# Load datasets\n",
    "dataframe = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/8april_looped_embeddings.csv')\n",
    "augmented_df = pd.read_csv('/Users/astrid/PycharmProjects/audioset-thesis-work/audioset/vggish/embeddings/augmented_dataset_1.csv')\n",
    "\n",
    "dataframe.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('mean_freq', axis=1, inplace=True)\n",
    "augmented_df.drop('gender', axis=1, inplace=True)\n",
    "\n",
    "def assign_age_group(age, age_groups):\n",
    "    for group_name, age_range in age_groups.items():\n",
    "        if age_range[0] <= age < age_range[1]:\n",
    "            return group_name\n",
    "    return 'Unknown'  # For any age that doesn't fit the defined groups\n",
    "\n",
    "# Define age groups\n",
    "age_groups = {\n",
    "    'kitten': (0, 0.5),\n",
    "    'adult': (0.5, 12),\n",
    "    'senior': (12, 20)\n",
    "}\n",
    "\n",
    "# Create a new column for the age group\n",
    "dataframe['age_group'] = dataframe['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "augmented_df['age_group'] = augmented_df['target'].apply(assign_age_group, age_groups=age_groups)\n",
    "\n",
    "print(dataframe['age_group'].value_counts())\n",
    "print(augmented_df['age_group'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "432a9515-e7f1-482c-bdaf-73cb07a9132e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = dataframe.iloc[:, :-4].values  # all columns except the last four\n",
    "\n",
    "# Encode the 'age_group' column as integers using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_y = label_encoder.fit_transform(dataframe['age_group'].values)\n",
    "\n",
    "# Use the encoded labels for splitting and one-hot encoding\n",
    "y = encoded_y  \n",
    "\n",
    "# Convert 'cat_id' column to numpy array to be used as groups array for GroupKFold\n",
    "groups = dataframe['cat_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8231e441-4472-4d9b-8ddf-dc285c07923b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b9ed6b-6d2f-491e-95d8-73d33edeef6a",
   "metadata": {},
   "source": [
    "## Run Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6cd63258-8a3b-4bc0-9ac6-ae8aa4b515b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outer_fold 1\n",
      "Train Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "067A    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "106A    14\n",
      "097B    14\n",
      "028A    13\n",
      "111A    13\n",
      "051A    12\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "036A    11\n",
      "063A    11\n",
      "068A    11\n",
      "014B    10\n",
      "016A    10\n",
      "071A    10\n",
      "005A    10\n",
      "072A     9\n",
      "051B     9\n",
      "033A     9\n",
      "045A     9\n",
      "015A     9\n",
      "013B     8\n",
      "094A     8\n",
      "095A     8\n",
      "010A     8\n",
      "050A     7\n",
      "027A     7\n",
      "031A     7\n",
      "099A     7\n",
      "117A     7\n",
      "053A     6\n",
      "008A     6\n",
      "007A     6\n",
      "037A     6\n",
      "023A     6\n",
      "025C     5\n",
      "075A     5\n",
      "021A     5\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "062A     4\n",
      "052A     4\n",
      "003A     4\n",
      "104A     4\n",
      "105A     4\n",
      "009A     4\n",
      "035A     4\n",
      "056A     3\n",
      "014A     3\n",
      "058A     3\n",
      "060A     3\n",
      "025B     2\n",
      "102A     2\n",
      "061A     2\n",
      "069A     2\n",
      "032A     2\n",
      "093A     2\n",
      "038A     2\n",
      "087A     2\n",
      "073A     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "091A     1\n",
      "041A     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "043A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "000B    19\n",
      "001A    14\n",
      "002A    13\n",
      "040A    10\n",
      "022A     9\n",
      "065A     9\n",
      "109A     6\n",
      "108A     6\n",
      "044A     5\n",
      "026A     4\n",
      "113A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "011A     2\n",
      "054A     2\n",
      "018A     2\n",
      "092A     1\n",
      "049A     1\n",
      "004A     1\n",
      "019B     1\n",
      "115A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    280\n",
      "X    256\n",
      "F    186\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    92\n",
      "F    66\n",
      "M    57\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 015A, 071A, 097B, 028A, 019A, 074...\n",
      "kitten    [014B, 111A, 047A, 042A, 050A, 043A, 041A, 045...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 116...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [006A, 001A, 103A, 022A, 065A, 002A, 000B, 026...\n",
      "kitten                 [044A, 040A, 046A, 109A, 049A, 115A]\n",
      "senior                             [113A, 054A, 108A, 011A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 59, 'kitten': 10, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 15, 'kitten': 6, 'senior': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '041A' '042A' '043A' '045A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '044A' '046A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "{'046A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'041A'}\n",
      "Moved to Test Set:\n",
      "{'041A'}\n",
      "Removed from Test Set\n",
      "{'046A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '002B' '003A' '005A' '007A' '008A' '009A' '010A' '013B' '014A'\n",
      " '014B' '015A' '016A' '019A' '020A' '021A' '023A' '023B' '024A' '025A'\n",
      " '025B' '025C' '026C' '027A' '028A' '029A' '031A' '032A' '033A' '034A'\n",
      " '035A' '036A' '037A' '038A' '039A' '042A' '043A' '045A' '046A' '047A'\n",
      " '048A' '050A' '051A' '051B' '052A' '053A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '061A' '062A' '063A' '066A' '067A' '068A' '069A' '070A'\n",
      " '071A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '090A' '091A'\n",
      " '093A' '094A' '095A' '096A' '097A' '097B' '099A' '100A' '101A' '102A'\n",
      " '104A' '105A' '106A' '110A' '111A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['000B' '001A' '002A' '004A' '006A' '011A' '012A' '018A' '019B' '022A'\n",
      " '026A' '026B' '040A' '041A' '044A' '049A' '054A' '064A' '065A' '092A'\n",
      " '103A' '108A' '109A' '113A' '115A']\n",
      "Length of X_train_val:\n",
      "784\n",
      "Length of y_train_val:\n",
      "784\n",
      "Length of groups_train_val:\n",
      "784\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten     85\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     116\n",
      "kitten     86\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     472\n",
      "senior    165\n",
      "kitten    147\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     116\n",
      "kitten     24\n",
      "senior     13\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 944, 2: 825, 1: 735})\n",
      "Epoch 1/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.8913 - accuracy: 0.6386\n",
      "Epoch 2/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.7023 - accuracy: 0.7161\n",
      "Epoch 3/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.6253 - accuracy: 0.7536\n",
      "Epoch 4/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5800 - accuracy: 0.7632\n",
      "Epoch 5/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5869 - accuracy: 0.7612\n",
      "Epoch 6/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5416 - accuracy: 0.7808\n",
      "Epoch 7/1500\n",
      "79/79 [==============================] - 0s 4ms/step - loss: 0.5292 - accuracy: 0.7808\n",
      "Epoch 8/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.5017 - accuracy: 0.7995\n",
      "Epoch 9/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4909 - accuracy: 0.7999\n",
      "Epoch 10/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4398 - accuracy: 0.8179\n",
      "Epoch 11/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4570 - accuracy: 0.8195\n",
      "Epoch 12/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4548 - accuracy: 0.8219\n",
      "Epoch 13/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4162 - accuracy: 0.8335\n",
      "Epoch 14/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4006 - accuracy: 0.8431\n",
      "Epoch 15/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.4166 - accuracy: 0.8351\n",
      "Epoch 16/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3788 - accuracy: 0.8494\n",
      "Epoch 17/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3983 - accuracy: 0.8462\n",
      "Epoch 18/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3894 - accuracy: 0.8478\n",
      "Epoch 19/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3772 - accuracy: 0.8502\n",
      "Epoch 20/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3693 - accuracy: 0.8558\n",
      "Epoch 21/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3654 - accuracy: 0.8586\n",
      "Epoch 22/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3458 - accuracy: 0.8634\n",
      "Epoch 23/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3533 - accuracy: 0.8582\n",
      "Epoch 24/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8654\n",
      "Epoch 25/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3529 - accuracy: 0.8610\n",
      "Epoch 26/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3313 - accuracy: 0.8682\n",
      "Epoch 27/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3220 - accuracy: 0.8794\n",
      "Epoch 28/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3169 - accuracy: 0.8698\n",
      "Epoch 29/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3294 - accuracy: 0.8678\n",
      "Epoch 30/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3167 - accuracy: 0.8762\n",
      "Epoch 31/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3298 - accuracy: 0.8674\n",
      "Epoch 32/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3098 - accuracy: 0.8790\n",
      "Epoch 33/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3150 - accuracy: 0.8790\n",
      "Epoch 34/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3150 - accuracy: 0.8790\n",
      "Epoch 35/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.3156 - accuracy: 0.8834\n",
      "Epoch 36/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2996 - accuracy: 0.8834\n",
      "Epoch 37/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2842 - accuracy: 0.8906\n",
      "Epoch 38/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2938 - accuracy: 0.8822\n",
      "Epoch 39/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2913 - accuracy: 0.8898\n",
      "Epoch 40/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.8790\n",
      "Epoch 41/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2819 - accuracy: 0.8910\n",
      "Epoch 42/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2892 - accuracy: 0.8882\n",
      "Epoch 43/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2808 - accuracy: 0.8890\n",
      "Epoch 44/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2888 - accuracy: 0.8862\n",
      "Epoch 45/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2781 - accuracy: 0.8830\n",
      "Epoch 46/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2702 - accuracy: 0.8962\n",
      "Epoch 47/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2752 - accuracy: 0.8978\n",
      "Epoch 48/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2670 - accuracy: 0.9026\n",
      "Epoch 49/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.8958\n",
      "Epoch 50/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2660 - accuracy: 0.8954\n",
      "Epoch 51/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2530 - accuracy: 0.9046\n",
      "Epoch 52/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2547 - accuracy: 0.9006\n",
      "Epoch 53/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2640 - accuracy: 0.9018\n",
      "Epoch 54/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2480 - accuracy: 0.9105\n",
      "Epoch 55/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2390 - accuracy: 0.9054\n",
      "Epoch 56/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2527 - accuracy: 0.9038\n",
      "Epoch 57/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9121\n",
      "Epoch 58/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2370 - accuracy: 0.9026\n",
      "Epoch 59/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2469 - accuracy: 0.9018\n",
      "Epoch 60/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2297 - accuracy: 0.9129\n",
      "Epoch 61/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2334 - accuracy: 0.9062\n",
      "Epoch 62/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2212 - accuracy: 0.9141\n",
      "Epoch 63/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2293 - accuracy: 0.9085\n",
      "Epoch 64/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2227 - accuracy: 0.9185\n",
      "Epoch 65/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2180 - accuracy: 0.9081\n",
      "Epoch 66/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2156 - accuracy: 0.9193\n",
      "Epoch 67/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2266 - accuracy: 0.9101\n",
      "Epoch 68/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2249 - accuracy: 0.9145\n",
      "Epoch 69/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2185 - accuracy: 0.9193\n",
      "Epoch 70/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2197 - accuracy: 0.9189\n",
      "Epoch 71/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2264 - accuracy: 0.9153\n",
      "Epoch 72/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1985 - accuracy: 0.9265\n",
      "Epoch 73/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2024 - accuracy: 0.9309\n",
      "Epoch 74/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2123 - accuracy: 0.9209\n",
      "Epoch 75/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2205 - accuracy: 0.9145\n",
      "Epoch 76/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.9173\n",
      "Epoch 77/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1907 - accuracy: 0.9301\n",
      "Epoch 78/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2116 - accuracy: 0.9201\n",
      "Epoch 79/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2232 - accuracy: 0.9141\n",
      "Epoch 80/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2075 - accuracy: 0.9229\n",
      "Epoch 81/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2133 - accuracy: 0.9233\n",
      "Epoch 82/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1969 - accuracy: 0.9305\n",
      "Epoch 83/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1952 - accuracy: 0.9269\n",
      "Epoch 84/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.2127 - accuracy: 0.9161\n",
      "Epoch 85/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1959 - accuracy: 0.9221\n",
      "Epoch 86/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1910 - accuracy: 0.9325\n",
      "Epoch 87/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1974 - accuracy: 0.9257\n",
      "Epoch 88/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1950 - accuracy: 0.9225\n",
      "Epoch 89/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1897 - accuracy: 0.9301\n",
      "Epoch 90/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1748 - accuracy: 0.9425\n",
      "Epoch 91/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1913 - accuracy: 0.9305\n",
      "Epoch 92/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1826 - accuracy: 0.9297\n",
      "Epoch 93/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1932 - accuracy: 0.9237\n",
      "Epoch 94/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1854 - accuracy: 0.9265\n",
      "Epoch 95/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1853 - accuracy: 0.9309\n",
      "Epoch 96/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1935 - accuracy: 0.9237\n",
      "Epoch 97/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1953 - accuracy: 0.9237\n",
      "Epoch 98/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1648 - accuracy: 0.9421\n",
      "Epoch 99/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1775 - accuracy: 0.9329\n",
      "Epoch 100/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1814 - accuracy: 0.9337\n",
      "Epoch 101/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1878 - accuracy: 0.9245\n",
      "Epoch 102/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1661 - accuracy: 0.9381\n",
      "Epoch 103/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1835 - accuracy: 0.9329\n",
      "Epoch 104/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1884 - accuracy: 0.9281\n",
      "Epoch 105/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1810 - accuracy: 0.9329\n",
      "Epoch 106/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1765 - accuracy: 0.9377\n",
      "Epoch 107/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1989 - accuracy: 0.9221\n",
      "Epoch 108/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1836 - accuracy: 0.9309\n",
      "Epoch 109/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1696 - accuracy: 0.9361\n",
      "Epoch 110/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1717 - accuracy: 0.9349\n",
      "Epoch 111/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1730 - accuracy: 0.9333\n",
      "Epoch 112/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1758 - accuracy: 0.9409\n",
      "Epoch 113/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1609 - accuracy: 0.9405\n",
      "Epoch 114/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1769 - accuracy: 0.9341\n",
      "Epoch 115/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1693 - accuracy: 0.9345\n",
      "Epoch 116/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1785 - accuracy: 0.9377\n",
      "Epoch 117/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9361\n",
      "Epoch 118/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1601 - accuracy: 0.9405\n",
      "Epoch 119/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1582 - accuracy: 0.9421\n",
      "Epoch 120/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1641 - accuracy: 0.9389\n",
      "Epoch 121/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9445\n",
      "Epoch 122/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9357\n",
      "Epoch 123/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1713 - accuracy: 0.9373\n",
      "Epoch 124/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1714 - accuracy: 0.9369\n",
      "Epoch 125/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1883 - accuracy: 0.9313\n",
      "Epoch 126/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1725 - accuracy: 0.9373\n",
      "Epoch 127/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1548 - accuracy: 0.9437\n",
      "Epoch 128/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1535 - accuracy: 0.9473\n",
      "Epoch 129/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9445\n",
      "Epoch 130/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1564 - accuracy: 0.9477\n",
      "Epoch 131/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1575 - accuracy: 0.9425\n",
      "Epoch 132/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1600 - accuracy: 0.9421\n",
      "Epoch 133/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1517 - accuracy: 0.9449\n",
      "Epoch 134/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9493\n",
      "Epoch 135/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1541 - accuracy: 0.9521\n",
      "Epoch 136/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1526 - accuracy: 0.9393\n",
      "Epoch 137/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1610 - accuracy: 0.9421\n",
      "Epoch 138/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1576 - accuracy: 0.9481\n",
      "Epoch 139/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1554 - accuracy: 0.9417\n",
      "Epoch 140/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9469\n",
      "Epoch 141/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9505\n",
      "Epoch 142/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1528 - accuracy: 0.9453\n",
      "Epoch 143/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1447 - accuracy: 0.9445\n",
      "Epoch 144/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1501 - accuracy: 0.9461\n",
      "Epoch 145/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.9349\n",
      "Epoch 146/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1514 - accuracy: 0.9505\n",
      "Epoch 147/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9441\n",
      "Epoch 148/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1428 - accuracy: 0.9501\n",
      "Epoch 149/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1459 - accuracy: 0.9457\n",
      "Epoch 150/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.9517\n",
      "Epoch 151/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1399 - accuracy: 0.9469\n",
      "Epoch 152/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9357\n",
      "Epoch 153/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1474 - accuracy: 0.9425\n",
      "Epoch 154/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9461\n",
      "Epoch 155/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9433\n",
      "Epoch 156/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9509\n",
      "Epoch 157/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1522 - accuracy: 0.9429\n",
      "Epoch 158/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1446 - accuracy: 0.9477\n",
      "Epoch 159/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1221 - accuracy: 0.9545\n",
      "Epoch 160/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1463 - accuracy: 0.9517\n",
      "Epoch 161/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1380 - accuracy: 0.9513\n",
      "Epoch 162/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9517\n",
      "Epoch 163/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1393 - accuracy: 0.9493\n",
      "Epoch 164/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1237 - accuracy: 0.9565\n",
      "Epoch 165/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9493\n",
      "Epoch 166/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9473\n",
      "Epoch 167/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1568 - accuracy: 0.9441\n",
      "Epoch 168/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1307 - accuracy: 0.9561\n",
      "Epoch 169/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1126 - accuracy: 0.9621\n",
      "Epoch 170/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9561\n",
      "Epoch 171/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9453\n",
      "Epoch 172/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1340 - accuracy: 0.9517\n",
      "Epoch 173/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9521\n",
      "Epoch 174/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9537\n",
      "Epoch 175/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9593\n",
      "Epoch 176/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1112 - accuracy: 0.9637\n",
      "Epoch 177/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9617\n",
      "Epoch 178/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.9481\n",
      "Epoch 179/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9529\n",
      "Epoch 180/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1358 - accuracy: 0.9525\n",
      "Epoch 181/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9545\n",
      "Epoch 182/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1191 - accuracy: 0.9561\n",
      "Epoch 183/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1257 - accuracy: 0.9553\n",
      "Epoch 184/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9553\n",
      "Epoch 185/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1485 - accuracy: 0.9433\n",
      "Epoch 186/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1328 - accuracy: 0.9513\n",
      "Epoch 187/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1289 - accuracy: 0.9573\n",
      "Epoch 188/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.9489\n",
      "Epoch 189/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1156 - accuracy: 0.9609\n",
      "Epoch 190/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9613\n",
      "Epoch 191/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1305 - accuracy: 0.9529\n",
      "Epoch 192/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1426 - accuracy: 0.9517\n",
      "Epoch 193/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9497\n",
      "Epoch 194/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1117 - accuracy: 0.9609\n",
      "Epoch 195/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9569\n",
      "Epoch 196/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1260 - accuracy: 0.9517\n",
      "Epoch 197/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9485\n",
      "Epoch 198/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9573\n",
      "Epoch 199/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1383 - accuracy: 0.9453\n",
      "Epoch 200/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1147 - accuracy: 0.9613\n",
      "Epoch 201/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9585\n",
      "Epoch 202/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1089 - accuracy: 0.9613\n",
      "Epoch 203/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9565\n",
      "Epoch 204/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1413 - accuracy: 0.9437\n",
      "Epoch 205/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1227 - accuracy: 0.9585\n",
      "Epoch 206/1500\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1173 - accuracy: 0.9597\n",
      "Epoch 207/1500\n",
      "47/79 [================>.............] - ETA: 0s - loss: 0.1188 - accuracy: 0.9548Restoring model weights from the end of the best epoch: 177.\n",
      "79/79 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9541\n",
      "Epoch 207: early stopping\n",
      "5/5 [==============================] - 0s 970us/step - loss: 0.7694 - accuracy: 0.7582\n",
      "5/5 [==============================] - 0s 720us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.80 (20/25)\n",
      "Before appending - Cat IDs: 0, Predictions: 0, Actuals: 0, Gender: 0\n",
      "After appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "Final Test Results - Loss: 0.769411563873291, Accuracy: 0.758169949054718, Precision: 0.6881888901524245, Recall: 0.7813144709696433, F1 Score: 0.6913434456547559\n",
      "Confusion Matrix:\n",
      " [[86  3 27]\n",
      " [ 4 20  0]\n",
      " [ 3  0 10]]\n",
      "outer_fold 2\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "103A    33\n",
      "057A    27\n",
      "074A    25\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "029A    17\n",
      "019A    17\n",
      "097A    16\n",
      "101A    15\n",
      "059A    14\n",
      "001A    14\n",
      "097B    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "111A    13\n",
      "051A    12\n",
      "025A    11\n",
      "036A    11\n",
      "005A    10\n",
      "040A    10\n",
      "071A    10\n",
      "014B    10\n",
      "022A     9\n",
      "015A     9\n",
      "065A     9\n",
      "045A     9\n",
      "072A     9\n",
      "095A     8\n",
      "094A     8\n",
      "031A     7\n",
      "027A     7\n",
      "008A     6\n",
      "108A     6\n",
      "109A     6\n",
      "053A     6\n",
      "023A     6\n",
      "037A     6\n",
      "023B     5\n",
      "070A     5\n",
      "044A     5\n",
      "021A     5\n",
      "034A     5\n",
      "052A     4\n",
      "003A     4\n",
      "105A     4\n",
      "009A     4\n",
      "026A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "056A     3\n",
      "012A     3\n",
      "113A     3\n",
      "014A     3\n",
      "060A     3\n",
      "011A     2\n",
      "102A     2\n",
      "032A     2\n",
      "069A     2\n",
      "018A     2\n",
      "093A     2\n",
      "054A     2\n",
      "038A     2\n",
      "019B     1\n",
      "090A     1\n",
      "100A     1\n",
      "110A     1\n",
      "115A     1\n",
      "092A     1\n",
      "004A     1\n",
      "041A     1\n",
      "076A     1\n",
      "096A     1\n",
      "026C     1\n",
      "073A     1\n",
      "049A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "000A    39\n",
      "002B    32\n",
      "047A    28\n",
      "067A    19\n",
      "042A    14\n",
      "116A    12\n",
      "039A    12\n",
      "068A    11\n",
      "063A    11\n",
      "016A    10\n",
      "033A     9\n",
      "051B     9\n",
      "010A     8\n",
      "013B     8\n",
      "099A     7\n",
      "117A     7\n",
      "050A     7\n",
      "007A     6\n",
      "075A     5\n",
      "025C     5\n",
      "087A     2\n",
      "061A     2\n",
      "025B     2\n",
      "043A     1\n",
      "066A     1\n",
      "048A     1\n",
      "088A     1\n",
      "091A     1\n",
      "024A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    263\n",
      "M    226\n",
      "F    177\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    111\n",
      "X     85\n",
      "F     75\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 015A, 001A, 103A, 071A, 097B, 028A, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 109A, 049A, 041...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 055A, 059A, 113...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [000A, 033A, 067A, 002B, 091A, 039A, 063A, 013...\n",
      "kitten                       [047A, 042A, 050A, 043A, 048A]\n",
      "senior                 [116A, 051B, 117A, 016A, 061A, 024A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 56, 'kitten': 11, 'senior': 16}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 18, 'kitten': 5, 'senior': 6}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A' '011A'\n",
      " '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A' '022A'\n",
      " '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A' '031A'\n",
      " '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A' '045A'\n",
      " '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A' '058A'\n",
      " '059A' '060A' '062A' '064A' '065A' '069A' '070A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "Unique Test Group IDs:\n",
      "['000A' '002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A'\n",
      " '039A' '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A'\n",
      " '067A' '068A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "No common groups found between train and test sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moved to Training/Validation Set:\n",
      "{'000A'}\n",
      "Removed from Training/Validation Set:\n",
      "{'070A'}\n",
      "Moved to Test Set:\n",
      "{'070A'}\n",
      "Removed from Test Set\n",
      "{'000A'}\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '003A' '004A' '005A' '006A' '008A' '009A'\n",
      " '011A' '012A' '014A' '014B' '015A' '018A' '019A' '019B' '020A' '021A'\n",
      " '022A' '023A' '023B' '025A' '026A' '026B' '026C' '027A' '028A' '029A'\n",
      " '031A' '032A' '034A' '035A' '036A' '037A' '038A' '040A' '041A' '044A'\n",
      " '045A' '046A' '049A' '051A' '052A' '053A' '054A' '055A' '056A' '057A'\n",
      " '058A' '059A' '060A' '062A' '064A' '065A' '069A' '071A' '072A' '073A'\n",
      " '074A' '076A' '090A' '092A' '093A' '094A' '095A' '096A' '097A' '097B'\n",
      " '100A' '101A' '102A' '103A' '104A' '105A' '106A' '108A' '109A' '110A'\n",
      " '111A' '113A' '115A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['002B' '007A' '010A' '013B' '016A' '024A' '025B' '025C' '033A' '039A'\n",
      " '042A' '043A' '047A' '048A' '050A' '051B' '061A' '063A' '066A' '067A'\n",
      " '068A' '070A' '075A' '087A' '088A' '091A' '099A' '116A' '117A']\n",
      "Length of X_train_val:\n",
      "700\n",
      "Length of y_train_val:\n",
      "700\n",
      "Length of groups_train_val:\n",
      "700\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     409\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     179\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     443\n",
      "senior    137\n",
      "kitten    120\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     145\n",
      "kitten     51\n",
      "senior     41\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 886, 2: 685, 1: 600})\n",
      "Epoch 1/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.9305 - accuracy: 0.6025\n",
      "Epoch 2/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.7169 - accuracy: 0.6891\n",
      "Epoch 3/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6540 - accuracy: 0.7167\n",
      "Epoch 4/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.6058 - accuracy: 0.7319\n",
      "Epoch 5/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5780 - accuracy: 0.7559\n",
      "Epoch 6/1500\n",
      "68/68 [==============================] - 0s 5ms/step - loss: 0.5768 - accuracy: 0.7596\n",
      "Epoch 7/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5503 - accuracy: 0.7748\n",
      "Epoch 8/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5304 - accuracy: 0.7807\n",
      "Epoch 9/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.5105 - accuracy: 0.7826\n",
      "Epoch 10/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4892 - accuracy: 0.7900\n",
      "Epoch 11/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4601 - accuracy: 0.8070\n",
      "Epoch 12/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4528 - accuracy: 0.8158\n",
      "Epoch 13/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4379 - accuracy: 0.8153\n",
      "Epoch 14/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4558 - accuracy: 0.8139\n",
      "Epoch 15/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4130 - accuracy: 0.8222\n",
      "Epoch 16/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4276 - accuracy: 0.8217\n",
      "Epoch 17/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.4036 - accuracy: 0.8314\n",
      "Epoch 18/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3978 - accuracy: 0.8342\n",
      "Epoch 19/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3847 - accuracy: 0.8420\n",
      "Epoch 20/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3945 - accuracy: 0.8300\n",
      "Epoch 21/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3848 - accuracy: 0.8383\n",
      "Epoch 22/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3776 - accuracy: 0.8434\n",
      "Epoch 23/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3683 - accuracy: 0.8498\n",
      "Epoch 24/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3494 - accuracy: 0.8586\n",
      "Epoch 25/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3521 - accuracy: 0.8623\n",
      "Epoch 26/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3599 - accuracy: 0.8489\n",
      "Epoch 27/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3355 - accuracy: 0.8655\n",
      "Epoch 28/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3268 - accuracy: 0.8618\n",
      "Epoch 29/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3553 - accuracy: 0.8609\n",
      "Epoch 30/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3316 - accuracy: 0.8692\n",
      "Epoch 31/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3213 - accuracy: 0.8669\n",
      "Epoch 32/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3227 - accuracy: 0.8756\n",
      "Epoch 33/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3249 - accuracy: 0.8701\n",
      "Epoch 34/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2952 - accuracy: 0.8756\n",
      "Epoch 35/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2903 - accuracy: 0.8853\n",
      "Epoch 36/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3001 - accuracy: 0.8743\n",
      "Epoch 37/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3052 - accuracy: 0.8752\n",
      "Epoch 38/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.3042 - accuracy: 0.8881\n",
      "Epoch 39/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2759 - accuracy: 0.8959\n",
      "Epoch 40/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2780 - accuracy: 0.8950\n",
      "Epoch 41/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2889 - accuracy: 0.8844\n",
      "Epoch 42/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2831 - accuracy: 0.8922\n",
      "Epoch 43/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2928 - accuracy: 0.8839\n",
      "Epoch 44/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2863 - accuracy: 0.8862\n",
      "Epoch 45/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2892 - accuracy: 0.8867\n",
      "Epoch 46/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2617 - accuracy: 0.8959\n",
      "Epoch 47/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2488 - accuracy: 0.9010\n",
      "Epoch 48/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2523 - accuracy: 0.8959\n",
      "Epoch 49/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2408 - accuracy: 0.9088\n",
      "Epoch 50/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2454 - accuracy: 0.8977\n",
      "Epoch 51/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2508 - accuracy: 0.9074\n",
      "Epoch 52/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2565 - accuracy: 0.9042\n",
      "Epoch 53/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2564 - accuracy: 0.9000\n",
      "Epoch 54/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2492 - accuracy: 0.8945\n",
      "Epoch 55/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2360 - accuracy: 0.9083\n",
      "Epoch 56/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2246 - accuracy: 0.9162\n",
      "Epoch 57/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2557 - accuracy: 0.9065\n",
      "Epoch 58/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2363 - accuracy: 0.9074\n",
      "Epoch 59/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2259 - accuracy: 0.9134\n",
      "Epoch 60/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2223 - accuracy: 0.9157\n",
      "Epoch 61/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2296 - accuracy: 0.9065\n",
      "Epoch 62/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2289 - accuracy: 0.9129\n",
      "Epoch 63/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2117 - accuracy: 0.9240\n",
      "Epoch 64/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2237 - accuracy: 0.9129\n",
      "Epoch 65/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2285 - accuracy: 0.9065\n",
      "Epoch 66/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2327 - accuracy: 0.9051\n",
      "Epoch 67/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2281 - accuracy: 0.9116\n",
      "Epoch 68/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9175\n",
      "Epoch 69/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2097 - accuracy: 0.9166\n",
      "Epoch 70/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1977 - accuracy: 0.9245\n",
      "Epoch 71/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2161 - accuracy: 0.9203\n",
      "Epoch 72/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2081 - accuracy: 0.9254\n",
      "Epoch 73/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2199 - accuracy: 0.9203\n",
      "Epoch 74/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1957 - accuracy: 0.9254\n",
      "Epoch 75/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1871 - accuracy: 0.9254\n",
      "Epoch 76/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2144 - accuracy: 0.9217\n",
      "Epoch 77/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9309\n",
      "Epoch 78/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2075 - accuracy: 0.9208\n",
      "Epoch 79/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2070 - accuracy: 0.9189\n",
      "Epoch 80/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1863 - accuracy: 0.9249\n",
      "Epoch 81/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1961 - accuracy: 0.9304\n",
      "Epoch 82/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1827 - accuracy: 0.9327\n",
      "Epoch 83/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1954 - accuracy: 0.9272\n",
      "Epoch 84/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.2086 - accuracy: 0.9162\n",
      "Epoch 85/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1917 - accuracy: 0.9240\n",
      "Epoch 86/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1847 - accuracy: 0.9291\n",
      "Epoch 87/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1815 - accuracy: 0.9355\n",
      "Epoch 88/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9291\n",
      "Epoch 89/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1791 - accuracy: 0.9291\n",
      "Epoch 90/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1919 - accuracy: 0.9235\n",
      "Epoch 91/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1886 - accuracy: 0.9318\n",
      "Epoch 92/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1859 - accuracy: 0.9300\n",
      "Epoch 93/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1672 - accuracy: 0.9401\n",
      "Epoch 94/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1864 - accuracy: 0.9277\n",
      "Epoch 95/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1645 - accuracy: 0.9415\n",
      "Epoch 96/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1704 - accuracy: 0.9341\n",
      "Epoch 97/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1733 - accuracy: 0.9397\n",
      "Epoch 98/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1675 - accuracy: 0.9378\n",
      "Epoch 99/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1767 - accuracy: 0.9318\n",
      "Epoch 100/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9397\n",
      "Epoch 101/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9374\n",
      "Epoch 102/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1779 - accuracy: 0.9258\n",
      "Epoch 103/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1686 - accuracy: 0.9374\n",
      "Epoch 104/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1799 - accuracy: 0.9378\n",
      "Epoch 105/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1668 - accuracy: 0.9351\n",
      "Epoch 106/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9369\n",
      "Epoch 107/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1811 - accuracy: 0.9254\n",
      "Epoch 108/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1630 - accuracy: 0.9401\n",
      "Epoch 109/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1670 - accuracy: 0.9420\n",
      "Epoch 110/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1654 - accuracy: 0.9392\n",
      "Epoch 111/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1603 - accuracy: 0.9406\n",
      "Epoch 112/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1825 - accuracy: 0.9341\n",
      "Epoch 113/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1529 - accuracy: 0.9415\n",
      "Epoch 114/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1803 - accuracy: 0.9277\n",
      "Epoch 115/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1619 - accuracy: 0.9397\n",
      "Epoch 116/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9397\n",
      "Epoch 117/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1484 - accuracy: 0.9452\n",
      "Epoch 118/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1571 - accuracy: 0.9456\n",
      "Epoch 119/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1499 - accuracy: 0.9433\n",
      "Epoch 120/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1472 - accuracy: 0.9452\n",
      "Epoch 121/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1549 - accuracy: 0.9424\n",
      "Epoch 122/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9475\n",
      "Epoch 123/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9346\n",
      "Epoch 124/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1482 - accuracy: 0.9470\n",
      "Epoch 125/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1646 - accuracy: 0.9424\n",
      "Epoch 126/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9480\n",
      "Epoch 127/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9443\n",
      "Epoch 128/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9489\n",
      "Epoch 129/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1453 - accuracy: 0.9507\n",
      "Epoch 130/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1408 - accuracy: 0.9521\n",
      "Epoch 131/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9507\n",
      "Epoch 132/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.9461\n",
      "Epoch 133/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1451 - accuracy: 0.9475\n",
      "Epoch 134/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1411 - accuracy: 0.9470\n",
      "Epoch 135/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1389 - accuracy: 0.9503\n",
      "Epoch 136/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1438 - accuracy: 0.9447\n",
      "Epoch 137/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1424 - accuracy: 0.9443\n",
      "Epoch 138/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9590\n",
      "Epoch 139/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1419 - accuracy: 0.9503\n",
      "Epoch 140/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1341 - accuracy: 0.9480\n",
      "Epoch 141/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1460 - accuracy: 0.9480\n",
      "Epoch 142/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9438\n",
      "Epoch 143/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9470\n",
      "Epoch 144/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9480\n",
      "Epoch 145/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9572\n",
      "Epoch 146/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1362 - accuracy: 0.9493\n",
      "Epoch 147/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1234 - accuracy: 0.9562\n",
      "Epoch 148/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1318 - accuracy: 0.9530\n",
      "Epoch 149/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9489\n",
      "Epoch 150/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1178 - accuracy: 0.9562\n",
      "Epoch 151/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9420\n",
      "Epoch 152/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1276 - accuracy: 0.9539\n",
      "Epoch 153/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1348 - accuracy: 0.9530\n",
      "Epoch 154/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1244 - accuracy: 0.9493\n",
      "Epoch 155/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1222 - accuracy: 0.9585\n",
      "Epoch 156/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1251 - accuracy: 0.9498\n",
      "Epoch 157/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9562\n",
      "Epoch 158/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9498\n",
      "Epoch 159/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1096 - accuracy: 0.9655\n",
      "Epoch 160/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1205 - accuracy: 0.9599\n",
      "Epoch 161/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1243 - accuracy: 0.9558\n",
      "Epoch 162/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1280 - accuracy: 0.9493\n",
      "Epoch 163/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9503\n",
      "Epoch 164/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1238 - accuracy: 0.9567\n",
      "Epoch 165/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1214 - accuracy: 0.9608\n",
      "Epoch 166/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1298 - accuracy: 0.9530\n",
      "Epoch 167/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1225 - accuracy: 0.9549\n",
      "Epoch 168/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9627\n",
      "Epoch 169/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1649 - accuracy: 0.9374\n",
      "Epoch 170/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1309 - accuracy: 0.9503\n",
      "Epoch 171/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9498\n",
      "Epoch 172/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9655\n",
      "Epoch 173/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1224 - accuracy: 0.9544\n",
      "Epoch 174/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1338 - accuracy: 0.9567\n",
      "Epoch 175/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1287 - accuracy: 0.9535\n",
      "Epoch 176/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1204 - accuracy: 0.9585\n",
      "Epoch 177/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1155 - accuracy: 0.9585\n",
      "Epoch 178/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9535\n",
      "Epoch 179/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9549\n",
      "Epoch 180/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9696\n",
      "Epoch 181/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9641\n",
      "Epoch 182/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1047 - accuracy: 0.9599\n",
      "Epoch 183/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9632\n",
      "Epoch 184/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9576\n",
      "Epoch 185/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1313 - accuracy: 0.9521\n",
      "Epoch 186/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9618\n",
      "Epoch 187/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1174 - accuracy: 0.9604\n",
      "Epoch 188/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9595\n",
      "Epoch 189/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1024 - accuracy: 0.9678\n",
      "Epoch 190/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1011 - accuracy: 0.9632\n",
      "Epoch 191/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1135 - accuracy: 0.9608\n",
      "Epoch 192/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9604\n",
      "Epoch 193/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9641\n",
      "Epoch 194/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1211 - accuracy: 0.9562\n",
      "Epoch 195/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1043 - accuracy: 0.9613\n",
      "Epoch 196/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0983 - accuracy: 0.9636\n",
      "Epoch 197/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0976 - accuracy: 0.9668\n",
      "Epoch 198/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0933 - accuracy: 0.9714\n",
      "Epoch 199/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1206 - accuracy: 0.9567\n",
      "Epoch 200/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9493\n",
      "Epoch 201/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9544\n",
      "Epoch 202/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1128 - accuracy: 0.9585\n",
      "Epoch 203/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1141 - accuracy: 0.9585\n",
      "Epoch 204/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9664\n",
      "Epoch 205/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0936 - accuracy: 0.9687\n",
      "Epoch 206/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9627\n",
      "Epoch 207/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9613\n",
      "Epoch 208/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0998 - accuracy: 0.9641\n",
      "Epoch 209/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1018 - accuracy: 0.9664\n",
      "Epoch 210/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9632\n",
      "Epoch 211/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1107 - accuracy: 0.9567\n",
      "Epoch 212/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1020 - accuracy: 0.9664\n",
      "Epoch 213/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1264 - accuracy: 0.9553\n",
      "Epoch 214/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9618\n",
      "Epoch 215/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0964 - accuracy: 0.9682\n",
      "Epoch 216/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0951 - accuracy: 0.9659\n",
      "Epoch 217/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0950 - accuracy: 0.9691\n",
      "Epoch 218/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9613\n",
      "Epoch 219/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9539\n",
      "Epoch 220/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1106 - accuracy: 0.9645\n",
      "Epoch 221/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0959 - accuracy: 0.9687\n",
      "Epoch 222/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1143 - accuracy: 0.9604\n",
      "Epoch 223/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0942 - accuracy: 0.9691\n",
      "Epoch 224/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1060 - accuracy: 0.9627\n",
      "Epoch 225/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.0985 - accuracy: 0.9641\n",
      "Epoch 226/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9636\n",
      "Epoch 227/1500\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1259 - accuracy: 0.9507\n",
      "Epoch 228/1500\n",
      "43/68 [=================>............] - ETA: 0s - loss: 0.1098 - accuracy: 0.9564Restoring model weights from the end of the best epoch: 198.\n",
      "68/68 [==============================] - 0s 1ms/step - loss: 0.1074 - accuracy: 0.9599\n",
      "Epoch 228: early stopping\n",
      "8/8 [==============================] - 0s 887us/step - loss: 0.8056 - accuracy: 0.7468\n",
      "8/8 [==============================] - 0s 825us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.76 (22/29)\n",
      "Before appending - Cat IDs: 153, Predictions: 153, Actuals: 153, Gender: 153\n",
      "After appending - Cat IDs: 390, Predictions: 390, Actuals: 390, Gender: 390\n",
      "Final Test Results - Loss: 0.8055605292320251, Accuracy: 0.746835470199585, Precision: 0.7591224345364473, Recall: 0.6872155983223118, F1 Score: 0.7035934931446337\n",
      "Confusion Matrix:\n",
      " [[121   1  23]\n",
      " [ 22  29   0]\n",
      " [ 14   0  27]]\n",
      "outer_fold 3\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "020A    23\n",
      "055A    20\n",
      "000B    19\n",
      "067A    19\n",
      "019A    17\n",
      "101A    15\n",
      "059A    14\n",
      "042A    14\n",
      "097B    14\n",
      "001A    14\n",
      "002A    13\n",
      "111A    13\n",
      "116A    12\n",
      "051A    12\n",
      "039A    12\n",
      "068A    11\n",
      "036A    11\n",
      "063A    11\n",
      "014B    10\n",
      "016A    10\n",
      "040A    10\n",
      "071A    10\n",
      "065A     9\n",
      "033A     9\n",
      "051B     9\n",
      "022A     9\n",
      "010A     8\n",
      "095A     8\n",
      "013B     8\n",
      "027A     7\n",
      "099A     7\n",
      "031A     7\n",
      "050A     7\n",
      "117A     7\n",
      "007A     6\n",
      "109A     6\n",
      "108A     6\n",
      "037A     6\n",
      "008A     6\n",
      "044A     5\n",
      "025C     5\n",
      "070A     5\n",
      "075A     5\n",
      "034A     5\n",
      "023B     5\n",
      "052A     4\n",
      "026A     4\n",
      "105A     4\n",
      "060A     3\n",
      "012A     3\n",
      "064A     3\n",
      "006A     3\n",
      "113A     3\n",
      "014A     3\n",
      "061A     2\n",
      "054A     2\n",
      "087A     2\n",
      "025B     2\n",
      "011A     2\n",
      "018A     2\n",
      "102A     2\n",
      "043A     1\n",
      "024A     1\n",
      "090A     1\n",
      "091A     1\n",
      "110A     1\n",
      "115A     1\n",
      "004A     1\n",
      "019B     1\n",
      "088A     1\n",
      "048A     1\n",
      "066A     1\n",
      "041A     1\n",
      "092A     1\n",
      "049A     1\n",
      "096A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "057A    27\n",
      "074A    25\n",
      "029A    17\n",
      "097A    16\n",
      "106A    14\n",
      "028A    13\n",
      "025A    11\n",
      "005A    10\n",
      "015A     9\n",
      "045A     9\n",
      "072A     9\n",
      "094A     8\n",
      "023A     6\n",
      "053A     6\n",
      "021A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "062A     4\n",
      "003A     4\n",
      "058A     3\n",
      "056A     3\n",
      "093A     2\n",
      "032A     2\n",
      "069A     2\n",
      "038A     2\n",
      "073A     1\n",
      "076A     1\n",
      "026C     1\n",
      "100A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "X    266\n",
      "M    237\n",
      "F    211\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "M    100\n",
      "X     82\n",
      "F     41\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 001A, 103A, 071A, 097B, 019...\n",
      "kitten    [044A, 014B, 111A, 040A, 046A, 047A, 042A, 109...\n",
      "senior    [055A, 059A, 113A, 116A, 051B, 054A, 117A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [015A, 028A, 074A, 062A, 029A, 005A, 072A, 009...\n",
      "kitten                                               [045A]\n",
      "senior     [093A, 097A, 057A, 106A, 104A, 056A, 058A, 094A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 53, 'kitten': 15, 'senior': 14}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 21, 'kitten': 1, 'senior': 8}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '004A' '006A' '007A' '008A' '010A'\n",
      " '011A' '012A' '013B' '014A' '014B' '016A' '018A' '019A' '019B' '020A'\n",
      " '022A' '023B' '024A' '025B' '025C' '026A' '026B' '027A' '031A' '033A'\n",
      " '034A' '036A' '037A' '039A' '040A' '041A' '042A' '043A' '044A' '046A'\n",
      " '047A' '048A' '049A' '050A' '051A' '051B' '052A' '054A' '055A' '059A'\n",
      " '060A' '061A' '063A' '064A' '065A' '066A' '067A' '068A' '070A' '071A'\n",
      " '075A' '087A' '088A' '090A' '091A' '092A' '095A' '096A' '097B' '099A'\n",
      " '101A' '102A' '103A' '105A' '108A' '109A' '110A' '111A' '113A' '115A'\n",
      " '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['003A' '005A' '009A' '015A' '021A' '023A' '025A' '026C' '028A' '029A'\n",
      " '032A' '035A' '038A' '045A' '053A' '056A' '057A' '058A' '062A' '069A'\n",
      " '072A' '073A' '074A' '076A' '093A' '094A' '097A' '100A' '104A' '106A']\n",
      "Length of X_train_val:\n",
      "714\n",
      "Length of y_train_val:\n",
      "714\n",
      "Length of groups_train_val:\n",
      "714\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     451\n",
      "kitten    162\n",
      "senior    101\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     137\n",
      "senior     77\n",
      "kitten      9\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 902, 1: 810, 2: 505})\n",
      "Epoch 1/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.8784 - accuracy: 0.6175\n",
      "Epoch 2/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.6528 - accuracy: 0.7442\n",
      "Epoch 3/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5935 - accuracy: 0.7632\n",
      "Epoch 4/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5154 - accuracy: 0.7912\n",
      "Epoch 5/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4945 - accuracy: 0.8029\n",
      "Epoch 6/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.5058 - accuracy: 0.7857\n",
      "Epoch 7/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4531 - accuracy: 0.8196\n",
      "Epoch 8/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4280 - accuracy: 0.8367\n",
      "Epoch 9/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.4031 - accuracy: 0.8254\n",
      "Epoch 10/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3991 - accuracy: 0.8453\n",
      "Epoch 11/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3880 - accuracy: 0.8498\n",
      "Epoch 12/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3925 - accuracy: 0.8430\n",
      "Epoch 13/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3675 - accuracy: 0.8530\n",
      "Epoch 14/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3392 - accuracy: 0.8647\n",
      "Epoch 15/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3563 - accuracy: 0.8611\n",
      "Epoch 16/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3446 - accuracy: 0.8687\n",
      "Epoch 17/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3440 - accuracy: 0.8737\n",
      "Epoch 18/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3413 - accuracy: 0.8714\n",
      "Epoch 19/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3393 - accuracy: 0.8615\n",
      "Epoch 20/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3284 - accuracy: 0.8669\n",
      "Epoch 21/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3203 - accuracy: 0.8787\n",
      "Epoch 22/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3267 - accuracy: 0.8809\n",
      "Epoch 23/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3136 - accuracy: 0.8760\n",
      "Epoch 24/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3093 - accuracy: 0.8728\n",
      "Epoch 25/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2786 - accuracy: 0.8886\n",
      "Epoch 26/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2958 - accuracy: 0.8872\n",
      "Epoch 27/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2805 - accuracy: 0.8917\n",
      "Epoch 28/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.3078 - accuracy: 0.8872\n",
      "Epoch 29/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2795 - accuracy: 0.8990\n",
      "Epoch 30/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2742 - accuracy: 0.8908\n",
      "Epoch 31/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2743 - accuracy: 0.8972\n",
      "Epoch 32/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2696 - accuracy: 0.8940\n",
      "Epoch 33/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2591 - accuracy: 0.8981\n",
      "Epoch 34/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2637 - accuracy: 0.8972\n",
      "Epoch 35/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2776 - accuracy: 0.8877\n",
      "Epoch 36/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2523 - accuracy: 0.9080\n",
      "Epoch 37/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2594 - accuracy: 0.9048\n",
      "Epoch 38/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2390 - accuracy: 0.9093\n",
      "Epoch 39/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2580 - accuracy: 0.9048\n",
      "Epoch 40/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2488 - accuracy: 0.9044\n",
      "Epoch 41/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2395 - accuracy: 0.9089\n",
      "Epoch 42/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2244 - accuracy: 0.9066\n",
      "Epoch 43/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2256 - accuracy: 0.9161\n",
      "Epoch 44/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2297 - accuracy: 0.9157\n",
      "Epoch 45/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2220 - accuracy: 0.9138\n",
      "Epoch 46/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.9116\n",
      "Epoch 47/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2221 - accuracy: 0.9157\n",
      "Epoch 48/1500\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.2145 - accuracy: 0.9206\n",
      "Epoch 49/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2338 - accuracy: 0.9102\n",
      "Epoch 50/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2192 - accuracy: 0.9184\n",
      "Epoch 51/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2090 - accuracy: 0.9256\n",
      "Epoch 52/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2203 - accuracy: 0.9193\n",
      "Epoch 53/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2223 - accuracy: 0.9080\n",
      "Epoch 54/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2069 - accuracy: 0.9233\n",
      "Epoch 55/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2077 - accuracy: 0.9166\n",
      "Epoch 56/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2145 - accuracy: 0.9129\n",
      "Epoch 57/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2049 - accuracy: 0.9188\n",
      "Epoch 58/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2032 - accuracy: 0.9256\n",
      "Epoch 59/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2107 - accuracy: 0.9175\n",
      "Epoch 60/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2092 - accuracy: 0.9202\n",
      "Epoch 61/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1850 - accuracy: 0.9292\n",
      "Epoch 62/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.2142 - accuracy: 0.9197\n",
      "Epoch 63/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9287\n",
      "Epoch 64/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1856 - accuracy: 0.9310\n",
      "Epoch 65/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1887 - accuracy: 0.9323\n",
      "Epoch 66/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1963 - accuracy: 0.9287\n",
      "Epoch 67/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1973 - accuracy: 0.9278\n",
      "Epoch 68/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1771 - accuracy: 0.9332\n",
      "Epoch 69/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1734 - accuracy: 0.9418\n",
      "Epoch 70/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1822 - accuracy: 0.9287\n",
      "Epoch 71/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1788 - accuracy: 0.9350\n",
      "Epoch 72/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1901 - accuracy: 0.9310\n",
      "Epoch 73/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1768 - accuracy: 0.9332\n",
      "Epoch 74/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1774 - accuracy: 0.9350\n",
      "Epoch 75/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1682 - accuracy: 0.9391\n",
      "Epoch 76/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1687 - accuracy: 0.9405\n",
      "Epoch 77/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1598 - accuracy: 0.9463\n",
      "Epoch 78/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1728 - accuracy: 0.9369\n",
      "Epoch 79/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1702 - accuracy: 0.9332\n",
      "Epoch 80/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1611 - accuracy: 0.9409\n",
      "Epoch 81/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1643 - accuracy: 0.9400\n",
      "Epoch 82/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1639 - accuracy: 0.9405\n",
      "Epoch 83/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1701 - accuracy: 0.9355\n",
      "Epoch 84/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1869 - accuracy: 0.9256\n",
      "Epoch 85/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1665 - accuracy: 0.9373\n",
      "Epoch 86/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1711 - accuracy: 0.9382\n",
      "Epoch 87/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1884 - accuracy: 0.9332\n",
      "Epoch 88/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9508\n",
      "Epoch 89/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1559 - accuracy: 0.9400\n",
      "Epoch 90/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1519 - accuracy: 0.9477\n",
      "Epoch 91/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1579 - accuracy: 0.9427\n",
      "Epoch 92/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9490\n",
      "Epoch 93/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.9441\n",
      "Epoch 94/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1509 - accuracy: 0.9427\n",
      "Epoch 95/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1536 - accuracy: 0.9427\n",
      "Epoch 96/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1444 - accuracy: 0.9481\n",
      "Epoch 97/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1448 - accuracy: 0.9472\n",
      "Epoch 98/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1432 - accuracy: 0.9495\n",
      "Epoch 99/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1479 - accuracy: 0.9499\n",
      "Epoch 100/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1500 - accuracy: 0.9477\n",
      "Epoch 101/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1567 - accuracy: 0.9409\n",
      "Epoch 102/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1363 - accuracy: 0.9490\n",
      "Epoch 103/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9504\n",
      "Epoch 104/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1415 - accuracy: 0.9517\n",
      "Epoch 105/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1250 - accuracy: 0.9581\n",
      "Epoch 106/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1367 - accuracy: 0.9495\n",
      "Epoch 107/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1329 - accuracy: 0.9522\n",
      "Epoch 108/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1381 - accuracy: 0.9499\n",
      "Epoch 109/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1433 - accuracy: 0.9508\n",
      "Epoch 110/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1308 - accuracy: 0.9558\n",
      "Epoch 111/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1574 - accuracy: 0.9378\n",
      "Epoch 112/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1345 - accuracy: 0.9531\n",
      "Epoch 113/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1344 - accuracy: 0.9508\n",
      "Epoch 114/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1395 - accuracy: 0.9477\n",
      "Epoch 115/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1296 - accuracy: 0.9549\n",
      "Epoch 116/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1203 - accuracy: 0.9626\n",
      "Epoch 117/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9585\n",
      "Epoch 118/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1268 - accuracy: 0.9526\n",
      "Epoch 119/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9562\n",
      "Epoch 120/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1235 - accuracy: 0.9630\n",
      "Epoch 121/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1240 - accuracy: 0.9590\n",
      "Epoch 122/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9590\n",
      "Epoch 123/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1252 - accuracy: 0.9567\n",
      "Epoch 124/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1304 - accuracy: 0.9499\n",
      "Epoch 125/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1176 - accuracy: 0.9630\n",
      "Epoch 126/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9540\n",
      "Epoch 127/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1247 - accuracy: 0.9576\n",
      "Epoch 128/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1349 - accuracy: 0.9522\n",
      "Epoch 129/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1160 - accuracy: 0.9608\n",
      "Epoch 130/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1187 - accuracy: 0.9581\n",
      "Epoch 131/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1275 - accuracy: 0.9567\n",
      "Epoch 132/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1405 - accuracy: 0.9490\n",
      "Epoch 133/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1101 - accuracy: 0.9644\n",
      "Epoch 134/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1170 - accuracy: 0.9621\n",
      "Epoch 135/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1332 - accuracy: 0.9499\n",
      "Epoch 136/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1324 - accuracy: 0.9581\n",
      "Epoch 137/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1028 - accuracy: 0.9648\n",
      "Epoch 138/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1162 - accuracy: 0.9621\n",
      "Epoch 139/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1293 - accuracy: 0.9540\n",
      "Epoch 140/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1016 - accuracy: 0.9662\n",
      "Epoch 141/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9630\n",
      "Epoch 142/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1186 - accuracy: 0.9571\n",
      "Epoch 143/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9562\n",
      "Epoch 144/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1256 - accuracy: 0.9590\n",
      "Epoch 145/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9738\n",
      "Epoch 146/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1042 - accuracy: 0.9648\n",
      "Epoch 147/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1056 - accuracy: 0.9630\n",
      "Epoch 148/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0991 - accuracy: 0.9657\n",
      "Epoch 149/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1034 - accuracy: 0.9639\n",
      "Epoch 150/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1109 - accuracy: 0.9630\n",
      "Epoch 151/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9590\n",
      "Epoch 152/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1152 - accuracy: 0.9553\n",
      "Epoch 153/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1040 - accuracy: 0.9662\n",
      "Epoch 154/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1063 - accuracy: 0.9648\n",
      "Epoch 155/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1066 - accuracy: 0.9621\n",
      "Epoch 156/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1082 - accuracy: 0.9680\n",
      "Epoch 157/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0926 - accuracy: 0.9707\n",
      "Epoch 158/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0928 - accuracy: 0.9738\n",
      "Epoch 159/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0946 - accuracy: 0.9689\n",
      "Epoch 160/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1021 - accuracy: 0.9603\n",
      "Epoch 161/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1177 - accuracy: 0.9590\n",
      "Epoch 162/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9680\n",
      "Epoch 163/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1013 - accuracy: 0.9648\n",
      "Epoch 164/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1045 - accuracy: 0.9585\n",
      "Epoch 165/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1061 - accuracy: 0.9621\n",
      "Epoch 166/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1072 - accuracy: 0.9635\n",
      "Epoch 167/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1025 - accuracy: 0.9657\n",
      "Epoch 168/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0911 - accuracy: 0.9698\n",
      "Epoch 169/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1121 - accuracy: 0.9621\n",
      "Epoch 170/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0941 - accuracy: 0.9666\n",
      "Epoch 171/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0826 - accuracy: 0.9729\n",
      "Epoch 172/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0903 - accuracy: 0.9689\n",
      "Epoch 173/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0919 - accuracy: 0.9684\n",
      "Epoch 174/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1105 - accuracy: 0.9594\n",
      "Epoch 175/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1290 - accuracy: 0.9535\n",
      "Epoch 176/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0958 - accuracy: 0.9662\n",
      "Epoch 177/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1062 - accuracy: 0.9653\n",
      "Epoch 178/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1123 - accuracy: 0.9585\n",
      "Epoch 179/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0949 - accuracy: 0.9666\n",
      "Epoch 180/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0852 - accuracy: 0.9725\n",
      "Epoch 181/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0845 - accuracy: 0.9702\n",
      "Epoch 182/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0997 - accuracy: 0.9639\n",
      "Epoch 183/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1192 - accuracy: 0.9594\n",
      "Epoch 184/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9707\n",
      "Epoch 185/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0916 - accuracy: 0.9702\n",
      "Epoch 186/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9612\n",
      "Epoch 187/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0884 - accuracy: 0.9702\n",
      "Epoch 188/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0823 - accuracy: 0.9684\n",
      "Epoch 189/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0897 - accuracy: 0.9671\n",
      "Epoch 190/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.1144 - accuracy: 0.9576\n",
      "Epoch 191/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0925 - accuracy: 0.9707\n",
      "Epoch 192/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0827 - accuracy: 0.9693\n",
      "Epoch 193/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0968 - accuracy: 0.9680\n",
      "Epoch 194/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0857 - accuracy: 0.9747\n",
      "Epoch 195/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0880 - accuracy: 0.9689\n",
      "Epoch 196/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0803 - accuracy: 0.9734\n",
      "Epoch 197/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0864 - accuracy: 0.9693\n",
      "Epoch 198/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0899 - accuracy: 0.9720\n",
      "Epoch 199/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0870 - accuracy: 0.9702\n",
      "Epoch 200/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9765\n",
      "Epoch 201/1500\n",
      "70/70 [==============================] - 0s 2ms/step - loss: 0.0822 - accuracy: 0.9716\n",
      "Epoch 202/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0821 - accuracy: 0.9734\n",
      "Epoch 203/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0846 - accuracy: 0.9689\n",
      "Epoch 204/1500\n",
      "70/70 [==============================] - 0s 1000us/step - loss: 0.0821 - accuracy: 0.9707\n",
      "Epoch 205/1500\n",
      "70/70 [==============================] - 0s 969us/step - loss: 0.0754 - accuracy: 0.9752\n",
      "Epoch 206/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0774 - accuracy: 0.9756\n",
      "Epoch 207/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9752\n",
      "Epoch 208/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0915 - accuracy: 0.9644\n",
      "Epoch 209/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0879 - accuracy: 0.9671\n",
      "Epoch 210/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0861 - accuracy: 0.9693\n",
      "Epoch 211/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0757 - accuracy: 0.9747\n",
      "Epoch 212/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0967 - accuracy: 0.9630\n",
      "Epoch 213/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0796 - accuracy: 0.9711\n",
      "Epoch 214/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.9752\n",
      "Epoch 215/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0938 - accuracy: 0.9702\n",
      "Epoch 216/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0847 - accuracy: 0.9693\n",
      "Epoch 217/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0814 - accuracy: 0.9707\n",
      "Epoch 218/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0811 - accuracy: 0.9711\n",
      "Epoch 219/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0795 - accuracy: 0.9743\n",
      "Epoch 220/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0830 - accuracy: 0.9707\n",
      "Epoch 221/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0982 - accuracy: 0.9662\n",
      "Epoch 222/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0868 - accuracy: 0.9738\n",
      "Epoch 223/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0767 - accuracy: 0.9761\n",
      "Epoch 224/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0817 - accuracy: 0.9720\n",
      "Epoch 225/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0809 - accuracy: 0.9707\n",
      "Epoch 226/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0759 - accuracy: 0.9720\n",
      "Epoch 227/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0699 - accuracy: 0.9793\n",
      "Epoch 228/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0765 - accuracy: 0.9743\n",
      "Epoch 229/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.9811\n",
      "Epoch 230/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0944 - accuracy: 0.9662\n",
      "Epoch 231/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0763 - accuracy: 0.9756\n",
      "Epoch 232/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0713 - accuracy: 0.9788\n",
      "Epoch 233/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0580 - accuracy: 0.9797\n",
      "Epoch 234/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0707 - accuracy: 0.9783\n",
      "Epoch 235/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0735 - accuracy: 0.9774\n",
      "Epoch 236/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9738\n",
      "Epoch 237/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9756\n",
      "Epoch 238/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0721 - accuracy: 0.9783\n",
      "Epoch 239/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9662\n",
      "Epoch 240/1500\n",
      "70/70 [==============================] - 0s 5ms/step - loss: 0.0628 - accuracy: 0.9829\n",
      "Epoch 241/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9707\n",
      "Epoch 242/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9702\n",
      "Epoch 243/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0900 - accuracy: 0.9716\n",
      "Epoch 244/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0782 - accuracy: 0.9756\n",
      "Epoch 245/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0720 - accuracy: 0.9729\n",
      "Epoch 246/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0775 - accuracy: 0.9747\n",
      "Epoch 247/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9635\n",
      "Epoch 248/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0802 - accuracy: 0.9716\n",
      "Epoch 249/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0667 - accuracy: 0.9793\n",
      "Epoch 250/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0691 - accuracy: 0.9756\n",
      "Epoch 251/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0745 - accuracy: 0.9734\n",
      "Epoch 252/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0692 - accuracy: 0.9774\n",
      "Epoch 253/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0807 - accuracy: 0.9729\n",
      "Epoch 254/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0751 - accuracy: 0.9752\n",
      "Epoch 255/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9756\n",
      "Epoch 256/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0638 - accuracy: 0.9788\n",
      "Epoch 257/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0739 - accuracy: 0.9738\n",
      "Epoch 258/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0709 - accuracy: 0.9779\n",
      "Epoch 259/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0777 - accuracy: 0.9752\n",
      "Epoch 260/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0862 - accuracy: 0.9711\n",
      "Epoch 261/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0752 - accuracy: 0.9756\n",
      "Epoch 262/1500\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0742 - accuracy: 0.9720\n",
      "Epoch 263/1500\n",
      "47/70 [===================>..........] - ETA: 0s - loss: 0.0669 - accuracy: 0.9794Restoring model weights from the end of the best epoch: 233.\n",
      "70/70 [==============================] - 0s 1ms/step - loss: 0.0653 - accuracy: 0.9779\n",
      "Epoch 263: early stopping\n",
      "7/7 [==============================] - 0s 957us/step - loss: 1.1585 - accuracy: 0.6816\n",
      "7/7 [==============================] - 0s 765us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adamax` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adamax`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Vote Accuracy for cat_id for this fold: 0.77 (23/30)\n",
      "Before appending - Cat IDs: 390, Predictions: 390, Actuals: 390, Gender: 390\n",
      "After appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "Final Test Results - Loss: 1.1585413217544556, Accuracy: 0.681614339351654, Precision: 0.6554329590914957, Recall: 0.7343508073435081, F1 Score: 0.6685815634745668\n",
      "Confusion Matrix:\n",
      " [[115   5  17]\n",
      " [  0   9   0]\n",
      " [ 49   0  28]]\n",
      "outer_fold 4\n",
      "Train Set Group Distribution:\n",
      "046A    63\n",
      "000A    39\n",
      "103A    33\n",
      "002B    32\n",
      "047A    28\n",
      "057A    27\n",
      "074A    25\n",
      "067A    19\n",
      "000B    19\n",
      "029A    17\n",
      "097A    16\n",
      "042A    14\n",
      "001A    14\n",
      "106A    14\n",
      "028A    13\n",
      "002A    13\n",
      "039A    12\n",
      "116A    12\n",
      "025A    11\n",
      "063A    11\n",
      "068A    11\n",
      "040A    10\n",
      "005A    10\n",
      "016A    10\n",
      "051B     9\n",
      "022A     9\n",
      "045A     9\n",
      "065A     9\n",
      "072A     9\n",
      "033A     9\n",
      "015A     9\n",
      "094A     8\n",
      "010A     8\n",
      "013B     8\n",
      "117A     7\n",
      "099A     7\n",
      "050A     7\n",
      "007A     6\n",
      "053A     6\n",
      "108A     6\n",
      "109A     6\n",
      "023A     6\n",
      "021A     5\n",
      "025C     5\n",
      "044A     5\n",
      "075A     5\n",
      "009A     4\n",
      "035A     4\n",
      "104A     4\n",
      "026A     4\n",
      "062A     4\n",
      "003A     4\n",
      "012A     3\n",
      "056A     3\n",
      "064A     3\n",
      "058A     3\n",
      "006A     3\n",
      "113A     3\n",
      "069A     2\n",
      "025B     2\n",
      "061A     2\n",
      "018A     2\n",
      "038A     2\n",
      "087A     2\n",
      "011A     2\n",
      "093A     2\n",
      "032A     2\n",
      "054A     2\n",
      "088A     1\n",
      "115A     1\n",
      "100A     1\n",
      "024A     1\n",
      "019B     1\n",
      "043A     1\n",
      "091A     1\n",
      "004A     1\n",
      "048A     1\n",
      "066A     1\n",
      "026C     1\n",
      "092A     1\n",
      "049A     1\n",
      "076A     1\n",
      "073A     1\n",
      "026B     1\n",
      "Name: cat_id, dtype: int64\n",
      "Testing Set Group Distribution:\n",
      "020A    23\n",
      "055A    20\n",
      "019A    17\n",
      "101A    15\n",
      "097B    14\n",
      "059A    14\n",
      "111A    13\n",
      "051A    12\n",
      "036A    11\n",
      "014B    10\n",
      "071A    10\n",
      "095A     8\n",
      "027A     7\n",
      "031A     7\n",
      "037A     6\n",
      "008A     6\n",
      "023B     5\n",
      "070A     5\n",
      "034A     5\n",
      "105A     4\n",
      "052A     4\n",
      "014A     3\n",
      "060A     3\n",
      "102A     2\n",
      "110A     1\n",
      "096A     1\n",
      "041A     1\n",
      "090A     1\n",
      "Name: cat_id, dtype: int64\n",
      "Training Set Gender Distribution BEFORE SWAP:\n",
      "M    268\n",
      "X    259\n",
      "F    182\n",
      "Name: gender, dtype: int64\n",
      "Testing Set Gender Distribution BEFORE SWAP:\n",
      "X    89\n",
      "F    70\n",
      "M    69\n",
      "Name: gender, dtype: int64\n",
      "Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "age_group\n",
      "adult     [006A, 000A, 033A, 015A, 001A, 103A, 028A, 074...\n",
      "kitten    [044A, 040A, 046A, 047A, 042A, 109A, 050A, 043...\n",
      "senior    [093A, 097A, 057A, 106A, 104A, 113A, 116A, 051...\n",
      "Name: cat_id, dtype: object\n",
      "Unique Cat IDs per Age Group in Testing Set:\n",
      "age_group\n",
      "adult     [071A, 097B, 019A, 020A, 101A, 095A, 034A, 027...\n",
      "kitten                             [014B, 111A, 041A, 110A]\n",
      "senior                             [055A, 059A, 051A, 090A]\n",
      "Name: cat_id, dtype: object\n",
      "Count of Unique Cat IDs per Age Group in Training/Validation Set:\n",
      "{'adult': 54, 'kitten': 12, 'senior': 18}\n",
      "Count of Unique Cat IDs per Age Group in Testing Set:\n",
      "{'adult': 20, 'kitten': 4, 'senior': 4}\n",
      "Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "No common groups found between train and test sets.\n",
      "Moved to Training/Validation Set:\n",
      "set()\n",
      "Removed from Training/Validation Set:\n",
      "set()\n",
      "Moved to Test Set:\n",
      "set()\n",
      "Removed from Test Set\n",
      "set()\n",
      "AFTER SWAP - Unique Training/Validation Group IDs:\n",
      "['000A' '000B' '001A' '002A' '002B' '003A' '004A' '005A' '006A' '007A'\n",
      " '009A' '010A' '011A' '012A' '013B' '015A' '016A' '018A' '019B' '021A'\n",
      " '022A' '023A' '024A' '025A' '025B' '025C' '026A' '026B' '026C' '028A'\n",
      " '029A' '032A' '033A' '035A' '038A' '039A' '040A' '042A' '043A' '044A'\n",
      " '045A' '046A' '047A' '048A' '049A' '050A' '051B' '053A' '054A' '056A'\n",
      " '057A' '058A' '061A' '062A' '063A' '064A' '065A' '066A' '067A' '068A'\n",
      " '069A' '072A' '073A' '074A' '075A' '076A' '087A' '088A' '091A' '092A'\n",
      " '093A' '094A' '097A' '099A' '100A' '103A' '104A' '106A' '108A' '109A'\n",
      " '113A' '115A' '116A' '117A']\n",
      "AFTER SWAP - Unique Test Group IDs:\n",
      "['008A' '014A' '014B' '019A' '020A' '023B' '027A' '031A' '034A' '036A'\n",
      " '037A' '041A' '051A' '052A' '055A' '059A' '060A' '070A' '071A' '090A'\n",
      " '095A' '096A' '097B' '101A' '102A' '105A' '110A' '111A']\n",
      "Length of X_train_val:\n",
      "709\n",
      "Length of y_train_val:\n",
      "709\n",
      "Length of groups_train_val:\n",
      "709\n",
      "No common groups found between train and test sets.\n",
      "Train Age Group Distribution BEFORE SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution BEFORE SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "Train Age Group Distribution AFTER SWAP:\n",
      "adult     432\n",
      "kitten    146\n",
      "senior    131\n",
      "Name: age_group, dtype: int64\n",
      "Testing Set Age Group Distribution AFTER SWAP:\n",
      "adult     156\n",
      "senior     47\n",
      "kitten     25\n",
      "Name: age_group, dtype: int64\n",
      "\n",
      " Starting training on unseen test set\n",
      "\n",
      "Age group distribution: Counter({0: 864, 1: 730, 2: 655})\n",
      "Epoch 1/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.9648 - accuracy: 0.5834\n",
      "Epoch 2/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.7321 - accuracy: 0.6870\n",
      "Epoch 3/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.7015 - accuracy: 0.7070\n",
      "Epoch 4/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.6269 - accuracy: 0.7390\n",
      "Epoch 5/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5930 - accuracy: 0.7523\n",
      "Epoch 6/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5785 - accuracy: 0.7546\n",
      "Epoch 7/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5440 - accuracy: 0.7746\n",
      "Epoch 8/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5275 - accuracy: 0.7830\n",
      "Epoch 9/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.5071 - accuracy: 0.7866\n",
      "Epoch 10/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4940 - accuracy: 0.7990\n",
      "Epoch 11/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4959 - accuracy: 0.8008\n",
      "Epoch 12/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4629 - accuracy: 0.8115\n",
      "Epoch 13/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4774 - accuracy: 0.8097\n",
      "Epoch 14/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4426 - accuracy: 0.8204\n",
      "Epoch 15/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4295 - accuracy: 0.8279\n",
      "Epoch 16/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4355 - accuracy: 0.8221\n",
      "Epoch 17/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4032 - accuracy: 0.8257\n",
      "Epoch 18/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3991 - accuracy: 0.8315\n",
      "Epoch 19/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4064 - accuracy: 0.8333\n",
      "Epoch 20/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3951 - accuracy: 0.8390\n",
      "Epoch 21/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3814 - accuracy: 0.8475\n",
      "Epoch 22/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.4010 - accuracy: 0.8341\n",
      "Epoch 23/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3742 - accuracy: 0.8537\n",
      "Epoch 24/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3640 - accuracy: 0.8564\n",
      "Epoch 25/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3504 - accuracy: 0.8524\n",
      "Epoch 26/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3396 - accuracy: 0.8675\n",
      "Epoch 27/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3451 - accuracy: 0.8666\n",
      "Epoch 28/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3460 - accuracy: 0.8595\n",
      "Epoch 29/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3480 - accuracy: 0.8590\n",
      "Epoch 30/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3338 - accuracy: 0.8697\n",
      "Epoch 31/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3408 - accuracy: 0.8617\n",
      "Epoch 32/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3106 - accuracy: 0.8791\n",
      "Epoch 33/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3217 - accuracy: 0.8666\n",
      "Epoch 34/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3246 - accuracy: 0.8684\n",
      "Epoch 35/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3291 - accuracy: 0.8711\n",
      "Epoch 36/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3118 - accuracy: 0.8764\n",
      "Epoch 37/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3260 - accuracy: 0.8737\n",
      "Epoch 38/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3232 - accuracy: 0.8684\n",
      "Epoch 39/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3049 - accuracy: 0.8799\n",
      "Epoch 40/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2946 - accuracy: 0.8795\n",
      "Epoch 41/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2779 - accuracy: 0.8933\n",
      "Epoch 42/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.3016 - accuracy: 0.8848\n",
      "Epoch 43/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2852 - accuracy: 0.8848\n",
      "Epoch 44/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2896 - accuracy: 0.8839\n",
      "Epoch 45/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2881 - accuracy: 0.8839\n",
      "Epoch 46/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2794 - accuracy: 0.8915\n",
      "Epoch 47/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2913 - accuracy: 0.8866\n",
      "Epoch 48/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2826 - accuracy: 0.8866\n",
      "Epoch 49/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2792 - accuracy: 0.8884\n",
      "Epoch 50/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2807 - accuracy: 0.8902\n",
      "Epoch 51/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2808 - accuracy: 0.8897\n",
      "Epoch 52/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2617 - accuracy: 0.8977\n",
      "Epoch 53/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2528 - accuracy: 0.9057\n",
      "Epoch 54/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2511 - accuracy: 0.9031\n",
      "Epoch 55/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2790 - accuracy: 0.8871\n",
      "Epoch 56/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2695 - accuracy: 0.8968\n",
      "Epoch 57/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2439 - accuracy: 0.9062\n",
      "Epoch 58/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2499 - accuracy: 0.9031\n",
      "Epoch 59/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2599 - accuracy: 0.9048\n",
      "Epoch 60/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2365 - accuracy: 0.9115\n",
      "Epoch 61/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2369 - accuracy: 0.9097\n",
      "Epoch 62/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2321 - accuracy: 0.9195\n",
      "Epoch 63/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2310 - accuracy: 0.9093\n",
      "Epoch 64/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2566 - accuracy: 0.9008\n",
      "Epoch 65/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2152 - accuracy: 0.9235\n",
      "Epoch 66/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2441 - accuracy: 0.9057\n",
      "Epoch 67/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2512 - accuracy: 0.9111\n",
      "Epoch 68/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2311 - accuracy: 0.9124\n",
      "Epoch 69/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2269 - accuracy: 0.9088\n",
      "Epoch 70/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2220 - accuracy: 0.9186\n",
      "Epoch 71/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2210 - accuracy: 0.9213\n",
      "Epoch 72/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2161 - accuracy: 0.9195\n",
      "Epoch 73/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2246 - accuracy: 0.9102\n",
      "Epoch 74/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2098 - accuracy: 0.9204\n",
      "Epoch 75/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2089 - accuracy: 0.9231\n",
      "Epoch 76/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2142 - accuracy: 0.9195\n",
      "Epoch 77/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2096 - accuracy: 0.9240\n",
      "Epoch 78/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2170 - accuracy: 0.9160\n",
      "Epoch 79/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2324 - accuracy: 0.9146\n",
      "Epoch 80/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2129 - accuracy: 0.9222\n",
      "Epoch 81/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2032 - accuracy: 0.9204\n",
      "Epoch 82/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1993 - accuracy: 0.9257\n",
      "Epoch 83/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2025 - accuracy: 0.9271\n",
      "Epoch 84/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1962 - accuracy: 0.9280\n",
      "Epoch 85/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2039 - accuracy: 0.9244\n",
      "Epoch 86/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1874 - accuracy: 0.9311\n",
      "Epoch 87/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2086 - accuracy: 0.9231\n",
      "Epoch 88/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2028 - accuracy: 0.9195\n",
      "Epoch 89/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2056 - accuracy: 0.9249\n",
      "Epoch 90/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2173 - accuracy: 0.9151\n",
      "Epoch 91/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2109 - accuracy: 0.9240\n",
      "Epoch 92/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2005 - accuracy: 0.9177\n",
      "Epoch 93/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2061 - accuracy: 0.9222\n",
      "Epoch 94/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2087 - accuracy: 0.9244\n",
      "Epoch 95/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2026 - accuracy: 0.9257\n",
      "Epoch 96/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1999 - accuracy: 0.9257\n",
      "Epoch 97/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1755 - accuracy: 0.9382\n",
      "Epoch 98/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2011 - accuracy: 0.9275\n",
      "Epoch 99/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1924 - accuracy: 0.9297\n",
      "Epoch 100/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1883 - accuracy: 0.9280\n",
      "Epoch 101/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1904 - accuracy: 0.9306\n",
      "Epoch 102/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.2044 - accuracy: 0.9217\n",
      "Epoch 103/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1861 - accuracy: 0.9337\n",
      "Epoch 104/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1840 - accuracy: 0.9324\n",
      "Epoch 105/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1780 - accuracy: 0.9337\n",
      "Epoch 106/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1812 - accuracy: 0.9364\n",
      "Epoch 107/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1633 - accuracy: 0.9444\n",
      "Epoch 108/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1678 - accuracy: 0.9378\n",
      "Epoch 109/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1920 - accuracy: 0.9235\n",
      "Epoch 110/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1742 - accuracy: 0.9346\n",
      "Epoch 111/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9364\n",
      "Epoch 112/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1732 - accuracy: 0.9395\n",
      "Epoch 113/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1744 - accuracy: 0.9342\n",
      "Epoch 114/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1737 - accuracy: 0.9369\n",
      "Epoch 115/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.9422\n",
      "Epoch 116/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1623 - accuracy: 0.9431\n",
      "Epoch 117/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1716 - accuracy: 0.9373\n",
      "Epoch 118/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1749 - accuracy: 0.9329\n",
      "Epoch 119/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1731 - accuracy: 0.9382\n",
      "Epoch 120/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1671 - accuracy: 0.9351\n",
      "Epoch 121/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1505 - accuracy: 0.9458\n",
      "Epoch 122/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1620 - accuracy: 0.9391\n",
      "Epoch 123/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1710 - accuracy: 0.9364\n",
      "Epoch 124/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1495 - accuracy: 0.9413\n",
      "Epoch 125/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1664 - accuracy: 0.9329\n",
      "Epoch 126/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1547 - accuracy: 0.9449\n",
      "Epoch 127/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1669 - accuracy: 0.9346\n",
      "Epoch 128/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1703 - accuracy: 0.9324\n",
      "Epoch 129/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1455 - accuracy: 0.9484\n",
      "Epoch 130/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1465 - accuracy: 0.9480\n",
      "Epoch 131/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1739 - accuracy: 0.9369\n",
      "Epoch 132/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1524 - accuracy: 0.9449\n",
      "Epoch 133/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1796 - accuracy: 0.9320\n",
      "Epoch 134/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1507 - accuracy: 0.9462\n",
      "Epoch 135/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1478 - accuracy: 0.9484\n",
      "Epoch 136/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1578 - accuracy: 0.9400\n",
      "Epoch 137/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1560 - accuracy: 0.9426\n",
      "Epoch 138/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1476 - accuracy: 0.9515\n",
      "Epoch 139/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9480\n",
      "Epoch 140/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1588 - accuracy: 0.9395\n",
      "Epoch 141/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1597 - accuracy: 0.9395\n",
      "Epoch 142/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1591 - accuracy: 0.9382\n",
      "Epoch 143/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1502 - accuracy: 0.9489\n",
      "Epoch 144/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1440 - accuracy: 0.9484\n",
      "Epoch 145/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9426\n",
      "Epoch 146/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1565 - accuracy: 0.9369\n",
      "Epoch 147/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1417 - accuracy: 0.9462\n",
      "Epoch 148/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1442 - accuracy: 0.9493\n",
      "Epoch 149/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1513 - accuracy: 0.9458\n",
      "Epoch 150/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1573 - accuracy: 0.9435\n",
      "Epoch 151/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1423 - accuracy: 0.9542\n",
      "Epoch 152/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1436 - accuracy: 0.9480\n",
      "Epoch 153/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1282 - accuracy: 0.9555\n",
      "Epoch 154/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1521 - accuracy: 0.9480\n",
      "Epoch 155/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1312 - accuracy: 0.9555\n",
      "Epoch 156/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1394 - accuracy: 0.9502\n",
      "Epoch 157/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1452 - accuracy: 0.9466\n",
      "Epoch 158/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1553 - accuracy: 0.9435\n",
      "Epoch 159/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1384 - accuracy: 0.9493\n",
      "Epoch 160/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1483 - accuracy: 0.9484\n",
      "Epoch 161/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1375 - accuracy: 0.9502\n",
      "Epoch 162/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1516 - accuracy: 0.9440\n",
      "Epoch 163/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1427 - accuracy: 0.9444\n",
      "Epoch 164/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1360 - accuracy: 0.9462\n",
      "Epoch 165/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1219 - accuracy: 0.9533\n",
      "Epoch 166/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1185 - accuracy: 0.9604\n",
      "Epoch 167/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1316 - accuracy: 0.9506\n",
      "Epoch 168/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9573\n",
      "Epoch 169/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1236 - accuracy: 0.9586\n",
      "Epoch 170/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1306 - accuracy: 0.9529\n",
      "Epoch 171/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1325 - accuracy: 0.9511\n",
      "Epoch 172/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1422 - accuracy: 0.9471\n",
      "Epoch 173/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1339 - accuracy: 0.9524\n",
      "Epoch 174/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1350 - accuracy: 0.9480\n",
      "Epoch 175/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1352 - accuracy: 0.9551\n",
      "Epoch 176/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1182 - accuracy: 0.9582\n",
      "Epoch 177/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1288 - accuracy: 0.9529\n",
      "Epoch 178/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9533\n",
      "Epoch 179/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1368 - accuracy: 0.9538\n",
      "Epoch 180/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1254 - accuracy: 0.9520\n",
      "Epoch 181/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1434 - accuracy: 0.9422\n",
      "Epoch 182/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1333 - accuracy: 0.9493\n",
      "Epoch 183/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1181 - accuracy: 0.9618\n",
      "Epoch 184/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1231 - accuracy: 0.9515\n",
      "Epoch 185/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1273 - accuracy: 0.9551\n",
      "Epoch 186/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1226 - accuracy: 0.9578\n",
      "Epoch 187/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1334 - accuracy: 0.9511\n",
      "Epoch 188/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9546\n",
      "Epoch 189/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1265 - accuracy: 0.9498\n",
      "Epoch 190/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1100 - accuracy: 0.9613\n",
      "Epoch 191/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1193 - accuracy: 0.9573\n",
      "Epoch 192/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1163 - accuracy: 0.9591\n",
      "Epoch 193/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9578\n",
      "Epoch 194/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9595\n",
      "Epoch 195/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1071 - accuracy: 0.9631\n",
      "Epoch 196/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1110 - accuracy: 0.9649\n",
      "Epoch 197/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1183 - accuracy: 0.9604\n",
      "Epoch 198/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1248 - accuracy: 0.9560\n",
      "Epoch 199/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9520\n",
      "Epoch 200/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1146 - accuracy: 0.9591\n",
      "Epoch 201/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1051 - accuracy: 0.9644\n",
      "Epoch 202/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1087 - accuracy: 0.9649\n",
      "Epoch 203/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1158 - accuracy: 0.9600\n",
      "Epoch 204/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1169 - accuracy: 0.9586\n",
      "Epoch 205/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1354 - accuracy: 0.9506\n",
      "Epoch 206/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1274 - accuracy: 0.9560\n",
      "Epoch 207/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1202 - accuracy: 0.9595\n",
      "Epoch 208/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1246 - accuracy: 0.9564\n",
      "Epoch 209/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1346 - accuracy: 0.9529\n",
      "Epoch 210/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1125 - accuracy: 0.9600\n",
      "Epoch 211/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9569\n",
      "Epoch 212/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1083 - accuracy: 0.9631\n",
      "Epoch 213/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1230 - accuracy: 0.9546\n",
      "Epoch 214/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1198 - accuracy: 0.9618\n",
      "Epoch 215/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1085 - accuracy: 0.9595\n",
      "Epoch 216/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1129 - accuracy: 0.9644\n",
      "Epoch 217/1500\n",
      "71/71 [==============================] - 0s 5ms/step - loss: 0.1245 - accuracy: 0.9520\n",
      "Epoch 218/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1084 - accuracy: 0.9627\n",
      "Epoch 219/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0904 - accuracy: 0.9720\n",
      "Epoch 220/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1286 - accuracy: 0.9533\n",
      "Epoch 221/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1092 - accuracy: 0.9649\n",
      "Epoch 222/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1142 - accuracy: 0.9569\n",
      "Epoch 223/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1303 - accuracy: 0.9520\n",
      "Epoch 224/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1067 - accuracy: 0.9604\n",
      "Epoch 225/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9618\n",
      "Epoch 226/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1017 - accuracy: 0.9662\n",
      "Epoch 227/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1073 - accuracy: 0.9635\n",
      "Epoch 228/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9578\n",
      "Epoch 229/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1054 - accuracy: 0.9622\n",
      "Epoch 230/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0874 - accuracy: 0.9684\n",
      "Epoch 231/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1108 - accuracy: 0.9564\n",
      "Epoch 232/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1249 - accuracy: 0.9582\n",
      "Epoch 233/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1036 - accuracy: 0.9595\n",
      "Epoch 234/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1103 - accuracy: 0.9595\n",
      "Epoch 235/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1165 - accuracy: 0.9564\n",
      "Epoch 236/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1008 - accuracy: 0.9653\n",
      "Epoch 237/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9573\n",
      "Epoch 238/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1055 - accuracy: 0.9627\n",
      "Epoch 239/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1080 - accuracy: 0.9640\n",
      "Epoch 240/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1076 - accuracy: 0.9604\n",
      "Epoch 241/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1180 - accuracy: 0.9604\n",
      "Epoch 242/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1099 - accuracy: 0.9613\n",
      "Epoch 243/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1032 - accuracy: 0.9671\n",
      "Epoch 244/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1209 - accuracy: 0.9564\n",
      "Epoch 245/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1091 - accuracy: 0.9600\n",
      "Epoch 246/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1010 - accuracy: 0.9640\n",
      "Epoch 247/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0996 - accuracy: 0.9667\n",
      "Epoch 248/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1113 - accuracy: 0.9609\n",
      "Epoch 249/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1284 - accuracy: 0.9511\n",
      "Epoch 250/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1116 - accuracy: 0.9609\n",
      "Epoch 251/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1118 - accuracy: 0.9573\n",
      "Epoch 252/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1090 - accuracy: 0.9649\n",
      "Epoch 253/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1136 - accuracy: 0.9564\n",
      "Epoch 254/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1068 - accuracy: 0.9631\n",
      "Epoch 255/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1102 - accuracy: 0.9595\n",
      "Epoch 256/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0923 - accuracy: 0.9698\n",
      "Epoch 257/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.1095 - accuracy: 0.9613\n",
      "Epoch 258/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0979 - accuracy: 0.9689\n",
      "Epoch 259/1500\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0885 - accuracy: 0.9689\n",
      "Epoch 260/1500\n",
      "44/71 [=================>............] - ETA: 0s - loss: 0.0931 - accuracy: 0.9688Restoring model weights from the end of the best epoch: 230.\n",
      "71/71 [==============================] - 0s 1ms/step - loss: 0.0932 - accuracy: 0.9689\n",
      "Epoch 260: early stopping\n",
      "8/8 [==============================] - 0s 873us/step - loss: 0.6172 - accuracy: 0.7939\n",
      "8/8 [==============================] - 0s 812us/step\n",
      "Majority Vote Accuracy for cat_id for this fold: 0.86 (24/28)\n",
      "Before appending - Cat IDs: 613, Predictions: 613, Actuals: 613, Gender: 613\n",
      "After appending - Cat IDs: 841, Predictions: 841, Actuals: 841, Gender: 841\n",
      "Final Test Results - Loss: 0.6172388195991516, Accuracy: 0.7938596606254578, Precision: 0.7593211313611464, Recall: 0.7817403164211676, F1 Score: 0.7695489124611593\n",
      "Confusion Matrix:\n",
      " [[129   4  23]\n",
      " [  3  22   0]\n",
      " [ 17   0  30]]\n",
      "\n",
      "Final Average F1-Score across all UNSEEN TEST sets: 0.708266853683779\n",
      "\n",
      "Final Average Loss across all UNSEEN TEST sets: 0.8376880586147308\n",
      "\n",
      "Final Average Accuracy across all UNSEEN TEST sets: 0.7451198548078537\n",
      "\n",
      "Final Average Precision across all UNSEEN TEST sets: 0.7155163537853785\n",
      "\n",
      "Final Average Recall across all UNSEEN TEST sets: 0.7461552982641577\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import numpy as np\n",
    "import optuna\n",
    "\n",
    "all_cat_ids = []\n",
    "all_predicted_age_groups = []\n",
    "all_actual_age_groups = []\n",
    "all_gender = []\n",
    "\n",
    "# Define the StratifiedGroupKFold splitter for 4 fold CV with random shuffling\n",
    "outer_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=int(random_seeds[4]))\n",
    "\n",
    "# unseen test set metrics\n",
    "unseen_losses, unseen_accuracies, unseen_precisions, unseen_recalls, unseen_f1 = [], [], [], [], []\n",
    "\n",
    "outer_fold = 0\n",
    "\n",
    "# Use the splitter to generate indices for training and testing sets\n",
    "# Note: GroupShuffleSplit.split returns indices, so we use it to index the arrays\n",
    "for train_val_idx, test_idx in outer_cv.split(X, y, groups):\n",
    "    outer_fold += 1\n",
    "    logger(f\"outer_fold {outer_fold}\", file=log_file_path)\n",
    "\n",
    "    # Convert indices back to DataFrame for easy manipulation\n",
    "    df_train_val = dataframe.iloc[train_val_idx]\n",
    "    df_test = dataframe.iloc[test_idx]\n",
    "\n",
    "    ##############################\n",
    "    # ASSESSING SPLITS BY CAT_ID #\n",
    "    ##############################\n",
    "    \n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test['cat_id'].value_counts()  \n",
    "    \n",
    "    # Log or print the distribution\n",
    "    logger(f\"Train Set Group Distribution:\\n{training_validation_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Group Distribution:\\n{testing_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test['gender'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Training Set Gender Distribution BEFORE SWAP:\\n{training_gender_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Gender Distribution BEFORE SWAP:\\n{testing_gender_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Group by 'age_group' and then list unique 'cat_id' within each age group\n",
    "    unique_cat_ids_per_age_group_train_val = df_train_val.groupby('age_group')['cat_id'].unique()\n",
    "    unique_cat_ids_per_age_group_test = df_test.groupby('age_group')['cat_id'].unique()\n",
    "    \n",
    "    # Log results\n",
    "    logger(f\"Unique Cat IDs per Age Group in Training/Validation Set:\\n{unique_cat_ids_per_age_group_train_val}\", file=log_file_path)\n",
    "    logger(f\"Unique Cat IDs per Age Group in Testing Set:\\n{unique_cat_ids_per_age_group_test}\", file=log_file_path)\n",
    "\n",
    "    # Calculate the count of unique identifiers per age group for training and testing set\n",
    "    counts_train_val = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_train_val.items()}\n",
    "    counts_test = {age_group: len(identifiers) for age_group, identifiers in unique_cat_ids_per_age_group_test.items()}\n",
    "\n",
    "    # Log the counts of unique identifiers per age group\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Training/Validation Set:\\n{counts_train_val}\", file=log_file_path)\n",
    "    logger(f\"Count of Unique Cat IDs per Age Group in Testing Set:\\n{counts_test}\", file=log_file_path)\n",
    "\n",
    "    #######################################################\n",
    "    # CONTINUE WITH ENSURING VALID AND APPROPRIATE SPLITS #\n",
    "    #######################################################\n",
    "    \n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    groups_train_val, groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # logging identifier splits \n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "\n",
    "    # check group splits\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Specify the cat_ids that must be in the training/validation set\n",
    "    specific_cat_ids = ['000A', '046A']\n",
    "    \n",
    "    # Perform the swapping operation\n",
    "    train_val_idx, test_idx = swap_cat_id_instances(dataframe, train_val_idx, test_idx, specific_cat_ids)\n",
    "    \n",
    "    # Re-assign the sets based on the updated indices\n",
    "    X_train_val, X_test = X[train_val_idx], X[test_idx]\n",
    "    y_train_val, y_test = y[train_val_idx], y[test_idx]\n",
    "    new_groups_train_val, new_groups_test = groups[train_val_idx], groups[test_idx]\n",
    "\n",
    "    # Find differences for training and test sets\n",
    "    moved_to_train_val, removed_from_train_val = find_group_differences(groups_train_val, new_groups_train_val)\n",
    "    moved_to_test, removed_from_test = find_group_differences(groups_test, new_groups_test)\n",
    "    \n",
    "    # Display the results\n",
    "    logger(f\"Moved to Training/Validation Set:\\n{moved_to_train_val}\", file=log_file_path)\n",
    "    logger(f\"Removed from Training/Validation Set:\\n{removed_from_train_val}\", file=log_file_path)\n",
    "    logger(f\"Moved to Test Set:\\n{moved_to_test}\", file=log_file_path)\n",
    "    logger(f\"Removed from Test Set\\n{removed_from_test}\", file=log_file_path)\n",
    "\n",
    "    # Update X_train_val, X_test, y_train_val, y_test, groups_train_val, groups_test based on updated indices\n",
    "    X_train_val = X[train_val_idx]\n",
    "    y_train_val = y[train_val_idx]\n",
    "    groups_train_val = groups[train_val_idx]\n",
    "    \n",
    "    X_test = X[test_idx]\n",
    "    y_test = y[test_idx]\n",
    "    groups_test = groups[test_idx]\n",
    "\n",
    "    # logging identifier splits again after potential swaps\n",
    "    unique_train_val_groups = np.unique(groups_train_val)\n",
    "    unique_test_groups = np.unique(groups_test)\n",
    "    \n",
    "    logger(f\"AFTER SWAP - Unique Training/Validation Group IDs:\\n{unique_train_val_groups}\", file=log_file_path)\n",
    "    logger(f\"AFTER SWAP - Unique Test Group IDs:\\n{unique_test_groups}\", file=log_file_path)\n",
    "    \n",
    "    # Verify the lengths are consistent\n",
    "    logger(f\"Length of X_train_val:\\n{len(X_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of y_train_val:\\n{len(y_train_val)}\", file=log_file_path)\n",
    "    logger(f\"Length of groups_train_val:\\n{len(groups_train_val)}\", file=log_file_path)\n",
    "\n",
    "    # Check group splits once more\n",
    "    check_initial_group_split(groups_train_val, groups_test)\n",
    "\n",
    "    # Convert the modified indices back to a DataFrame representing the updated df_train_val\n",
    "    df_train_val_updated = dataframe.iloc[train_val_idx].copy()\n",
    "    df_test_updated = dataframe.iloc[test_idx].copy()\n",
    "\n",
    "    ############################\n",
    "    # LOGGING AGE GROUP SPLITS #\n",
    "    ############################\n",
    "\n",
    "    # Get the distribution of age groups of age groups before the update\n",
    "    training_validation_age_group_distribution = df_train_val['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test['age_group'].value_counts()\n",
    "    \n",
    "    # Logthe distribution\n",
    "    logger(f\"Train Age Group Distribution BEFORE SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution BEFORE SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "\n",
    "    # Get the distribution of age groups after the update\n",
    "    training_validation_age_group_distribution = df_train_val_updated['age_group'].value_counts()\n",
    "    testing_age_group_distribution = df_test_updated['age_group'].value_counts()\n",
    "    \n",
    "    # Log the distribution\n",
    "    logger(f\"Train Age Group Distribution AFTER SWAP:\\n{training_validation_age_group_distribution}\", file=log_file_path)\n",
    "    logger(f\"Testing Set Age Group Distribution AFTER SWAP:\\n{testing_age_group_distribution}\", file=log_file_path)\n",
    "    \n",
    "    ########################################################\n",
    "    # TRACKING FINAL CAT_IDs & GENDER AFTER REDISTRIBUTION #\n",
    "    ########################################################\n",
    "\n",
    "    # Get the distribution of groups\n",
    "    training_validation_group_distribution = df_train_val_updated['cat_id'].value_counts()  \n",
    "    testing_group_distribution = df_test_updated['cat_id'].value_counts()  \n",
    "\n",
    "    # Log gender distribution in training and testing datasets\n",
    "    training_gender_distribution = df_train_val_updated['gender'].value_counts()\n",
    "    testing_gender_distribution = df_test_updated['gender'].value_counts()\n",
    "    \n",
    "    logger(f\"\\n Starting training on unseen test set\\n\", file=log_file_path)\n",
    "\n",
    "    ######################\n",
    "    # ADD AUGMENTED DATA #\n",
    "    ######################\n",
    "\n",
    "    # Filter out augmented data derived from groups in the training set\n",
    "    augmented_df_filtered = augmented_df[augmented_df['cat_id'].isin(groups_train_val)]\n",
    "    \n",
    "    # Convert augmented_df_filtered to numpy array (excluding target and other columns)\n",
    "    augmented_array = augmented_df_filtered.drop(columns=['target', 'cat_id', 'age_group']).values\n",
    "    \n",
    "    # Concatenate the filtered augmented data with X_train_val\n",
    "    X_train_val = np.concatenate([X_train_val, augmented_array])\n",
    "\n",
    "    # Extract target labels from the filtered augmented DataFrame\n",
    "    y_augmented = augmented_df_filtered['age_group'].values\n",
    "    \n",
    "    # Encode the labels using the same LabelEncoder instance\n",
    "    y_augmented_encoded = label_encoder.transform(y_augmented)\n",
    "    \n",
    "    # Concatenate the encoded augmented labels with y_train_val\n",
    "    y_train_val = np.concatenate([y_train_val, y_augmented_encoded])\n",
    "\n",
    "    # Calculate the distribution of age groups in y_train_val\n",
    "    age_group_distribution = Counter(y_train_val)\n",
    "    print(\"Age group distribution:\", age_group_distribution)\n",
    "\n",
    "    # EarlyStopping callback: monitor 'loss' instead of 'val_loss' for the test set\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='loss',  \n",
    "        min_delta=0.001, \n",
    "        patience=30,  \n",
    "        verbose=1,  \n",
    "        restore_best_weights=True  \n",
    "    )\n",
    "\n",
    "    # One final shuffle to ensure randomness\n",
    "    outer_shuffle_idx = np.random.permutation(len(X_train_val))\n",
    "    X_train_val = X_train_val[outer_shuffle_idx]\n",
    "    y_train_val = y_train_val[outer_shuffle_idx]\n",
    "    \n",
    "    # Scale the features\n",
    "    scaler_full = StandardScaler().fit(X_train_val)\n",
    "    X_train_full_scaled = scaler_full.transform(X_train_val)\n",
    "    X_test_scaled = scaler_full.transform(X_test)\n",
    "    \n",
    "    # Encode the labels\n",
    "    y_train_full_encoded = to_categorical(y_train_val)\n",
    "    y_test_encoded = to_categorical(y_test)\n",
    "\n",
    "    #######################\n",
    "    # BUILD & TRAIN MODEL #\n",
    "    #######################\n",
    "\n",
    "    optimizers = {\n",
    "        'Adamax': Adamax(learning_rate=0.00038188800331973483)\n",
    "    }\n",
    "    \n",
    "    # Full model definition with dynamic number of layers\n",
    "    model_full = Sequential()\n",
    "    model_full.add(Dense(480, activation='relu', input_shape=(X_train_full_scaled.shape[1],)))  # units_l0 and activation from parameters\n",
    "    model_full.add(BatchNormalization())\n",
    "    model_full.add(Dropout(0.27188281261238406))  \n",
    "    model_full.add(Dense(3, activation='softmax'))  \n",
    "    \n",
    "    optimizer = optimizers['Adamax']  # optimizer_key from parameters\n",
    "    \n",
    "    # Compile the model\n",
    "    model_full.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model on the full training set\n",
    "    history_full = model_full.fit(X_train_full_scaled, y_train_full_encoded, epochs=1500, batch_size=32,\n",
    "                                  verbose=1, callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluate the model on the held-out test set\n",
    "    test_loss, test_accuracy = model_full.evaluate(X_test_scaled, y_test_encoded)\n",
    "\n",
    "    y_test_pred_prob = model_full.predict(X_test_scaled)\n",
    "    y_test_pred = np.argmax(y_test_pred_prob, axis=1)  \n",
    "    y_test_true = np.argmax(y_test_encoded, axis=1)    \n",
    "\n",
    "    ################################\n",
    "    # LOG MAJORITY VOTE PER CAT_ID #\n",
    "    ################################\n",
    "\n",
    "    # Convert numeric predictions back to age group labels\n",
    "    predicted_age_groups = label_encoder.inverse_transform(y_test_pred)\n",
    "    actual_labels = label_encoder.inverse_transform(y_test_true)\n",
    "    \n",
    "    # Map predictions and actual labels back to cat_ids\n",
    "    test_results = pd.DataFrame({\n",
    "        'cat_id': df_test_updated['cat_id'],\n",
    "        'predicted_age_group': predicted_age_groups,\n",
    "        'actual_age_group': actual_labels\n",
    "    })\n",
    "    \n",
    "    # Group by cat_id and aggregate predicted age groups\n",
    "    majority_votes = test_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "    actual_groups = test_results.groupby('cat_id')['actual_age_group'].first()\n",
    "    \n",
    "    # Calculate the accuracy of majority voting\n",
    "    correct_predictions = (majority_votes == actual_groups).sum()\n",
    "    total_cats = len(actual_groups)\n",
    "    majority_vote_accuracy = correct_predictions / total_cats\n",
    "    \n",
    "    # Log the accuracy of the majority voting\n",
    "    logger(f\"Majority Vote Accuracy for cat_id for this fold: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)\n",
    "\n",
    "    ####################################################\n",
    "    # COLLECT PREDICTIONS FOR GENDER & CAT_ID ANALYSIS #\n",
    "    ####################################################\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'Before appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Extend lists with current fold results\n",
    "    all_cat_ids.extend(df_test_updated['cat_id'].tolist())\n",
    "    all_predicted_age_groups.extend(predicted_age_groups)\n",
    "    all_actual_age_groups.extend(actual_labels)\n",
    "    all_gender.extend(df_test_updated['gender'].tolist())\n",
    "\n",
    "    # Debugging print statements\n",
    "    print(f'After appending - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')\n",
    "    \n",
    "    # Calculate additional metrics\n",
    "    test_precision = precision_score(y_test_true, y_test_pred, average='macro')  # Use 'macro' for imbalanced datasets\n",
    "    test_recall = recall_score(y_test_true, y_test_pred, average='macro')\n",
    "    test_f1 = f1_score(y_test_true, y_test_pred, average='macro')\n",
    "\n",
    "    # prepare averages of outer metrics\n",
    "    unseen_f1.append(test_f1)\n",
    "    unseen_losses.append(test_loss)\n",
    "    unseen_accuracies.append(test_accuracy)\n",
    "    unseen_precisions.append(test_precision)\n",
    "    unseen_recalls.append(test_recall)\n",
    "\n",
    "    # Print final test results\n",
    "    logger(f\"Final Test Results - Loss: {test_loss}, Accuracy: {test_accuracy}, Precision: {test_precision}, Recall: {test_recall}, F1 Score: {test_f1}\", file=log_file_path)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    logger(f\"Confusion Matrix:\\n {cm}\", file=log_file_path)\n",
    "\n",
    "# Calculate the overall average metrics\n",
    "unseen_set_avg_f1 = np.mean(unseen_f1)\n",
    "unseen_set_avg_loss = np.mean(unseen_losses)\n",
    "unseen_set_avg_acc = np.mean(unseen_accuracies)\n",
    "unseen_set_avg_precision = np.mean(unseen_precisions)\n",
    "unseen_set_avg_recall = np.mean(unseen_recalls)\n",
    "\n",
    "logger(f\"\\nFinal Average F1-Score across all UNSEEN TEST sets: {unseen_set_avg_f1}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Loss across all UNSEEN TEST sets: {unseen_set_avg_loss}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Accuracy across all UNSEEN TEST sets: {unseen_set_avg_acc}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Precision across all UNSEEN TEST sets: {unseen_set_avg_precision}\", file=log_file_path)\n",
    "logger(f\"\\nFinal Average Recall across all UNSEEN TEST sets: {unseen_set_avg_recall}\\n\", file=log_file_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b90d80b-198d-4293-a1a0-73a65f6588d0",
   "metadata": {},
   "source": [
    "## Majority Voting on Total Entries per cat_id "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e9dd5399-64d4-435b-9e73-cb31f16d042d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking - Cat IDs: 841, Predictions: 841, Actuals: 841, Gender: 841\n"
     ]
    }
   ],
   "source": [
    "# Debugging print statements\n",
    "print(f'Checking - Cat IDs: {len(all_cat_ids)}, Predictions: {len(all_predicted_age_groups)}, Actuals: {len(all_actual_age_groups)}, Gender: {len(all_gender)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9d9869f0-e23f-4453-a329-395fa44f1208",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Create results df\n",
    "full_results = pd.DataFrame({\n",
    "    'cat_id': all_cat_ids,\n",
    "    'predicted_age_group': all_predicted_age_groups,\n",
    "    'actual_age_group': all_actual_age_groups,\n",
    "    'all_gender': all_gender\n",
    "})\n",
    "\n",
    "# create a correct majority prediction column\n",
    "full_results['correct'] = full_results['predicted_age_group'] == full_results['actual_age_group']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a7f81185-7b51-497f-98cb-80c4b8f35388",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Majority Vote Accuracy for cat_id: 0.79 (87/110)\n"
     ]
    }
   ],
   "source": [
    "# Group by cat_id and aggregate predicted age groups\n",
    "majority_votes = full_results.groupby('cat_id')['predicted_age_group'].agg(lambda x: x.mode()[0])\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first()\n",
    "\n",
    "# Calculate the accuracy of majority voting\n",
    "correct_predictions = (majority_votes == actual_groups).sum()\n",
    "total_cats = len(actual_groups)\n",
    "majority_vote_accuracy = correct_predictions / total_cats\n",
    "\n",
    "logger(f\"Overall Majority Vote Accuracy for cat_id: {majority_vote_accuracy:.2f} ({correct_predictions}/{total_cats})\", file=log_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "adbd2ca0-3dea-4b42-bfad-93138cb1ae30",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Detailed df with predictions, actual labels, and comparison including majority vote\n",
    "detailed_results = full_results.groupby('cat_id').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'Predictions': list(x['predicted_age_group']),\n",
    "        'Majority Vote': x['predicted_age_group'].mode()[0],\n",
    "        'Actual Age Group': x['actual_age_group'].iloc[0],\n",
    "        'Correct Majority Vote': x['predicted_age_group'].mode()[0] == x['actual_age_group'].iloc[0]\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Convert to DataFrame for better formatting and display\n",
    "detailed_results = pd.DataFrame(detailed_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b1ea4fe7-6ee1-4185-a009-b864d9aa6b85",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat_id</th>\n",
       "      <th>Predictions</th>\n",
       "      <th>Majority Vote</th>\n",
       "      <th>Actual Age Group</th>\n",
       "      <th>Correct Majority Vote</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000B</td>\n",
       "      <td>[adult, adult, kitten, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>062A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>072A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>071A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>070A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>069A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>068A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>067A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>066A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>065A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, senior, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>064A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>059A</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>074A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>058A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>056A</td>\n",
       "      <td>[senior, senior, adult]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>055A</td>\n",
       "      <td>[adult, senior, senior, adult, adult, adult, s...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>053A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>051A</td>\n",
       "      <td>[senior, senior, adult, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001A</td>\n",
       "      <td>[adult, adult, senior, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>049A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>045A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>044A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>073A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>075A</td>\n",
       "      <td>[adult, adult, senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>042A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, adult, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>101A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>116A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>115A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>113A</td>\n",
       "      <td>[senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>111A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>108A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>105A</td>\n",
       "      <td>[adult, adult, adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>104A</td>\n",
       "      <td>[senior, senior, adult, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>103A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, senior, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>102A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>100A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>076A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>099A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>097B</td>\n",
       "      <td>[senior, adult, kitten, adult, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>096A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>095A</td>\n",
       "      <td>[senior, adult, adult, senior, senior, adult, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>094A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>091A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>090A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>088A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>087A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>043A</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>050A</td>\n",
       "      <td>[kitten, adult, kitten, kitten, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>041A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>025C</td>\n",
       "      <td>[adult, senior, adult, senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>023B</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>040A</td>\n",
       "      <td>[kitten, kitten, kitten, kitten, kitten, kitte...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>022A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ki...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>021A</td>\n",
       "      <td>[adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>020A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>019B</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>019A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>018A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>016A</td>\n",
       "      <td>[senior, adult, senior, adult, senior, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>015A</td>\n",
       "      <td>[adult, adult, senior, adult, senior, adult, s...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>014B</td>\n",
       "      <td>[kitten, kitten, kitten, adult, kitten, kitten...</td>\n",
       "      <td>kitten</td>\n",
       "      <td>kitten</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>014A</td>\n",
       "      <td>[adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>013B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>012A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>010A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>009A</td>\n",
       "      <td>[senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>008A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>007A</td>\n",
       "      <td>[adult, senior, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>006A</td>\n",
       "      <td>[senior, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>004A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>003A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>002B</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>002A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>025A</td>\n",
       "      <td>[senior, adult, adult, adult, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>023A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>117A</td>\n",
       "      <td>[senior, senior, adult, senior, adult, senior,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>senior</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>031A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>034A</td>\n",
       "      <td>[adult, senior, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>026A</td>\n",
       "      <td>[adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>033A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>032A</td>\n",
       "      <td>[kitten, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>037A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>029A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>028A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>027A</td>\n",
       "      <td>[adult, adult, adult, adult, senior, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>039A</td>\n",
       "      <td>[adult, adult, adult, adult, adult, adult, adu...</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>035A</td>\n",
       "      <td>[kitten, adult, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>adult</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>110A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>109A</td>\n",
       "      <td>[adult, adult, kitten, kitten, adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>051B</td>\n",
       "      <td>[senior, adult, adult, adult, senior, senior, ...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>005A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>106A</td>\n",
       "      <td>[adult, adult, senior, senior, adult, adult, a...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>048A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>047A</td>\n",
       "      <td>[adult, adult, adult, kitten, kitten, adult, k...</td>\n",
       "      <td>adult</td>\n",
       "      <td>kitten</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>038A</td>\n",
       "      <td>[kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>036A</td>\n",
       "      <td>[senior, senior, senior, senior, adult, adult,...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>025B</td>\n",
       "      <td>[senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>052A</td>\n",
       "      <td>[adult, senior, senior, senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>011A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>054A</td>\n",
       "      <td>[adult, senior]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>057A</td>\n",
       "      <td>[adult, adult, adult, senior, adult, adult, ad...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>093A</td>\n",
       "      <td>[adult, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>092A</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>060A</td>\n",
       "      <td>[adult, kitten, kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>061A</td>\n",
       "      <td>[senior, adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>063A</td>\n",
       "      <td>[senior, senior, senior, senior, senior, senio...</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>026C</td>\n",
       "      <td>[kitten]</td>\n",
       "      <td>kitten</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>026B</td>\n",
       "      <td>[senior]</td>\n",
       "      <td>senior</td>\n",
       "      <td>adult</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>024A</td>\n",
       "      <td>[adult]</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>097A</td>\n",
       "      <td>[senior, senior, senior, adult, adult, senior,...</td>\n",
       "      <td>adult</td>\n",
       "      <td>senior</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat_id                                        Predictions Majority Vote Actual Age Group  Correct Majority Vote\n",
       "0     000B  [adult, adult, kitten, senior, adult, adult, a...         adult            adult                   True\n",
       "68    062A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "78    072A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "77    071A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "76    070A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "75    069A                                     [adult, adult]         adult            adult                   True\n",
       "74    068A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "73    067A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "72    066A                                            [adult]         adult            adult                   True\n",
       "71    065A  [senior, adult, adult, adult, adult, senior, s...         adult            adult                   True\n",
       "70    064A                              [adult, adult, adult]         adult            adult                   True\n",
       "65    059A  [senior, adult, adult, adult, senior, senior, ...        senior           senior                   True\n",
       "80    074A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "64    058A                            [senior, senior, adult]        senior           senior                   True\n",
       "62    056A                            [senior, senior, adult]        senior           senior                   True\n",
       "61    055A  [adult, senior, senior, adult, adult, adult, s...        senior           senior                   True\n",
       "59    053A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "56    051A  [senior, senior, adult, adult, senior, senior,...        senior           senior                   True\n",
       "1     001A  [adult, adult, senior, adult, adult, adult, ad...         adult            adult                   True\n",
       "54    049A                                           [kitten]        kitten           kitten                   True\n",
       "51    045A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "50    044A           [kitten, kitten, kitten, kitten, kitten]        kitten           kitten                   True\n",
       "79    073A                                            [adult]         adult            adult                   True\n",
       "81    075A               [adult, adult, senior, adult, adult]         adult            adult                   True\n",
       "48    042A  [kitten, kitten, kitten, kitten, adult, kitten...        kitten           kitten                   True\n",
       "96    101A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "108   116A  [senior, senior, senior, senior, adult, senior...        senior           senior                   True\n",
       "107   115A                                           [kitten]        kitten           kitten                   True\n",
       "106   113A                           [senior, senior, senior]        senior           senior                   True\n",
       "105   111A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "102   108A    [senior, senior, senior, senior, adult, senior]        senior           senior                   True\n",
       "100   105A                      [adult, adult, adult, senior]         adult            adult                   True\n",
       "99    104A                    [senior, senior, adult, senior]        senior           senior                   True\n",
       "98    103A  [adult, adult, adult, adult, senior, senior, a...         adult            adult                   True\n",
       "97    102A                                     [adult, adult]         adult            adult                   True\n",
       "95    100A                                            [adult]         adult            adult                   True\n",
       "82    076A                                            [adult]         adult            adult                   True\n",
       "94    099A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "93    097B  [senior, adult, kitten, adult, adult, adult, a...         adult            adult                   True\n",
       "91    096A                                            [adult]         adult            adult                   True\n",
       "90    095A  [senior, adult, adult, senior, senior, adult, ...         adult            adult                   True\n",
       "89    094A  [senior, senior, senior, senior, senior, senio...        senior           senior                   True\n",
       "86    091A                                            [adult]         adult            adult                   True\n",
       "85    090A                                           [senior]        senior           senior                   True\n",
       "84    088A                                            [adult]         adult            adult                   True\n",
       "83    087A                                     [adult, adult]         adult            adult                   True\n",
       "49    043A                                           [kitten]        kitten           kitten                   True\n",
       "55    050A  [kitten, adult, kitten, kitten, kitten, kitten...        kitten           kitten                   True\n",
       "47    041A                                   [kitten, kitten]        kitten           kitten                   True\n",
       "30    025C              [adult, senior, adult, senior, adult]         adult            adult                   True\n",
       "26    023B                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "46    040A  [kitten, kitten, kitten, kitten, kitten, kitte...        kitten           kitten                   True\n",
       "24    022A  [adult, adult, adult, senior, adult, adult, ki...         adult            adult                   True\n",
       "23    021A                [adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "22    020A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "21    019B                                            [adult]         adult            adult                   True\n",
       "20    019A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "19    018A                                    [adult, senior]         adult            adult                   True\n",
       "18    016A  [senior, adult, senior, adult, senior, senior,...        senior           senior                   True\n",
       "17    015A  [adult, adult, senior, adult, senior, adult, s...         adult            adult                   True\n",
       "16    014B  [kitten, kitten, kitten, adult, kitten, kitten...        kitten           kitten                   True\n",
       "15    014A                              [adult, adult, adult]         adult            adult                   True\n",
       "14    013B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "13    012A                             [senior, adult, adult]         adult            adult                   True\n",
       "11    010A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "10    009A                      [senior, adult, adult, adult]         adult            adult                   True\n",
       "9     008A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "8     007A        [adult, senior, adult, adult, adult, adult]         adult            adult                   True\n",
       "7     006A                             [senior, adult, adult]         adult            adult                   True\n",
       "5     004A                                            [adult]         adult            adult                   True\n",
       "4     003A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "3     002B  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "2     002A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "28    025A  [senior, adult, adult, adult, adult, adult, ad...         adult            adult                   True\n",
       "25    023A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "109   117A  [senior, senior, adult, senior, adult, senior,...        senior           senior                   True\n",
       "37    031A  [adult, adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "40    034A               [adult, senior, adult, adult, adult]         adult            adult                   True\n",
       "31    026A                       [adult, adult, adult, adult]         adult            adult                   True\n",
       "39    033A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "38    032A                                    [kitten, adult]         adult            adult                   True\n",
       "43    037A         [adult, adult, adult, adult, adult, adult]         adult            adult                   True\n",
       "36    029A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "35    028A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "34    027A  [adult, adult, adult, adult, senior, adult, ad...         adult            adult                   True\n",
       "45    039A  [adult, adult, adult, adult, adult, adult, adu...         adult            adult                   True\n",
       "41    035A                      [kitten, adult, adult, adult]         adult            adult                   True\n",
       "104   110A                                            [adult]         adult           kitten                  False\n",
       "103   109A       [adult, adult, kitten, kitten, adult, adult]         adult           kitten                  False\n",
       "57    051B  [senior, adult, adult, adult, senior, senior, ...         adult           senior                  False\n",
       "6     005A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "101   106A  [adult, adult, senior, senior, adult, adult, a...         adult           senior                  False\n",
       "53    048A                                            [adult]         adult           kitten                  False\n",
       "52    047A  [adult, adult, adult, kitten, kitten, adult, k...         adult           kitten                  False\n",
       "44    038A                                   [kitten, kitten]        kitten            adult                  False\n",
       "42    036A  [senior, senior, senior, senior, adult, adult,...        senior            adult                  False\n",
       "29    025B                                   [senior, senior]        senior            adult                  False\n",
       "58    052A                    [adult, senior, senior, senior]        senior            adult                  False\n",
       "12    011A                                    [senior, adult]         adult           senior                  False\n",
       "60    054A                                    [adult, senior]         adult           senior                  False\n",
       "63    057A  [adult, adult, adult, senior, adult, adult, ad...         adult           senior                  False\n",
       "88    093A                                     [adult, adult]         adult           senior                  False\n",
       "87    092A                                           [senior]        senior            adult                  False\n",
       "66    060A                            [adult, kitten, kitten]        kitten            adult                  False\n",
       "67    061A                                    [senior, adult]         adult           senior                  False\n",
       "69    063A  [senior, senior, senior, senior, senior, senio...        senior            adult                  False\n",
       "33    026C                                           [kitten]        kitten            adult                  False\n",
       "32    026B                                           [senior]        senior            adult                  False\n",
       "27    024A                                            [adult]         adult           senior                  False\n",
       "92    097A  [senior, senior, senior, adult, adult, senior,...         adult           senior                  False"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 10)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "sorted_detailed_results = detailed_results.sort_values(by='Correct Majority Vote', ascending=False)\n",
    "\n",
    "# Display the sorted DataFrame\n",
    "sorted_detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e03366c5-a807-4159-b0c1-8dc88c04d91e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Majority Votes per Class:\n",
      "actual_age_group\n",
      "adult     63\n",
      "kitten    11\n",
      "senior    13\n",
      "Name: Majority_Correct, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create df for majority votes\n",
    "majority_df = majority_votes.reset_index()\n",
    "majority_df.columns = ['cat_id', 'Majority_Vote']\n",
    "\n",
    "# Get actual groups for each cat_id\n",
    "actual_groups = full_results.groupby('cat_id')['actual_age_group'].first().reset_index()\n",
    "\n",
    "# Merge majority votes with actual groups\n",
    "majority_with_actual = pd.merge(majority_df, actual_groups, on='cat_id')\n",
    "\n",
    "# Add a column to check if the majority vote was correct\n",
    "majority_with_actual['Majority_Correct'] = majority_with_actual['Majority_Vote'] == majority_with_actual['actual_age_group']\n",
    "\n",
    "# Count correct majority votes per class\n",
    "correct_majority_votes_per_class = majority_with_actual.groupby('actual_age_group')['Majority_Correct'].sum()\n",
    "\n",
    "print(\"Correct Majority Votes per Class:\")\n",
    "print(correct_majority_votes_per_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0ae8f86b-0e19-49df-a07c-f64383bce76b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics for Majority Votes:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult           73             63  86.301370\n",
      "1           kitten           15             11  73.333333\n",
      "2           senior           22             13  59.090909\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = majority_with_actual.groupby('actual_age_group').agg(\n",
    "    total_count=('Majority_Correct', 'size'),  # Total number of cases per class\n",
    "    correct_count=('Majority_Correct', 'sum')  # Sum of correct predictions per class\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log detailed stats\n",
    "print(\"Detailed Class Statistics for Majority Votes:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6350c1b2-050d-4bf3-98f1-5a80b3078d3a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1ea2c7-0827-4ffa-9777-ad2891e5f49b",
   "metadata": {},
   "source": [
    "## Detailed Class Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "94fbe612-c62e-4abe-8ec3-d75940a993cb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed Class Statistics:\n",
      "  actual_age_group  total_count  correct_count   accuracy\n",
      "0            adult          554            451  81.407942\n",
      "1           kitten          109             80  73.394495\n",
      "2           senior          178             95  53.370787\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count and the number of correct predictions for each class\n",
    "class_stats = full_results.groupby('actual_age_group').agg(\n",
    "    total_count=('correct', 'size'),\n",
    "    correct_count=('correct', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each class\n",
    "class_stats['accuracy'] = class_stats['correct_count'] / class_stats['total_count'] * 100\n",
    "\n",
    "# Log the detailed stats\n",
    "print(\"Detailed Class Statistics:\")\n",
    "print(class_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "50e84f5a-909e-47ca-93ca-2e92abf7ded1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Detailed Class Statistics for Majority Votes\n",
    "styled_barplot(class_stats, 'actual_age_group', 'accuracy', \n",
    "               'Accuracy of Majority Votes by Age Group', \n",
    "               'Age Group', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ac46e-4cc6-4237-925c-524d47e83b46",
   "metadata": {},
   "source": [
    "## Gender Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1743599c-0d3f-4b10-a095-02c36218c679",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy by Gender:\n",
      "  all_gender  count  correct  accuracy\n",
      "0          F    218      172     78.90\n",
      "1          M    337      262     77.74\n",
      "2          X    286      192     67.13\n"
     ]
    }
   ],
   "source": [
    "# Group by gender and calculate the total count, correct predictions, and accuracy\n",
    "gender_stats = full_results.groupby('all_gender').agg(\n",
    "    count=('cat_id', 'size'),  # Total number of cases per gender\n",
    "    correct=('correct', 'sum')  # Sum of correct predictions per gender\n",
    ").reset_index()\n",
    "\n",
    "# Calculate accuracy for each gender\n",
    "gender_stats['accuracy'] = (gender_stats['correct'] / gender_stats['count'] * 100).round(2)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(\"Accuracy by Gender:\")\n",
    "print(gender_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b8546267-aa7a-4e4b-b60e-810f836ebf8b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot for Accuracy by Gender\n",
    "styled_barplot(gender_stats, 'all_gender', 'accuracy', \n",
    "               'Accuracy by Gender', \n",
    "               'Gender', 'Accuracy (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a613636d-22ae-4629-ac86-daf482925194",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
